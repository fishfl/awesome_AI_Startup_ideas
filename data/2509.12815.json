{
    "id": "2509.12815",
    "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation",
    "summary": "This paper introduces Hunyuan3D Studio, an end-to-end AI content creation platform integrated with advanced neural modules, aimed at revolutionizing the game production pipeline through the automation and simplification of the generation process for game-ready 3D assets.",
    "abstract": "The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.",
    "category1": "Application Implementation",
    "category2": "Creative Industry",
    "category3": "Non-Agent",
    "authors": "Biwen Lei,Yang Li,Xinhai Liu,Shuhui Yang,Lixin Xu,Jingwei Huang,Ruining Tang,Haohan Weng,Jian Liu,Jing Xu,Zhen Zhou,Yiling Zhu,Jiankai Xing,Jiachen Xu,Changfeng Ma,Xinhao Yan,Yunhan Yang,Chunshi Wang,Duoteng Xu,Xueqi Ma,Yuguang Chen,Jing Li,Mingxin Yang,Sheng Zhang,Yifei Feng,Xin Huang,Di Luo,Zebin He,Puhua Jiang,Changrong Hu,Zihan Qin,Shiwei Miao,Haolin Liu,Yunfei Zhao,Zeqiang Lai,Qingxiang Lin,Zibo Zhao,Kunhong Li,Xianghui Yang,Huiwen Shi,Xin Yang,Yuxuan Wang,Zebin Yao,Yihang Lian,Sicong Liu,Xintong Han,Wangchen Qin,Caisheng Ouyang,Jianyin Liu,Tianwen Yuan,Shuai Jiang,Hong Duan,Yanqi Niu,Wencong Lin,Yifu Sun,Shirui Huang,Lin Niu,Gu Gong,Guojian Xiao,Bojian Zheng,Xiang Yuan,Qi Chen,Jie Xiao,Dongyang Zheng,Xiaofeng Yang,Kai Liu,Jianchen Zhu,Lifu Wang,Qinglin Lu,Jie Liu,Liang Dong,Fan Jiang,Ruibin Chen,Lei Wang,Chao Zhang,Jiaxin Lin,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Yinhe Wu,Jiayao Du,Jupeng Chen,Xinyue Mao,Dongyuan Guo,Yixuan Tang,Yulin Tsai,Yonghao Tan,Jiaao Yu,Junlin Yu,Keren Zhang,Yifan Li,Peng Chen,Tian Liu,Di Wang,Yuhong Liu,Linus,Jie Jiang,Zhuo Chen,Chunchao Guo",
    "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "comments": "Comments:Technical Report",
    "keypoint": "Hunyuan3D Studio is an end-to-end AI-powered platform for game-ready 3D asset generation.\nThe system transforms a single concept image or text into a production-quality 3D model with optimized geometry and PBR textures.\nIt integrates advanced neural modules including Part-level 3D Generation, Polygon Generation, Semantic UV, and Texture Synthesis.\nThe pipeline supports multimodal input such as text-to-image and image-to-multi-view synthesis.\nControllable Image Generation includes style transfer and A-pose standardization for character models.\nHigh-Fidelity Geometry Generation uses diffusion-based models to produce detailed 3D shapes from 2D inputs.\nBounding box and multi-view image conditions improve geometric accuracy and alignment.\nPart-level 3D Generation decomposes models into semantic components using P3-SAM and X-Part.\nP3-SAM enables native 3D part segmentation with high precision and robustness using a large-scale annotated dataset.\nX-Part allows controllable, high-fidelity shape decomposition using bounding box cues and semantic features.\nPolygon Generation (PolyGen) employs an autoregressive model for face-by-face retopologization into low-poly meshes.\nPolyGen uses Blocked and Patchified Tokenization (BPT) and a two-stage training process with Masked DPO.\nSemantic UV Unwrapping (SeamGPT) generates artist-style cutting seams via auto-regressive prediction of 3D line segments.\nSeamGPT uses point cloud conditioning on vertices and edges for better alignment with mesh topology.\nTexture Synthesis produces physically accurate PBR textures from text or image prompts with non-destructive editing.\nMultimodal texture editing supports both text and image guidance using unified latent conditioning.\n4K Material Map Generation creates tileable, high-resolution material balls from textual descriptions.\nAnimation Module supports both humanoid and general character rigging and skinning.\nHumanoid animation uses a template-based approach with T-pose standardization and motion retargeting.\nGeneral character animation uses autoregressive skeleton generation and topology-aware skinning.\nThe system outputs assets ready for integration into game engines like Unity and Unreal Engine.\nHunyuan3D Studio significantly reduces iteration time and lowers technical barriers in 3D content creation.\nExperimental results show state-of-the-art performance in geometry, part decomposition, UV unwrapping, and texture quality.\nUser studies confirm superior boundary quality and editability of UV maps compared to existing methods.",
    "date": "2025-09-18",
    "paper": "Tencent Hunyuan\nHunyuan3D Studio: End-to-End AI Pipeline for\nGame-Ready 3D Asset Generation\nTencent Hunyuan3D\nhttps://3d.hunyuan.tencent.com\nFigure 1: High quality 3D assets generated by Hunyuan3D Studio.\nAbstract\nThe creation of high-quality 3D assets, a cornerstone of modern game develop-\nment, has long been characterized by labor-intensive and specialized workflows.\nThis paper presents Hunyuan3D Studio, an end-to-end AI-powered content\ncreation platform designed to revolutionize the game production pipeline by\nautomating and streamlining the generation of game-ready 3D assets. At its\ncore, Hunyuan3D Studio integrates a suite of advanced neural modules (such as\nPart-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive\nand user-friendly system. This unified framework allows for the rapid trans-\nformation of a single concept image or textual description into a fully-realized,\nproduction-quality 3D model complete with optimized geometry and high-fidelity\nPBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not\nonly visually compelling but also adhere to the stringent technical requirements\nof contemporary game engines, significantly reducing iteration time and lowering\nthe barrier to entry for 3D content creation. By providing a seamless bridge from\ncreative intent to technical asset, Hunyuan3D Studio represents a significant leap\nforward for AI-assisted workflows in game development and interactive media.\n1\nIntroduction\nThe demand for high-fidelity 3D content has surged, driven by the expanding frontiers of 3D\ngames, virtual production, and the metaverse. However, traditional 3D asset creation remains a\ncomplex, time-consuming, and often costly endeavor, typically requiring expertise across multiple\nsoftware suites for modeling, UV mapping, texturing, and rigging. This process can form a\nbottleneck in game production, limiting creative iteration and accessibility.\nRecent advancements in generative AI (Zhang et al., 2024a; Xiang et al., 2025; Weng et al., 2024;\nLei et al., 2024), particularly diffusion models, have driven rapid progress in specific areas of 3D\n1\narXiv:2509.12815v1  [cs.CV]  16 Sep 2025\ncontent creation, such as geometry generation where systems like Hunyuan3D series (Yang et al.,\n2024; Zhao et al., 2025; Lai et al., 2025b; Hunyuan3D et al., 2025b; Lai et al., 2025a) demonstrate\nscalable, high-resolution asset synthesis from single images or text prompts. However, despite\nthese breakthroughs in shape formation, the field continues to struggle with integrating these\nadvances into assets that simultaneously meet the dual demands of high visual fidelity and\ntechnical readiness for real-time rendering in game engines. Many existing solutions address\nonly isolated parts of the pipeline (e.g., generating geometry without game-optimized topology\nor producing textures that lack material accuracy), leaving artists with the challenging task of\nintegrating and refining these outputs into a usable, performant asset.\nTo bridge this critical gap, we introduce Hunyuan3D Studio, a comprehensive AI platform that\nreimagines the entire 3D asset creation workflow from the ground up. Our system is built upon the\nfoundation of large-scale generative models but extends far beyond them into a fully integrated\nproduction environment. This report details the architecture of this end-to-end pipeline, which\nis designed to transform a high-level creative concept into a game-engine-ready asset with min-\nimal manual intervention. We demonstrate that this integrated approach not only significantly\naccelerates content creation but also democratizes 3D artistry by lowering the technical barriers to\nproducing production-quality assets.\nThis paper is organized as follows: Section 2 provides a comprehensive overview of the Hun-\nyuan3D Studio pipeline and its core modules. Sections 3 through 9 delve into the technical\nspecifics of each module. Finally, Section 10 discusses conclusions, limitations, and future work.\n2\nHunyuan3D Studio Pipeline\nThe Hunyuan3D Studio pipeline is architected as a sequential yet modular workflow, where each\nstage processes the asset and enriches it with data crucial for the next. This design ensures that\nthe entire process—from an initial idea to a deployed game asset—is seamless, automated, and\nmaintains the highest possible fidelity. The pipeline, as shown in Figure. 2, comprises seven core\ntechnological modules, each addressing a fundamental stage in the asset creation process:\nControllable\nImage Generation\nPolygon \nGeneration\nSemantic\nUV Unwrapping\nTexture Synthesis \nand Editing\nAnimation\nGeometry \nGeneration\nPart-level \n3D Generation\nFigure 2: The pipeline of Hunyuan3D Studio.\n• Controllable Image Generation (Concept Design): The pipeline initiates with multi-\nmodal input processing, supporting text-to-image and image-to-multi-view synthesis. A\ndedicated A-Pose standardization module ensures character models maintain consistent\nskeletal orientation, while neural style transfer adapts visual aesthetics to match target\ngame art styles.\n• High-Fidelity Geometry Generation: This module generates detailed 3D geometry (high-\npoly mesh) from single or multi-view images, leveraging advanced diffusion-based ar-\nchitectures to ensure geometric alignment with input references and preserve intricate\nsurface details.\n• Part-level 3D Generation: Using connectivity analysis and semantic partitioning algo-\nrithms, complex models are automatically decomposed into logical, functional components\n(e.g., a rifle’s magazine, barrel, and stock), enabling independent editing and animation.\n2\n• Polygon Generation (PolyGen): This module abandons traditional graphics-based retopol-\nogy methods and instead employs an autoregressive model for face-by-face generation\nto construct low-polygon assets. Taking a point cloud of the geometric surface as input,\nPolyGen intelligently retopologizes high-fidelity meshes, producing game-ready assets\nwith low vertex counts and well-structured, deformation-aware edge flow.\n• Semantic UV Unwrapping: This module implements context-aware UV segmentation\nthat groups surfaces by material type and texel density requirements, minimizing seams\nand ensuring efficient texture space utilization.\n• Texture Synthesis and Editing: Integrating generative models, the system produces\nphysically-accurate PBR texture sets from text or image prompts, supported by a non-\ndestructive editing layer for refinement via natural language commands.\n• Animation Module: The final automation stage infers joint placement and bone hierar-\nchies, calculating vertex weights to create ready-to-animate assets that are configured for\nstandard game engines.\nThese modules are orchestrated through a unified asset graph, where outputs from each stage\npropagate metadata to downstream processes. This enables parametric control, where high-level\nartistic adjustments cascade through the entire pipeline, and reversibility, allowing for incremental\nupdates without full recomputation. The final output is configured and exported with all necessary\nspecifications for the target game engine, such as Unity or Unreal Engine.\nThis modular yet integrated approach ensures Hunyuan3D Studio addresses the full spectrum\nof game asset creation—from conceptualization to engine integration—while maintaining artistic\ncontrol and technical rigor. The following sections will provide a detailed technical dissection of\neach module’s architecture and functionality.\n3\nControllable Image Generation\nOur controllable image generation pipeline leverages state-of-the-art open-source models, com-\nprising modules for image stylization and pose standardization, as described below.\n3.1\nImage Stylization\nInput\nChibi\nSteampunk\nVoxel\nFuturistic\nHand-drawn\nLow-poly\nFigure 3: Visualization results of our image stylization module with pre-defined styles.\nOur image stylization module enables users to generate 3D design drawings in diverse, pre-defined\npopular game art styles through a configurable option prior to 3D model generation, as shown\nin Figure. 3. It employs a multi-style image-to-image generation model based on Qwen-Image-\nEdit Wu et al. (2025) and further adapted with Low-Rank Adaptation (LoRA) Hu et al. (2022).\nThe model processes a user-provided subject image with a textual style instruction formatted as\n“Change the style to {style type} 3D model. White Background.” to produce a stylized output that\nmaintains content consistency with the input image while faithfully adhering to the specified\n3\nartistic style. The training data is constructed in a triplet format {input reference image, style type,\nstylized 3D design drawing}, which establishes a precise correspondence between photorealistic\nsubject images and their stylized counterparts. For text-to-image stylization, where no reference\nimage is provided, the system first generates a reference image from the text prompt using an\nin-house general text-to-image model, and then processes it through the same image-to-image\nstylization pipeline to achieve the final stylized output.\n3.2\nPose Standardization\nStandardizing the pose (e.g., to an A-pose) from an arbitrary character reference image requires\nsimultaneously achieving precise pose control and maintaining strict character consistency. In\naddition, it involves the removal of background elements and props from reference images. To\nachieve this, we leverage a character image with arbitrary poses/viewpoints as the reference and\ninject it as a conditioning input into FLUX.1-dev DiT to guide the generation process, as shown in\nFigure. 4.\nFigure 4: Overall workflow of our pose standardization module.\nFigure 5: Visualization results of our pose standardization module.\n• Dataset Construction. We first construct image pairs based on rendered character data\nin the following format: [character image with arbitrary poses/viewpoints, standard A-pose\nfront views of the same character]. Subsequently, rendered data that includes props (such as\nhandheld weapons and pedestals) is processed through state-of-the-art editing models,\nincluding Flux-Kontext Batifol et al. (2025). This step isolates the character by removing all\nprops and background elements, ensuring the subject’s consistency is maintained. Finally,\nthe resulting image pairs are manually curated and incorporated into the dataset to equip\nthe model with the capability for prop and background removal.\n• Model Training. We implement the progressive learning strategy, starting with an initial\nresolution of 512 × 512 pixels and increasing to 768 × 768. Consequently, the model learns\nto extract more intricate features, which substantially improves the fidelity of generated\n4\noutputs, particularly in reproducing detailed facial characteristics and complex clothing\ntextures. In addition, reference images of the same character under different scenarios are\nrandomly selected and injected as conditional inputs, thereby achieving generalized pose\ncontrol and consistent generation performance. Moreover, we have assembled supplemen-\ntary high-quality datasets specifically targeting challenging categories, including half-body\nportraits, non-human humanoids, and anthropomorphic characters. These datasets are\nused for post-training techniques like Supervised Fine-Tuning (SFT) and Direct Preference\nOptimization (DPO) to bolster the model’s generalizability and robustness.\n4\nHigh-Fidelity Geometry Generation\n4.1\nPreliminary\nOur geometry-generation pipeline is built upon the state-of-the-art Hunyuan3D 2.1 (Hunyuan3D\net al., 2025b) and Hunyuan3D 2.5 (Lai et al., 2025a) frameworks. The Hunyuan3D 2.1 framework\ncomprises two modules:\n• Hunyuan3D-ShapeVAE — a variational encoder–decoder transformer that first com-\npresses and then reconstructs 3-D geometry. The encoder receives a point cloud endowed\nwith 3-D positions and surface normals {xi = (Pi, Ni) | Pi, Ni ∈R3}, and embeds it into\ncompact shape latents z via a Vector-Set Transformer with importance sampling (Zhang\net al., 2023). The decoder employs z to query a 3-D neural field Fg ∈RD×H×W×d on a\nuniform grid Qg ∈RD×H×W×3, and subsequently maps Fg to signed-distance values\nSg ∈RD×H×W.\n• Hunyuan3D-DiT — a flow-based diffusion model that operates in the latent space of\nShapeVAE. The network stacks 21 Transformer layers, each enhanced with a Mixture-of-\nExperts (MoE) sub-layer to significantly enlarge capacity and expressive power. Hunyuan3D-\nDiT is trained to map gaussian noises to shape latents z with the flow matching objec-\ntive (Lipman et al., 2022; Esser et al., 2024).\n4.2\nConditional Generation\n(a) 3D shape generation pipeline\n(b) Image-to-multiview pipeline\nFigure 6: Overview of conditional generation.\nHunyuan3D-DiT is primarily conditioned on a single input image, which is first resized to 518 ×\n518, removed background and then encoded by a frozen DINOv2 backbone (Oquab et al., 2023) to\nobtain the image latent cI ∈RB×L×C, which is subsequently fused into the generated shape latent\nvia cross attention.\nTo provide additional geometric and prior guidance, Hunyuan3D-Studio supplies two additional\ncontrol signals: an explicit 3-D bounding box and multi-view images. The overall pipeline is\nillustrated in Figure 6(a).\n4.2.1\nBounding Box Condition\nTo better align the generated 3D assets with user intent, we introduce a bounding box as the\nsimplest 3-D control signal to steer the DiT model. Given a bounding box, we first encode its\nheight, width and length into a single shape latent cB ∈RB×1×C with a two-layer MLP. Then, cB is\nconcatenated with cI along the sequence dimension to form the final conditioning vector.\n5\nDuring training we deliberately misalign the object proportions between the image and the point\ncloud—via mild deformation of either modality—to force the network to rely on the bounding-box\nsignal.\n4.2.2\nGenerated Multi-view Image Condition\nTo harness the powerful capabilities of image-generation models, we use multi-view images\nproduced by an image diffusion model as the condition for character generation.\nImage to multiview image generation. As, shown in Figure 6(b), to synthesize high-fidelity\nmultiview images from a single input, we introduce a lightweight module built upon a pretrained\ntext-to-image foundation model (Li et al., 2024). This is achieved by training a Low-Rank Adap-\ntation (LoRA) layer (Hu et al., 2022). The training process begins with the curation of a dataset\ncomprising object-centric images from arbitrary camera poses, each paired with its corresponding\nground-truth multiview images. During training, both the input single-view and target multiview\nimages are encoded into their latent representations using the model’s pretrained Variational Au-\ntoencoder (VAE). The LoRA layer is conditioned on two sources of information: (1) the noise-free\nlatent of the single-view image, which is concatenated with the noised multiview latent, providing\nstructural guidance; and (2) a semantic condition vector extracted from the input image using a\npretrained SigLIP vision encoder (Zhai et al., 2023). The LoRA parameters are then optimized\nusing a standard flow-matching loss.\nMulti-view image injection. Similar to the single image condition, we first encode all the images\ninto image latents {ci\nI|i = org, front, left, back, right}. Each non-original view is marked by a\nsinusoidal positional embedding with a fixed index. After positional encoding, the latents from\ngenerated views are concatenated with the original-image latent to form the final condition.\n4.3\nVisualization\nBbox condition. As illustrated in Figure 7, the bbox control signal not only succeeds in producing\nhigh-quality geometry when image-only geometric generation fails, but also generates 3D assets\nwith appropriate proportions and well-structured forms according to the given bbox.\nFigure 7: 3D geometry generated with bounding box control.\nGenerated multi-view image condition. Leveraging a state-of-the-art multi-view generation\nmodel as guidance, our approach produces high-fidelity 3D character assets, as exemplified in\nFigure 8.\nFigure 8: 3D geometry generated with generated multi-view image control.\n6\nFigure 9: Our part level shape generation results.\n5\nPart-level 3D Generation\nGenerating 3D shapes at part level is pivotal for downstream applications such as mesh retopology,\nUV mapping, and 3D printing. However, existing part-based generation methods often lack\nsufficient controllability, produce inadequate geometric quality in generated parts, and suffer from\nlimited semantic coherence. This section establishes a new paradigm for creating production-ready,\neditable, and structurally sound 3D assets. Fig. 9 shows our part-level shape generation results.\nAs shown in Figure. 10, given an input image, we first obtain the holistic shape using Huyuan3D\n2.5 Lai et al. (2025a). The holistic mesh is then fed to part detection module P3-SAM Ma et al.\n(2025) to obtain the semantic features and part bounding boxes. Finally, X-Part Yan et al. (2025)\ndecompose the holistic shape into parts. P3-SAM and X-Part will be introduced in section 5.1\nand 5.2\nFigure 10: Pipeline of our image to 3D part generation. Given an input image, we first obtain the\nholistic shape using Huyuan3D 2.5 Lai et al. (2025a). The holistic mesh is then fed to part detection\nmodule P3-SAM Ma et al. (2025) to obtain the semantic features and part bounding boxes.\n7\nFigure 11: Training pipeline of P3-SAM.\n5.1\nP3-SAM: Native 3D Part Segmentation Ma et al. (2025)\n3D part segmentation is a fundamental step in our part generation pipeline. In this section, we\npropose a native 3D Point-Promptable Part segmentation model termed P3-SAM, designed to\nfully automate the segmentation of any complex 3D objects into components with precise mask\nand strong robustness. As a pioneering promptable image segmentation work, SAM provides\na feasible implementation approach. However, our method focuses on achieving precise part\nsegmentation automatically, and we simplify the architecture of SAM. Without adopting the\ncomplex segmentation decoder and multiple types of prompts from SAM, our model is designed\nto handle only one positive point prompt.\nSpecifically, as shown in Fig. 11, P3-SAM contains a feature extractor, three segmentation heads,\nand an IoU prediction head. We employ PointTransformerV3 as our feature extractor and integrate\nits features from different levels as extracted point-wise features. The input point prompt and\nfeature are fused and passed to the segmentation heads to predict three multi-scale masks and an\nIoU (Intersection of Union) predictor is utilized to evaluate the quality of the masks.\nTo automatically segment an object, as shown in Fig. 12, we apply our segmentation model\nusing point prompts sampled by FPS (Farthest Point Sampling) and utilize NMS (Non-Maximum\nSuppression) to merge redundant masks. The point-level masks are then projected onto mesh faces\nto obtain the part segmentation results.\nAnother key aspect of this method is to eliminate the influence of 2D SAM, and rely exclusively\non raw 3D part supervision for training a native 3D segmentation model. While existing 3D part\nsegmentation datasets are either too small or lack part annotation, this work addresses the data\nscarcity by developing an automated part annotation pipeline for artist-created meshes and used it\nto generate a dataset comprising 3.7 million meshes with high-quality part-level masks. Our model\ndemonstrates excellent scalability with this dataset and achieves robust, precise, and globally\ncoherent part segmentation.\nFor more details of P3-SAM, please refer to the paper Ma et al. (2025).\nComparison with SOTA.\nWe evaluate each method on three datasets: PartObj-Tiny, PartObj-\nTiny-WT, and PartNetE. PartObj-Tiny is a subset of Objarvse, containing 200 data samples across\n8 categories, with manually annotated part segmentation information. PartObj-Tiny-WT is the\nwatertight version of PartObj-Tiny. To evaluate the performance of various networks on watertight\ndata, we converted the meshes from PartObj-Tiny to watertight versions and successfully obtained\n189 watertight meshes. PartNetE, derived from PartNet-Mobility, contains 1,906 shapes covering 45\nobject categories in the form of point clouds. We also evaluate various networks on it to verify their\ngeneralization performance on point cloud. Table 1 and 2 confirming the superior performance of\nP3-SAM under diverse conditions.\n8\nFigure 12: Pipeline of automatic segmentation using P3-SAM.\nTask\nMethod\nHuman\nAnimals\nDaily\nBuild.\nTrans.\nPlants\nFood\nElec.\nAVG.\nFind3D\n23.99\n23.99\n22.67\n16.03\n14.11\n21.77\n25.71\n19.83\n21.28\nFully\nSAMPart3D\n55.03\n57.98\n49.17\n40.36\n47.38\n62.14\n64.59\n51.15\n53.47\nSeg. w/o\nSAMesh\n66.03\n60.89\n56.53\n41.03\n46.89\n65.12\n60.56\n57.81\n56.86\nConnect.\nPartField\n54.52\n58.07\n56.46\n42.47\n49.09\n59.16\n55.4\n56.29\n53.93\nOurs\n60.77\n59.43\n62.98\n50.82\n57.72\n70.53\n54.04\n61.96\n59.88\nSeg. w/\nPartField\n80.85\n83.43\n77.83\n69.66\n73.85\n80.21\n85.27\n82.30\n79.18\nConnect.\nOurs\n80.77\n86.46\n80.97\n67.77\n68.44\n90.30\n92.90\n81.52\n81.14\nInteract.\nPoint-SAM\n8.63\n9.38\n17.47\n11.19\n7.63\n13.95\n23.02\n12.73\n13.00\nOurs\n49.01\n53.45\n52.36\n38.50\n51.52\n62.57\n50.80\n51.86\n51.23\nTable 1: The comparison of our method with previous methods on PartObjectarverse-Tiny. The first\ntwo blocks represent class-agnostic part segmentation without and with connectivity, respectively,\nand the last block represents interactive segmentation.\nTask\nFully Segmentation w/o Connectivity\nInteractive Seg.\nMethod\nFind3D\nSAMPart3D\nSAMesh\nPartField\nOurs\nPoint-SAM\nOurs\nPartObj-Tiny-WT\nwait\nwait\nwait\nwait\n55.35\n13.11\n49.11\nPartNetE\n21.69\n56.17\n26.66\n59.1\n65.39\n15.06\n63.48\nTable 2: The comparison of our method with previous methods on the watertight version of\nPartObjectarverse-Tiny.\n5.2\nX -Part: high-fidelity and structure-coherent shape decomposition Yan et al. (2025)\nThis section shows how to decompose the shapes into parts. Decomposing a complete 3D shape\ninto meaningful semantic parts would greatly facilitate various downstream tasks. For instance,\nbreaking down a complex geometry into simpler parts can significantly ease the process of mesh\nre-topology and uv-unwrapping. However, generating shapes at the part level presents two major\nchallenges: 1) The decomposed geometry must maintain meaningful part-level semantics, and 2)\nThe generation process must recover geometrically plausible structures for internal regions.\nMainstream part-generation methods adopt the latent vecset diffusion framework Zhao et al.\n(2025), where each part is represented as an independent set of latent codes for diffusion.\nThe generation process can be executed independently for individual parts (e.g., HoloPart Yang\net al. (2025a)) or simultaneously for all parts (e.g., PartCrafter Lin et al. (2025), PartPacker Tang\net al. (2025)) to enhance part synchronization. Furthermore, multi-view or 3D segmentation are fre-\nquently employed for better part decomposition Yang et al. (2025a;b). However, these approaches\nare highly sensitive to inaccuracies in the segmentation results. Alternative works Lin et al. (2025);\nTang et al. (2025) do not explicitly rely on segmentation, but they still fail to offer controllable\npart-based generation and often produce decomposed parts with ambiguous boundary.\nMotivated by these observations, we introduce X -Part, a controllable and editable diffusion\nframework, which enables semantically meaningful and structurally coherent part generation. Our\nobjective is to generate high-fidelity and structure-coherent part geometries from a given object\npoint cloud, while ensuring flexible controllability over the decomposition process. Figure. 13\n9\nFigure 13: Pipeline of our shape decomposition.\nshows the pipeline of our shape decomposition method.\nFirst, to achieve the controllability, we propose a part-level cues extraction module that uses bound-\ning boxes as prompts to indicate part locations and scales, instead of directly using segmentation\nresults as input. Compared with fine-grained and point-level segmentation cues, bounding boxes\nprovide a coarser form of guidance, which mitigates overfitting to the input. Besides, the bounding\nbox provides additional volume scale information for the partially visible part, benefiting the\ngeneration and controllability.\nSecond, despite inaccuracies in the segmentation results, we notice that the high-dimension point-\nwise semantic feature is free from the information compression caused by the cluster algorithm\nor prediction head used in Liu et al. (2025c), resulting in more accurate semantic representa-\ntions. Therefore, we carefully introduce the semantic features into our framework with delicately\ndesigned feature perturbation, which benefits the meaningful part decomposition.\nThird, we integrate X -Part into a bounding box based part editing pipeline. It supports local\nediting, such as merging a small number of parts within an object and adjusting their scales, to\nfacilitate interactive part generation. To prove the effectiveness of X -Part, we conducted extensive\nexperiments on various benchmarks. Our results show that X -Part achieves state-of-the-art\nperformance in part-level decomposition and generation.\nFor more details of X -Part, please refer to the paper Yan et al. (2025).\nComparison with SOTA. We evaluate our method on 200 samples from the ObjaversePart-Tiny\ndataset, each comprising rendered images and corresponding ground-truth part geometries. To\nassess geometric quality, we employ Chamfer Distance (CD) and F-Score. The F-Score is computed\nat two different thresholds [0.1, 0.5] to capture both coarse-level and fine-level geometric alignment.\nPrior to metric computation, each object is normalized to the range [−1, 1]. To ensure pose-agnostic\nevaluation, we rotate each object by [0, 90, 180, 270] degrees and report the best score among these\norientations as the final metric. As shown in Table 3 and Figure 14 our method outperforms all\nbaselines.\nMethod\nCD↓\nFscore-0.1↑\nFscore-0.5↑\nSAMPart3D\n0.\n0.\nPartField\n0.17\n0.68\n0.57\nHoloPart\n0.26\n0.59\n0.43\nOmniPart\n0.23\n0.63\n0.46\nOurs\n0.11\n0.80\n0.71\nTable 3: Shape Decomposition Results\n10\nFigure 14: Shape Decomposition Results.\n6\nPolygon Generation with Auto-regressive Models\nIn this section, our goal is to generate a clean topology for shapes produced by geometry generative\nmodels or given by users. Despite the delicate shapes produced in previous sections, they typically\nconsist of a huge amount of messy triangles and are hard to be directly applied in downstream\napplications (e.g., UV segmentation and rigging). Therefore, we leverage an auto-regressive model\nto directly predict vertices and faces from the point cloud of generated shapes. Our model is\n11\ntrained in two stages, including pre-training and post-training as shown in Figure 15.\n...\n...\nAuto-Regressive\nMesh Decoder\nHourglass Transformer\nbc\noc\nb1\no1\nbn\non\n<E>\n…\n…\n…\n<B>\nSurface\nSampling\nShape\nEncoder\nShape Latent\n…\n1\nStage 1: Mesh Generation Pre-training\nDecode\nBPT Tokens\n<B> BOS Token\n<E> EOS Token\nTuned\nParameters\nFrozen\nParameters\nReference\nMesh Decoder\nPolicy\nMesh Decoder\nMask\nDPO\nStage 2: Reinforcement Post-training\n2\n×2000\nV.S.\nGenerated\nCandidates\nchosen\nreject\nInput\n(𝟒𝟎𝟗𝟔𝟎, 𝟔)\nBLOCKED AND PATCHIFIED TOKENIZATION\n’\nJudger\nFigure 15: Mesh-RFT Framework Overview. The pipeline comprises two stages: 1) Mesh Genera-\ntion Pre-training using an Hourglass AutoRegressive Transformer and a Shape Encoder; and 2)\nReinforcement Post-training which employs Mask DPO with reference and policy networks for\nsubsequent refinement.\n6.1\nMesh Pretrianing with Efficient Tokenization and Architecture\nMesh Tokenization.\nTo model meshes with the next-token-prediction paradigm, the first step\nis to tokenize them into 1D sequences. We leverage the Blocked and Patchified Tokenization\n(BPT) (Weng et al., 2025) as the basic tokenization for meshes. Specifically, BPT combines two\ncore mechanisms: 1) Block-wise Indexing partitions 3D coordinates into discrete spatial blocks,\nconverting Cartesian coordinates into block-offset indexes to exploit spatial locality. 2) Patch\nAggregation further compresses face-level data by selecting high-degree vertices as patch centers\nand aggregating connected faces into unified patches. Each patch is encoded as a center vertex\nfollowed by peripheral vertices, reducing vertex repetition and enhancing spatial coherence. With\nBPT, both training and inference efficiency are significantly improved.\nNetwork Architecture.\nOur polygon generative model consists of a point cloud encoder and\nan auto-regressive mesh decoder. The point cloud encoder is highly motivated by Michelangelo\n(Zhao et al., 2023) and Hunyuan3D series (Zhao et al., 2025; Hunyuan3D et al., 2025b), which\napplies the Perceiver (Jaegle et al., 2021) architecture to encode point cloud into condition tokens\ncp. Then, we leverage the Hourglass Transformer (Hao et al., 2024) as the mesh decoder backbone,\nconditioned on point cloud tokens by cross attention layers.\nTraining & Inference Scheme.\nThe mesh token distribution p(mi) is modeled with Hourglass\nTransformer with parameter θ, maximizing the log probability. The cross-attention is leveraged for\nvarious conditions cp.\nL(θ) =\n|m|\n∏\ni=1\np(mi|m1:i−1, cp; θ),\n(1)\nTo further leverage the high-poly mesh data and improve training efficiency, we adopt the truncated\ntraining strategy (Hao et al., 2024). Specifically, for each training iteration, we randomly selected a\nslice of mesh sequence with a fixed number of faces (e.g., 4k faces). And in the inference stages, we\napply the rolling cache strategy to reduce the gap between the training and inference stages.\n6.2\nMesh Post-Training with Topology-Aware Masked DPO\nPreference Dataset Construction.\nWe establish a pipeline for constructing the preference dataset\nused in the second-stage fine-tuning, which consists of candidate generation, multi-metric evalua-\ntion, and preference ranking.\nFor each input point cloud P, we generate eight candidate meshes {M1\nP, M2\nP, . . . , M8\nP} using\nthe pre-trained model Gpre\nθ\n. Each candidate is evaluated using three metrics: Boundary Edge Ratio\n(BER) and Topology Score (TS) for topological quality, and Hausdorff Distance (HD) for geometric\nconsistency.\n12\nOriginal Mesh\nPoint Cloud\nPost-train\nPre-train\nFigure 16: The effectiveness of the post-training stage. The post-training stage enhances the mesh\ncompleteness (Row #1) and connectivity (Row #2) and reduces the broken faces (Row #3).\nA preference relation Mi\nP ≻Mj\nP is defined if and only if:\nBER(Mi\nP) < BER(Mj\nP)\n∧\nTS(Mi\nP) > TS(Mj\nP)\n∧\nHD(Mi\nP) < HD(Mj\nP)\n(2)\nWe compile preference triplets (P, M+\nP, M−\nP) from all pairwise comparisons to form the dataset.\nMasked Direct Preference Optimization.\nTo address localized geometric imperfections and\ninconsistent face density, we leverage Masked Direct Preference Optimization (M-DPO) (Liu et al.,\n2025b), which extends DPO with quality-aware localization masks.\nWe define a binary masking function ϕ(M) ∈{0, 1}|M| that identifies high-quality regions\n(value 1) versus low-quality regions (value 0) based on per-face quality assessment. Each region\ncorresponds to a subsequence in the block patch tokenization (BPT). A subsequence is classified as\na high-quality region only if all faces within it have a quad ratio above a predefined threshold and\nthe average topology score exceeds another threshold.\nLet Gref = Gpre\nθ\nbe the frozen reference model and Gψ be the trainable policy. The M-DPO objective\nis:\nLM-DPO(πψ; πref) = −E(P,M+\nP,M−\nP)∼D\n\u0002\nlog σ\n\u0000βL+(P, M+\nP) −βL−(P, M−\nP)\n\u0001\u0003\n(3)\nwhere the positive and negative terms are:\nL+(P, M+\nP) = log\n\f\fπψ(M+\nP|P) ⊙ϕ(M+\nP)\n\f\f\n1\n\f\fπref(M+\nP|P) ⊙ϕ(M+\nP)\n\f\f\n1\nL−(P, M−\nP) = log\n\f\fπψ(M−\nP|P) ⊙(1 −ϕ(M−\nP))\n\f\f\n1\n\f\fπref(M−\nP|P) ⊙(1 −ϕ(M−\nP))\n\f\f\n1\n(4)\nHere, ⊙denotes element-wise multiplication and | · |1 is the ℓ1 norm. M-DPO enables targeted\nrefinement of low-quality regions while preserving satisfactory areas.\n13\nOriginal Mesh\nPoint Cloud\nBPT\nDeepMesh\nMeshAny\nOurs\nFigure 17: Generalization results on dense, out-of-distribution meshes. Our model demonstrates\nsuperior geometric fidelity and surface continuity, maintaining high-quality reconstruction even\nunder complex and unseen input conditions.\nOriginal Mesh\nPart-Aware \nGenerated Mesh\nPart-Aware \nLow-Poly Mesh\nFigure 18: Part-aware polygon generation. With shapes segmented into several parts as input,\nour model can generate the corresponding meshes conditioned on partial point clouds separately\nwithout further fine-tuning.\n14\n6.3\nExperiments\nPre-training vs. post-training.\nIn Figure 16, we show the improvement after the post-training. In\nour experiments, we found that the post-training stage is crucial for improving the completeness\nand topology quality of the generated meshes.\nComparison with existing methods.\nAs shown in Figure 17, we compare our model with\nexisting polygon generation methods. Our model can generate much more complex meshes with\nsignificantly improved topology quality and stability.\nPart-aware polygon generation.\nWith shapes segmented into several parts as input, our model\ncan generate the corresponding meshes conditioned on partial point clouds separately without\nfurther fine-tuning as shown in Figure 18. This would be much easier for the model to generate\nthe topology for complicated meshes.\n7\nSemantic UV\nThe results of traditional UV unwrapping methods often lack semantic significance, which notably\naffects the quality of downstream texturing and the efficiency of resource utilization. Consequently,\nthese traditional methods cannot be directly applied in professional pipelines, such as those used\nin game development and film production. To handle this challenge, we introduce SeamGPT, a\nnovel framework that generates artist-style cutting seams through an auto-regressive approach.\nOur method formulates surface cutting as a sequence prediction problem, where cutting seams\nare represented as an ordered series of 3D line segments. Given an input mesh M, our goal is\nto generate seam edges S = {si}i∈[Ns]. The overview of SeamGPT is shown in Fig. 19. We first\nintroduce our seam representation strategy in Sec. 7.1, which encodes cutting seams as sequential\ntokens. In Sec. 7.2, we detail our auto-regressive generation process, which mimics the sequential\ndecision-making of professional artists.\nFigure 19: SeamGPT architecture: Point cloud encoder extracts shape context; Causal transformer\ndecoder generates axis-ordered seam coordinates. Color indicates the prediction order is of the\nseam segments (red to blue).\n7.1\nMesh Seam Representation\nA seam sequence S of Ns segments {si}i∈[Ns] is defined as: S = {s1, s2, . . . sNs}, where each segment\nsi is a 3D line segment represented by two vertices: si = (pi\nh, pi\nt), i.e. head and tail. Each vertex p is\ndefined by its 3D coordinates: p = (x, y, z). Thus, a seam sequence can be decomposed at multiple\nlevels:\nS = {s1, s2, . . . sNs}\nSegment\nlevel\n= {p1\nh, p1\nt , p2\nh, p2\nt , . . . , pNh\nt , pNh\nt }\nPoint\nlevel\n(5)\n= {x1\nh, y1\nh, z1\nh, x1\nt , y1\nt , z1\nt , . . . , xNs\nt , yNs\nt , zNs\nt }\nCoord.\nlevel\n15\nSeam ordering. For an auto-regressive model to function properly, a consistent order of sequences\nis required. Following existing practice for mesh generation Siddiqui et al. (2023); Weng et al.\n(2024); Hao et al. (2024) and wireframe generation Ma et al. (2024), we first sort vertices yzx order,\nwhere y represents the vertical axis, and then sort two vertices within an edge lexicographically,\nplacing the lowest yzx-ordered vertex first. Finally, seam edges are sorted in ascending yzx-order\nbased on the sorted values of their vertices. The resulting order can be seen through the color\ncoding of the generated meshes presented in Figure 19, i.e. from red to blue.\nQuantization of coordinates. Autoregressive models typically sample from a multinomial dis-\ntribution over a discrete set of possible values. To adhere to this convention, we quantize vertex\ncoordinates into a fixed number of discrete bins. The quantization resolution—determined by\nthe number of bins—directly affects the precision of the predicted seam. Higher quantization\nlevels yield more detailed and accurate representations but also increase the complexity of the\ngeneration process. To balance precision and tractability, we employ 1024-level quantization,\nenabling effective representation of complex seams.\n7.2\nAutoregressive Seam Prediction\nIn autoregressive seam prediction, a seam sequence S is generated by sequentially predicting\neach coordinate ci based on its conditional probability given all previously generated coordinates\nP(ci|c<i). The probability of the entire seam is then given by the joint probability of all its\ncoordinates:\nP(S) =\n6Ns\n∏\ni=1\nP(ci|c<i).\n(6)\nGlobal Shape Conditioning. Point clouds are a flexible and universal 3D representation that can\nbe efficiently derived from other 3D formats, including meshes. We use a point cloud encoder to\nextract representative features for characterizing the input 3D shapes. In the context of surface\ncutting, seams are encouraged to align with the vertices and edges of the original mesh, such\nthat cutting the mesh along seams does not create excessive extra faces. To guide the decoder in\nproducing vertex and edge-aligned seam placement, instead of sampling point clouds uniformly,\nwe sample structural points only on vertices and along edges. Specifically, we sample a total of\n61,440 points, evenly split between: 30,720 points on vertices and 30,720 points on edges. If the\ninput mesh has fewer than 30,720 vertices, we use repeated over-sampling. Points along an edge\nare sampled uniformly by interpolating between its start and end points with K samples, where K\nis determined based on the edge’s length. Finally, the input points are fed into a jointly trained\npoint cloud encoder from Team (2025), which processes the point cloud through a series of cross-\nand self-attention layers and compresses the point cloud to a latent shape embedding of length\n3072 and dimension 1024. Another option to create shape embeddings is to use mesh encoders,\nsuch as Zhou et al. (2020). However, the computational cost of mesh encoder does not scale well\nwhen the input has a large number of vertices. We show in the ablation study that point cloud\nconditioning produces much better results than mesh conditioning.\nSeam Count control. Given an input shape, multiple possible suitable cutting solutions exist.\nDepending on the application requirements, one can make many cuts to decompose a mesh, or\njust a few. To regulate the cutting granularity, we concatenate a length embedding to the shape\nembedding. We find that modulating the length embedding directly controls cutting granularity.\nHourGlass Decoder Architecture. Following Hao et al. (2024), we build an hourglass-like au-\ntoregressive decoder architecture to sequence at multiple levels of abstraction. The architecture\nemploys multiple Transformer stacks at each level, with transitions between levels managed by\ncausality-preserving shortening and upsampling layers that bridge these hierarchical stages. There\nare three hierarchical levels: coordinates, vertices, and edges. The input coordinate sequence is\nshortened by a factor of 3 at the vertex level. It is then further shortened by a factor of 2 at the\nedge level. Both shortening and upsampling layers are implemented to preserve causality. The\nexpanded sequence is combined with higher-resolution sequences from earlier levels via residual\nconnections, similar to U-Nets.\nTraining Strategy. We employ two loss functions to for model training: a cross-entropy loss for\ntoken prediction and a KL-divergence loss to regularize the shape embedding space, ensuring\nit remains compact and continuous. Training begins with a 2,000-step warm-up phase and is\nparallelized across 64 Nvidia H20 GPUs (98GB Mem.) with a total batch size of 128. The model\nconverges after one week of training. During training, we first scale all samples to fit within a cubic\n16\nFigure 20: Qualitative UV flatten results on FAM benchmark ( Nefertiti, Cow, and Fandisk).\nbounding box in the range of −1 to 1. We then apply data augmentation techniques, including\nrandom scaling within [0.95, 1.05], random vertex jitter, and random rotation.\nBimba\nLucy\nOgre\nArmadi.\nBunny\nNefert.\nDragon\nHomer\nHappy\nFandi.\nSpot\nArm\nCow\nAvg.\nXatalas Young (2024)\n15.44\n0.01\n0.66\n0.17\n61.84\n0.03\n0.22\n7.51\n99.84\n8.41\n12.77\n29.98\n1.94\n18.37\nNuvo Srinivasan et al. (2024)\n19.12\n57.89\n26.22\n114.21\n16.84\n20.92\n61.03\n21.92\n267.43\n19.76\n12.93\n37.34\n12.70\n52.95\nFAM Zhang et al. (2024b)\n12.10\n35.14\n11.55\n59.87\n7.33\n11.21\n904.89\n14.19\n23.00\n12.21\n9.37\n20.98\n8.49\n86.95\nEdge-CLS\n8.14\n22.85\n23.54\n11.18\n3.91\n4.67\n15.39\n20.16\n27.37\n12.47\n5.77\n87.20\n9.20\n19.37\nOurs\n10.68\n0.01\n2.01\n2.47\n50.47\n0.12\n0.56\n10.28\n61.68\n8.15\n5.95\n14.88\n2.24\n13.04\nTable 4: Quantitative results on Flatten-Anything benchmark using the face distortion metric.\nBowl\nBall\nSheep\nDriver\nChicken\nApple\nGiraffe\nBottle\nAvg.\nXatalas Young (2024)\n0.91\n0.26\n1.19\n4.61\n2.36\n3.11\n2.85\n0.57\n1.98\nNuvo Srinivasan et al. (2024)\n3.99\n1.33\n10.43\n33.07\n9.79\n15.39\n21.04\n6.02\n12.63\nFAM Zhang et al. (2024b)\n3.80\n0.81\n6.45\n15.33\n18.98\n6.64\n11.77\n4.36\n8.52\nOurs\n0.49\n0.31\n1.39\n4.25\n1.86\n4.02\n2.59\n0.67\n1.95\nTable 5: Quantitative results on Toys4K Benchmark using the face distortion metric.\n7.3\nExperiment\nBenchmarks and Evaluation Metric. We conduct experiments on a diverse collection of 3D surface\nmodels from Flatten Anything (FAM) Zhang et al. (2024b), which primarily includes low-poly\nmeshes, CAD models, and 3D scanned meshes. We also evaluate on Toys4K Stojanov et al. (2021),\na dataset of non-manifold artist-created meshes. Our evaluation leverages the Mesh distortion\nmetrics, which is computed as the average conformal energy over all triangular faces of the mesh.\nBaselines and Implementations. We compare SeamGPT against several state-of-the-art methods\nfor mesh UV-unwrapping. XAtlas Young (2024) employs a bottom-up approach with bounded dis-\ntortion charts. Nuvo Srinivasan et al. (2024) leverages neural fields with explicit parameterization\nconstraints. FAM Zhang et al. (2024b) implements interpretable sub-networks in a bi-directional\ncycle mapping framework. We also built another baseline called Edge-CLS, which takes a mesh as\ninput and uses graph convolution and Transformer layers to compute per-edge features. These\nfeatures are then fed into an MLP classifier to predict whether each edge is a seam edge or not (i.e.,\nthis is an edge classification baseline). We train Edge-CLS on the same training set and use the\nsame UV-unwrapping process as SeamGPT.\n17\nSeamGPT-based UV-unwrapping. Once SeamGPT generates cutting seams, we implement a\nstreamlined unwrapping process to create practical UV maps. We first map each predicted seam\npoint to its nearest vertex on the input mesh, then connect these vertices through shortest geodesic\npaths along the mesh edges. We then cut the mesh by duplicating vertices along these paths,\ncreating independent boundaries for flattening. Finally, we apply Blender’s Minimum Stretch\nalgorithm to the segmented mesh, optimizing UV coordinates to evenly distribute stretching\nwhile preserving the semantic structure defined by our seams. This process yields low-distortion\nUV mappings that respect functional and aesthetic boundaries, improving upon conventional\nautomated methods.\nComparison results. Tables 4 and 5, along with Figure 20, present qualitative and quantitative\nresults. SeamGPT achieves the best performance across all metrics. In contrast: XAtlas generates\nover-fragmented cuts, FAM fails to produce subtle cuts consistently, Edge-CLS performs well only\non sharp edge features but struggles with generating seams on smooth, featureless regions. Our\nmethod consistently produces semantic and reasonable cuts regardless of surface characteristics.\nUser study. To further assess our method’s practical utility, we conducted a user study with 20\nprofessional 3D artists evaluating Boundary quality and Editability. Boundary quality measures\nhow unfragmented a UV map is, while editability reflects how well the mapping supports appear-\nance editing. Participants rated UV unwrappings from all methods on a 5-point scale. As shown\nin Table 6, SeamGPT significantly outperforms existing methods in both metrics.\nFandisk\nCow\nNefertiti\nAvg.\nBoundary ↑\nEditability ↑\nBoundary ↑\nEditability ↑\nBoundary ↑\nEditability ↑\nBoundary ↑\nEditability ↑\nXatalas Young (2024)\n4.42\n4.42\n2.68\n2.37\n2.79\n2.47\n3.30\n3.09\nNuvo Srinivasan et al. (2024)\n1.32\n1.32\n1.16\n1.21\n1.42\n1.42\n1.30\n1.32\nFAM Zhang et al. (2024b)\n1.74\n1.53\n1.84\n1.53\n2.05\n1.84\n1.88\n1.63\nEdge-CLS\n3.89\n3.84\n2.79\n2.32\n2.58\n2.16\n3.09\n2.77\nOurs\n4.37\n4.32\n4.16\n4.16\n3.47\n3.58\n4.00\n4.02\nTable 6: User Study about Boundary quality and Editability.\n7.4\nAblation Study\nFigure 21: Ablation of point sampling strategy.\nPoint cloud sampling strategy. As shown in\nFigure. 21, when conditioned on point clouds\nuniformly sampled across the mesh surface, the\ngenerated seams remain logically valid from\na surface-cutting perspective but may not pre-\ncisely align with the input mesh’s vertices and\nedges. In contrast, sampling point clouds along\nedges and vertices produces seams that nat-\nurally conform to the mesh topologies. This\ncould prevents creating excessive extra mesh faces. We also found that sampling along edges and\nvertices significantly improves model convergence, as the transformer gains explicit positional\nawareness of potential cutting coordinates.\nFigure 22: Ablation study of encoder and de-\ncoder.\nMesh encoder vs Point cloud encoder. An\nalternative approach for generating shape em-\nbeddings employs mesh encoders, as demon-\nstrated by Zhou et al. Zhou et al. (2020). We im-\nplemented an encoder combining graph convo-\nlutions (operating on both vertices and edges)\nwith a full self-attention transformer. This en-\ncoder produces vertex-wise tokens that are sub-\nsequently fed to the decoder via cross-attention\nmechanisms. As shown in Figure 22, point-\ncloud encoder yields superior results compared\nto mesh encoders. Furthermore, the computa-\ntional cost of our mesh encoder scales poorly\nwith increasing vertex counts. Mesh encoder-\nbased methods often fail to accurately capture\nthe precise positions of original vertices, result-\ning in significant misalignment between the\n18\ngenerated seam edges and the original mesh.\nDoes Pointer networks works? In the case that the cutting seam forms a subset of the edges\nin the mesh, we can also adopt the Pointer Network Vinyals et al. (2015) architecture, which\nauto-regressively produce the pointers to the mesh edges. We follow the implementation of\nPolygen Nash et al. (2020) to build a pointer network with a mesh encoder that produces edge-\nwise embedding and a casual transformer to create pointers to the edges that lie on the seams\nauto-regressively. Pointer network struggles to generate consistent seams, often resulting in\ndiscontinuous cuts as demonstrated in Figure 22.\nSeam length control and diversity. We define R as the ratio of seam segment count to the number\nof mesh vertices. Empirically, valid cutting seams typically has R value within the range [0.1,0.35].\nAbove this range result in over-cutting, while values below it lead to insufficient cuts. As shown in\nFigure 23, controlling R allows us to adjust the granularity of the cuts. Additionally, due to the\nnon-deterministic nature of autoregressive transformers, we can generate diverse valid cutting\nseams from the same length control.\nFigure 23: Seam length control and diversity. We can control the cutting granularity by adjusting\nseam length. Diverse valid cutting seams can be generated.\n8\nTexture Generation and Editing\nTexture generation and editing technologies have critical importance in 3D asset creation. Phys-\nically Based Rendering (PBR) workflows rely on accurate texture maps to emulate real-world\nmaterial behaviors under varying lighting conditions. High quality texture bridge the gap between\ngeometric abstraction and perceptual realism, enhencing immersion and aesthetic coherence.\nwe introduced a high-fidelity texture synthesis methodology in Zhao et al. (2025); Hunyuan3D et al.\n(2025a); Lai et al. (2025a) that lift a 2D diffusion model into a geometry-conditioned multi-view\ngenerative model i, subsequently baking its outputs into high-resolution texture maps via view\nprojection. This framework systematically addresses two critical challenges in multi-view based\ntexture generation:\n• Cross-view consistency and geometric alignment in Feng et al. (2025).\n• Expansion of RGB textures into photorealistic PBR material textures in He et al. (2025).\nIn this report, we extend our texture generation framework into a comprehensive system support-\ning multimodal texture editing. First, we augment our existing multi-view PBR material generation\nmodel to accommodate text and image-guided multimodal editing. Second, we propose a material-\nbased 3D segmentation method that generates part-wise material segmentation maps from input\ngeometry-only meshes, enabling localized texture editing. Finally, we introduce a 4K material ball\ngeneration model that synthesizes high-resolution tileable texture balls—including Base Color,\nMetallic, Roughness, and Normal maps—from textual prompts, facilitating professional artistic\nworkflows.\n19\nTransform the axe blade \ninto mossy iron and the \nhandle into wooden.\nTurn the axe into lacquerware \nwith gold wire outlines and \nauspicious cloud textures.\nTransform the car into a \nwooden material with a \ndelicate relief on the surface.\nTransform the car into an orange, \npink, and purple gradient color, \nkeeping the gold border.\nChange the warrior's armor \nto gold armor and the green \ncloth to chocolate brown.\nTransform the warrior into \na purple astrologer with \nsunset gradient boots.\nInput Textured Mesh\nEdited by Text\nEdited by Image\nTransform the ship into a \nsmooth green glazed material \nwith hand-applied gold \ntracing to outline its contours.\nTransform the ship into a \nclassical polished wood \nmaterial with red lacquer \npatterns on the surface.\nTransform the flower into a \ncloisonné-style material with \na gradient of turquoise and \nreddish-brown tones.\nTransform the flower into an \namethyst crystal material \nwith gold-edged petals.\nFigure 24: visualization results of multimodal texture editing: Perform text- and image-guided\nediting on textured meshes.\n8.1\nMultimodal Texture Editing\nWe introduce a text-guided texture editing model trained on a meticulously curated dataset of\n80k high-quality 3D assets with PBR materials. These assets were rendered into multi-view HDR\nimages, and a Vision-Language Model (VLM) was employed to generate descriptive captions\nfor textures and editing instructions. Leveraging the Flux Kontext framework, we constructed\nextensive image editing pairs across multiple viewpoints. Our texture foundation model then\ninferred consistent multi-view textures from these pairs, synthesizing a large-scale corpus of\ntext-texture pairs for fine-tuning the editing model. During training, we followed flux kontext to\nunify textual prompts and reference image features into a joint latent sequence. Starting from a\nbase texture generation model, the system was optimized end-to-end using 30,000 text-texture\n20\npairs, resulting in a unified model capable of texture synthesis and editing under both textual and\nvisual guidance.\nWe introduce a streamlined Mixture of Experts (MoE) architecture for the image-guided texture\nediting model to handle diverse image inputs. To determine whether an input image aligns with\nthe target geometry, we compute the CLIP similarity between geometrically rendered views and\nthe input image. If the guidance image exhibits high geometric correspondence with the target\nmesh, we inject image features via a VAE encoder; otherwise, we use CLIP image embeddings for\nfeature infusion—analogous to IP-Adapter’s methodology. This adaptive conditioning mechanism\nensures robust texture editing under arbitrary image conditions.\nThe fascinating multimodal editing results are demonstrated in Fig. 24, indicating that we can\nperform material editing with diverse styles on objects in games, such as props and characters,\nboth globally and locally.\n8.2\n4K Matertial Map Generation\nWe innovatively adapt the 3D VAE framework 25—originally designed for encoding continuous\nvideo frames—to compress multi-domain material data (renders, base color, bump, roughness,\nmetallic, etc.) into unified latent representations, enabling scalable 4K-resolution texture synthesis.\nSpecifically, we fine-tune the 3D VAE using textured 3D assets to achieve domain-invariant feature\nextraction, resulting in a PBR-VAE module. Subsequently, we fine-tune a 3D Diffusion Transformer\n(DiT) with material ball datasets to establish the core architecture of our material ball generation\nmodel.\nFabric texture, a pattern \nof flowers and leaves on \na blue background\nOrange fruits on white \nbackground with natural \ngreen and gray leaves\nInput Prompt\nGenerating PBR materials\nEditing textures by parts\nFigure 25: Visualization of material generation framework.\n9\nAnimation Module\n9.1\nMethod overview\nIn this section, we present the animation module, which is composed of two main branches:\nthe humanoid character animation module and the general character animation module. Each\ncharacter input is first processed by a detection module. If the input is identified as a humanoid\ncharacter, it is directed to the humanoid animation branch; otherwise, it is routed to the general\nbranch.\nThe humanoid branch consists of a template-based auto-rigging module and a motion retargeting\nmodule. To balance the accuracy of skeleton generation with ease of use, we adopt 22 body joints\nas the template skeleton. We follow Guo et al. (2025b) to construct our rigging and skinning\nmodel. However, unlike Guo et al. (2025b), which does not incorporate rig-related information\nduring skinning prediction, our model integrates both skeletal and vertex features to achieve more\n21\nFigure 26: Comparison with UniRig Zhang et al. (2025). Left: results of rigging for a general\ncharacter using our generated mesh. Right: results of skinning applied to the general character\ninput.\naccurate results. In addition, our system includes a pose standardization module that converts\nuser-provided models in arbitrary poses into a canonical T-pose. Feeding T-pose models into the\nmotion retargeting module yields more reliable and precise outcomes.\nIn contrast, the general branch integrates an autoregressive skeleton generation module with\na geometry topology-aware skinning module. Since general characters vary in both skeletal\ntopology and the number of joints, most existing approaches to skeleton generation are based on\nautoregressive techniques, such as Song et al. (2025); Zhang et al. (2025); Guo et al. (2025a); Liu\net al. (2025a). These architectures have already demonstrated stability and accuracy in skeleton\ngeneration tasks, and our module is built upon these autoregressive methods. With respect to\nthe skinning module, prior algorithms typically consider only mesh vertices and skeletal joints\nas input features, while paying little attention to the topological relationships among them. In\ncontrast, our skinning module explicitly incorporates these topological relationships, leading to\nmore robust and stable results.\n9.2\nImplement details\nWe used internally purchased and manually annotated datasets, which consist of approximately\n80,000 high-quality general-character samples and 10,000 humanoid samples. All modules em-\nploy Zhao et al. (2023) as the mesh encoder, while the autoregressive module is implemented using\nOPT-350M Zhang et al. (2022) as the transformer backbone.\nFor the humanoid auto-rigging and skinning module, we apply motion-based data augmentation\nduring training. The model is trained with a batch size of 6 on 8 H20 GPUs for 3 days. For the\ngeneral-purpose rigging module, we train with a batch size of 16 on 24 H20 GPUs for 2 days. For\nthe general-purpose skinning module, we adopt a batch size of 16 and train on 8 GPUs for 2 days.\n9.3\nQualitative results\nAs shown in Figure 26, our method produces more detailed results with fewer errors on gen-\neral characters. Moreover, for the general skinning task, since our algorithm incorporates both\nskeletal and mesh topology information, it achieves higher overall accuracy compared to existing\napproaches.\n10\nConclusion\nHunyuan3D Studio represents a paradigm shift in 3D content creation by integrating generative\nAI into a seamless, end-to-end pipeline that transforms single-image or text inputs into game-ready\nassets with optimized geometry, PBR textures, and engine-compatible topology. Our system’s core\ninnovation lies in its modular yet unified architecture, which combines advanced neural models for\ngeometry generation, component-aware segmentation, automated retopology (PolyGen), semantic\nUV unwrapping, and texture synthesis—all orchestrated to bridge the gap between creative intent\nand technical execution. Experimental validation confirms that assets generated by Hunyuan3D\n22\nStudio meet the stringent requirements of modern game engines while significantly reducing\nproduction time and technical barriers. By democratizing access to high-quality 3D content, the\nplatform empowers both artists and developers to iterate rapidly and focus on creativity rather\nthan manual workflows.\n11\nContributors\n• Project Sponsors: Jie Jiang, Linus, Yuhong Liu, Di Wang, Tian Liu, Peng Chen\n• Project Leaders: Chunchao Guo, Zhuo Chen\n• Core Contributors:\n– PolyGen: Biwen Lei, Jing Xu, Yiling Zhu, Haohan Weng, Jian Liu, Zhen Zhou, Jiankai Xing\n– Part: Yang Li, Jiachen Xu, Changfeng Ma, Xinhao Yan, Yunhan Yang, Chunshi Wang\n– UV: Xinhai Liu, Duoteng Xu, Xueqi Ma, Yuguang Chen, Jing Li\n– Texture: Shuhui yang, Mingxin Yang, Sheng Zhang, Yifei feng, Xin Huang, Di Luo, Zebin He\n– Animation: Lixin Xu, Puhua Jiang, Changrong Hu, Zihan Qin, Shiwei Miao\n– Geometry: Jingwei Huang, Haolin Liu, Yunfei Zhao, Zeqiang Lai, Qingxiang Lin, Zibo Zhao,\nKunhong Li, Huiwen Shi\n– Image: Ruining Tang, Xianghui Yang, Xin Yang, Yuxuan Wang, Zebin Yao\n• Contributors:\n– Engineering: Yihang Lian, Sicong Liu, Xintong Han, Wangchen Qin, Caisheng Ouyang,\nJianyin Liu, Tianwen Yuan, Shuai Jiang, Hong Duan, Yanqi Niu, Wencong Lin, Yifu Sun,\nShirui Huang, Lin Niu, Gu Gong, Guojian Xiao, Bojian Zheng, Xiang Yuan, Qi Chen, Jie Xiao,\nDongyang Zheng, Xiaofeng Yang, Kai Liu, Jianchen Zhu\n– Data: Lifu Wang, Qinglin Lu, Jie Liu, Liang Dong, Fan Jiang, Ruibin Chen, Lei Wang, Chao\nZhang, Jiaxin Lin, Hao Zhang, Zheng Ye, Peng He, Runzhou Wu, Yinhe Wu, Jiayao Du,\nJupeng Chen\n– Art Designer: Xinyue Mao, Dongyuan Guo, Yixuan Tang, Yulin Tsai, Yonghao Tan, Jiaao Yu,\nJunlin Yu, Keren Zhang, Yifan Li\n23\nReferences\nStephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dock-\nhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow\nmatching for in-context image generation and editing in latent space. arXiv e-prints, pp. arXiv–\n2506, 2025.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In Proceedings of the International Conference on Machine Learning\n(ICML), 2024.\nYifei Feng, Mingxin Yang, Shuhui Yang, Sheng Zhang, Jiaao Yu, Zibo Zhao, Yuhong Liu, Jie\nJiang, and Chunchao Guo. Romantex: Decoupling 3d-aware rotary positional embedded multi-\nattention network for texture synthesis, 2025. URL https://arxiv.org/abs/2503.19011.\nJingfeng Guo, Jian Liu, Jinnan Chen, Shiwei Mao, Changrong Hu, Puhua Jiang, Junlin Yu, Jing Xu,\nQi Liu, Lixin Xu, et al. Auto-connect: Connectivity-preserving rigformer with direct preference\noptimization. arXiv preprint arXiv:2506.11430, 2025a.\nZhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, and Ran Zhang. Make-it-\nanimatable: An efficient framework for authoring animation-ready 3d characters. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025b.\nZekun Hao, David W Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like\n3d mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024.\nZebin He, Mingxin Yang, Shuhui Yang, Yixuan Tang, Tao Wang, Kaihao Zhang, Guanying Chen,\nYuhong Liu, Jie Jiang, Chunchao Guo, and Wenhan Luo. Materialmvp: Illumination-invariant\nmaterial generation via multi-view pbr diffusion, 2025. URL https://arxiv.org/abs/2503.102\n89.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nWeizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022.\nTeam Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He,\nDi Luo, Haolin Liu, Yunfei Zhao, Qingxiang Lin, Zeqiang Lai, Xianghui Yang, Huiwen Shi, Zibo\nZhao, Bowen Zhang, Hongyu Yan, Lifu Wang, Sicong Liu, Jihong Zhang, Meng Chen, Liang\nDong, Yiwen Jia, Yulin Cai, Jiaao Yu, Yixuan Tang, Dongyuan Guo, Junlin Yu, Hao Zhang, Zheng\nYe, Peng He, Runzhou Wu, Shida Wei, Chao Zhang, Yonghao Tan, Yifu Sun, Lin Niu, Shirui\nHuang, Bojian Zheng, Shu Liu, Shilin Chen, Xiang Yuan, Xiaofeng Yang, Kai Liu, Jianchen Zhu,\nPeng Chen, Tian Liu, Di Wang, Yuhong Liu, Linus, Jie Jiang, Jingwei Huang, and Chunchao Guo.\nHunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material,\n2025a. URL https://arxiv.org/abs/2506.15442.\nTeam Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He,\nDi Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to high-fidelity 3d assets\nwith production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025b.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.\nPerceiver: General perception with iterative attention. In International conference on machine\nlearning, pp. 4651–4664. PMLR, 2021.\nZeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang,\nMingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets\ngeneration with ultimate details. arXiv preprint arXiv:2506.16504, 2025a.\nZeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang,\nQingxiang Lin, Jingwei Huang, Yuhong Liu, et al. Unleashing vecset diffusion model for fast\nshape generation. arXiv preprint arXiv:2503.16302, 2025b.\nBiwen Lei, Kai Yu, Mengyang Feng, Miaomiao Cui, and Xuansong Xie. Diffusiongan3d: Boosting\ntext-guided 3d generation and domain adaptation by combining 3d gans and diffusion priors. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10487–10497,\n2024.\n24\nZhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang,\nXingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen\nZhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li,\nJihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu\nTao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen,\nZhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and\nQinglin Lu. Hunyuan-dit: A powerful multi-resolution diffusion transformer with fine-grained\nchinese understanding, 2024.\nYuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina\nFragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion\ntransformers, 2025. URL https://arxiv.org/abs/2506.05573.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching\nfor generative modeling. arXiv preprint arXiv:2210.02747, 2022.\nIsabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, and Zifan Shi.\nRiganything: Template-free autoregressive rigging for diverse 3d assets. ACM Transactions on\nGraphics (TOG), 44(4):1–12, 2025a.\nJian Liu, Jing Xu, Song Guo, Jing Li, Jingfeng Guo, Jiaao Yu, Haohan Weng, Biwen Lei, Xianghui\nYang, Zhuo Chen, et al. Mesh-rft: Enhancing mesh generation via fine-grained reinforcement\nfine-tuning. arXiv preprint arXiv:2505.16761, 2025b.\nMinghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun\nGao. Partfield: Learning 3d feature fields for part segmentation and beyond, 2025c.\nChangfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen\nGuo, Zhuo Chen, and Chunchao Guo. P3-sam: Native 3d part segmentation. arXiv preprint\narXiv:2509.06784, 2025.\nXueqi Ma, Yilin Liu, Wenjun Zhou, Ruowei Wang, and Hui Huang. Generating 3d house wire-\nframes with semantics. In European Conference on Computer Vision, pp. 223–240. Springer, 2024.\nCharlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive\ngenerative model of 3d meshes. In International conference on machine learning, pp. 7220–7229.\nPMLR, 2020.\nMaxime Oquab, Timoth´ee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao\nHuang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nico-\nlas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand\nJoulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023.\nYawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav\nRosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-\nonly transformers. arXiv preprint arXiv:2311.15475, 2023.\nChaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew,\nXiaoyang Guo, Fayao Liu, Jiashi Feng, and Guosheng Lin. Magicarticulate: Make your 3d\nmodels articulation-ready. In Proceedings of the Computer Vision and Pattern Recognition Conference\n(CVPR), pp. 15998–16007, June 2025.\nPratul P Srinivasan, Stephan J Garbin, Dor Verbin, Jonathan T Barron, and Ben Mildenhall. Nuvo:\nNeural uv mapping for unruly 3d representations. In European Conference on Computer Vision, pp.\n18–34. Springer, 2024.\nStefan Stojanov, Anh Thai, and James M. Rehg. Using shape to categorize: Low-shot learning with\nan explicit shape bias. CVPR, 2021.\nJiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang\nZeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume\npacking. arXiv preprint arXiv:2506.09980, 2025.\nTencent Hunyuan3D Team. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured\n3d assets generation, 2025.\n25\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural information\nprocessing systems, 28, 2015.\nHaohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong\nLiu, Jie Jiang, Chunchao Guo, Tong Zhang, Shenghua Gao, and C. L. Philip Chen. Scaling mesh\ngeneration via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024.\nHaohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong\nLiu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. In\nProceedings of the Computer Vision and Pattern Recognition Conference, pp. 11093–11103, 2025.\nChenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai\nBai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324,\n2025.\nJianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen,\nXin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In\nProceedings of the Computer Vision and Pattern Recognition Conference, pp. 21469–21480, 2025.\nXinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang\nLai, Yunfei Zhao, Zhuo Chen, and Chunchao Guo. X-part: high-fidelity and structure-coherent\nshape decomposition. arXiv preprint arXiv:2509.08643, 2025.\nXianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai\nLiu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d 1.0: A unified framework for\ntext-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024.\nYunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-\nPei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint\narXiv:2504.07943, 2025a.\nYunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu,\nDing Liang, Yan-Pei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic\ndecoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025b.\nJonathan Young. xatlas - mesh parameterization / uv unwrapping library, 2024. URL https:\n//github.com/jpcy/xatlas.\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language\nimage pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision\n(ICCV), pp. 11975–11986, 2023.\nBiao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape\nrepresentation for neural fields and generative diffusion models. ACM Transactions On Graphics\n(TOG), 42(4):1–16, 2023.\nJia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. One model to rig\nthem all: Diverse skeleton rigging with unirig. arXiv preprint arXiv:2504.12451, 2025.\nLongwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan\nXu, and Jingyi Yu. Clay: A controllable large-scale generative model for creating high-quality 3d\nassets. ACM Transactions on Graphics (TOG), 43(4):1–20, 2024a.\nQijian Zhang, Junhui Hou, Wenping Wang, and Ying He. Flatten anything: Unsupervised neural\nsurface parameterization. arXiv preprint arXiv:2405.14633, 2024b.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOpt: Open pre-trained transformer language models, 2022.\nZibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu,\nand Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text\naligned latent representation. Advances in neural information processing systems, 36:73969–73982,\n2023.\n26\nZibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin\nYang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high\nresolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025.\nYi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, and Yaser Sheikh.\nFully convolutional mesh autoencoder using efficient spatially varying kernels. Advances in\nneural information processing systems, 33:9251–9262, 2020.\n27\n",
    "content": "# Tencent Hunyuan3D Studio Paper Interpretation\n\n## 1. Core Content and Key Contributions\n\n**Core Content:**  \nThis paper introduces **Hunyuan3D Studio**, an end-to-end, AI-driven 3D asset generation platform developed by Tencent. The system aims to revolutionize the 3D content creation pipeline in game development by automatically converting a single concept image or text description into a \"production-ready\" 3D model complete with optimized geometry, high-fidelity PBR materials, and animation readiness.\n\n**Key Contributions:**\n- Proposes and implements a **complete end-to-end AI pipeline**, integrating all critical stages from concept design to engine deployment (image generation → high-precision geometry generation → part-level decomposition → polygon retopology → semantic UV unwrapping → material synthesis & editing → automatic rigging & animation).\n- Develops a series of innovative neural modules, including:\n  - **Part-level 3D Generation**: Semantic segmentation and structured generation based on components;\n  - **PolyGen**: Autoregressive modeling for low-polygon mesh generation;\n  - **SeamGPT**: Transformer-based, semantics-aware UV seam prediction;\n  - **4K Material Ball Generation**: High-quality, tileable material ball generation.\n- Achieves **production-grade output quality**, enabling direct use of generated assets in mainstream game engines such as Unity and Unreal, meeting real-time rendering requirements.\n- Constructs large-scale training datasets (e.g., 3.7 million 3D models with part-level annotations), advancing subtasks like part segmentation.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nHunyuan3D Studio achieves significant technical breakthroughs across multiple dimensions:\n\n| Module | Innovation |\n|-------|------------|\n| **Controllable Image Generation** | Supports style transfer (cartoon, cyberpunk, etc.) and pose standardization (A-pose alignment) to ensure input consistency. |\n| **Geometry Generation** | Introduces **bounding box control signals** and **multi-view image guidance**, improving geometric proportion accuracy and detail fidelity. |\n| **Part-level 3D Generation** | Proposes **P3-SAM** (native 3D part segmentation model) and **X-Part** (controllable part decomposition framework), enabling part-level modeling with high semantic consistency. |\n| **Polygon Generation (PolyGen)** | Uses an **autoregressive model to generate low-poly meshes face-by-face**, replacing traditional retopology tools and producing deformation-friendly topology; employs **Masked DPO reinforcement learning post-training** to optimize local quality. |\n| **Semantic UV Unwrapping (SeamGPT)** | Models UV seam prediction as a sequence generation task, using a **causal Transformer** to produce artist-style semantic seams that significantly outperform traditional algorithms (e.g., XAtlas). User studies show substantial advantages in seam completeness and editability. |\n| **Texture Synthesis & Editing** | Supports **dual-modal (text + image) guided material editing**, enabling both local and global material modifications; introduces a **4K material ball generation model** for easy reuse of art resources. |\n| **Animation Module** | Differentiates between humanoid and non-humanoid characters, handling bone rigging and skinning weights accordingly; supports T-pose standardization and motion retargeting, improving animation preparation efficiency. |\n\nOverall, its greatest innovation lies in **integrating fragmented AI generation capabilities into a seamless, highly automated, and industry-standard-compliant workflow**, solving the long-standing problem of AI-generated 3D models being “visually appealing but unusable.”\n\n---\n\n## 3. Startup Project Recommendations\n\nBased on the core technologies of Hunyuan3D Studio, here are several highly promising entrepreneurial directions:\n\n### 🚀 Startup Project One: **GameAsset.AI — SaaS Platform for Game Developers' 3D Asset Generation**\n\n#### Positioning\nA cloud-based 3D asset generation service targeting indie game developers, small studios, and educational users, offered via pay-per-use or subscription models.\n\n#### Core Features\n- Text/sketch → FBX/GLTF assets importable into Unity/Unreal (with LOD, collision meshes)\n- Style control support (pixel art, low-poly, realistic, etc.)\n- Built-in material library + AI-powered material editor\n- Automatic UV mapping + lightweight topology + basic skeletal rigging\n- Plugin integration: one-click export to major game engines\n\n#### Business Model\n- Free Tier: 5 basic assets per month\n- Pro Tier: $29/month, unlimited generation + advanced styles + batch export\n- Enterprise Tier: Custom private deployment + API access\n\n#### Competitive Advantages\n- Vertical optimization (output tailored for game performance)\n- Native Chinese language support (ideal for domestic small teams)\n- Cost reduction through partnerships with Tencent Cloud and GPU vendors\n\n---\n\n### 🎨 Startup Project Two: **StyleMesh — AI-Powered 3D Artistic Style Transfer Platform**\n\n#### Positioning\nEmpowers ordinary users to create 3D digital collectibles/NFTs/metaverse avatars with specific artistic styles.\n\n#### Core Features\n- Users upload a 3D model or select a template\n- Input prompts (e.g., “cyber-mecha,” “Dunhuang mural texture”)\n- AI automatically repaints materials, adjusts shapes, adds decorative elements\n- Outputs compatible with platforms like Decentraland and Roblox\n- Supports blockchain verification and NFT minting\n\n#### Target Users\n- Digital artists\n- NFT creators\n- Metaverse social platform users\n\n#### Revenue Model\n- Generation fees + NFT transaction commissions\n- Style pack subscriptions (official or third-party artist releases)\n- Revenue sharing with Web3 platforms\n\n---\n\n### 🏭 Startup Project Three: **IndusModeler — Industrial-Grade 3D Modeling Automation Solution**\n\n#### Positioning\nAn AI-assisted modeling system serving industries such as manufacturing, architectural visualization, and virtual exhibition halls.\n\n#### Core Features\n- Input product photos/sketches → automatically generate parametric CAD-compatible models\n- Supports part decomposition and assembly relationship recognition\n- Automatically generates engineering drawings and material specifications\n- Enables batch variant generation (color, size, configuration)\n\n#### Use Cases\n- Rapid creation of e-commerce 3D product models (furniture, appliances)\n- Fast BIM model construction for architecture\n- Digital twin modeling for industrial equipment\n\n#### Partnership Opportunities\n- Integration with Autodesk, Siemens, Dassault Systèmes\n- Becoming a 3D content provider for e-commerce platforms (JD, Alibaba)\n\n---\n\n### 💡 Summary and Recommendation\n\n> **The best entry point is “GameAsset.AI”** — because:\n> 1. The gaming industry has massive demand for 3D assets with relatively high standardization;\n> 2. Indie developers have strong willingness to pay and the market is fragmented, making it easier to enter;\n> 3. It can directly leverage the Hunyuan3D Studio technology stack for rapid MVP development;\n> 4. It easily fosters a community ecosystem (UGC sharing, plugin marketplace).\n\nFuture expansion could extend into film pre-visualization, VR content creation, and beyond, ultimately building the next-generation **AI-native 3D content operating system**.",
    "github": "",
    "hf": ""
}