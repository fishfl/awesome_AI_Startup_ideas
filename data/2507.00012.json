{
    "id": "2507.00012",
    "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information",
    "summary": "This paper proposes a new training method called CMIM, which trains the model by minimizing the cross-entropy loss and the conditional mutual information of the output probability distribution after temperature normalization. It is also shown that the resulting CMIM model is distillable.",
    "abstract": "A deep neural network (DNN) is said to be undistillable if, when used as a black-box input-output teacher, it cannot be distilled through knowledge distillation (KD). In this case, the distilled student (referred to as the knockoff student) does not outperform a student trained independently with label smoothing (LS student) in terms of prediction accuracy. To protect intellectual property of DNNs, it is desirable to build undistillable DNNs. To this end, it is first observed that an undistillable DNN may have the trait that each cluster of its output probability distributions in response to all sample instances with the same label should be highly concentrated to the extent that each cluster corresponding to each label should ideally collapse into one probability distribution. Based on this observation and by measuring the concentration of each cluster in terms of conditional mutual information (CMI), a new training method called CMI minimized (CMIM) method is proposed, which trains a DNN by jointly minimizing the conventional cross entropy (CE) loss and the CMI values of all temperature scaled clusters across the entire temperature spectrum. The resulting CMIM model is shown, by extensive experiments, to be undistillable by all tested KD methods existing in the literature. That is, the knockoff students distilled by these KD methods from the CMIM model underperform the respective LS students. In addition, the CMIM model is also shown to performs better than the model trained with the CE loss alone in terms of their own prediction accuracy.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang",
    "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:27 pages, 6 figures, Transactions on Machine Learning Research",
    "keypoint": "- The paper introduces the concept of undistillable deep neural networks (DNNs), which cannot be effectively distilled through knowledge distillation (KD) methods.\n- An undistillable DNN should have highly concentrated output probability distributions for each class, ideally collapsing into one probability distribution per label.\n- Conditional mutual information (CMI) is used to measure the concentration of output clusters, and minimizing CMI leads to more compact clusters.\n- A new training method called CMI Minimized (CMIM) is proposed, which jointly minimizes cross entropy loss and CMI values across all temperature-scaled clusters.\n- CMIM models are shown to be undistillable by all tested KD methods; knockoff students distilled from these models underperform compared to label smoothing (LS) students.\n- CMIM models also demonstrate improved classification accuracy over models trained with conventional cross entropy loss alone.\n- Theoretical extensions include measuring CMI for power-transformed clusters, allowing different temperature scaling for each class to simulate adaptive adversaries.\n- Experimental results on CIFAR-100, TinyImageNet, and ImageNet datasets confirm the effectiveness of the CMIM approach in making models both undistillable and accurate.\n- Visualization shows that CMIM-trained models have highly concentrated output clusters near simplex corners, resembling one-hot vectors.\n- CMIM outperforms existing defense methods in resisting a wide range of KD-based attacks while maintaining or improving model performance.",
    "date": "2025-07-03",
    "paper": "arXiv:2507.00012v1  [cs.LG]  13 Jun 2025\nPublished in Transactions on Machine Learning Research (06/2025)\nTowards Undistillable Models by Minimizing Conditional\nMutual Information\nLinfeng Ye∗\nlinfeng.ye@mail.utoronto.ca\nEdward S. Rogers Sr. Department of Electrical and Computer Engineering\nUniversity of Toronto\nShayan Mohajer Hamidi∗\nsmohajer@stanford.edu\nDepartment of Electrical Engineering\nStanford University\nEn-Hui Yang\nehyang@uwaterloo.ca\nDepartment of Electrical and Computer Engineering\nUniversity of Waterloo\nReviewed on OpenReview: https: // openreview. net/ forum? id= jVABSsD4Vf\nAbstract\nA deep neural network (DNN) is said to be undistillable if, when used as a black-box input-\noutput teacher, it cannot be distilled through knowledge distillation (KD). In this case, the\ndistilled student (referred to as the knockoff student) does not outperform a student trained\nindependently with label smoothing (LS student) in terms of prediction accuracy. To protect\nintellectual property of DNNs, it is desirable to build undistillable DNNs. To this end, it is\nfirst observed that an undistillable DNN may have the trait that each cluster of its output\nprobability distributions in response to all sample instances with the same label should be\nhighly concentrated to the extent that each cluster corresponding to each label should ideally\ncollapse into one probability distribution. Based on this observation and by measuring the\nconcentration of each cluster in terms of conditional mutual information (CMI), a new\ntraining method called CMI minimized (CMIM) method is proposed, which trains a DNN\nby jointly minimizing the conventional cross entropy (CE) loss and the CMI values of all\ntemperature scaled clusters across the entire temperature spectrum. The resulting CMIM\nmodel is shown, by extensive experiments, to be undistillable by all tested KD methods\nexisting in the literature. That is, the knockoff students distilled by these KD methods\nfrom the CMIM model underperform the respective LS students. In addition, the CMIM\nmodel is also shown to performs better than the model trained with the CE loss alone in\nterms of their own prediction accuracy.\nThe code for the paper is publicly available at\nhttps://anonymous.4open.science/r/CMIM-605C/README.md.\n1\nIntroduction\nOriginally aiming for model compression, knowledge distillation (Buciluˇa et al., 2006; Hinton et al., 2015)\n(KD) has received significant attention from both academia and industry in recent years due to its remarkable\neffectiveness. The essence of KD is to transfer the knowledge of a pre-trained large model (teacher) to a\nsmaller model (student).\nBuilding on the work of Hinton et al. (2015), numerous follow-up works have\nendeavored to enhance the performance of KD (Romero et al., 2014; Anil et al., 2018; Park et al., 2019) and\nto gain deeper insights into why distillation is effective (Phuong & Lampert, 2019; Mobahi et al., 2020; Ye\net al., 2024; Allen-Zhu & Li, 2020; Menon et al., 2021; Borup & Andersen, 2021).\n∗Authors contributed equally.\n1\nPublished in Transactions on Machine Learning Research (06/2025)\nIn the scenario where the teacher does not want its knowledge to be transferred, however, KD is undesirable\nand indeed poses a threat to intellectual property (IP) of the teacher (Shokri & Shmatikov, 2015). Developing\nand training a high-quality large DNN requires significant investments of time, effort, finances, and resources,\nincluding extensive data annotation and computational infrastructure. Developers of the large DNN may\nwant to prevent the knowledge of the large DNN from being transferred by their competitors. However, once\nthe large DNN is released as a “black box”, anyone can apply a logit-based KD method (or equivalently a\ndistribution-based KD method (Zheng & YANG, 2024)) to distill the DNN as a teacher. The goal is to train\na student, referred to as a knockoff student in the context of DNN IP protection, that mimics the teacher’s\nbehavior to gain competitive advantages. As such, in this case, it would be desirable for the developers to\nbuild the large DNN so that it is undistillable. The question, of course, is how.\nBefore delving deeper into the above question, let us first clarify what we mean by saying that a DNN is\nundistillable. At this point, we invoke the concept of distillable DNN introduced recently in Yang & Ye\n(2024):\nDefinition 1. [Distillability of a DNN (Yang & Ye, 2024)] When used as a black-box input-output teacher,\na DNN is said to be distillable1 with respect to a student if there exists a KD method which, when applied\nto the teacher and student, yields a knockoff student outperforming the student trained alone with label\nsmoothing (LS student) in terms of prediction accuracy.\nTherefore, a DNN is undistillable if no knockoff student can outperform the respective LS student regardless\nof which logit-based KD method is used. Since there are so many logit-based KD methods and so many\nstudents, what type of a DNN is undistillable? In comparison with the training of LS student, Definition 1\nprovides some insight into a trait that an undistillable DNN may possess. For each label, consider the cluster\nof the output probability distributions of the DNN in response to all input sample instances with that label. If\neach cluster corresponding to each label is highly concentrated to the extent that all probability distributions\nwithin the cluster more or less collapse into one probability distribution, then the student training within\nKD is similar to that of the respective LS student regardless of which logit-based KD method and which\nstudent are applied. In this case, one would expect that no knockoff student would perform significantly\nbetter than the respective LS student. Therefore, a DNN possessing this trait will likely be undistillable.\nFrom a theoretical perspective, KD relies on the diversity within the teacher’s predicted distributions to\nconvey richer contextual information to the student. The greater the diversity in the cluster of predictions\nfor a given class, the more nuanced guidance the student can receive (see Ye et al. (2024)). Conversely,\nwhen the teacher’s outputs become highly concentrated—meaning predictions for inputs with the same label\nconverge to a single probability vector—this contextual richness is lost. The student, in this case, receives\nlargely redundant information, rendering the distillation process as ineffective as direct training with label\nsmoothing.\nTo quantify this notion of concentration, we turn to conditional mutual information (CMI) (Yang et al.,\n2025). Specifically, let X denote the random input sample to the DNN, and Y be the ground truth label of\nX. Let ˆY denote the random label predicted by the DNN in response to input X. It was shown in Yang\net al. (2025) that for each label y, the label specific CMI I(X; ˆY | Y = y) measures the concentration of\nthe cluster corresponding to label y, and the CMI I(X; ˆY | Y ) measures the average concentration across all\nclusters. To build an undistillable DNN, one then is motivated to minimize jointly the conventional cross\nentropy (CE) loss and the CMI I(X; ˆY | Y ).\nIn this paper, we will go one step further. In KD (Hinton et al., 2015), temperature scaling of logits is often\napplied. It was shown in Zheng & YANG (2024) that logit temperature scaling with temperature T can\nbe equivalently achieved by power transform of the output probability distribution with power α = 1/T.\nFurther, it was demonstrated in Ye et al. (2024) that the purpose of temperature scaling or power transform\nis to enlarge the CMI values of temperature scaled (or power transformed) clusters, and enlarging CMI\nvalues in turn improves the performance of distilled students. Since here we want to achieve the opposite,\n1There are two reasons for us to adopt this definition of distillability. First, as shown theoretically in Zheng & YANG (2024),\nknowledge distillation reduces to label smoothing in the limit as the temperature approaches infinity. Therefore, if the teacher is\ndistillable, the distilled student should perform no worse than the LS student. Second, if a knockoff student cannot outperform\nthe LS student, then there is no incentive for the teacher to be leveraged since the LS student can be trained on its own.\n2\nPublished in Transactions on Machine Learning Research (06/2025)\nwe want to make sure that all CMI values of all power transformed clusters can be made small. To this end,\nwe further extend the label specific CMI I(X; ˆY | Y = y) and the CMI I(X; ˆY | Y ) to I(X; ˆY α| Y = y) and\nI(X; ˆY α[Y ]| Y ), respectively, so that I(X; ˆY α| Y = y) measures the concentration of the power transformed\ncluster corresponding to label y with power α, and I(X; ˆY α[Y ]| Y ) measures the average concentration across\nall power transformed clusters with power α[Y ], where different clusters may be power transformed with\ndifferent power α. Notably, allowing separate temperature scaling (i.e., power transformation) for each class\nis essential, as an adaptive adversary may apply class-specific scaling to selectively alter cluster concentration\nand improve the knockoff student’s performance.\nBased on the above discussion and towards building undistillable DNNs, we then propose a new training\nmethod called CMI minimized method, which trains a DNN by jointly minimizing the CE loss and all CMI\nvalues of all power transformed clusters, i.e., jointly minimizing the CE loss and I(X; ˆY α[Y ]| Y ), ∀α[Y ] > 0.\nThe resulting trained DNN is referred to as the CMI minimized (CMIM) DNN. The contributions of the\npaper are summarized as follows:\n• An insight is provided that in order for a DNN to be undistillable, it is desirable for the DNN to possess the\ntrait that each cluster of the DNN’s output probability distributions corresponding to each label is highly\nconcentrated to the extent that all probability distributions within the cluster more or less collapse into one\nprobability distribution close to the one-hot probability vector of that label.\n• We extend the label specific CMI I(X; ˆY | Y = y) and the CMI I(X; ˆY | Y ) to I(X; ˆY α| Y = y) and\nI(X; ˆY α[Y ]| Y ), respectively, so that I(X; ˆY α| Y = y) measures the concentration of the power transformed\ncluster corresponding to label y with power α, and I(X; ˆY α[Y ]| Y ) measures the average concentration across\nall power transformed clusters with power α[Y ], where different clusters may be power transformed with\ndifferent power α.\n• We develop a novel training method dubbed CMI minimized method to train a DNN by jointly minimizing\nthe CE loss and all CMI values of all power transformed clusters with the resulting trained DNN referred to\nas the CMIM DNN.\n• To the best of our knowledge, our method is the first in the literature capable of training undistillable DNNs\nthat remain robust against a wide range of KD methods. Furthermore, for the notion of undistillability, we\nare the first to employ the formal definition introduced in Yang & Ye (2024).\n• We show, by extensive experiments over three popular image classification datasets, namely CIFAR-100\n(Krizhevsky et al., 2012), TinyImageNet (Le & Yang, 2015) and ImageNet (Deng et al., 2009), that CMIM\nDNNs have very small CMI values and are indeed undistillable by all tested KD methods existing in the\nliterature. That is, the knockoff students distilled by these KD methods from the CMIM models underperform\nthe respective LS students. On the other hand, models trained by defense training methods proposed in the\nliterature are all distillable.\n• In addition, we show that the CMIM models achieve a higher classification accuracy compared to those\ntrained with the conventional CE loss.\n2\nRelated Works\nIn this section, we mention some defense methods against the threat posed by knockoff students attempting\nto steal the IP of pre-trained DNNs via logit-based KD methods. For a thorough review of related works,\nincluding detailed discussions about recent logit-based KD methods, please refer to Appendix B. These\ndefense methods can be mainly categorized into two groups: (i) model stealing resistant training methods\nthat specifically train DNNs to reduce the accuracy of knockoff students while maintaining the original\nclassification accuracy of the model (Ma et al., 2021; Wang et al., 2022); and (ii) post-training defense\nmethods that perform minimal perturbations to the pre-trained model’s predictions to mislead the knockoff\nstudent (Lee et al., 2019; Orekondy et al., 2020; Cheng & Cheng, 2023). Nonetheless, in Section 5, we will\nshow that models trained by all these defense methods are indeed distillable.\n3\nPublished in Transactions on Machine Learning Research (06/2025)\n3\nNotation and Preliminaries\n3.1\nNotation\nThe set of real numbers is denoted by R. Vectors are denoted by bold-face letters (e.g., w). The i-th element\nof vector w is denoted by w[i]. For two vectors u, v ∈RC, the inequality u ≤v implies that u[i] ≤v[i],\n∀i ∈[C]. For a positive integer K, let [K] ≜{1, ...K}. Assume that there are C class labels with [C] as\nthe set of class labels. Let P([C]) denote the set of all C dimensional probability distributions. For any two\nprobability distributions P1, P2 ∈P([C]), the CE and Kullback-Leibler (KL) divergence between P1 and P2\nare denoted by H(P1, P2) and KL(P1, P2), respectively. For any y ∈[C] and P ∈P([C]), write the CE of\nthe one-hot probability distribution corresponding to y and P as H(y, P).\nFor any differentiable function f(·), ∇wf(·) denotes its gradient vector w.r.t. vector w.\nFor any pair of random variables (X, Y ), denote its joint probability distribution by PX,Y (x, y) or simply\nP(x, y) whenever there is no ambiguity, the marginal distribution of Y by PY (y), and the conditional\ndistribution of Y given X = x by PY |X(·|x). The mutual information between two random variables X and\nY is denoted by I(X, Y ), and the CMI of X and Y given a third random variable Z is I(X, Y |Z).\nWe regard a classification DNN as a mapping from raw data x ∈Rd to a probability distribution qx ∈P([C]).\nGiven a DNN: x ∈Rd →qx, let θ denote its weight vector consisting of all its connection weights; whenever\nthere is no ambiguity, we also write qx as qx,θ.\n3.2\nLabel Smoothing\nLabel smoothing (LS) (Pereyra et al., 2017) is a regularization technique that prevents peaked output\nprobability distributions, leading to better generalization, by minimizing the objective function:\nLLS = (1 −ϵ)H(y, qx) + ϵH(u, qx),\n(1)\nwhere u is the uniform distribution over C classes, and ϵ controls the strength of the regularization.\n3.3\nPower Transform of Probability Distribution\nIn a “black-box” teacher setting, where only the output probability vectors (and not the logits) of the\nteacher are accessible to the public, applying temperature scaling directly over the logits of the teacher is not\nfeasible in training knockoff students. In this case, KD training can resort to applying “power transformation\nof probability distribution” directly to the output probability vectors (Zheng & YANG, 2024). Specifically,\ngiven P ∈P([C]), and a non-negative real number α, the power transform of P is another probability\ndistribution define as\nP α[i] =\n(P[i])α\nP\nj∈[C](P[j])α ,\n∀i ∈[C].\n(2)\nIt is not hard to verify that the power transformed probability distribution P α is equal to the softmax of the\nlogits scaled by temperature T = 1/α. Therefore, temperature scaling can be equivalently operated directly\non the output probability distribution through power transform.\n3.4\nCMI value of a DNN\nAs discussed in Yang et al. (2025), for a classifier f : x ∈Rd →qx, let ˆY be the random label predicted by\nthe f with probability qX[ ˆY ] in respond to the input X. For each cluster corresponding to label y ∈[C], we\n4\nPublished in Transactions on Machine Learning Research (06/2025)\nhave\nI(X; ˆY |Y = y) =\nX\nx\nPX|Y (x|y)\n\" C\nX\ni=1\nP ˆY |XY ( ˆY = i|x, y) ln\nP ˆY |XY ( ˆY = i|x, y)\nP ˆY |Y ( ˆY = i|Y = y)\n#\n(3)\n= EX|Y\n\" C\nX\ni=1\nqX[i] ln\nqX[i]\nP ˆY |Y ( ˆY = i|Y = y)\n!\n|Y = y\n#\n= EX|Y [KL (qX, sy) |Y = y] ,\n(4)\nwhere P ˆY |XY ( ˆY = i|x, y) = qx[i] follows from the Markov chain Y →X →ˆY , and sy = P ˆY |Y (·|y) =\nEX|Y [qX|Y = y]. I(X; ˆY |Y = y) measures the concentration of the cluster corresponding to label y ∈[C].\nAveraging over all clusters corresponding to all labels y, we get\nI(X; ˆY |Y ) =\nX\ny∈[C]\nPY (y)I(X; ˆY |Y = y) = EXY [KL (qX, sY )] .\n(5)\nI(X; ˆY |Y ) measures the average concentration across all clusters.\nWhen the distribution PX,Y\nis unknown, we can approximate the CMI of f by its empirical value\nfrom a data sample (a training dataset or mini-batch thereof) D = {(xi, yi)}m\ni=1.\nTo this end, let\nDy = {1 ≤j ≤m : yj = y}.\nDenote the size of Dy by |Dy|.\nThe empirical values of each label specific\nCMI and the CMI can be calculated as follows\nIemp(X; ˆY |Y = y) =\n1\n|Dy|\nX\ni∈Dy\nKL(qxi, semp\ny\n),\n(6)\nIemp(X; ˆY |Y ) = 1\nm\nm\nX\ni=1\nKL(qxi, semp\nyi\n),\n(7)\nwhere\nsemp\ny\n=\n1\n|Dy|\nX\ni∈Dy\nqxi, ∀y ∈[C].\n(8)\n4\nCMI Minimized Method\nIn this section, we present our CMI minimized method.\nWe begin with extending I(X; ˆY |Y = y) and\nI(X; ˆY |Y ) to the case of power transformed clusters.\n4.1\nInformation Quantities for Power Transformed Clusters\nConsider a classification DNN: f : x ∈Rd →qx which maps input sample instances x with different labels\ninto clusters of probability distributions qx in the space P([C]), with one cluster per label. For each label\ny ∈[C], apply the power transform with power α to each probability distribution qx within the cluster\ncorresponding to the label y. Then, we obtain a power transformed cluster. To measure the concentration\nof the power transformed cluster, we extend I(X; ˆY |Y = y) to the following information quantity\nI(X; ˆY α|Y = y) = EX|Y [KL (qα\nX, sy,α) |Y = y] ,\n(9)\nwhere sy,α = EX|Y [qα\nX|Y = y]. Note that if we regard ˆY α as the random label predicted by f with probability\nqα\nX( ˆY α) in response to the input sample X, i.e., given X, ˆY α is equal to a label c with probability qα\nX(c),\n∀c ∈[C], then I(X; ˆY α|Y = y) is exactly the CMI between X and ˆY α given Y = y. Thus, I(X; ˆY α|Y = y)\nmeasures the concentration of the power transformed cluster corresponding to y.\nNow, we go one step further and allow different clusters to be power transformed with different powers.\nSuppose that the cluster corresponding to label y is power transformed with power α[y]. Let ˆY α[Y ] be the\nrandom label predicted by f with probability qα[Y ]\nX\n( ˆY α[Y ]) in response to the input sample X given Y . That\n5\nPublished in Transactions on Machine Learning Research (06/2025)\nis, given Y = y and X = x, ˆY α[Y ] is equal to c with probability qα[y]\nx\n(c) for any c ∈[C]. We can then extend\nI(X; ˆY |Y ) to I(X; ˆY α[Y ]|Y )\nI(X; ˆY α[Y ]|Y ) = EXY\nh\nKL\n\u0010\nqα[Y ]\nX\n, sY,α[Y ]\n\u0011i\n,\n(10)\n=\nX\ny∈[C]\nPY (y)\nh\nEX|Y\nh\nKL\n\u0010\nqα[y]\nX\n, sy,α[y]\n\u0011\n|Y = y\nii\n(11)\n=\nX\ny∈[C]\nPY (y)I(X; ˆY α[y]|Y = y),\n(12)\nwhere for each y ∈[C],\nsy,α[y] = P ˆY α[Y ]|Y (·|y) =\nX\nx\nPX|Y (x|y)qα[y]\nx\n= EX|Y\nh\nqα[y]\nX\n|Y = y\ni\n.\n(13)\nNote that I(X; ˆY α[Y ]|Y ) is exactly the CMI between X and ˆY α[Y ] given Y and measures the average con-\ncentration across all power transformed clusters with power function α[Y ]. However, Y , X, and ˆY α[Y ] do\nnot form a Markov chain anymore.\nWhen the distribution PX,Y is unknown, we can approximate I(X; ˆY α[Y ]|Y = y) and I(X; ˆY α[Y ]|Y ) by their\nrespective empirical values from a data sample (a training dataset or mini-batch thereof) D = {(xi, yi)}m\ni=1:\nIemp(X; ˆY α[Y ]|Y = y) =\n1\n|Dy|\nX\ni∈Dy\nKL(qα[y]\nxi , semp\ny,α[y]),\n(14)\nIemp(X; ˆY α[Y ]|Y ) = 1\nm\nm\nX\ni=1\nKL(qα[yi]\nxi\n, semp\nyi,α[yi]),\n(15)\nwhere\nsemp\ny,α[y] =\n1\n|Dy|\nX\ni∈Dy\nqα[y]\nxi , ∀y ∈[C].\n(16)\n0.00\n0.25\n0.50\n0.75\n1.00\nα\n0.00\n0.02\n0.04\n0.06\n0.08\nCMI\nbeaver\norchids\nmotorcycle\nFigure\n1:\nThe\nCMI\nvalue\nI(X; ˆY α[Y ]|Y\n=\ny)\nfor\nthree\nrandomly\nselected\nclasses\ny\n=\n{beaver, orchids, motorcycle}\nvs the power transform factor α;\nthe model is ResNet-50 pre-trained\non CIFAR-100.\nAs observed, as\nα grows, the CMI value becomes\nlarger, peaks, and then gradually\nbecomes smaller.\nAs discussed in Section 1, an undistillable DNN should exhibit the\ntrait that each of these clusters is highly concentrated and ideally\ncollapses into a single probability distribution that closely resembles\nthe one-hot probability vector for that label.\nRemark 1. While we leverage the concept of CMI from Yang et al.\n(2025), the way it is calculated in our work significantly differs from\nhow it is calculated in Yang et al. (2025).\nIn Yang et al. (2025),\nI(X; ˆY |Y ) is calculated under the assumption of a Markov chain\nY →X →ˆY . In contrast, we quantify cluster compactness using\nI(X; ˆY α[Y ]|Y ), where ˆY α[Y ] explicitly depends on Y , violating the\nMarkov assumption.\nMoreover, minimizing I(X; ˆY α[Y ]|Y ) over all\npossible values of α introduces additional challenges, which we ad-\ndress in the next subsection.\n4.2\nFramework\nfor Minimizing CMI Values of Power Transformed Clusters\nTowards building an undistillable DNN, we now train a DNN f : x ∈\nRd →qx by jointly minimizing the CE loss and all CMI values of all\npower transformed clusters. Let\nα =\n\u0002\nα[1], α[2], . . . , α[C]\n\u0003\n,\nand write each qx as qx,θ. In our CMI minimized method, the objec-\ntive function we want to minimize is\nEXY\nh\nH(Y, qX,θ)\ni\n+ λ max\nα\nI(X; ˆY α[Y ]|Y ),\n(17)\n6\nPublished in Transactions on Machine Learning Research (06/2025)\nwhere λ > 0 is a hyper-parameter trading the CE loss with the maximum CMI, and the maximization over\nα is taken over the region 0 ≤α[i] ≤β, 1 ≤i ≤C. The optimization problem then becomes\nmin\nθ\nn\nEXY\nh\nH(Y, qX,θ)\ni\n+ λ max\nα\nI(X; ˆY α[Y ]|Y )\no\n= min\nθ\nn\nEXY\nh\nH(Y, qX,θ)\ni\n+ λ max\nα\nX\ny\nPY [y]I(X; ˆY α[y]|Y = y)\no\n(18)\n= min\nθ\nn\nEXY\nh\nH(Y, qX,θ)\ni\n+ λ\nX\ny\nPY [y] max\nα[y] I(X; ˆY α[y]|Y = y)\no\n.\n(19)\nIn order to get a better understanding about the behavior of the second term in the objective function of\nequation 18 w.r.t. α, we depict in Figure 1 I(X; ˆY α[Y ]|Y = y) vs α[y] for three randomly-selected classes\ny using a pre-trained ResNet-50 on CIFAR-100. In Figure 1, maxα I(X; ˆY α|Y = y) is achieved at a value\nof α which is between 0.25 and 0.75. In Theorem 2 of Appendix D, we further show that for each label y,\nI(X; ˆY α|Y = y) as a function of α is continuously differentiable.\nHowever, finding an algorithmic solution to the min-max problem in equation 18 to equation 19 is challenging.\nTo overcome this difficulty, we next develop a more tractable expression for maxα I(X; ˆY α|Y = y). At this\npoint, we invoke the following theorem, which will be proved in Appendix E.\nTheorem 1. For any label y,\nmax\nα\nI(X; ˆY α|Y = y) = lim\nω→∞\n1\nω ln 1\nβ\nZ β\n0\nexp {ωI(X; ˆY α|Y = y)}dα.\n(20)\nTherefore, when ω is large, maxα I(X; ˆY α|Y = y) can be approximated by\nmax\nα\nI(X; ˆY α|Y = y) ≈1\nω ln 1\nβ\nZ β\n0\nexp {ωI(X; ˆY α|Y = y)}dα\n(21)\n≈1\nω ln\n\"\n1\nN\nN\nX\ni=1\nexp {ωI(X; ˆY αi|Y = y)}\n#\n,\n(22)\nwhere N is relatively large, and αi = iβ/N.\nNow plugging equation 22 into equation 19, we have\nmin\nθ\nn\nEXY\nh\nH(Y, qX,θ)\ni\n+ λ\nω\nX\ny\nPY [y] ln\n\"\n1\nN\nN\nX\ni=1\nexp {ωI(X; ˆY αi|Y = y)}\n# o\n.\n(23)\nNote that the second term in the objective function of equation 23 is not amenable to parallel computation\nvia GPU due to the dependency of KL divergence on sy,αi, the centroid of the power transformed cluster\ncorresponding to Y = y with power αi. To get around this difficulty, we follow the approach in Yang et al.\n(2025) and introduce dummy distributions Qy,i ∈P([C]) for each (y, i) to rewrite I(X; ˆY αi|Y = y) as follows\nI(X; ˆY αi|Y = y) = EX|Y\n\u0002\nKL\n\u0010\nqαi\nX,θ, sy,αi\n\u0011\n| Y = y\n\u0003\n= min\nQy,i EX|Y\n\u0002\nKL\n\u0010\nqαi\nX,θ, Qy,i\n\u0011\n| Y = y\n\u0003\n,\n(24)\nwhere the minimum in the above is achieved when\nQy,i = sy,αi = EX|Y\nh\nqαi\nX,θ|Y = y\ni\n.\n(25)\n7\nPublished in Transactions on Machine Learning Research (06/2025)\nCombining equation 24 with equation 23, we are led to solve the double minimization problem 2\nmin\nθ\nn\nEXY\nh\nH(Y, qX,θ)\ni\n+ λ\nω\nX\ny\nPY [y] ln\n\"\n1\nN\nN\nX\ni=1\nexp {ω min\nQy,i EX|Y\n\u0002\nKL\n\u0010\nqαi\nX,θ, Qy,i\n\u0011\n| Y = y\n\u0003\n}\n# o\n(26)\n= min\nθ\nmin\n{Qy,i}y∈[C],i∈[N]\nn\nEXY\nh\nH(Y, qX,θ)\ni\n+ λ\nω\nX\ny\nPY [y] ln\n\"\n1\nN\nN\nX\ni=1\nexp {ωEX|Y\n\u0002\nKL\n\u0010\nqαi\nX,θ, Qy,i\n\u0011\n| Y = y\n\u0003\n}\n# o\n(27)\nWhen the distribution PX,Y is unknown, it can be approximated by its empirical distribution from a data\nsample (a training dataset or mini-batch thereof) D = {(xi, yi)}m\ni=1. The objective function in the double\nminimization equation 27 then becomes\nJD(θ, {Qy,i}y∈[C],i∈[N]) =\n1\n|D|\nX\n(x,y)∈D\nH(y, qx,θ)+\nλ\nω\nX\ny\n|Dy|\n|D| ln\n\n1\nN\nN\nX\ni=1\nexp { ω\n|Dy|\nX\nj∈Dy\nKL\n\u0010\nqαi\nXj,θ, Qy,i\n\u0011\n}\n\n.\n(28)\n4.3\nAlgorithm for Solving the Optimization in equation 27\nThe double minimization optimization problem in equation 27 naturally lends us an alternating algorithm\nthat optimizes θ and {Qy,i}y∈[C],i∈[N] alternatively to minimize the objective function in equation 27 or\nEquation (28), given the other is fixed.\nGiven {Qy,i}y∈[C],i∈[N], θ can be updated using the same first-order optimization method as in conventional\ndeep learning, such as stochastic gradient descent applied over mini-batches.\nFollowing Yang et al. (2025), given θ, for each class y, {Qy,i}i∈[N] can be updated according to equation 25\nin the following manner: (1) we randomly sample a mini-batch of samples |By| instances from the training\nset with ground truth label y; (2) {Qy,i}i∈[N] can be updated as\nQy,i =\nP\nx∈By qαi\nx,θ\n|By|\n∀i ∈[N].\n(29)\nThe proposed alternating algorithm for optimization problem equation 27 is summarized in Algorithm 1\n3. To simplify our notation, we use (·)t\nb to indicate parameters at the b-th batch updation during the t-th\nalternating iteration of the algorithm. We further write (·)t\nB as (·)t whenever needed, set (·)t\n0 = (·)t−1.\nRemark 2. The compactness of output clusters alone is insufficient to ensure an undistillable DNN. Undis-\ntillability is a significantly stronger property. For instance, LS can improve the compactness of the feature\nspace of a DNN, which may make the output probability of the DNN more compact (Müller et al., 2019).\nHowever, this compactness is not enough to make the DNN undistillable (see Section 5)\n5\nExperiments\nIn this section, we demonstrate the effectiveness of CMIM by comparing it with several state-of-the-art\nalternatives.\nSpecifically, we first report the accuracy that a knockoff student can achieve by deploying\n2In practice, solving the double minimization problem introduces only minor runtime overhead, as the inner optimization\nproblem has an analytic solution and can be parallelized efficiently on modern GPUs. To demonstrate the efficiency of CMIC,\nwe report the wall-clock training time and compare the computational overhead of CMIM and CE in Appendix J.\n3If the impact of the random mini-batch sampling and stochastic gradient descent is ignored, the alternating algorithm\nis guaranteed to converge in theory since given θ, the optimal {Qy,i}y∈[C],i∈[N] can be found analytically via equation 29,\nalthough it may not converge to a global minimum.\n8\nPublished in Transactions on Machine Learning Research (06/2025)\nAlgorithm 1: Conditional Mutual Information Minimized (CMIM) Method.\nInput: Training set T , mini-batches {Bb}b∈[B], number of epochs T, λ, β, ω, N\nInitialization: Initialize θ0 and Q0\ny,i y∈[C],i∈[N].\nfor t = 1 to T do\n[Sampling αi] Randomly select N samples {αi}i∈[N] from interval [0, β].\nfor b = 1 to B do\n[Updating Qy,i] For each class y, construct mini-batch {By}y∈[C]. Update Qt\ny,i, ∀y ∈[C]; ∀i ∈[N],\naccording to Equation (29).\n[Updating θ] Fix Qt\ny,i y∈[C],i∈[N]. Update θt\nb−1 to θt\nb by stochastic gradient descent over the objective\nfunction 28.\nend\nend\nOutput: Global model θT.\ndifferent logit-based KD (attack) methods in Section 5.1. In all the experiments, when testing the distillibality\nof the trained DNNs using the benchmark defense methods and CMIM, we compare the knockoff student’s\naccuracy (i) when it attempts to steal the IP of protected DNN using logit-based (attack) methods with (ii)\nwhen it trains its model using the LS. If the former outperforms the latter, we conclude that the knockoff\nmakes the underlying DNN distillable. Next, in Section 5.2, we report the classification accuracy of the\nprotected models trained by the different defense methods. Lastly, in Section 5.3, we visualize the output\ncluster of models trained by CMIM, CE and NT.\n5.1\nKnockoff Student Accuracy\n• Datasets: We conduct extensive experiments on three image classification dataset, namely CIFAR-100\n(Krizhevsky et al., 2012) TinyImageNet (Le & Yang, 2015) and ImageNet (Deng et al., 2009). For description\nof each dataset, please refer to Appendix F.\n• Models: To show the effectiveness of CMIM, we use different model architectural families for teacher and\nknockoff student models. To this end, we pick models from VGG family (Simonyan & Zisserman, 2015),\nResNet family (He et al., 2016) (shortened as RN), ShuffleNetV2 (Ma et al., 2018), shortened as SNV2, and\nMobilenetv2 (Sandler et al., 2018) shortened as MNV2. Particularly, we have conducted experiments on the\nfollowing (teacher-student) pairs for each dataset: (i) for CIFAR-100, we use four pairs {(VGG16-VGG11),\n(VGG16-SNV2), (RN50-VGG11), (RN50-RN18)}; (ii) for TinyImageNet, we use two pairs {(RN34-RN18),\n(RN50-SNV2)}; and for ImageNet we use two pairs {(RN34-RN18),(RN34-MNV2)}.\n• Defense benchmark methods: For comprehensive comparisons, we benchmark CMIM with seven\nrecently published defense methods: MAD (Orekondy et al., 2020), APGP (Cheng & Cheng, 2023), RSP\n(Lee et al., 2019), ST (Ma et al., 2022), NT (Ma et al., 2021), SNT (Wang et al., 2022), and LS4 (Müller\net al., 2019).\n• Logit-based KD (attack) methods: We use three logit-based KD methods that are primarily designed\nfor when the teacher-student models are in cooperating mode, namely KD (Hinton et al., 2015), DKD (Zhao\net al., 2022), DIST (Huang et al., 2022a); and four logit-based KD attacks methods that a knockoff student\ncan deploy to make the protected DNNs possibly distillable, namely MKD (Yang & Ye, 2024), HTC (Jandial\net al., 2022), AVG (Keser & Toreyin, 2023), Knockoff (Orekondy et al., 2019). We report all the training\nsetups, including all the hyper-parameters used for both defense and attack methods in Appendix G.1.\n• Results: The accuracy that a knockoff student can achieve using the various (defense-attack) combinations\nis summarized in Table 1. For accuracy variances, please refer to Appendix I. In the table, we use the notation\n“K-student” to denote a knockoff student. The numbers in the column titled “Best” represent the highest\n4Although LS is not a defense method per se, it is observed that the models trained by LS reduce the knockoff student’s\naccuracy. We discuss the rationale behind this in Appendix C.\n9\nPublished in Transactions on Machine Learning Research (06/2025)\nTable 1: Top-1 accuracy (%) of the knockoff student on CIFAR-100, TinyImageNet and ImageNet dataset\n(the results for CIFAR-100 and TinyImageNet are averaged over 3 runs). Green upward arrows (↑) and\nred downward arrows (↓) indicate whether the knockoff student was able to render the underlying DNN\ndistillable.\nCIFAR-100\nDefense\nModel\nK-student\nLS\nKD\nMKD\nDKD\nDIST\nHTC\nAVG\nKnockoff\nBest\nMAD\nVGG16\nVGG11\n71.94\n68.55 ↓\n72.08 ↑\n53.32 ↓\n69.21 ↓\n71.19 ↓\n70.03 ↓\n61.44 ↓\n72.08 ↑\nSNV2\n72.65\n72.50 ↓\n72.46 ↓\n7.64\n↓\n69.91 ↓\n71.37 ↓\n72.86 ↑\n70.87 ↓\n72.86 ↑\nRN50\nVGG11\n71.94\n72.00 ↑\n72.04 ↑\n54.29 ↓\n71.57 ↓\n70.76 ↓\n70.73 ↓\n61.73 ↓\n72.04 ↑\nRN18\n78.76\n77.76 ↓\n78.79 ↑\n43.73 ↓\n73.76 ↓\n77.89 ↓\n78.61 ↓\n73.92 ↓\n78.79 ↑\nAPGP\nVGG16\nVGG11\n71.94\n71.92 ↑\n72.27 ↑\n27.24 ↓\n69.25 ↓\n70.08 ↓\n72.01 ↓\n45.98 ↓\n72.27 ↑\nSNV2\n72.65\n73.10 ↑\n73.75 ↑\n12.52 ↓\n71.04 ↓\n71.66 ↓\n73.20 ↑\n9.48\n↓\n73.75 ↑\nRN50\nVGG11\n71.94\n71.91 ↓\n72.11 ↑\n9.74\n↓\n69.48 ↓\n71.36 ↓\n71.92 ↑\n34.71 ↓\n72.11 ↑\nRN18\n78.76\n78.04 ↓\n79.06 ↑\n62.71 ↓\n77.32 ↓\n77.82 ↓\n77.90 ↓\n2.57\n↓\n79.06 ↑\nRSP\nVGG16\nVGG11\n71.94\n71.42 ↓\n72.04 ↑\n70.22 ↓\n70.80 ↓\n70.40 ↓\n71.56 ↓\n31.04 ↓\n72.04 ↑\nSNV2\n72.65\n73.55 ↑\n72.95 ↑\n67.45 ↓\n72.19 ↓\n71.46 ↓\n72.27 ↓\n26.09 ↓\n73.55 ↑\nRN50\nVGG11\n71.94\n71.97 ↑\n72.01 ↑\n69.53 ↓\n72.18 ↑\n70.87 ↓\n70.85 ↓\n46.68 ↓\n72.18 ↑\nRN18\n78.76\n77.78 ↓\n77.79 ↓\n77.01 ↓\n78.88 ↑\n78.00 ↓\n78.13 ↓\n55.86 ↓\n78.88 ↑\nNT\nVGG16\nVGG11\n71.94\n71.40 ↓\n73.44 ↑\n71.47 ↓\n71.33 ↓\n70.77 ↓\n71.58 ↓\n63.56 ↓\n73.44 ↑\nSNV2\n72.65\n72.44 ↓\n72.70 ↑\n6.24\n↓\n72.04 ↓\n70.75 ↓\n72.83 ↑\n6.32\n↓\n72.83 ↑\nRN50\nVGG11\n71.94\n72.01 ↑\n72.03 ↑\n71.55 ↓\n71.88 ↓\n70.16 ↓\n71.94 ↓\n62.94 ↓\n72.03 ↑\nRN18\n78.76\n78.41 ↓\n78.92 ↑\n79.26 ↑\n78.99 ↑\n77.94 ↓\n78.33 ↓\n68.96 ↓\n79.26 ↑\nSNT\nVGG16\nVGG11\n71.94\n72.06 ↑\n72.28 ↑\n4.92\n↓\n71.98 ↑\n70.60 ↓\n71.63 ↓\n64.08 ↓\n72.06 ↑\nSNV2\n72.65\n72.94 ↑\n73.17 ↑\n72.78 ↑\n72.22 ↓\n71.22 ↓\n72.74 ↑\n6.22\n↓\n73.17 ↑\nRN50\nVGG11\n71.94\n72.02 ↑\n72.12 ↑\n72.32 ↑\n71.70 ↓\n70.66 ↓\n71.65 ↓\n62.94 ↓\n72.32 ↑\nRN18\n78.76\n78.25 ↓\n78.48 ↓\n78.82 ↑\n78.14 ↓\n78.45 ↓\n78.38 ↓\n67.71 ↓\n78.82 ↑\nST\nVGG16\nVGG11\n71.94\n72.09 ↑\n72.01 ↑\n71.63 ↓\n71.93 ↓\n71.16 ↓\n71.63 ↓\n63.32 ↓\n72.09 ↑\nSNV2\n72.65\n72.64 ↓\n72.67 ↑\n70.53 ↓\n72.24 ↓\n71.32 ↓\n72.42 ↓\n69.46 ↓\n72.67 ↑\nRN50\nVGG11\n71.94\n72.00 ↑\n72.13 ↑\n71.62 ↓\n71.76 ↓\n70.54 ↓\n71.73 ↓\n65.43 ↓\n72.13 ↑\nRN18\n78.76\n78.96 ↑\n79.02 ↑\n78.35 ↓\n78.31 ↓\n78.36 ↓\n78.81 ↑\n72.87 ↓\n79.02 ↑\nLS\nVGG16\nVGG11\n71.94\n71.90 ↓\n72.00 ↑\n71.57 ↓\n70.89 ↓\n70.66 ↓\n71.76 ↓\n63.49 ↓\n72.00 ↑\nSNV2\n72.65\n72.87 ↑\n73.52 ↑\n70.01 ↓\n71.49 ↓\n71.70 ↓\n73.01 ↑\n65.20 ↓\n73.52 ↑\nRN50\nVGG11\n71.94\n71.82 ↓\n71.99 ↑\n71.95 ↓\n70.77 ↓\n70.86 ↓\n71.88 ↓\n62.29 ↓\n71.99 ↑\nRN18\n78.76\n77.72 ↓\n77.82 ↓\n79.37 ↑\n78.33 ↓\n78.31 ↓\n77.91 ↓\n63.36 ↓\n79.37 ↑\nCMIM\nVGG16\nVGG11\n71.94\n71.87 ↓\n71.64 ↓\n71.56 ↓\n70.34 ↓\n71.71 ↓\n71.42 ↓\n66.89 ↓\n71.87 ↓\nSNV2\n72.65\n72.53 ↓\n71.44 ↓\n72.46 ↓\n71.45 ↓\n71.59 ↓\n71.94 ↓\n64.45 ↓\n72.53 ↓\nRN50\nVGG11\n71.94\n71.54 ↓\n71.34 ↓\n71.77 ↓\n71.86 ↓\n69.32 ↓\n71.70 ↓\n60.58 ↓\n71.86 ↓\nRN18\n78.76\n78.21 ↓\n78.16 ↓\n78.13 ↓\n77.56 ↓\n77.23 ↓\n78.64 ↓\n65.88 ↓\n78.64 ↓\nTinyImageNet\nRSP\nRN34\nRN18\n63.56\n63.54 ↓\n64.32 ↑\n64.01 ↑\n63.27 ↓\n63.54 ↓\n62.15 ↓\n55.43 ↓\n64.32 ↑\nRN50\nSNV2\n60.61\n60.18 ↓\n60.76 ↑\n56.26 ↓\n56.43 ↓\n60.96 ↑\n60.15 ↓\n54.01 ↓\n60.96 ↑\nST\nRN34\nRN18\n63.56\n63.96 ↑\n64.12 ↑\n63.25 ↓\n63.51 ↓\n63.49 ↓\n63.84 ↑\n57.42 ↓\n64.12 ↑\nRN50\nSNV2\n60.61\n61.23 ↑\n61.36 ↑\n60.43 ↓\n60.32 ↓\n60.22 ↓\n61.13 ↑\n55.84 ↓\n61.36 ↑\nNT\nRN34\nRN18\n63.56\n63.27 ↓\n64.49 ↑\n64.67 ↑\n63.43 ↓\n63.50 ↓\n64.43 ↑\n53.11 ↓\n64.67 ↑\nRN50\nSNV2\n60.61\n59.57 ↓\n61.55 ↑\n31.55 ↓\n60.03 ↓\n60.98 ↑\n60.31 ↓\n50.94 ↓\n61.55 ↑\nLS\nRN34\nRN18\n63.56\n63.74 ↑\n64.01 ↑\n64.23 ↑\n63.51 ↓\n64.20 ↑\n63.04↓\n57.43 ↓\n64.23 ↑\nRN50\nSNV2\n60.61\n60.32 ↓\n60.93 ↑\n60.74 ↑\n60.11 ↓\n60.46 ↓\n60.14 ↓\n52.96 ↓\n60.93 ↑\nCMIM\nRN34\nRN18\n63.53\n62.89 ↓\n63.15 ↓\n62.94 ↓\n63.28 ↓\n61.57 ↓\n62.96 ↓\n56.13 ↓\n63.28 ↓\nRN50\nSNV2\n60.61\n57.57 ↓\n59.32 ↓\n60.58 ↓\n59.41 ↓\n59.33 ↓\n60.42 ↓\n56.91 ↓\n60.58 ↓\nImageNet\nST\nRN34\nRN18\n70.89\n70.74 ↓\n71.02 ↑\n70.02 ↓\n69.94 ↓\n70.91 ↑\n71.00 ↑\n63.24 ↓\n71.02 ↑\nMNV2\n70.93\n71.03 ↑\n71.25 ↑\n69.32 ↓\n70.53 ↓\n70.69 ↓\n71.06 ↑\n54.53 ↓\n71.25 ↑\nCMIM\nRN34\nRN18\n70.89\n70.44 ↓\n70.69 ↓\n69.97 ↓\n70.59 ↓\n70.63 ↓\n70.53 ↓\n59.34 ↓\n70.69 ↓\nMNV2\n70.93\n70.21 ↓\n70.72 ↓\n69.97 ↓\n70.44 ↓\n70.86 ↓\n70.20 ↓\n55.24 ↓\n70.86 ↓\n10\nPublished in Transactions on Machine Learning Research (06/2025)\naccuracy obtained for each respective row, indicating the best possible performance a knockoff student can\nachieve using any of the listed distillation methods.\nAs observed in Table 1, regardless of whether the teacher-student architectures are the same or different,\nDNNs trained with CMIM remain undistillable across all distillation methods. This is in stark contrast to\nDNNs trained using other defense techniques, which can still be successfully distilled to a certain degree.\nThe results indicate that prior defense strategies do not offer complete resistance against knockoff students,\nwhereas CMIM effectively prevents distillation, making it significantly more robust in protecting model\nknowledge.\nWe also perform an ablation study on the hyperparameters of CMIM—namely β, N, and ω—as detailed in\nAppendix K. The key findings are as follows:\n• Effect of β: The accuracy of the knockoff student drops sharply when β ≥1, suggesting that\nlarge values may destabilize optimization or excessively penalize the CMI term. This highlights the\nimportance of carefully tuning β to effectively balance the cross-entropy and CMI objectives.\n• Effect of the number of samples N: Increasing the number of power samples N leads to a\nmonotonic decrease in the knockoff student’s accuracy. This indicates that moderate values of N\nare sufficient to capture the necessary diversity for robust CMI estimation.\n• Effect of the power coefficient ω: Setting ω > 30 can lead to numerical instability, resulting\nin NaNs due to excessive exponentiation.\nInterestingly, the worst knockoff student performance\nis observed at ω = 20 and ω = 30, suggesting that these settings best approximate the extreme\nconcentration behavior of ω →∞, thereby enhancing undistillability.\nAdditionally, we investigate CMIC’s impact on model calibration in Appendix M.\n5.2\nAccuracy of Protected Models\nIn this section, we report the top-1 accuracy of the protected models in Table 1 trained using the benchmark\ndefense methods with those trained by CMIM. The results are summarized in Tables 2 and 3. As observed,\nthe models trained by CMIM have the highest classification accuracy compared to the benchmark methods.\nThis is because for the models trained by CMIM, the clusters corresponding to the output probability of the\nDNNs are very concentrated, facilitating easier classification of samples from different classes.\nTable 2: Top-1 accuracy (%) of models trained by defense methods on CIFAR-100 and TinyImageNet. The\nbest and second best results are bolded and underlined, respectively.\nCIFAR100\nTinyImageNet\nModel\nCE\nMAD\nAPGP\nRSP\nST\nNT\nSNT\nLS\nCMIM\nModel\nCE\nRSP\nST\nNT\nLS\nCMIM\nVGG16\n73.75\n73.75\n73.84\n73.71\n73.75\n73.75\n72.59\n73.90\n73.84\nRN34\n65.39\n65.21\n65.39\n65.23\n65.45\n65.99\nRN50\n77.81\n77.81\n77.56\n77.63\n77.81\n77.31\n77.77\n78.45\n78.72\nRN50\n66.14\n65.91\n66.13\n66.06\n66.09\n66.93\nTable 3: Top-1 accuracy (%) of models trained by defense methods on ImageNet.\nImageNet\nModel\nCE\nST\nCMIM\nRN34\n73.31\n73.30\n73.69\nThe results in Table 2 motivate us to test the top-1 accuracy of additional models trained by CMIM and\ncompare them with those trained by CE loss (see Appendix H).\nIt is worth noting that the primary focus of this paper is not on increasing the accuracy of DNNs but on\ndeveloping a method to train undistillable DNNs. While many existing methods in the literature can enhance\na DNN’s accuracy, they do not address the critical challenge of making DNNs undistillable.\n11\nPublished in Transactions on Machine Learning Research (06/2025)\n(a) LS\n(b) Nasty teacher\n(c) CMIM\nFigure 2: Visualization of three projected probability clusters for ResNet-50 trained on CIFAR-100 using (a)\nLS, (b) NT, and (c) CMIM.\nOur approach is the first in the literature that effectively trains undistillable models robust against a wide\nrange of existing KD methods. The improvement in accuracy observed in our results is a by-product of\nour method and not its primary goal. This improvement arises from the unique properties of our approach\nrather than replicating the effects of label smoothing or similar techniques.\n5.3\nVisualizing the Output Clusters\nIn this subsection, we aim to visualize the output clusters for models trained using CE, NT, and CMIM.\nTo achieve this, we follow the visualization approach introduced by Yang et al. (2025). Specifically, we\nrandomly select three labels from the CIFAR-100 dataset. For each probability distribution corresponding\nto these three labels, we extract only the probabilities associated with these selected labels and normalize\nthem to form three-dimensional probability vectors. These vectors are then projected onto a two-dimensional\nsimplex, allowing us to visualize the clustering behavior of each model. By applying this transformation,\nwe obtain a clear representation of how the models distribute their probability mass across different output\ncategories.\nThe resulting simplexes for ResNet-50 models trained with LS, Nasty Teacher, and the CMIM framework are\nshown in Figure 6 5. To ensure a consistent comparison, we applied the same power transform α = 4 for all\nvisualizations. As observed, the clusters for the model trained with CMIM are highly concentrated near the\ncorners of the simplex, closely resembling one-hot vectors. This indicates that the output distributions are\nhighly compact, making it difficult for a knockoff student to surpass LS regularization when attempting to\ndistill knowledge from a CMIM-trained model. This increased compactness plays a crucial role in enhancing\nthe model’s resistance to knowledge distillation.\nLastly, we clarify the distinction between “highly concentrated output clusters” and “overly confident pre-\ndictions”. A highly concentrated output cluster does not necessarily imply that the model produces overly\nconfident predictions. This is because the clusters can be concentrated around points that are not close to\none-hot labels (the corners of the probability simplex). As a result, the model can have concentrated outputs\nwithout being overly confident. These are two separate concepts.\nTo illustrate this, we train an RN50 model on the CIFAR-100 dataset using three different methods: CMIM,\nCE, and LS. After training, we evaluate the model’s confidence by measuring the average entropy of its\noutput probability vectors on the test dataset. The results, presented in Table 4, indicate that the entropy\nfor CMIM is higher than that for CE, demonstrating that CMIM produces less confident output probabilities.\nAdditionally, our experiments in Table 1 further support the above-mentioned claim: the CMIM method\ngenerates more concentrated output clusters while also improving the model’s accuracy on the held-out test\ndataset compared to models trained with the conventional CE loss. This observation is consistent with the\n5In Appendix L, we present the visualization of three different projected probability clusters for ResNet-50 trained on\nCIFAR-100.\n12\nPublished in Transactions on Machine Learning Research (06/2025)\nTable 4: Entropy Value of the Models Trained by CMIM, CE and LS\nMethods\nCMIM\nCE\nLS\nEntropy\n0.102\n0.064\n0.152\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nα\n0.00\n0.02\n0.04\n0.06\nCMI\nCE\nMAD\nAPGP\nRSP\nNT\nSNT\nCMIM (ours)\n(a) ResNet-50\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nα\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nCMI\n(b) VGG16\nFigure 3: The CMI values for the models trained by different KD-resistance defence methods Vs. power\ntransform value α for (a) ResNet-50, and (b) VGG16 trained on CIFAR-100 dataset. Compared to all other\ndefense methods in the literature, CMIM effectively reduces peak CMI values under temperature scaling,\nwhich prevents the teacher model from being distilled by the knockoff student.\nfindings of Yang et al. (2025), which demonstrate that training DNNs to produce highly concentrated output\nclusters can lead to improved test accuracy.\n5.4\nWhy Prior Defense Methods Can be Made Distillable?\nIn this section, we address the question of why DNNs trained using all prior KD-resistance defense meth-\nods can still be made distillable, as demonstrated in Section 5.1. The key reason behind this lies in the\nfundamental characteristics of these defense methods and their susceptibility to distillation under certain\nconditions. Specifically, by appropriately tuning the power transform parameter α, the models trained using\nthese defense methods can attain a relatively high CMI value in comparison to our proposed method, CMIM\n(as illustrated in Figure 3). As though, compared to the CMIM model, all other defense methods provide\nmore information for the student to leverage in improving their own performance.\nThis suggests that, despite their initial resistance, the defense methods fail to enforce undistillability rigor-\nously across all distillation settings. When the correct α value is selected during the distillation process, a\nlogit-based KD approach can leverage this property to effectively distill knowledge from these supposedly\nresistant DNNs. Consequently, these models can still be exploited to produce distilled students that outper-\nform the baseline LS student, demonstrating that prior defense strategies do not provide robust protection\nagainst all KD methods.\n6\nConclusion and Future Directions\nIn this paper, from an information-theoretic perspective, we proposed a defence method against the threat\nposed by knockoff students attempting to steal the IP of pre-trained DNNs via logit-based KD methods. In\nparticular, we proposed to minimize the CMI of the protected DNN across different power transform hyper-\nparameter values α, while minimizing the conventional CE loss simultaneously. We referred to model trained\nby these framework as CMIM models. By conducting a series of experiments, we showed that, unlike the\nprior defense methods proposed in the literature, a knockoff student cannot render CMIM models distillable.\nIn addition, we showed that the models trained by CMIM achieve higher classification accuracy compared\nto those trained by CE loss.\n13\nPublished in Transactions on Machine Learning Research (06/2025)\nDespite these promising results, our work has certain limitations. First, the evaluation of CMIM models\nis primarily empirical, as providing a formal theoretical proof of undistillability remains an open challenge.\nSecond, our approach introduces additional computational overhead compared to the conventional training\nusing CE loss.\nFor future work, we aim to extend the CMIM method beyond standard classification tasks. Potential direc-\ntions include adapting it for multi-label classification, regression problems, and safeguarding the intellectual\nproperty of cutting-edge models such as LLMs, CLIP, and diffusion models. Another promising direction is\nto enhance the model’s undistillability by adapting CMIM for integration with a contrastive learning frame-\nwork, which regularizes the latent space rather than the output probability space. By broadening the scope\nof CMIM, we hope to further enhance its applicability and effectiveness across a wider range of machine\nlearning paradigms.\nReferences\nZeyuan Allen-Zhu and Yuanzhi Li.\nTowards understanding ensemble, knowledge distillation and self-\ndistillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.\nJure An, Doetsch Peter, and Pylvänäinen Thomas. Relation knowledge distillation. In International Con-\nference on Learning Representations, 2021.\nRohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E\nHinton.\nLarge scale distributed neural network training through online distillation.\narXiv preprint\narXiv:1804.03235, 2018.\nKenneth Borup and Lars N Andersen. Even your teacher needs guidance: Ground-truth targets dampen\nregularization imposed by self-distillation. Advances in Neural Information Processing Systems, 34:5316–\n5327, 2021.\nCristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the\n12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 535–541, 2006.\nAnda Cheng and Jian Cheng. Apgp: Accuracy-preserving generative perturbation for defending against\nmodel cloning attacks. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 1–5, 2023. doi: 10.1109/ICASSP49357.2023.10094956.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee,\n2009.\nKangning Guo, Hu Shengyuan, Yan Junjie, Liu Xin, Xu Dongbao, and Wang Ningbo. Logit-like knowledge\ndistillation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 10186–10193,\n2021.\nShayan Mohajer Hamidi, Xizhen Deng, Renhao Tan, Linfeng Ye, and Ahmed Hussein Salamah. How to\ntrain the teacher model for effective knowledge distillation. In European Conference on Computer Vision,\npp. 1–18. Springer, 2024.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi:\n10.1109/CVPR.2016.90.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015. URL\nhttps://arxiv.org/abs/1503.02531.\nTao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu.\nKnowledge distillation from a stronger\nteacher. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in\nNeural Information Processing Systems, 2022a. URL https://openreview.net/forum?id=157Usp_kbi.\n14\nPublished in Transactions on Machine Learning Research (06/2025)\nTao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Knowledge distillation from a stronger teacher.\nAdvances in Neural Information Processing Systems, 35:33716–33727, 2022b.\nSurgan Jandial, Yash Khasbage, Arghya Pal, Vineeth N. Balasubramanian, and Balaji Krishnamurthy.\nDistilling the undistillable: Learning from a nasty teacher. In Shai Avidan, Gabriel Brostow, Moustapha\nCissé, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision – ECCV 2022, pp. 587–603,\nCham, 2022. Springer Nature Switzerland. ISBN 978-3-031-19778-9.\nReyhan Kevser Keser and Behcet Ugur Toreyin. Averager student: Distillation from undistillable teacher,\n2023. URL https://openreview.net/forum?id=4isz71_aZN.\nDiederik P Kingma and Jimmy Ba.\nAdam:\nA method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014.\nAlex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).\nUniversity of Toronto, 2012. URL http://www.cs.toronto.edu/~kriz/cifar.html.\nYa Le and Xuan S. Yang.\nTiny imagenet visual recognition challenge.\n2015.\nURL https://api.\nsemanticscholar.org/CorpusID:16664790.\nYann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller.\nEfficient backprop.\nIn Neural\nnetworks: Tricks of the trade, pp. 9–50. Springer, 2002.\nTaesung Lee, Benjamin Edwards, Ian Molloy, and Dong Su. Defending against neural network model stealing\nattacks using deceptive perturbations. In 2019 IEEE Security and Privacy Workshops (SPW), pp. 43–49,\n2019. doi: 10.1109/SPW.2019.00020.\nHaoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang Wang. Undistillable:\nMaking a nasty teacher that cannot teach students. In International Conference on Learning Representa-\ntions, 2021. URL https://openreview.net/forum?id=0zvfm-nZqQs.\nHaoyu Ma, Yifan Huang, Tianlong Chen, Hao Tang, Chenyu You, Zhangyang Wang, and Xiaohui Xie.\nStingy teacher: Sparse logits suffice to fail knowledge distillation, 2022. URL https://openreview.net/\nforum?id=ae7BJIOxkxH.\nNingning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient\ncnn architecture design.\nIn Proceedings of the European conference on computer vision (ECCV), pp.\n116–131, 2018.\nAditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar. A statistical\nperspective on distillation. In International Conference on Machine Learning, pp. 7632–7642. PMLR,\n2021.\nHossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in hilbert\nspace. Advances in Neural Information Processing Systems, 33:3351–3361, 2020.\nDaniel Moldovan, Ionescu Bogdan, Drimbă Alexandru, and Marius Popescu. Path-kg: Knowledge distillation\nwith path-level guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npp. 1709–1718, 2019.\nRafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help?\nAdvances in\nneural information processing systems, 32, 2019.\nTribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-box\nmodels.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\n4954–4963, 2019.\nTribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses against\ndnn model stealing attacks. In International Conference on Learning Representations, 2020. URL https:\n//openreview.net/forum?id=SyevYxHtDB.\n15\nPublished in Transactions on Machine Learning Research (06/2025)\nWonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 3967–3976, 2019.\nGabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing neural\nnetworks by penalizing confident output distributions, 2017. URL https://openreview.net/forum?id=\nHkCjNI5ex.\nMary Phuong and Christoph Lampert.\nTowards understanding knowledge distillation.\nIn International\nconference on machine learning, pp. 5142–5151. PMLR, 2019.\nRadim Rehurek and Petr Sojka. Gensim–python framework for vector space modelling. NLP Centre, Faculty\nof Informatics, Masaryk University, Brno, Czech Republic, 3(2), 2011.\nHerbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathematical\nStatistics, 22(3):400 – 407, 1951. doi: 10.1214/aoms/1177729586. URL https://doi.org/10.1214/aoms/\n1177729586.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\nAhmed H Salamah, Shayan Mohajer Hamidi, and En-Hui Yang. A coded knowledge distillation framework\nfor image classification based on adaptive jpeg encoding. Pattern Recognition, 158:110966, 2025.\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:\nInverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 4510–4520, 2018.\nReza Shokri and Vitaly Shmatikov.\nPrivacy-preserving deep learning.\nIn 2015 53rd Annual Aller-\nton Conference on Communication, Control, and Computing (Allerton), pp. 909–910, 2015.\ndoi:\n10.1109/ALLERTON.2015.7447103.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\nIn International Conference on Learning Representations, 2015.\nZi Wang, Chengcheng Li, and Husheng Li. Adversarial training of anti-distilled neural network with semantic\nregulation of class confidence. In 2022 IEEE International Conference on Image Processing (ICIP), pp.\n3576–3580, 2022. doi: 10.1109/ICIP46576.2022.9897169.\nEn-hui Yang and Linfeng Ye. Markov knowledge distillation: Make nasty teachers trained by self-undermining\nknowledge distillation fully distillable. In European Conference on Computer Vision. Springer, 2024.\nEn-hui Yang, Shayan Mohajer Hamidi, Linfeng Ye, Renhao Tan, and Beverly Yang. Conditional mutual\ninformation constrained deep learning: Framework and preliminary results. In 2024 IEEE International\nSymposium on Information Theory (ISIT), pp. 569–574, 2024. doi: 10.1109/ISIT57864.2024.10619241.\nEn-Hui Yang, Shayan Mohajer Hamidi, Linfeng Ye, Renhao Tan, and Beverly Yang. Conditional mutual\ninformation constrained deep learning for classification.\nIEEE Transactions on Neural Networks and\nLearning Systems, pp. 1–13, 2025. doi: 10.1109/TNNLS.2025.3540014.\nLinfeng Ye, Shayan Mohajer Hamidi, Renhao Tan, and En-hui Yang. Bayes conditional distribution esti-\nmation for knowledge distillation based on conditional mutual information. In The Twelfth International\nConference on Learning Representations, 2024. URL https://openreview.net/forum?id=yV6wwEbtkR.\nBorui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. Decoupled knowledge distillation. In\nProceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pp. 11953–11962,\n2022.\nKaixiang Zheng and ENHUI YANG. Knowledge distillation based on tranformed teacher matching. In The\nTwelfth Interna2ional Conference on Learning Representations, 2024. URL https://openreview.net/\nforum?id=MJ74K7uDGGl.\n16\nPublished in Transactions on Machine Learning Research (06/2025)\nA\nAppendix\nB\nA Thorough Study of Related Works\nB.1\nLogit-based KD methods\nKnowledge Distillation (KD) has become a cornerstone technique for compressing large teacher models into\nsmaller student models. This section reviews key logit-based KD methods and explores their advancements.\nHinton et al. (2015) introduced the foundational concept of KD, using KL divergence to match the student’s\nsoftmax outputs to the teacher’s. This work laid the groundwork for logit-based methods, as the softmax\noutput directly relates to the logits.\nLater, Moldovan et al. (2019) proposed Path-KD, a method that\nutilizes the paths leading to the correct class in both teacher and student models for distillation. While not\ndirectly logit-based, it demonstrates the effectiveness of aligning decision-making processes. Guo et al. (2021)\nproposed Logit-Like Distillation, addressing the capacity gap by matching the ranking of logits instead of their\nexact values [4]. This approach allows the student to learn the essential ordering of classes even with limited\ncapacity. An et al. (2021) proposed relation knowledge distillation (RKD), focusing on aligning relationships\nbetween class logits rather than individual values. This approach improves the student’s ability to generalize\nto unseen data. Zhao et al. (2022) introduced decoupled knowledge distillation (DKD), where it decouples\nthe classical KD loss into two parts: target class knowledge distillation and non-target class knowledge\ndistillation. Huang et al. (2022b) proposed DIST where they designed a KD method to distill better from\na stronger teacher; indeed they claim that preserving the relations between the predictions of teacher and\nstudent would suffice for an effective KD. Borup & Andersen (2021) provided theoretical arguments for the\nimportance of weighting the teacher outputs with the ground-truth targets when performing self-distillation\nwith kernel ridge regressions along with a closed form solution for the optimal weighting parameter. Hamidi\net al. (2024) propose to train the teacher model using mean square loss instead of the conventional cross\nentropy loss. Salamah et al. (2025) proposed a KD framework that leverages adaptive JPEG encoding to\nmake the teacher’s output responses less peaky for image classification tasks.\nB.2\nDefense methods against Logit-based KD\nAs also discussed in Section 2, the defense methods against the threat posed by knockoff students attempting\nto steal the IP of pre-trained DNNs via logit-based KD methods can be categorized into two approaches.\nHere, we elaborate on these two approaches.\n(I) Model stealing resistant training: In this approach, DNNs are trained to reduce the accuracy of\nknockoff students while maintaining the original classification accuracy of the model. In particular, Ma\net al. (2021) proposed a training algorithm named self-undermining KD to create nasty teachers (NT) that\nprevent knowledge leakage and unauthorized model stealing through KD, without compromising model\naccuracy. The nasty teacher is trained by minimizing the following objective function:\nLNT = H(y, qx) −ϵ KL(˜qx, qx),\n(30)\nwhere ˜qx is a output of a pre-trained standard model.\nSubsequently, Wang et al. (2022) proposed semantic nasty teachers (SNT) which improve the model stealing\nresistance of NT by disentangling semantic relationships in the output logits during teacher model training,\nwhich is crucial for successful KD.\n(II) post-training defence methods: The aim of these approaches is to deceive the knockoff by imposing\nminimal perturbations to the model’s predictions. Lee et al. (2019) tested a variety of possible perturbation\nforms, and found that the reverse sigmoid perturbation (RSP) to be the most effective one. Orekondy et al.\n(2020) introduced maximizing angular deviation (MAD), a technique that perturbs the output probabilities,\nleading to an adversarial gradient signal that deviates significantly from the original gradient of the knockoff.\nTo this end, they applied a randomly initialized model as the surrogate for the potential knockoff. More\nrecently, Cheng & Cheng (2023) proposed a plug-and-play generative perturbation model, dubbed as accuracy\n17\nPublished in Transactions on Machine Learning Research (06/2025)\npreserving generative perturbation (APGP), which can effectively defend KD-based model cloning, while\npreserve the model utility.\nB.3\nAttack Methods Using Logit-based KD\nJandial et al. (2022) sought to circumvent the defense of nasty teachers and steal (or extract) its information.\nSpecifically, they analyzed nasty teacher from two different angles and subsequently leverage them carefully\nto develop simple yet efficient methodologies, named as HTC and SCM, which enhance learning from nasty\nteacher.\nIn AVG (Keser & Toreyin, 2023), the authors noted that undistillable teachers exhibit multiple peaks in\ntheir softmax response, which are transferred to the student models. These peaks are considered to be the\nprimary factor that misleads the student models. To mitigate the influence of the multiple peaks in the\nsoftmax response of teachers, they proposed transferring the mean of features with the same labels as the\nsoft labels.\nOrekondy et al. (2019) introduced a technique called \"Knockoff Nets\" that allows an attacker to steal the\nfunctionality of black-box models.\nRemarkably, the attacker only needs to interact with the model by\nfeeding it input data and observing the resulting predictions. By training a new model (\"knockoff\") on these\ninput-prediction pairs, the attacker can create a copycat model that performs similarly to the original black\nbox.\nC\nWhy LS reduce the knockoff student’s accuracy?\nOriginal aiming to prevent overfitting and improve generalization, label smoothing was observed by Müller\net al. (2019) to reduce the accuracy of the knockoff student. The researchers found that \"label smoothing\nencourages examples to lie in tight, equally separated clusters\". Consequently, label smoothing reduces the\ncontextual information in the teacher model’s output (Yang et al., 2024).\nD\nPower Transformation of Mutual Information\nThe following theorem implies that for each label y, I(X; ˆY α|Y = y) as a function of α is continuously\ndifferentiable.\nTheorem 2. Let (X, Z) be a pair of random variables, where Z is discrete, and X can be either discrete or\ncontinuous. Let PZ|X[·|x] denote the conditional probability distribution of Z given X = x. Additionally, let\nP α\nZ|X[·|x] denote the power transformed version of PZ|X[·|x] with power α, Zα denote the random variable\nthe conditional distribution of which given X = x is P α\nZ|X[·|x], and qα denote the probability distribution of\nZα. Then, the following holds\n∂I(X; Zα)\n∂α\n= 1\nα\nX\nx\nPX[x]\nn\u0000m2(P α\nZ|X[·|x]) −m2\n1(P α\nZ|X[·|x])\n\u0001\n−Cov(P α\nZ|X[·|x], qα)\no\n,\n(31)\nwhere for probability vectors P and Q,\nm1(P)\n∆=\nX\nj\nP[j]\n\u0000−ln(P[j])\n\u0001\n= H(P),\n(Shannon entropy)\n(32a)\nm2(P)\n∆=\nX\nj\nP[j]\n\u0000−ln(P[j])\n\u00012,\n(Second moment)\n(32b)\nCov(P, Q)\n∆=\nX\nj\nP[j]\n\u0010\n−ln(P[j]) −m1(P)\n\u0011\u0010\n−ln(Q[j]) −\nX\ni\nP[i](−ln(Q[i]))\n\u0011\n.\n(32c)\n18\nPublished in Transactions on Machine Learning Research (06/2025)\nProof. To simplify our notation, we denote the conditional distributions PZ|X[·|x] and P α\nZ|X[j|x] by px and\npα\nx, respectively. Decompose I(X; Zα) as follows\nI(X; Zα) = H(Zα) −H(Zα|X)\n= H(qα) −\nX\nx\nP[x]H(pα\nx)\n(33)\nwhere for any random variables U and V , H(V ) and H(V |U) are the entropy of V and the conditional\nentropy of V given U, respectively, and H(p) denotes the entropy of the probability distribution p. Then\nthe partial derivative of I(X; Zα) w.r.t. α is equal to\n∂I(X; Zα)\n∂α\n= ∂H(qα)\n∂α\n−\nX\nx\nP[x]∂H(pα\nx)\n∂α\n.\n(34)\nTo continue, we first compute the partial derivative in the second term of the RHS of equation 34\n∂H(pα\nx)\n∂α\n=\n−∂P\nj pα\nx[j] ln(pα\nx[j])\n∂α\n= −\nX\nj\n\u0010\nln(pα\nx[j]) + 1\n\u0011∂pα\nx[j]\n∂α\n= −\nX\nj\n\u0010\nln(pα\nx[j]) + 1\n\u0011\n×\n(px[j])α ln(px[j])\n\u0010 P\ni(px[i])α\u0011\n−(px[j])α\u0010 P\ni(px[i])α ln px[i]\n\u0011\n\u0010 P\ni(px[i])α\n\u00112\n= −\nX\nj\n\u0010\nln(pα\nx[j]) + 1\n\u0011\u0010\npα\nx[j]\u0000ln(px[j]) −\nX\ni\npα\nx[i] ln(px[i])\u0001\u0011\n= −1\nα\nX\nj\n\u0010\nln(pα\nx[j]) + 1\n\u0011\npα\nx[j]\n\u0010\nln(px[j])α −\nX\ni\npα\nx[i|] ln(px[i])α\u0011\n(35)\n= −1\nα\nX\nj\n\u0010\nln(pα\nx[j]) + 1\n\u0011\npα\nx[j]\n\u0010\nln(pα\nx[j]) −\nX\ni\npα\nx[i] ln(pα\nx[i])\n\u0011\n= −1\nα\n X\nj\npα\nx[j]\u0000ln(pα\nx[j])\u00012 −\n\u0010 X\nj\npα\nx[j] ln(pα\nx[j])\n\u0011\u0010 X\ni\npα\nx[i] ln(pα\nx[i])\n\u0011!\n= −1\nα\n\u0010\nm2(pα\nx) −m2\n1(pα\nx)\n\u0011\n(36)\nNote that\nqα =\nX\nx\nP[x]pα\nx.\n19\nPublished in Transactions on Machine Learning Research (06/2025)\nThen we have\n∂H(qα)\n∂α\n=\n−∂P\nj qα[j] ln(qα[j])\n∂α\n= −\nX\nj\n\u0010\nln(qα[j]) + 1\n\u0011∂qα[j]\n∂α\n= −\nX\nj\n\u0010\nln(qα[j]) + 1\n\u0011 X\nx\nP[x]∂pα\nx[j]\n∂α\n= −1\nα\nX\nj\n\u0010\nln(qα[j]) + 1\n\u0011 X\nx\nP[x]pα\nx[j]\n\u0010\nln(pα\nx[j]) + m1(pα\nx)\n\u0011\n(37)\n= −1\nα\nX\nj\n\u0010\nln(qα[j])\n\u0011 X\nx\nP[x]pα\nx[j]\n\u0010\nln(pα\nx[j]) + m1(pα\nx)\n\u0011\n= −1\nα\nX\nx\nP[x]\nX\nj\n\u0010\nln(qα[j])\n\u0011\npα\nx[j]\n\u0010\nln(pα\nx[j]) + m1(pα\nx)\n\u0011\n= −1\nα\nX\nx\nP[x]\n X\nj\npα\nx[j] ln(pα\nx[j])\n\u0010\nln(qα[j])\n\u0011\n−m1(pα\nx)\nX\nj\npα\nx[j]\n\u0010\n−ln(qα[j])\n\u0011!\n= −1\nα\nX\nx\nP[x] Cov (pα\nx, qα)\n(38)\nwhere equation 37 is due to equation 35.\nFrom Equations (36) and (38), Theorem 2 follows.\nE\nProof of Theorem 1\nTheorem 1 follows from Theorem 2 and the following lemma.\nLemma 1. Let g(t) be a continuously differentiable function over [0, β]. Then the following holds:\nmax\nt\ng(t) = lim\nω→∞\n1\nω ln 1\nβ\nZ β\n0\nexp {ωg(t)}dt.\n(39)\nProof. Let t∗be an optimal point at which\ng(t∗) = max\nt\ng(t).\nFor any ϵ > 0, let N(t∗, ϵ) denote a closed interval containing t∗with length ϵ. It is easy to verify that\n1\nω ln 1\nβ\nZ β\n0\nexp {ωg(t)}dt ≤g(t∗)\nwhich implies that\nlim sup\nω→∞\n1\nω ln 1\nβ\nZ β\n0\nexp {ωg(t)}dt ≤g(t∗).\n(40)\nOn the other hand,\n1\nω ln 1\nβ\nZ β\n0\nexp {ωg(t)}dt ≥1\nω ln 1\nβ\nZ\nN(t∗,ϵ)\nexp {ωg(t)}dt\n≥1\nω ln ϵ\nβ exp {ω\nmin\nt∈N(t∗,ϵ) g(t)}\n=\nmin\nt∈N(t∗,ϵ) g(t) + 1\nω ln ϵ\nβ .\n(41)\n20\nPublished in Transactions on Machine Learning Research (06/2025)\nLetting ω →∞in equation 41 yields\nlim inf\nω→∞\n1\nω ln 1\nβ\nZ β\n0\nexp {ωg(t)}dt ≥\nmin\nt∈N(t∗,ϵ) g(t).\n(42)\nNote that equation 42 is valid for any ϵ > 0. Letting ϵ →0 in equation 42, we have\nlim inf\nω→∞\n1\nω ln 1\nβ\nZ β\n0\nexp {ωg(t)}dt ≥g(t∗).\n(43)\nThen equation 39 follows from equation 40 and equation 43. This completes the proof of Lemma 1.\nF\nDatasets description\n• CIFAR-100 (Krizhevsky et al., 2012) dataset contains 50K training and 10K test color images, each\nwith size 32 × 32, categorized into 100 classes.\n• TinyImageNet (Le & Yang, 2015) contains 120K color images across 200 classes, each with a reso-\nlution of 64 × 64 pixels. For each class, there are 500 training images, 50 validation images and 50\ntest images.\n• ImageNet (Deng et al., 2009) is a large-scale dataset used in visual recognition tasks, containing\naround 1.2 million training and 50K validation images.\nG\nExperiments setup\nAll experiments detailed in this paper were conducted using a publicly available national high-performance\ncomputer. For each experiment, we utilized 16 CPU cores, 64 GB of memory, and one NVIDIA V100 GPU.\nThe software environment comprised Python 3.10, PyTorch 1.13, and CUDA 11.\nFor all experiments, including defenses and attacks, the SGD optimizer (Robbins & Monro, 1951; LeCun\net al., 2002) with a learning rate of 0.1 is used unless otherwise specified.\nFor the CIFAR-100 and TinyImageNet datasets, we train the model for 200 epochs, decaying the learning\nrate by 0.1 at epochs 60, 120, 160.\nFor ImageNet, we follow the standard PyTorch practice 6.\nThe batch size is 128 for both CIFAR-100 and TinyImageNet, and 256 for ImageNet.\nTo get the accuracy that a knockoff student can achieve using label smoothing, we have tested a wide spectrum\nof label smoothing factor ϵ = {0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}, and selected the\nvalue that yielded a classification accuracy exceeding that of all knockoff students.\nIn the CMIM method, we set T = 20 and tested λ = 0.1, 0.25, 0.5, 1, selecting the value that minimized the\nCMI value while maintaining or improving classification accuracy.\nG.1\nDefense setup\nWe used the following parameters and settings for the defense models used in Section 5.\nG.1.1\nDefense setup on CIFAR-100 and TinyImagenet\n1. MAD: We employ a randomly initialized VGG-8 as adversary’s architecture, and following the\nimplementation of MAD-argmax.\n2. APGP: We apply a 3 layer MLP with residual connection as the generative model and set λ = 0.1\nfor all experiments.\n6https://github.com/pytorch/vision/tree/main/references/classification\n21\nPublished in Transactions on Machine Learning Research (06/2025)\n3. RSP: We use α = 1 and β = 20 for all the experiments.\n4. NT: To ensure a acceptable accuracy sacrifice, we test three different ϵ values and select the largest\none that results in an accuracy drop of less than 0.5%. Specifically, we use ϵ = 0.01 for ResNet-50\nand ϵ = 0.005 for VGG-16 on the CIFAR-100 dataset, while for TinyImageNet, we use ϵ = 0.001 for\nboth ResNet-34 and ResNet-50.\n5. SNT: We use the pretrained word2vect model namely \"fasttext-wiki-news-subwords-300\" provided\nby Gensim (Rehurek & Sojka, 2011), and set λ = 0.2 for all experiments.\n6. ST: We use the teacher model trained by CE as the underlying model, and use the sparse ratio of\n10% as suggested in their paper for all experiments.\n7. LS: We apply label smoothing factor 0f 0.05 for all experiments.\n8. ISTM: We set the binary search parameters to Tb = 20 and αmax = 2000. We use λ = 0.2 for\nResNet-50 and λ = 0.5 for VGG-16 on the CIFAR-100 dataset, while for TinyImageNet, we use\nλ = 0.1 for ResNet-34 and λ = 0.5 ResNet-50.\nG.1.2\nDefense setup on Imagenet\n1. ST: We use the teacher model trained by CE as the underlying model, and use the sparse ratio of\n10% as suggested in their paper for all experiments.\n2. ISTM: We set the binary search parameters to Tb = 20 and αmax = 2000. We use λ = 0.2 for all\nthe experiments.\nG.2\nAttack setup\nG.2.1\nAttack setup on CIFAR-100 and TinyImagenet\nWe use power transform parameter α = 0.25 (or equivalently T = 4) for all experiments unless otherwise\nspecified.\n1. KD: We set the CE-KL trade-off coefficient to λ = 0.9.\n2. MKD: We use the intrinsic dimension of 3 for CIFAR-100, and 5 for TinyImageNet.\nWe em-\nployed the Adam optimizer (Kingma & Ba, 2014) with learning rate 10−3 for the trainable Markov\ntransform.\n3. DKD: We test alpha, beta pairs of {1, 4} and {2, 8}, and report the one with best accuracy.\n4. DIST: We useβ = 1.0, γ = 1.0, τ = 1.0 for all experiments.\n5. HTC: We use α = 0.05(T = 20), λ = 0.01 for all experiments.\n6. AVG: λ = 0.9.\n7. Knockoff: We follow the implementation of the original paper.\nG.2.2\nAttack setup on Imagenet\nWe use α = 1 (T = 1) for all experiments unless otherwise specified.\n1. KD: λ = 0.9.\n2. MKD: We use the intrinsic dimension of 16. We employed the Adam optimizer (Kingma & Ba,\n2014) with learning rate 10−3 for the trainable Markov transform.\n3. DKD: We test alpha, beta pairs of {1, 4} and {2, 8}, and report the one with best accuracy.\n22\nPublished in Transactions on Machine Learning Research (06/2025)\nH\nAccuracy of Protected models\nIn this section, we report the top-1 accuracy of some additional models that are trained using CMIM and\ncompare them with those trained by CE method. To this end, we use 10 well-known models for CIFAR-\n100 dataset namely ResNet (RN)-{18, 34, 50, 101, 152}, SqueezeNet (SQN), ResNext (RNXT) 50, MobileNet\n(MN), Xception (XCP), DenseNet (DN) 121; and 2 models namely RN-{34, 50} for TinyImageNet and\nImageNet. We follow the same training recipe as the one in Section 5.1. The results for CIFAR-100 and\n(Tiny-)ImageNet are listed in Table 5 and Table 6, respectively. As seen, the top-1 accuracy for all models\ntrained by CMIM is consistently higher than those trained by CE counterpart, with the gain up to 1.15%.\nTable 5: Top-1 accuracy (%) of models trained by CE and CMIM methods on CIFAR-100.\nCIFAR-100\nModel\nCE\nCMIM\nModel\nCE\nCMIM\nRN18\n76.05\n77.20\nSQN\n69.32\n70.64\nRN34\n77.20\n77.54\nRNXT50\n78.71\n79.12\nRN50\n77.81\n77.93\nMN\n67.26\n67.51\nRN101\n79.07\n79.12\nXCP\n77.37\n77.64\nRN152\n79.21\n79.43\nDN121\n79.16\n79.33\nTable 6: Top-1 accuracy (%) of models trained by CE and CMIM methods on TinyImageNet and ImageNet.\nTinyImageNet\nImageNet\nModel\nCE\nCMIM\nModel\nCE\nCMIM\nRN34\n65.39\n65.99\nRN34\n73.31\n73.69\nRN50\n66.14\n66.93\nRN50\n76.15\n76.40\n23\nPublished in Transactions on Machine Learning Research (06/2025)\nI\nVariance of Table 1\nTable 7: Top-1 accuracy (%) and variance of the knockoff student on CIFAR-100 and TinyImageNet dataset\n(averaged over 3 runs)\nCIFAR-100\nDefense\nModel\nK-student\nLS\nKD\nMKD\nDKD\nDIST\nHTC\nAVG\nKnockoff\nMAD\nVGG16\nVGG11\n71.94±0.09\n68.55±0.20\n72.08±0.29\n53.32±0.38\n69.21±0.09\n71.19±0.03\n70.03±0.07\n61.44±0.06\nSNV2\n72.65±0.18\n72.50±0.11\n72.46±0.13\n7.64\n± 0.70\n69.91 ± 0.18\n71.37±0.24\n72.86±0.18\n70.87±0.26\nRN50\nVGG11\n71.94±0.09\n72.00±0.21\n72.04±0.13\n54.29±0.52\n71.57±0.31\n70.76±0.20\n70.73±0.22\n61.73±0.23\nRN18\n78.76±0.08\n77.76±0.23\n78.79±0.23\n43.73±0.55\n73.76±0.10\n77.89±0.25\n78.61±0.15\n73.92±0.19\nAPGP\nVGG16\nVGG11\n71.94±0.09\n71.92±0.19\n72.27±0.21\n27.24±0.54\n69.25±0.14\n70.08±0.23\n72.01±0.20\n45.98±0.45\nSNV2\n72.65±0.18\n73.10±0.27\n73.75±0.23\n12.52±0.30\n71.04±0.31\n71.66±0.13\n73.20±0.27\n9.48\n± 0.73\nRN50\nVGG11\n71.94±0.09\n71.91 ± 0.17\n72.11 ± 0.23\n9.74\n± 0.86\n69.48 ± 0.11\n71.38 ± 0.25\n71.92 ± 0.14\n34.71 ± 0.30\nRN18\n78.76 ± 0.08\n78.04 ± 0.21\n79.06 ± 0.14\n62.71 ± 0.29\n77.32 ± 0.13\n77.82 ± 0.13\n77.90 ± 0.15\n2.57\n± 0.95\nRSP\nVGG16\nVGG11\n71.94 ± 0.09\n71.42 ± 0.24\n72.04 ± 0.13\n70.22 ± 0.19\n70.80 ± 0.17\n70.40 ± 0.17\n71.56 ± 0.06\n31.04 ± 0.62\nSNV2\n72.65 ± 0.18\n73.55 ± 0.34\n72.95 ± 0.36\n67.45 ± 0.20\n72.19 ± 0.44\n71.46 ± 0.41\n72.27 ± 0.27\n26.09 ± 0.40\nRN50\nVGG11\n71.94 ± 0.09\n71.97 ± 0.17\n72.01 ± 0.20\n69.53 ± 0.12\n72.18 ± 0.21\n70.87 ± 0.14\n70.85 ± 0.17\n46.68 ± 0.60\nRN18\n78.76 ± 0.08\n77.78 ± 0.09\n77.79 ± 0.16\n77.01 ± 0.09\n78.88 ± 0.21\n78.00 ± 0.26\n78.13 ± 0.12\n55.86 ± 0.18\nNT\nVGG16\nVGG11\n71.94 ± 0.09\n71.40 ± 0.34\n73.44 ± 0.16\n71.47 ± 0.14\n71.33 ± 0.18\n70.77 ± 0.23\n71.58 ± 0.09\n63.56 ± 0.16\nSNV2\n72.65 ± 0.18\n72.44 ± 0.43\n72.70 ± 0.35\n6.24\n± 0.51\n72.04 ± 0.19\n70.75 ± 0.13\n72.83 ± 0.20\n6.32 ± 0.26\nRN50\nVGG11\n71.94 ± 0.09\n72.01 ± 0.25\n72.03 ± 0.19\n71.55 ± 0.36\n71.88 ± 0.31\n70.16 ± 0.29\n71.94 ± 0.18\n62.94 ± 0.24\nRN18\n78.76 ± 0.08\n78.41 ± 0.25\n78.92 ± 0.14\n79.26 ± 0.29\n78.99 ± 0.14\n77.94 ± 0.22\n78.33 ± 0.05\n68.96 ± 0.18\nSNT\nVGG16\nVGG11\n71.94 ± 0.09\n72.06 ± 0.22\n72.28 ± 0.12\n4.92\n± 0.22\n71.98 ± 0.18\n70.60 ± 0.13\n71.63 ± 0.10\n64.08 ± 0.19\nSNV2\n72.65 ± 0.18\n72.94 ± 0.41\n73.17 ± 0.13\n72.78 ± 0.20\n72.22 ± 0.24\n71.22 ± 0.18\n72.74 ± 0.20\n6.22\n± 0.59\nRN50\nVGG11\n71.94 ± 0.09\n72.02 ± 0.19\n72.12 ± 0.39\n72.32 ± 0.33\n71.70 ± 0.39\n70.66 ± 0.17\n71.65 ± 0.20\n62.94 ± 0.29\nRN18\n78.76 ± 0.08\n78.25 ± 0.05\n78.48 ± 0.24\n78.82 ± 0.30\n78.14 ± 0.28\n78.45 ± 0.15\n78.38 ± 0.13\n67.71 ± 0.20\nST\nVGG16\nVGG11\n71.94 ± 0.09\n72.09 ± 0.23\n72.01 ± 0.14\n71.63 ± 0.16\n71.93 ± 0.12\n71.16 ± 0.27\n71.63 ± 0.18\n63.32 ± 0.14\nSNV2\n72.65 ± 0.18\n72.64 ± 0.15\n72.67 ± 0.21\n70.53 ± 0.48\n72.24 ± 0.39\n71.32 ± 0.38\n72.42 ± 0.12\n69.46 ± 0.28\nRN50\nVGG11\n71.94 ± 0.09\n72.00 ± 0.19\n72.13 ± 0.13\n71.62 ± 0.24\n71.76 ± 0.29\n70.54 ± 0.33\n71.73 ± 0.11\n65.43 ± 0.24\nRN18\n78.76 ± 0.08\n78.96 ± 0.26\n79.02 ± 0.06\n78.35 ± 0.09\n78.31 ± 0.14\n78.36 ± 0.25\n78.81 ± 0.14\n72.87 ± 0.08\nLS\nVGG16\nVGG11\n71.94 ± 0.09\n71.90 ± 0.18\n72.00 ± 0.06\n71.57 ± 0.26\n70.89 ± 0.12\n70.66 ± 0.17\n71.76 ± 0.10\n63.49 ± 0.21\nSNV2\n72.65 ± 0.18\n72.87 ± 0.28\n73.52 ± 0.25\n70.01 ± 0.32\n71.49 ± 0.38\n71.70 ± 0.42\n73.01 ± 0.27\n65.20 ± 0.14\nRN50\nVGG11\n71.94 ± 0.09\n71.82 ± 0.28\n71.99 ± 0.16\n71.95 ± 0.33\n70.77 ± 0.39\n70.86 ± 0.24\n71.88 ± 0.16\n62.29 ± 0.10\nRN18\n78.76 ± 0.08\n77.72 ± 0.30\n77.82 ± 0.12\n79.37 ± 0.19\n78.33 ± 0.06\n78.31 ± 0.21\n77.91 ± 0.07\n63.36 ± 0.17\nCMIM\nVGG16\nVGG11\n71.94 ± 0.09\n71.87 ± 0.24\n71.64 ± 0.07\n71.56 ± 0.03\n70.34 ± 0.09\n71.71 ± 0.14\n71.42 ± 0.05\n66.89 ± 0.11\nSNV2\n72.65 ± 0.18\n72.53 ± 0.21\n71.44 ± 0.16\n72.46 ± 0.20\n71.45 ± 0.31\n71.59 ± 0.24\n71.94 ± 0.20\n64.45 ± 0.24\nRN50\nVGG11\n71.94 ± 0.09\n71.54 ± 0.30\n71.34 ± 0.16\n71.77 ± 0.06\n71.86 ± 0.28\n69.32 ± 0.09\n71.70 ± 0.22\n60.58 ± 0.17\nRN18\n78.76 ± 0.08\n78.21 ± 0.13\n78.16 ± 0.09\n78.13 ± 0.06\n77.56 ± 0.06\n77.23 ± 0.09\n78.64 ± 0.06\n65.88 ± 0.09\nTinyImageNet\nRSP\nRN34\nRN18\n63.56 ± 0.06\n63.54 ± 0.09\n64.32 ± 0.07\n64.01 ± 0.07\n63.27 ± 0.16\n63.54 ± 0.07\n62.15 ± 0.14\n55.43 ± 0.12\nRN50\nSNV2\n60.61 ± 0.15\n60.18 ± 0.26\n60.76 ± 0.16\n56.26 ± 0.16\n56.43 ± 0.11\n60.96 ± 0.22\n60.15 ± 0.20\n54.01 ± 0.22\nST\nRN34\nRN18\n63.56 ± 0.06\n63.96 ± 0.13\n64.12 ± 0.07\n63.25 ± 0.10\n63.51 ± 0.14\n63.49 ± 0.19\n63.84 ± 0.08\n57.42 ± 0.08\nRN50\nSNV2\n60.61 ± 0.15\n61.23 ± 0.24\n61.36 ± 0.14\n60.43 ± 0.14\n60.32 ± 0.24\n60.22 ± 0.17\n61.13 ± 0.13\n55.84 ± 0.11\nNT\nRN34\nRN18\n63.56 ± 0.06\n63.27 ± 0.14\n64.49 ± 0.17\n64.67 ± 0.16\n63.43 ± 0.20\n63.50 ± 0.10\n64.43 ± 0.11\n53.11 ± 0.06\nRN50\nSNV2\n60.61 ± 0.15\n59.57 ± 0.22\n61.55 ± 0.12\n31.55 ± 0.28\n60.03 ± 0.23\n60.98 ± 0.27\n60.31 ± 0.17\n50.94 ± 0.15\nLS\nRN34\nRN18\n63.56 ± 0.06\n63.74 ± 0.08\n64.01 ± 0.14\n64.23 ± 0.11\n63.51 ± 0.07\n64.20 ± 0.16\n63.04 ± 0.13\n57.43 ± 0.10\nRN50\nSNV2\n60.61 ± 0.15\n60.32 ± 0.24\n60.93 ± 0.15\n60.74 ± 0.26\n60.11 ± 0.28\n60.46 ± 0.14\n60.14 ± 0.21\n52.96 ± 0.23\nCMIM\nRN34\nRN18\n63.53 ± 0.06\n62.89 ± 0.03\n63.15 ± 0.08\n62.94 ± 0.03\n63.28 ± 0.05\n61.57 ± 0.06\n62.96 ± 0.06\n56.13 ± 0.04\nRN50\nSNV2\n60.61 ± 0.15\n57.57 ± 0.20\n59.32 ± 0.17\n60.58 ± 0.12\n59.41 ± 0.09\n59.33 ± 0.10\n60.42 ± 0.04\n56.91 ± 0.05\nUpon reviewing the variance estimates, we see that certain cases, such as the (RN50, VGG11) pair on\nCIFAR-100, might suggest that the CMIM-trained model could be rendered distillable under naive statistical\ninterpretations. For example, when the DIST method is applied to the CMIM model, the accuracy is reported\nas, which could potentially exceed the LS student accuracy of if variance is simply added to the mean.\nHowever, this approach of directly comparing mean values with added variances does not provide an accurate\nor fair assessment of undistillability. To address this, we conducted a more rigorous analysis where, across\nfive different seeds, we compared the accuracy of the knock-off Student (VGG11 in this case) trained via\nlabel smoothing and the DIST method applied to RN50 trained with CMIM.\nThe results of this comprehensive comparison are summarized in the following table, which demonstrates\nthat the CMIM model achieves undistillability across all different seeds.\nJ\nComputational Overhead\nIn this section we compare the computational overhead of our method with the CE counterpart on CIFAR-100\ndataset.\n24\nPublished in Transactions on Machine Learning Research (06/2025)\nTable 8: Variance analysis of Table 7\nSeed 1\nSeed 2\nSeed 3\nSeed 4\nSeed 5\nLS\n72.28\n71.83\n72.06\n72.25\n71.53\nCMIM\n72.01\n71.74\n71.93\n72.12\n71.27\nTable 9: Training times of CMIM and CE for ResNet-10 and VGG-16 on the CIFAR-100 dataset.\nCE\nCMIM\nRN50\n4 hours 43 minutes\n5 hours 13 minutes\nVGG16\n2 hours 57 minutes\n3 hours 25 minutes\nNote that the training time for CMIM is slightly higher than that of conventional CE method. This is\nprimarily due to the additional inference samples required to estimate the CMI per class. We believe this\nis a reasonable trade-off given the significant benefits of the method. In addition, note that the number\nof samples N does not have any effects on the training time CMIM; this is because the power transform\napplied to the teacher’s output probabilities, and when calculating the gradients during the backpropagation,\ndifferent values of α does not change the gradients.\nAdditionally, we note that among the existing KD protection methods in the literature, only CMIM (our\nmethod) and ST are scalable to larger datasets like ImageNet. This scalability is due to the significant\ncomputational complexity of other benchmark methods, which limits their applicability to smaller datasets.\nK\nAblation on Hyper-parameters\nIn this section, we perform an ablation study to analyze the impact of three key hyper-parameters of CMIM:\nβ, the number of samples N, and ω.\nFor all experiments in this study, we employ the VGG-16 and SNV2 models as the teacher-student pair,\nusing the CIFAR-100 dataset as the evaluation benchmark.\nK.1\nRange of β\nIn this section, we examine the impact of β on the performance of CMIM. For this analysis, we fix N = 50\nand ω = 25, while varying the value of β. The results are shown in Figure 4 show that the accuracy of the\nknockoff student decreases significantly when β ≥1, highlighting the importance of choosing an appropriate\nβ to effectively balance cross-entropy and CMI objectives.\nFigure 4: The student accuracy when distilled from the teacher model trained by different β values.\n25\nPublished in Transactions on Machine Learning Research (06/2025)\nK.2\nNumber of power samples N\nIn this section, we analyze the influence of the number of samples N on the performance of the knockoff\nstudent. For this study, we set β = 2 and ω = 25, varying N to observe its impact. The results, presented in\nFigure 5, reveal that the accuracy of the knockoff student declines monotonically as N increases, suggesting\nthat a moderate sampling size is enable to capture sufficient diversity for robust estimation.\nFigure 5: The student accuracy when distilled from the teacher model trained by different number of sample\nN.\nK.3\nPower Coefficient ω\nIn this section, we investigate the effect of the power coefficient ω on the knockoff student’s performance.\nFor this analysis, we fix β = 2 and N = 25, while varying ω. The results are summarized in Table 10.\nNotably, when ω > 30, the simulation frequently results in NaN values due to excessively large exponent\nvalues. Furthermore, the table shows that at ω = 20 or ω = 30, the knockoff student’s accuracy reaches its\nminimum, indicating that this value effectively approximates the behavior of ω = ∞.\nTable 10: The student accuracy when distilled from the teacher model trained by different values of ω.\nValue of ω\n1\n2\n5\n10\n15\n20\n25\n30\n40\n50\n100\n200\nKnock-off Student Accuracy\n72.65\n72.67\n72.55\n72.56\n72.55\n72.52\n72.53\n72.52\nNaN\nNaN\nNaN\nNaN\n26\nPublished in Transactions on Machine Learning Research (06/2025)\nL\nVisualization of Different Three Projected Probability Clusters\n(a) LS\n(b) Nasty teacher\n(c) CMIM\nFigure 6: Visualization of three projected probability clusters for ResNet-50 trained on CIFAR-100 using (a)\nLS, (b) NT, and (c) CMIM.\nIn this section, we randomly selected three additional output clusters from the CIFAR-100 dataset, oak,\nraccoon, and ray, and visualized them using the method introduced in Yang et al. (2025).\nThe same\nconclusion continues to hold.\nM\nEffect of CMIM on Model Calibration\nIn this section, we investigate the impact of the CMIM method on model calibration. To do so, we report the\nExpected Calibration Error (ECE) for ResNet-50 trained with CE, LS, NT and CMIM on the CIFAR-100\ndataset. As shown in Table 11, the CMIM-trained model achieves calibration on par with its CE-trained\ncounterpart and consistently outperforms both the LS and NT counterparts.\nTable 11: Expected calibration error of three projected probability clusters for ResNet-50 trained on CIFAR-\n100 using CE, LS, NT and CMIM.\nDefense\nCE\nLS\nNT\nCMIM\nECE\n9.42%\n21.48%\n16.47%\n11.24%\nAccuracy\n77.81%\n78.45%\n77.31%\n78.72%\n27\n",
    "content": "# Paper Interpretation and Analysis\n\n## 1. Core Content and Main Contributions\n\nThe core contribution of this paper is the proposal of a new deep neural network (DNN) training method called the **CMI Minimization Method (CMIM)**, which aims to make models \"undistillable.\" An undistillable model is one in which even with the use of **Knowledge Distillation (KD)** techniques, a student model cannot extract effective information from the teacher model to achieve performance better than that of a student model trained directly using **Label Smoothing (LS)**.\n\n### Main Contributions:\n\n- **Theoretical Insight**: The paper identifies that for a DNN to be undistillable, its output probability distribution should be highly concentrated within the same class, ideally approaching a one-hot vector for that class.\n- **Extension of Conditional Mutual Information (CMI)**: The authors extend traditional CMI to apply to clusters after various power transformations, enabling measurement of the concentration level of output distributions under different temperature scalings.\n- **CMI Minimization Method**: A novel training method is introduced that jointly minimizes cross-entropy loss (CE loss) and the CMI values across all power-transformed clusters. This effectively prevents model distillation.\n- **Experimental Validation**: Extensive experiments on multiple image classification datasets (e.g., CIFAR-100, TinyImageNet, and ImageNet) demonstrate that CMIM models are indeed undistillable and achieve higher classification accuracy compared to models trained solely with CE loss.\n- **First Broad Robustness Achieved**: This is the first study to train models robust against a wide range of KD methods.\n\n## 2. Breakthroughs and Innovations\n\n- **Definition of Undistillable Models**: The paper formally defines an undistillable model as one where any logit-based knowledge distillation method cannot yield a student model that outperforms one trained solely with label smoothing.\n- **CMI Minimization Approach**: By simultaneously minimizing CE loss and CMI values, the method sharpens the model's output distribution, thereby reducing the effectiveness of distillation. This not only enhances the model’s resistance to distillation but also improves its own classification accuracy.\n- **Integration of Theory and Practice**: The paper provides both theoretical foundations and extensive experimental validation, demonstrating the method’s effectiveness across major benchmark datasets.\n\n## 3. Startup Project Suggestions\n\nBased on the content of this paper, here are several startup ideas:\n\n### 1. **AI Model Intellectual Property Protection Service**\n   - **Concept**: Develop an AI model protection tool based on the CMIM method to help businesses and developers prevent their deep learning models from being extracted via knowledge distillation.\n   - **Market Positioning**: Target companies that need to protect their AI model IP, especially those offering cloud services or API interfaces.\n   - **Business Model**: SaaS model, charging per usage, providing plugin or API integration for model protection during training.\n\n### 2. **Security-Enhanced AI Model Training Platform**\n   - **Concept**: Create an AI model training platform integrated with the CMIM method to ensure trained models offer high accuracy while resisting distillation.\n   - **Market Positioning**: Aim at enterprises deploying high-security AI models, such as companies in finance, healthcare, and other critical sectors.\n   - **Business Model**: Offer customized model training services, charging either a one-time fee or subscription-based access.\n\n### 3. **AI Model Distillation Detection and Evaluation Tool**\n   - **Concept**: Develop a tool to detect and evaluate whether existing AI models are vulnerable to knowledge distillation attacks, and provide improvement suggestions (e.g., retraining with CMIM).\n   - **Market Positioning**: Target enterprise internal security teams or third-party security auditing firms.\n   - **Business Model**: Provide software licensing or consulting services to help companies identify risks and implement mitigation strategies.\n\n### 4. **Adversarial Training and Defense Research Platform**\n   - **Concept**: Build an open platform for researchers and developers to explore and test various adversarial training and defense techniques, including applications of the CMIM method.\n   - **Market Positioning**: Academic researchers, industrial R&D teams, and AI security enthusiasts.\n   - **Business Model**: Open-source basic version with premium features available via paid subscriptions.\n\n### 5. **AI Model Compression and Optimization Service**\n   - **Concept**: Utilize the CMIM method to optimize model size and inference speed while maintaining security, offering end-to-end model compression solutions.\n   - **Market Positioning**: Companies deploying lightweight AI models on edge devices.\n   - **Business Model**: Offer model compression services, charging per project or based on model size.\n\nThese startup projects can fully leverage the research outcomes from the paper while addressing the growing demand in the AI industry for enhanced model security, efficiency, and intellectual property protection.",
    "github": "",
    "hf": ""
}