{
    "id": "/lukas-blecher/LaTeX-OCR",
    "issues": "137",
    "watch": "85",
    "fork": "1.2k",
    "star": "15.4k",
    "topics": [
        "python",
        "machine-learning",
        "ocr",
        "latex",
        "deep-learning",
        "image-processing",
        "pytorch",
        "dataset",
        "transformer",
        "vit",
        "image2text",
        "im2text",
        "im2latex",
        "im2markup",
        "math-ocr",
        "vision-transformer",
        "latex-ocr"
    ],
    "license": "MIT License",
    "languages": [
        "Python,96.7%",
        "JavaScript,1.7%",
        "Jupyter Notebook,1.5%"
    ],
    "contributors": [],
    "about": "pix2tex: Using a ViT to convert images of equations into LaTeX code.",
    "is_AI": "y",
    "category": "Research/Reading Tools",
    "summary": "```markdown\n# pix2tex Project Analysis\n\n## 1. Core Content and Problems Solved\n\n**Core Content:**  \n`pix2tex` is a deep learning-based LaTeX Optical Character Recognition (OCR) system that automatically converts images of mathematical formulas into corresponding LaTeX code. The project employs a **Vision Transformer (ViT)** as the encoder and a **Transformer decoder** to build an end-to-end image-to-text generation model.\n\n**Main Problems Addressed:**  \n- Manual input of complex mathematical expressions is inefficient and error-prone.\n- Traditional OCR tools struggle to accurately recognize math expressions with intricate formatting.\n- Academic writing and educational material preparation often require extracting formula code from screenshots or scanned documents.\n\nWith this tool, users can simply screenshot or upload an image containing a mathematical expression, and the system outputs editable LaTeX source code—significantly improving digitization efficiency in research, education, and publishing.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n- ✅ **ViT + Transformer Architecture for Formula Images:**  \n  Unlike traditional CNN-RNN or CNN-Attention approaches (e.g., im2markup), this project uses Vision Transformers to encode image features, which are better at capturing long-range dependencies and complex structures—making it particularly effective for highly structured visual content like mathematical formulas.\n\n- ✅ **Automated Resolution Optimization in Preprocessing:**  \n  An auxiliary neural network predicts the optimal input resolution, dynamically resizing images to match the training data distribution. This significantly improves recognition robustness on real-world images.\n\n- ✅ **Out-of-the-box Multi-Platform Support:**  \n  Offers three usage modes: command-line interface, GUI (with screenshot support and real-time rendering), and API access—compatible with both Windows and Linux (including Wayland), ensuring a user-friendly experience.\n\n- ✅ **Integrated MathJax Real-Time Preview:**  \n  The GUI renders recognition results instantly, allowing users to verify correctness immediately and enhancing interactive usability.\n\n- ✅ **Lightweight Deployment Options:**  \n  Supports Docker-based API deployment, making it easy to integrate into other systems or deploy as a cloud service.\n\n- ✅ **Open Source and Retrainable:**  \n  Provides not only pre-trained models but also full training pipelines, data generation methods, and configuration files. Users can customize datasets and tokenizers, offering strong extensibility.\n\n---\n\n## 3. Entrepreneurial Opportunities and Business Ideas\n\n### Idea 1: Academic Writing Assistant SaaS Platform  \n> **Product Form:** Web app + plugins (Word / Overleaf / Notion)  \n- Allow users to upload handwritten notes, paper screenshots, or PDF pages; automatically extract formulas and insert them into documents.  \n- Combine with general OCR to enable automated transcription of mixed text-and-image content.  \n- Business Model: Subscription-based pricing with team collaboration features.\n\n### Idea 2: Smart Teaching Tool — Teacher’s Assistant App  \n> **Target Users:** High school/college teachers, tutors, online course creators  \n- Take photos of exercises to recognize formulas and import them directly into lesson plans or question banks.  \n- Automatically generate LaTeX templates for problems, supporting batch exam paper exports.  \n- Can be extended with AI-powered problem generation and solution features for a closed-loop workflow.\n\n### Idea 3: LaTeX Collaboration Plugin Marketplace  \n> **Direction:** Develop plugins for VS Code / Overleaf / Obsidian  \n- Enable users to capture screenshots via shortcut keys during writing, seamlessly inserting recognized formulas at the cursor.  \n- Include features like history tracking, formula library management, and version comparison.  \n- Monetization: Charge via plugin marketplaces or offer freemium models with premium add-ons.\n\n### Idea 4: Mobile Formula Recognition App (with Handwriting Support)  \n> **Key Feature:** Photo capture & handwriting input → LaTeX output  \n- Students photograph textbook or blackboard formulas and convert them into searchable, copyable digital content.  \n- Integrate with note-taking apps (e.g., Notion, OneNote) to create a “Smart STEM Notebook.”  \n- Potential expansion into photo-based homework help and AI tutoring systems.\n\n### Idea 5: Enterprise Document Automation Solution  \n> **Clients:** Publishers, EdTech companies, standardized testing organizations  \n- Automate processing of mathematical content from large volumes of historical exams and scanned textbooks.  \n- Integrate with CMS or LMS (Learning Management Systems) to digitize legacy materials.  \n- Offer private deployment and custom model training (e.g., for specific fonts or symbol sets).\n\n---\n\n💡 **Summary & Recommendations:**  \n`pix2tex` is a technically mature AI tool with clear application scenarios—exemplifying how a \"small model can address big needs.\" The most promising entrepreneurial path lies in **vertical-specific content digitization tools**, especially in education and research. By integrating into existing productivity software ecosystems, startups can achieve rapid deployment and build sustainable competitive advantages.\n```",
    "text": "pix2tex - LaTeX OCR\nThe goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code.\nUsing the model\nTo run the model you need Python 3.7+\nIf you don't have PyTorch installed. Follow their instructions\nhere\n.\nInstall the package\npix2tex\n:\npip install \"pix2tex[gui]\"\nModel checkpoints will be downloaded automatically.\nThere are three ways to get a prediction from an image.\nYou can use the command line tool by calling\npix2tex\n. Here you can parse already existing images from the disk and images in your clipboard.\nThanks to\n@katie-lim\n, you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with\nlatexocr\n. From here you can take a screenshot and the predicted latex code is rendered using\nMathJax\nand copied to your clipboard.\nUnder linux, it is possible to use the GUI with\ngnome-screenshot\n(which comes with multiple monitor support). For other Wayland compositers,\ngrim\nand\nslurp\nwill be used for wlroots-based Wayland compositers and\nspectacle\nfor KDE Plasma. Note that\ngnome-screenshot\nis not compatible with wlroots or Qt based compositers. Since\ngnome-screenshot\nwill be preferred when available, you may have to set the environment variable\nSCREENSHOT_TOOL\nto\ngrim\nor\nspectacle\nin these cases (other available values are\ngnome-screenshot\nand\npil\n).\nIf the model is unsure about the what's in the image it might output a different prediction every time you click \"Retry\". With the\ntemperature\nparameter you can control this behavior (low temperature will produce the same result).\nYou can use an API. This has additional dependencies. Install via\npip install -U \"pix2tex[api]\"\nand run\npython -m pix2tex.api.run\nto start a\nStreamlit\ndemo that connects to the API at port 8502. There is also a docker image  available for the API:\nhttps://hub.docker.com/r/lukasblecher/pix2tex\ndocker pull lukasblecher/pix2tex:api\ndocker run --rm -p 8502:8502 lukasblecher/pix2tex:api\nTo also run the streamlit demo run\ndocker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py\nand navigate to\nhttp://localhost:8501/\nUse from within Python\nfrom\nPIL\nimport\nImage\nfrom\npix2tex\n.\ncli\nimport\nLatexOCR\nimg\n=\nImage\n.\nopen\n(\n'path/to/image.png'\n)\nmodel\n=\nLatexOCR\n()\nprint\n(\nmodel\n(\nimg\n))\nThe model works best with images of smaller resolution. That's why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it's not perfect and might not be able to handle huge images optimally, so don't zoom in all the way before taking a picture.\nAlways double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.\nWant to use the package?\nI'm trying to compile a documentation right now.\nVisit here:\nhttps://pix2tex.readthedocs.io/\nTraining the model\nInstall a couple of dependencies\npip install \"pix2tex[train]\"\n.\nFirst we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run\npython -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl\nTo use your own tokenizer pass it via\n--tokenizer\n(See below).\nYou can find my generated training data on the\nGoogle Drive\nas well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.\nEdit the\ndata\n(and\nvaldata\n) entry in the config file to the newly generated\n.pkl\nfile. Change other hyperparameters if you want to. See\npix2tex/model/settings/config.yaml\nfor a template.\nNow for the actual training run\npython -m pix2tex.train --config path_to_config_file\nIf you want to use your own data you might be interested in creating your own tokenizer with\npython -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json\nDon't forget to update the path to the tokenizer in the config file and set\nnum_tokens\nto your vocabulary size.\nModel\nThe model consist of a ViT [\n1\n] encoder with a ResNet backbone and a Transformer [\n2\n] decoder.\nPerformance\nBLEU score\nnormed edit distance\ntoken accuracy\n0.88\n0.10\n0.60\nData\nWe need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g.\nwikipedia\n,\narXiv\n. We also use the formulae from the\nim2latex-100k\n[\n3\n] dataset.\nAll of it can be found\nhere\nDataset Requirements\nIn order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools:\nXeLaTeX\nImageMagick\nwith\nGhostscript\n. (for converting pdf to png)\nNode.js\nto run\nKaTeX\n(for normalizing Latex code)\nPython 3.7+ & dependencies (specified in\nsetup.py\n)\nFonts\nLatin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math\nTODO\nadd more evaluation metrics\ncreate a GUI\nadd beam search\nsupport handwritten formulae (kinda done, see training colab notebook)\nreduce model size (distillation)\nfind optimal hyperparameters\ntweak model structure\nfix data scraping and scrape more data\ntrace the model (\n#2\n)\nContribution\nContributions of any kind are welcome.\nAcknowledgment\nCode taken and modified from\nlucidrains\n,\nrwightman\n,\nim2markup\n,\narxiv_leaks\n,\npkra: Mathjax\n,\nharupy: snipping tool\nReferences\n[1]\nAn Image is Worth 16x16 Words\n[2]\nAttention Is All You Need\n[3]\nImage-to-Markup Generation with Coarse-to-Fine Attention",
    "readme": "# pix2tex - LaTeX OCR\n\n[![GitHub](https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR)](https://github.com/lukas-blecher/LaTeX-OCR) [![Documentation Status](https://readthedocs.org/projects/pix2tex/badge/?version=latest)](https://pix2tex.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![PyPI - Downloads](https://img.shields.io/pypi/dm/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![GitHub all releases](https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&logo=github)](https://github.com/lukas-blecher/LaTeX-OCR/releases) [![Docker Pulls](https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb) [![Hugging Face Spaces](https://img.shields.io/badge/🤗%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/lukbl/LaTeX-OCR)\n\nThe goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code. \n\n![header](https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png)\n\n## Using the model\nTo run the model you need Python 3.7+\n\nIf you don't have PyTorch installed. Follow their instructions [here](https://pytorch.org/get-started/locally/).\n\nInstall the package `pix2tex`: \n\n```\npip install \"pix2tex[gui]\"\n```\n\nModel checkpoints will be downloaded automatically.\n\nThere are three ways to get a prediction from an image. \n1. You can use the command line tool by calling `pix2tex`. Here you can parse already existing images from the disk and images in your clipboard.\n\n2. Thanks to [@katie-lim](https://github.com/katie-lim), you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with `latexocr`. From here you can take a screenshot and the predicted latex code is rendered using [MathJax](https://www.mathjax.org/) and copied to your clipboard.\n\n    Under linux, it is possible to use the GUI with `gnome-screenshot` (which comes with multiple monitor support). For other Wayland compositers, `grim` and `slurp` will be used for wlroots-based Wayland compositers and `spectacle` for KDE Plasma. Note that `gnome-screenshot` is not compatible with wlroots or Qt based compositers. Since `gnome-screenshot` will be preferred when available, you may have to set the environment variable `SCREENSHOT_TOOL` to `grim` or `spectacle` in these cases (other available values are `gnome-screenshot` and `pil`).\n\n    ![demo](https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif)\n\n    If the model is unsure about the what's in the image it might output a different prediction every time you click \"Retry\". With the `temperature` parameter you can control this behavior (low temperature will produce the same result).\n\n3. You can use an API. This has additional dependencies. Install via `pip install -U \"pix2tex[api]\"` and run\n    ```bash\n    python -m pix2tex.api.run\n    ```\n    to start a [Streamlit](https://streamlit.io/) demo that connects to the API at port 8502. There is also a docker image  available for the API: https://hub.docker.com/r/lukasblecher/pix2tex [![Docker Image Size (latest by date)](https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex)\n\n    ```\n    docker pull lukasblecher/pix2tex:api\n    docker run --rm -p 8502:8502 lukasblecher/pix2tex:api\n    ```\n    To also run the streamlit demo run\n    ```\n    docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py\n    ```\n    and navigate to http://localhost:8501/\n\n4. Use from within Python\n    ```python\n    from PIL import Image\n    from pix2tex.cli import LatexOCR\n    \n    img = Image.open('path/to/image.png')\n    model = LatexOCR()\n    print(model(img))\n    ```\n\nThe model works best with images of smaller resolution. That's why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it's not perfect and might not be able to handle huge images optimally, so don't zoom in all the way before taking a picture. \n\nAlways double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.\n\n**Want to use the package?**\n\nI'm trying to compile a documentation right now. \n\nVisit here: https://pix2tex.readthedocs.io/ \n\n\n## Training the model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb)\n\nInstall a couple of dependencies `pip install \"pix2tex[train]\"`.\n1. First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run \n\n```\npython -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl\n```\nTo use your own tokenizer pass it via `--tokenizer` (See below).\n\nYou can find my generated training data on the [Google Drive](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO) as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.\n\n2. Edit the `data` (and `valdata`) entry in the config file to the newly generated `.pkl` file. Change other hyperparameters if you want to. See `pix2tex/model/settings/config.yaml` for a template.\n3. Now for the actual training run \n```\npython -m pix2tex.train --config path_to_config_file\n```\n\nIf you want to use your own data you might be interested in creating your own tokenizer with\n```\npython -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json\n```\nDon't forget to update the path to the tokenizer in the config file and set `num_tokens` to your vocabulary size.\n\n## Model\nThe model consist of a ViT [[1](#References)] encoder with a ResNet backbone and a Transformer [[2](#References)] decoder.\n\n### Performance\n| BLEU score | normed edit distance | token accuracy |\n| ---------- | -------------------- | -------------- |\n| 0.88       | 0.10                 | 0.60           |\n\n## Data\nWe need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. [wikipedia](https://www.wikipedia.org), [arXiv](https://www.arxiv.org). We also use the formulae from the [im2latex-100k](https://zenodo.org/record/56198#.V2px0jXT6eA) [[3](#References)] dataset.\nAll of it can be found [here](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO)\n\n### Dataset Requirements\nIn order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools: \n* [XeLaTeX](https://www.ctan.org/pkg/xetex)\n* [ImageMagick](https://imagemagick.org/) with [Ghostscript](https://www.ghostscript.com/index.html). (for converting pdf to png)\n* [Node.js](https://nodejs.org/) to run [KaTeX](https://github.com/KaTeX/KaTeX) (for normalizing Latex code)\n* Python 3.7+ & dependencies (specified in `setup.py`)\n\n### Fonts\nLatin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math\n\n\n## TODO\n- [x] add more evaluation metrics\n- [x] create a GUI\n- [ ] add beam search\n- [ ] support handwritten formulae (kinda done, see training colab notebook)\n- [ ] reduce model size (distillation)\n- [ ] find optimal hyperparameters\n- [ ] tweak model structure\n- [ ] fix data scraping and scrape more data\n- [ ] trace the model ([#2](https://github.com/lukas-blecher/LaTeX-OCR/issues/2))\n\n\n## Contribution\nContributions of any kind are welcome.\n\n## Acknowledgment\nCode taken and modified from [lucidrains](https://github.com/lucidrains), [rwightman](https://github.com/rwightman/pytorch-image-models), [im2markup](https://github.com/harvardnlp/im2markup), [arxiv_leaks](https://github.com/soskek/arxiv_leaks), [pkra: Mathjax](https://github.com/pkra/MathJax-single-file), [harupy: snipping tool](https://github.com/harupy/snipping-tool)\n\n## References\n[1] [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)\n\n[2] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n\n[3] [Image-to-Markup Generation with Coarse-to-Fine Attention](https://arxiv.org/abs/1609.04938v2)\n",
    "author": "lukas-blecher",
    "project": "LaTeX-OCR",
    "date": "2025-10-03"
}