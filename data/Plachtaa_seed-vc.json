{
    "id": "/Plachtaa/seed-vc",
    "issues": "86",
    "watch": "48",
    "fork": "367",
    "star": "3.1k",
    "topics": [
        "voice-conversion",
        "singing-voice-conversion"
    ],
    "license": "GNU General Public License v3.0",
    "languages": [
        "Python,97.4%",
        "Cuda,1.5%"
    ],
    "contributors": [
        "https://avatars.githubusercontent.com/u/112609742?s=64&v=4",
        "https://avatars.githubusercontent.com/u/34035011?s=64&v=4",
        "https://avatars.githubusercontent.com/u/88027106?s=64&v=4",
        "https://avatars.githubusercontent.com/u/1288038?s=64&v=4",
        "https://avatars.githubusercontent.com/u/12210038?s=64&v=4",
        "https://avatars.githubusercontent.com/u/50707839?s=64&v=4",
        "https://avatars.githubusercontent.com/u/198223882?s=64&v=4",
        "https://avatars.githubusercontent.com/u/22633385?s=64&v=4",
        "https://avatars.githubusercontent.com/u/39916032?s=64&v=4",
        "https://avatars.githubusercontent.com/u/43167455?s=64&v=4",
        "https://avatars.githubusercontent.com/u/56507185?s=64&v=4",
        "https://avatars.githubusercontent.com/u/94414189?s=64&v=4"
    ],
    "about": "zero-shot voice conversion & singing voice conversion, with real-time support",
    "is_AI": "y",
    "category": "Agent/Robot",
    "summary": "Sure! Here's the English translation of your content:\n\n---\n\n# Seed-VC Project Analysis\n\n## 1. Core Content and Problems Solved\n\n**Seed-VC** is an advanced **zero-shot voice conversion (VC)** and **singing voice conversion (SVC)** system that supports real-time audio processing. Its core objectives are:\n\n- **High-quality voice cloning and conversion without training**: Only 1–30 seconds of target speaker reference audio is required to convert the source speech into a specified voice.\n- **Supports multiple use cases**:\n  - General voice conversion (e.g., voice modulation, dubbing)\n  - Singing voice conversion (e.g., converting normal singing into a specific singer’s timbre)\n  - Real-time voice conversion (suitable for online meetings, live streaming, gaming, etc.)\n\nThis technology addresses several key issues in traditional voice conversion:\n- Heavy training data requirements and long training times\n- Complex deployment and high latency, making real-time interaction difficult\n- Poor performance on high-frequency, wide-dynamic-range audio like singing\n\nBy integrating advanced encoder, diffusion model, and vocoder architectures, Seed-VC achieves a \"plug-and-play\" zero-shot voice conversion capability that balances quality and efficiency.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### ✅ Zero-Shot + Real-Time Capabilities\n\n- **True zero-shot voice conversion**: No training required—simply input a short reference audio to perform conversion.\n- **Supports real-time voice conversion** (~300ms algorithm latency + ~100ms device latency), ideal for interactive applications like virtual anchors, remote meetings, and voice-based social platforms.\n\n### ✅ Unified Multi-Task Framework\n\n- The same system supports:\n  - Daily speech conversion (VC)\n  - Singing voice conversion (SVC)\n  - Accent/emotion transfer (new in V2)\n- Multiple pre-trained model versions (v1/v2) for different use cases (real-time vs. offline, speech vs. singing).\n\n### ✅ Ultra-Low Resource Fine-Tuning\n\n- **One-shot fine-tuning support**: Only one utterance per speaker is needed to significantly improve timbre similarity.\n- **Fast fine-tuning**: Optimization can be completed in ~2 minutes with ~100 training steps on a T4 GPU, drastically lowering the barrier for personalization.\n\n### ✅ Advanced Architecture Design (V2 Highlights)\n\n- **CFM (Flow Matching) + AR (Autoregressive) joint modeling** for more natural prosody and accent transfer.\n- **ASTRAL-Quantization technology** effectively decouples speech content from speaker identity, enhancing anonymity and generalization.\n- Supports **accent conversion and emotion transfer**, going beyond traditional VC capabilities.\n\n### ✅ User-Friendly and Engineering Optimizations\n\n- Offers CLI, Web UI, and real-time GUI interfaces.\n- Cross-platform support: Mac M-series chips, Windows, Linux.\n- Auto-model download with domestic mirror acceleration (`HF_ENDPOINT=https://hf-mirror.com`).\n- Open-source training code and Colab tutorials for researchers to reproduce and extend.\n\n---\n\n## 3. Entrepreneurial Directions\n\nBased on Seed-VC's powerful capabilities, here are several promising startup directions:\n\n### 🎤 Direction 1: AI Virtual Streamer / Digital Human Voice Customization Platform  \n> **Product Form**: SaaS platform where users upload their voice samples to generate a real-time \"AI voice actor\" for streaming or short video dubbing.\n>\n> **Advantages**:\n> - Real-time VC allows streamers to use an \"ideal voice\" without revealing their real one.\n> - Supports multi-character switching (e.g., anime dubbing), gender conversion, age simulation.\n> - Can be integrated into tools like OBS, Discord, Zoom as a plugin.\n\n### 🎵 Direction 2: AI Singing Tutoring & Karaoke Entertainment App  \n> **Product Form**: Mobile app or mini-program where users sing a cappella, and the app instantly converts the voice into celebrity timbres like Jay Chou or GEM.\n>\n> **Key Features**:\n> - \"I Am a Singer\" experience: anyone can sing like their idols.\n> - Integrated singing score + voice conversion.\n> - Supports exporting high-quality MV videos for social sharing.\n\n### 💼 Direction 3: Enterprise-Grade Voice Anonymization Service  \n> **Product Form**: Privacy-preserving voice communication solutions for customer service, mental health counseling, telemedicine, etc.\n>\n> **Core Use Case**:\n> - Use V2’s `anonymization-only` mode to convert employee voices into neutral, average-sounding voices to protect identities.\n> - Low-latency real-time processing without affecting communication fluency.\n> - Complies with GDPR and other privacy regulations.\n\n### 🌐 Direction 4: Globalized Voice Localization Platform  \n> **Product Form**: Help companies quickly localize English videos/courses into Chinese or other languages with regional accents.\n>\n> **Innovation Points**:\n> - Not just translation—preserves the original speaker’s tone while changing the accent (e.g., Sichuan, Cantonese, Northeast).\n> - Supports \"one speaker, multiple languages\" with consistent timbre—ideal for brand-specific announcers.\n\n### 🧠 Direction 5: Personalized AI Assistant Voice Engine  \n> **Product Form**: Customizable voice interface for smart hardware (e.g., smart speakers, in-car systems).\n>\n> **Differentiation**:\n> - Users can train a personal AI voice assistant using a family member’s or friend’s voice (just one sentence needed).\n> - Supports emotion modulation (e.g., a \"gentle mom voice\" reminding kids to do homework).\n> - Strong emotional connection value in family settings.\n\n---\n\n## Summary\n\nSeed-VC, with its **zero-shot, high-quality, real-time, and easy fine-tuning** capabilities, has become one of the most practically valuable technologies in the field of voice conversion. It not only advances academic research but also opens the door to a new era of voice interaction—where not only **what is said** but also **who is speaking** becomes fully controllable and customizable.\n\n--- \n\nLet me know if you'd like this translated into a different tone (e.g., academic, business, technical documentation) or formatted for a specific platform like a presentation or website!",
    "text": "Seed-VC\nEnglish |\n简体中文\n|\n日本語\nreal-time-demo.webm\nCurrently released model supports\nzero-shot voice conversion\n🔊 ,\nzero-shot real-time voice conversion\n🗣️ and\nzero-shot singing voice conversion\n🎶. Without any training, it is able to clone a voice given a reference speech of 1~30 seconds.\nWe support further fine-tuning on custom data to increase performance on specific speaker/speakers, with extremely low data requirement\n(minimum 1 utterance per speaker)\nand extremely fast training speed\n(minimum 100 steps, 2 min on T4)\n!\nReal-time voice conversion\nis support, with algorithm delay of ~300ms and device side delay of ~100ms, suitable for online meetings, gaming and live streaming.\nTo find a list of demos and comparisons with previous voice conversion models, please visit our\ndemo page\n🌐  and\nEvaluaiton\n📊.\nWe are keeping on improving the model quality and adding more features.\nEvaluation📊\nSee\nEVAL.md\nfor objective evaluation results and comparisons with other baselines.\nInstallation📥\nSuggested python 3.10 on Windows, Mac M Series (Apple Silicon) or Linux.\nWindows and Linux:\npip install -r requirements.txt\nMac M Series:\npip install -r requirements-mac.txt\nFor Windows users, you may consider install\ntriton-windows\nto enable\n--compile\nusage, which gains speed up on V2 models:\npip install triton-windows==3.2.0.post13\nUsage🛠️\nWe have released 4 models for different purposes:\nVersion\nName\nPurpose\nSampling Rate\nContent Encoder\nVocoder\nHidden Dim\nN Layers\nParams\nRemarks\nv1.0\nseed-uvit-tat-xlsr-tiny (\n🤗\n📄\n)\nVoice Conversion (VC)\n22050\nXLSR-large\nHIFT\n384\n9\n25M\nsuitable for real-time voice conversion\nv1.0\nseed-uvit-whisper-small-wavenet (\n🤗\n📄\n)\nVoice Conversion (VC)\n22050\nWhisper-small\nBigVGAN\n512\n13\n98M\nsuitable for offline voice conversion\nv1.0\nseed-uvit-whisper-base (\n🤗\n📄\n)\nSinging Voice Conversion (SVC)\n44100\nWhisper-small\nBigVGAN\n768\n17\n200M\nstrong zero-shot performance, singing voice conversion\nv2.0\nhubert-bsqvae-small (\n🤗\n📄\n)\nVoice & Accent Conversion (VC)\n22050\nASTRAL-Quantization\nBigVGAN\n512\n13\n67M(CFM) + 90M(AR)\nBest in suppressing source speaker traits\nCheckpoints of the latest model release will be downloaded automatically when first run inference.\nIf you are unable to access huggingface for network reason, try using mirror by adding\nHF_ENDPOINT=https://hf-mirror.com\nbefore every command.\nCommand line inference:\npython inference.py --source\n<\nsource-wav\n>\n--target\n<\nreferene-wav\n>\n--output\n<\noutput-dir\n>\n--diffusion-steps 25\n#\nrecommended 30~50 for singingvoice conversion\n--length-adjust 1.0\n--inference-cfg-rate 0.7\n--f0-condition False\n#\nset to True for singing voice conversion\n--auto-f0-adjust False\n#\nset to True to auto adjust source pitch to target pitch level, normally not used in singing voice conversion\n--semi-tone-shift 0\n#\npitch shift in semitones for singing voice conversion\n--checkpoint\n<\npath-to-checkpoint\n>\n--config\n<\npath-to-config\n>\n--fp16 True\nwhere:\nsource\nis the path to the speech file to convert to reference voice\ntarget\nis the path to the speech file as voice reference\noutput\nis the path to the output directory\ndiffusion-steps\nis the number of diffusion steps to use, default is 25, use 30-50 for best quality, use 4-10 for fastest inference\nlength-adjust\nis the length adjustment factor, default is 1.0, set <1.0 for speed-up speech, >1.0 for slow-down speech\ninference-cfg-rate\nhas subtle difference in the output, default is 0.7\nf0-condition\nis the flag to condition the pitch of the output to the pitch of the source audio, default is False, set to True for singing voice conversion\nauto-f0-adjust\nis the flag to auto adjust source pitch to target pitch level, default is False, normally not used in singing voice conversion\nsemi-tone-shift\nis the pitch shift in semitones for singing voice conversion, default is 0\ncheckpoint\nis the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface.(\nseed-uvit-whisper-small-wavenet\nif\nf0-condition\nis\nFalse\nelse\nseed-uvit-whisper-base\n)\nconfig\nis the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface\nfp16\nis the flag to use float16 inference, default is True\nSimilarly, to use V2 model, you can run:\npython inference_v2.py --source\n<\nsource-wav\n>\n--target\n<\nreferene-wav\n>\n--output\n<\noutput-dir\n>\n--diffusion-steps 25\n#\nrecommended 30~50 for singingvoice conversion\n--length-adjust 1.0\n#\nsame as V1\n--intelligibility-cfg-rate 0.7\n#\ncontrols how clear the output linguistic content is, recommended 0.0~1.0\n--similarity-cfg-rate 0.7\n#\ncontrols how similar the output voice is to the reference voice, recommended 0.0~1.0\n--convert-style\ntrue\n#\nwhether to use AR model for accent & emotion conversion, set to false will only conduct timbre conversion similar to V1\n--anonymization-only\nfalse\n#\nset to true will ignore reference audio but only anonymize source speech to an \"average voice\"\n--top-p 0.9\n#\ncontrols the diversity of the AR model output, recommended 0.5~1.0\n--temperature 1.0\n#\ncontrols the randomness of the AR model output, recommended 0.7~1.2\n--repetition-penalty 1.0\n#\npenalizes the repetition of the AR model output, recommended 1.0~1.5\n--cfm-checkpoint-path\n<\npath-to-cfm-checkpoint\n>\n#\npath to the checkpoint of the CFM model, leave to blank to auto-download default model from huggingface\n--ar-checkpoint-path\n<\npath-to-ar-checkpoint\n>\n#\npath to the checkpoint of the AR model, leave to blank to auto-download default model from huggingface\nVoice Conversion Web UI:\npython app_vc.py --checkpoint\n<\npath-to-checkpoint\n>\n--config\n<\npath-to-config\n>\n--fp16 True\ncheckpoint\nis the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (\nseed-uvit-whisper-small-wavenet\n)\nconfig\nis the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface\nThen open the browser and go to\nhttp://localhost:7860/\nto use the web interface.\nSinging Voice Conversion Web UI:\npython app_svc.py --checkpoint\n<\npath-to-checkpoint\n>\n--config\n<\npath-to-config\n>\n--fp16 True\ncheckpoint\nis the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (\nseed-uvit-whisper-base\n)\nconfig\nis the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface\nV2 model Web UI:\npython app_vc_v2.py --cfm-checkpoint-path\n<\npath-to-cfm-checkpoint\n>\n--ar-checkpoint-path\n<\npath-to-ar-checkpoint\n>\ncfm-checkpoint-path\nis the path to the checkpoint of the CFM model, leave to blank to auto-download default model from huggingface\nar-checkpoint-path\nis the path to the checkpoint of the AR model, leave to blank to auto-download default model from huggingface\nyou may consider adding\n--compile\nto gain ~x6 speed-up on AR model inference\nIntegrated Web UI:\npython app.py --enable-v1 --enable-v2\nThis will only load pretrained models for zero-shot inference. To use custom checkpoints, please run\napp_vc.py\nor\napp_svc.py\nas above.\nIf you have limited memory, remove\n--enable-v2\nor\n--enable-v1\nto only load one of the model sets.\nReal-time voice conversion GUI:\npython real-time-gui.py --checkpoint-path\n<\npath-to-checkpoint\n>\n--config-path\n<\npath-to-config\n>\ncheckpoint\nis the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (\nseed-uvit-tat-xlsr-tiny\n)\nconfig\nis the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface\nImportant\nIt is strongly recommended to use a GPU for real-time voice conversion.\nSome performance testing has been done on a NVIDIA RTX 3060 Laptop GPU, results and recommended parameter settings are listed below:\nModel Configuration\nDiffusion Steps\nInference CFG Rate\nMax Prompt Length\nBlock Time (s)\nCrossfade Length (s)\nExtra context (left) (s)\nExtra context (right) (s)\nLatency (ms)\nInference Time per Chunk (ms)\nseed-uvit-xlsr-tiny\n10\n0.7\n3.0\n0.18s\n0.04s\n2.5s\n0.02s\n430ms\n150ms\nYou can adjust the parameters in the GUI according to your own device performance, the voice conversion stream should work well as long as Inference Time is less than Block Time.\nNote that inference speed may drop if you are running other GPU intensive tasks (e.g. gaming, watching videos)\nExplanations for real-time voice conversion GUI parameters:\nDiffusion Steps\nis the number of diffusion steps to use, in real-time case usually set to 4~10 for fastest inference;\nInference CFG Rate\nhas subtle difference in the output, default is 0.7, set to 0.0 gains about 1.5x speed-up;\nMax Prompt Length\nis the maximum length of the prompt audio, setting to a low value can speed up inference, but may reduce similarity to prompt speech;\nBlock Time\nis the time length of each audio chunk for inference, the higher the value, the higher the latency, note this value must be greater than the inference time per block, set according to your hardware condition;\nCrossfade Length\nis the time length of crossfade between audio chunks, normally not needed to change;\nExtra context (left)\nis the time length of extra history context for inference, the higher the value, the higher the inference time, but can increase stability;\nExtra context (right)\nis the time length of extra future context for inference, the higher the value, the higher the inference time and latency, but can increase stability;\nThe algorithm delay is appoximately calculated as\nBlock Time * 2 + Extra context (right)\n, device side delay is usually of ~100ms. The overall delay is the sum of the two.\nYou may wish to use\nVB-CABLE\nto route audio from GUI output stream to a virtual microphone.\n(GUI and audio chunking logic are modified from\nRVC\n, thanks for their brilliant implementation!)\nTraining🏋️\nFine-tuning on custom data allow the model to clone someone's voice more accurately. It will largely improve speaker similarity on particular speakers, but may slightly increase WER.\nA Colab Tutorial is here for you to follow:\nPrepare your own dataset. It has to satisfy the following:\nFile structure does not matter\nEach audio file should range from 1 to 30 seconds, otherwise will be ignored\nAll audio files should be in on of the following formats:\n.wav\n.flac\n.mp3\n.m4a\n.opus\n.ogg\nSpeaker label is not required, but make sure that each speaker has at least 1 utterance\nOf course, the more data you have, the better the model will perform\nTraining data should be as clean as possible, BGM or noise is not desired\nChoose a model configuration file from\nconfigs/presets/\nfor fine-tuning, or create your own to train from scratch.\nFor fine-tuning, it should be one of the following:\n./configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml\nfor real-time voice conversion\n./configs/presets/config_dit_mel_seed_uvit_whisper_small_wavenet.yml\nfor offline voice conversion\n./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml\nfor singing voice conversion\nRun the following command to start training:\npython train.py \n--config\n<\npath-to-config\n>\n--dataset-dir\n<\npath-to-data\n>\n--run-name\n<\nrun-name\n>\n--batch-size 2\n--max-steps 1000\n--max-epochs 1000\n--save-every 500\n--num-workers 0\nwhere:\nconfig\nis the path to the model config, choose one of the above for fine-tuning or create your own for training from scratch\ndataset-dir\nis the path to the dataset directory, which should be a folder containing all the audio files\nrun-name\nis the name of the run, which will be used to save the model checkpoints and logs\nbatch-size\nis the batch size for training, choose depends on your GPU memory.\nmax-steps\nis the maximum number of steps to train, choose depends on your dataset size and training time\nmax-epochs\nis the maximum number of epochs to train, choose depends on your dataset size and training time\nsave-every\nis the number of steps to save the model checkpoint\nnum-workers\nis the number of workers for data loading, set to 0 for Windows\nSimilarly, to train V2 model, you can run: (note that V2 training script supports multi-GPU training)\naccelerate launch train_v2.py \n--dataset-dir\n<\npath-to-data\n>\n--run-name\n<\nrun-name\n>\n--batch-size 2\n--max-steps 1000\n--max-epochs 1000\n--save-every 500\n--num-workers 0\n--train-cfm\nIf training accidentially stops, you can resume training by running the same command again, the training will continue from the last checkpoint. (Make sure\nrun-name\nand\nconfig\narguments are the same so that latest checkpoint can be found)\nAfter training, you can use the trained model for inference by specifying the path to the checkpoint and config file.\nThey should be under\n./runs/<run-name>/\n, with the checkpoint named\nft_model.pth\nand config file with the same name as the training config file.\nYou still have to specify a reference audio file of the speaker you'd like to use during inference, similar to zero-shot usage.\nTODO📝\nRelease code\nRelease pretrained models:\nHuggingface space demo:\nHTML demo page:\nDemo\nStreaming inference\nReduce streaming inference latency\nDemo video for real-time voice conversion\nSinging voice conversion\nNoise resiliency for source audio\nPotential architecture improvements\nU-ViT style skip connections\nChanged input to OpenAI Whisper\nTime as Token\nCode for training on custom data\nFew-shot/One-shot speaker fine-tuning\nChanged to BigVGAN from NVIDIA for singing voice decoding\nWhisper version model for singing voice conversion\nObjective evaluation and comparison with RVC/SoVITS for singing voice conversion\nImprove audio quality\nNSF vocoder for better singing voice conversion\nFix real-time voice conversion artifact while not talking (done by adding a VAD model)\nColab Notebook for fine-tuning example\nReplace whisper with more advanced linguistic content extractor\nMore to be added\nAdd Apple Silicon support\nRelease paper, evaluations and demo page for V2 model\nKnown Issues\nOn Mac - running\nreal-time-gui.py\nmight raise an error\nModuleNotFoundError: No module named '_tkinter'\n, in this case a new Python version\nwith Tkinter support\nshould be installed. Refer to\nThis Guide on stack overflow\nfor explanation of the problem and a detailed fix.\nCHANGELOGS🗒️\n2024-04-16\nReleased V2 model for voice and accent conversion, with better anonymization of source speaker\n2025-03-03:\nAdded Mac M Series (Apple Silicon) support\n2024-11-26:\nUpdated v1.0 tiny version pretrained model, optimized for real-time voice conversion\nSupport one-shot/few-shot single/multi speaker fine-tuning\nSupport using custom checkpoint for webUI & real-time GUI\n2024-11-19:\narXiv paper released\n2024-10-28:\nUpdated fine-tuned 44k singing voice conversion model with better audio quality\n2024-10-27:\nAdded real-time voice conversion GUI\n2024-10-25:\nAdded exhaustive evaluation results and comparisons with RVCv2 for singing voice conversion\n2024-10-24:\nUpdated 44kHz singing voice conversion model, with OpenAI Whisper as speech content input\n2024-10-07:\nUpdated v0.3 pretrained model, changed speech content encoder to OpenAI Whisper\nAdded objective evaluation results for v0.3 pretrained model\n2024-09-22:\nUpdated singing voice conversion model to use BigVGAN from NVIDIA, providing large improvement to high-pitched singing voices\nSupport chunking and  streaming output for long audio files in Web UI\n2024-09-18:\nUpdated f0 conditioned model for singing voice conversion\n2024-09-14:\nUpdated v0.2 pretrained model, with smaller size and less diffusion steps to achieve same quality, and additional ability to control prosody preservation\nAdded command line inference script\nAdded installation and usage instructions\nAcknowledgements🙏\nAmphion\nfor providing computational resources and inspiration!\nVevo\nfor theoretical foundation of V2 model\nMegaTTS3\nfor multi-condition CFG inference implemented in V2 model\nASTRAL-quantiztion\nfor the amazing speaker-disentangled speech tokenizer used by V2 model\nRVC\nfor foundationing the real-time voice conversion\nSEED-TTS\nfor the initial idea",
    "readme": "# Seed-VC  \n[![Hugging Face](https://img.shields.io/badge/🤗%20Hugging%20Face-Demo-blue)](https://huggingface.co/spaces/Plachta/Seed-VC)  [![arXiv](https://img.shields.io/badge/arXiv-2411.09943-<COLOR>.svg)](https://arxiv.org/abs/2411.09943)\n\n*English | [简体中文](README-ZH.md) | [日本語](README-JA.md)*  \n\n[real-time-demo.webm](https://github.com/user-attachments/assets/86325c5e-f7f6-4a04-8695-97275a5d046c)\n\nCurrently released model supports *zero-shot voice conversion* 🔊 , *zero-shot real-time voice conversion* 🗣️ and *zero-shot singing voice conversion* 🎶. Without any training, it is able to clone a voice given a reference speech of 1~30 seconds.  \n\nWe support further fine-tuning on custom data to increase performance on specific speaker/speakers, with extremely low data requirement **(minimum 1 utterance per speaker)** and extremely fast training speed **(minimum 100 steps, 2 min on T4)**!\n\n**Real-time voice conversion** is support, with algorithm delay of ~300ms and device side delay of ~100ms, suitable for online meetings, gaming and live streaming.\n\nTo find a list of demos and comparisons with previous voice conversion models, please visit our [demo page](https://plachtaa.github.io/seed-vc/)🌐  and [Evaluaiton](EVAL.md)📊.\n\nWe are keeping on improving the model quality and adding more features.\n\n## Evaluation📊\nSee [EVAL.md](EVAL.md) for objective evaluation results and comparisons with other baselines.\n## Installation📥\nSuggested python 3.10 on Windows, Mac M Series (Apple Silicon) or Linux.\nWindows and Linux:\n```bash\npip install -r requirements.txt\n```\n\nMac M Series:\n```bash\npip install -r requirements-mac.txt\n```\n\nFor Windows users, you may consider install `triton-windows` to enable `--compile` usage, which gains speed up on V2 models:\n```bash\npip install triton-windows==3.2.0.post13\n```\n\n## Usage🛠️\nWe have released 4 models for different purposes:\n\n| Version | Name                                                                                                                                                                                                                       | Purpose                        | Sampling Rate | Content Encoder                                                        | Vocoder | Hidden Dim | N Layers | Params             | Remarks                                                |\n|---------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|---------------|------------------------------------------------------------------------|---------|------------|----------|--------------------|--------------------------------------------------------|\n| v1.0    | seed-uvit-tat-xlsr-tiny ([🤗](https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_uvit_tat_xlsr_ema.pth)[📄](configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml))                                                     | Voice Conversion (VC)          | 22050         | XLSR-large                                                             | HIFT    | 384        | 9        | 25M                | suitable for real-time voice conversion                |\n| v1.0    | seed-uvit-whisper-small-wavenet ([🤗](https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_seed_v2_uvit_whisper_small_wavenet_bigvgan_pruned.pth)[📄](configs/presets/config_dit_mel_seed_uvit_whisper_small_wavenet.yml)) | Voice Conversion (VC)          | 22050         | Whisper-small                                                          | BigVGAN | 512        | 13       | 98M                | suitable for offline voice conversion                  |\n| v1.0    | seed-uvit-whisper-base ([🤗](https://huggingface.co/Plachta/Seed-VC/blob/main/DiT_seed_v2_uvit_whisper_base_f0_44k_bigvgan_pruned_ft_ema.pth)[📄](configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml))       | Singing Voice Conversion (SVC) | 44100         | Whisper-small                                                          | BigVGAN | 768        | 17       | 200M               | strong zero-shot performance, singing voice conversion |\n| v2.0    | hubert-bsqvae-small ([🤗](https://huggingface.co/Plachta/Seed-VC/blob/main/v2)[📄](configs/v2/vc_wrapper.yaml))                                                                                                            | Voice & Accent Conversion (VC) | 22050         | [ASTRAL-Quantization](https://github.com/Plachtaa/ASTRAL-quantization) | BigVGAN | 512        | 13       | 67M(CFM) + 90M(AR) | Best in suppressing source speaker traits              |\n\nCheckpoints of the latest model release will be downloaded automatically when first run inference.  \nIf you are unable to access huggingface for network reason, try using mirror by adding `HF_ENDPOINT=https://hf-mirror.com` before every command.\n\nCommand line inference:\n```bash\npython inference.py --source <source-wav>\n--target <referene-wav>\n--output <output-dir>\n--diffusion-steps 25 # recommended 30~50 for singingvoice conversion\n--length-adjust 1.0\n--inference-cfg-rate 0.7\n--f0-condition False # set to True for singing voice conversion\n--auto-f0-adjust False # set to True to auto adjust source pitch to target pitch level, normally not used in singing voice conversion\n--semi-tone-shift 0 # pitch shift in semitones for singing voice conversion\n--checkpoint <path-to-checkpoint>\n--config <path-to-config>\n --fp16 True\n```\nwhere:\n- `source` is the path to the speech file to convert to reference voice\n- `target` is the path to the speech file as voice reference\n- `output` is the path to the output directory\n- `diffusion-steps` is the number of diffusion steps to use, default is 25, use 30-50 for best quality, use 4-10 for fastest inference\n- `length-adjust` is the length adjustment factor, default is 1.0, set <1.0 for speed-up speech, >1.0 for slow-down speech\n- `inference-cfg-rate` has subtle difference in the output, default is 0.7 \n- `f0-condition` is the flag to condition the pitch of the output to the pitch of the source audio, default is False, set to True for singing voice conversion  \n- `auto-f0-adjust` is the flag to auto adjust source pitch to target pitch level, default is False, normally not used in singing voice conversion\n- `semi-tone-shift` is the pitch shift in semitones for singing voice conversion, default is 0  \n- `checkpoint` is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface.(`seed-uvit-whisper-small-wavenet` if `f0-condition` is `False` else `seed-uvit-whisper-base`)\n- `config` is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface  \n- `fp16` is the flag to use float16 inference, default is True\n\nSimilarly, to use V2 model, you can run:\n```bash\npython inference_v2.py --source <source-wav>\n--target <referene-wav>\n--output <output-dir>\n--diffusion-steps 25 # recommended 30~50 for singingvoice conversion\n--length-adjust 1.0 # same as V1\n--intelligibility-cfg-rate 0.7 # controls how clear the output linguistic content is, recommended 0.0~1.0\n--similarity-cfg-rate 0.7 # controls how similar the output voice is to the reference voice, recommended 0.0~1.0\n--convert-style true # whether to use AR model for accent & emotion conversion, set to false will only conduct timbre conversion similar to V1\n--anonymization-only false # set to true will ignore reference audio but only anonymize source speech to an \"average voice\"\n--top-p 0.9 # controls the diversity of the AR model output, recommended 0.5~1.0\n--temperature 1.0 # controls the randomness of the AR model output, recommended 0.7~1.2\n--repetition-penalty 1.0 # penalizes the repetition of the AR model output, recommended 1.0~1.5\n--cfm-checkpoint-path <path-to-cfm-checkpoint> # path to the checkpoint of the CFM model, leave to blank to auto-download default model from huggingface\n--ar-checkpoint-path <path-to-ar-checkpoint> # path to the checkpoint of the AR model, leave to blank to auto-download default model from huggingface\n```\n\n\nVoice Conversion Web UI:\n```bash\npython app_vc.py --checkpoint <path-to-checkpoint> --config <path-to-config> --fp16 True\n```\n- `checkpoint` is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (`seed-uvit-whisper-small-wavenet`)\n- `config` is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface  \n\nThen open the browser and go to `http://localhost:7860/` to use the web interface.\n\nSinging Voice Conversion Web UI:\n```bash\npython app_svc.py --checkpoint <path-to-checkpoint> --config <path-to-config> --fp16 True\n```\n- `checkpoint` is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (`seed-uvit-whisper-base`)\n- `config` is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface  \n\nV2 model Web UI:\n```bash\npython app_vc_v2.py --cfm-checkpoint-path <path-to-cfm-checkpoint> --ar-checkpoint-path <path-to-ar-checkpoint>\n```\n- `cfm-checkpoint-path` is the path to the checkpoint of the CFM model, leave to blank to auto-download default model from huggingface\n- `ar-checkpoint-path` is the path to the checkpoint of the AR model, leave to blank to auto-download default model from huggingface\n- you may consider adding `--compile` to gain ~x6 speed-up on AR model inference  \n- \nIntegrated Web UI:\n```bash\npython app.py --enable-v1 --enable-v2\n```\nThis will only load pretrained models for zero-shot inference. To use custom checkpoints, please run `app_vc.py` or `app_svc.py` as above.  \nIf you have limited memory, remove `--enable-v2` or `--enable-v1` to only load one of the model sets.\n\nReal-time voice conversion GUI:\n```bash\npython real-time-gui.py --checkpoint-path <path-to-checkpoint> --config-path <path-to-config>\n```\n- `checkpoint` is the path to the model checkpoint if you have trained or fine-tuned your own model, leave to blank to auto-download default model from huggingface. (`seed-uvit-tat-xlsr-tiny`)\n- `config` is the path to the model config if you have trained or fine-tuned your own model, leave to blank to auto-download default config from huggingface  \n\n> [!IMPORTANT]\n> It is strongly recommended to use a GPU for real-time voice conversion.\n> Some performance testing has been done on a NVIDIA RTX 3060 Laptop GPU, results and recommended parameter settings are listed below:\n\n| Model Configuration             | Diffusion Steps | Inference CFG Rate | Max Prompt Length | Block Time (s) | Crossfade Length (s) | Extra context (left) (s) | Extra context (right) (s) | Latency (ms) | Inference Time per Chunk (ms) |\n|---------------------------------|-----------------|--------------------|-------------------|----------------|----------------------|--------------------------|---------------------------|--------------|-------------------------------| \n| seed-uvit-xlsr-tiny             | 10              | 0.7                | 3.0               | 0.18s          | 0.04s                | 2.5s                     | 0.02s                     | 430ms        | 150ms                         |\n\nYou can adjust the parameters in the GUI according to your own device performance, the voice conversion stream should work well as long as Inference Time is less than Block Time.  \nNote that inference speed may drop if you are running other GPU intensive tasks (e.g. gaming, watching videos)  \n\nExplanations for real-time voice conversion GUI parameters:\n- `Diffusion Steps` is the number of diffusion steps to use, in real-time case usually set to 4~10 for fastest inference;\n- `Inference CFG Rate` has subtle difference in the output, default is 0.7, set to 0.0 gains about 1.5x speed-up;\n- `Max Prompt Length` is the maximum length of the prompt audio, setting to a low value can speed up inference, but may reduce similarity to prompt speech;\n- `Block Time` is the time length of each audio chunk for inference, the higher the value, the higher the latency, note this value must be greater than the inference time per block, set according to your hardware condition;\n- `Crossfade Length` is the time length of crossfade between audio chunks, normally not needed to change;\n- `Extra context (left)` is the time length of extra history context for inference, the higher the value, the higher the inference time, but can increase stability;\n- `Extra context (right)` is the time length of extra future context for inference, the higher the value, the higher the inference time and latency, but can increase stability;\n\nThe algorithm delay is appoximately calculated as `Block Time * 2 + Extra context (right)`, device side delay is usually of ~100ms. The overall delay is the sum of the two.\n\nYou may wish to use [VB-CABLE](https://vb-audio.com/Cable/) to route audio from GUI output stream to a virtual microphone.  \n\n*(GUI and audio chunking logic are modified from [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI), thanks for their brilliant implementation!)*\n\n## Training🏋️\nFine-tuning on custom data allow the model to clone someone's voice more accurately. It will largely improve speaker similarity on particular speakers, but may slightly increase WER.  \nA Colab Tutorial is here for you to follow: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1R1BJTqMsTXZzYAVx3j1BiemFXog9pbQG?usp=sharing)\n1. Prepare your own dataset. It has to satisfy the following:\n    - File structure does not matter\n    - Each audio file should range from 1 to 30 seconds, otherwise will be ignored\n    - All audio files should be in on of the following formats: `.wav` `.flac` `.mp3` `.m4a` `.opus` `.ogg`\n    - Speaker label is not required, but make sure that each speaker has at least 1 utterance\n    - Of course, the more data you have, the better the model will perform\n    - Training data should be as clean as possible, BGM or noise is not desired\n2. Choose a model configuration file from `configs/presets/` for fine-tuning, or create your own to train from scratch.\n    - For fine-tuning, it should be one of the following:\n        - `./configs/presets/config_dit_mel_seed_uvit_xlsr_tiny.yml` for real-time voice conversion\n        - `./configs/presets/config_dit_mel_seed_uvit_whisper_small_wavenet.yml` for offline voice conversion\n        - `./configs/presets/config_dit_mel_seed_uvit_whisper_base_f0_44k.yml` for singing voice conversion\n3. Run the following command to start training:\n```bash\npython train.py \n--config <path-to-config> \n--dataset-dir <path-to-data>\n--run-name <run-name>\n--batch-size 2\n--max-steps 1000\n--max-epochs 1000\n--save-every 500\n--num-workers 0\n```\nwhere:\n- `config` is the path to the model config, choose one of the above for fine-tuning or create your own for training from scratch\n- `dataset-dir` is the path to the dataset directory, which should be a folder containing all the audio files\n- `run-name` is the name of the run, which will be used to save the model checkpoints and logs\n- `batch-size` is the batch size for training, choose depends on your GPU memory.\n- `max-steps` is the maximum number of steps to train, choose depends on your dataset size and training time\n- `max-epochs` is the maximum number of epochs to train, choose depends on your dataset size and training time\n- `save-every` is the number of steps to save the model checkpoint\n- `num-workers` is the number of workers for data loading, set to 0 for Windows    \n\nSimilarly, to train V2 model, you can run: (note that V2 training script supports multi-GPU training)\n```bash\naccelerate launch train_v2.py \n--dataset-dir <path-to-data>\n--run-name <run-name>\n--batch-size 2\n--max-steps 1000\n--max-epochs 1000\n--save-every 500\n--num-workers 0\n--train-cfm\n```\n\n4. If training accidentially stops, you can resume training by running the same command again, the training will continue from the last checkpoint. (Make sure `run-name` and `config` arguments are the same so that latest checkpoint can be found)\n\n5. After training, you can use the trained model for inference by specifying the path to the checkpoint and config file.\n    - They should be under `./runs/<run-name>/`, with the checkpoint named `ft_model.pth` and config file with the same name as the training config file.\n    - You still have to specify a reference audio file of the speaker you'd like to use during inference, similar to zero-shot usage.\n\n## TODO📝\n- [x] Release code\n- [x] Release pretrained models: [![Hugging Face](https://img.shields.io/badge/🤗%20Hugging%20Face-SeedVC-blue)](https://huggingface.co/Plachta/Seed-VC)\n- [x] Huggingface space demo: [![Hugging Face](https://img.shields.io/badge/🤗%20Hugging%20Face-Space-blue)](https://huggingface.co/spaces/Plachta/Seed-VC)\n- [x] HTML demo page: [Demo](https://plachtaa.github.io/seed-vc/)\n- [x] Streaming inference\n- [x] Reduce streaming inference latency\n- [x] Demo video for real-time voice conversion\n- [x] Singing voice conversion\n- [x] Noise resiliency for source audio\n- [ ] Potential architecture improvements\n    - [x] U-ViT style skip connections\n    - [x] Changed input to OpenAI Whisper\n    - [x] Time as Token\n- [x] Code for training on custom data\n- [x] Few-shot/One-shot speaker fine-tuning\n- [x] Changed to BigVGAN from NVIDIA for singing voice decoding\n- [x] Whisper version model for singing voice conversion\n- [x] Objective evaluation and comparison with RVC/SoVITS for singing voice conversion\n- [x] Improve audio quality\n- [ ] NSF vocoder for better singing voice conversion\n- [x] Fix real-time voice conversion artifact while not talking (done by adding a VAD model)\n- [x] Colab Notebook for fine-tuning example\n- [x] Replace whisper with more advanced linguistic content extractor\n- [ ] More to be added\n- [x] Add Apple Silicon support\n- [ ] Release paper, evaluations and demo page for V2 model\n\n## Known Issues\n- On Mac - running `real-time-gui.py` might raise an error `ModuleNotFoundError: No module named '_tkinter'`, in this case a new Python version **with Tkinter support** should be installed. Refer to [This Guide on stack overflow](https://stackoverflow.com/questions/76105218/why-does-tkinter-or-turtle-seem-to-be-missing-or-broken-shouldnt-it-be-part) for explanation of the problem and a detailed fix.\n\n\n## CHANGELOGS🗒️\n- 2024-04-16\n    - Released V2 model for voice and accent conversion, with better anonymization of source speaker\n- 2025-03-03:\n    - Added Mac M Series (Apple Silicon) support\n- 2024-11-26:\n    - Updated v1.0 tiny version pretrained model, optimized for real-time voice conversion\n    - Support one-shot/few-shot single/multi speaker fine-tuning\n    - Support using custom checkpoint for webUI & real-time GUI\n- 2024-11-19:\n    - arXiv paper released\n- 2024-10-28:\n    - Updated fine-tuned 44k singing voice conversion model with better audio quality\n- 2024-10-27:\n    - Added real-time voice conversion GUI\n- 2024-10-25:\n    - Added exhaustive evaluation results and comparisons with RVCv2 for singing voice conversion\n- 2024-10-24:\n    - Updated 44kHz singing voice conversion model, with OpenAI Whisper as speech content input\n- 2024-10-07:\n    - Updated v0.3 pretrained model, changed speech content encoder to OpenAI Whisper\n    - Added objective evaluation results for v0.3 pretrained model\n- 2024-09-22:\n    - Updated singing voice conversion model to use BigVGAN from NVIDIA, providing large improvement to high-pitched singing voices\n    - Support chunking and  streaming output for long audio files in Web UI\n- 2024-09-18:\n    - Updated f0 conditioned model for singing voice conversion\n- 2024-09-14:\n    - Updated v0.2 pretrained model, with smaller size and less diffusion steps to achieve same quality, and additional ability to control prosody preservation\n    - Added command line inference script\n    - Added installation and usage instructions\n\n## Acknowledgements🙏\n- [Amphion](https://github.com/open-mmlab/Amphion) for providing computational resources and inspiration!\n- [Vevo](https://github.com/open-mmlab/Amphion/tree/main/models/vc/vevo) for theoretical foundation of V2 model\n- [MegaTTS3](https://github.com/bytedance/MegaTTS3) for multi-condition CFG inference implemented in V2 model\n- [ASTRAL-quantiztion](https://github.com/Plachtaa/ASTRAL-quantization) for the amazing speaker-disentangled speech tokenizer used by V2 model\n- [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI) for foundationing the real-time voice conversion\n- [SEED-TTS](https://arxiv.org/abs/2406.02430) for the initial idea\n",
    "author": "Plachtaa",
    "project": "seed-vc",
    "date": "2025-09-18"
}