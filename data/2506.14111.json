{
    "id": "2506.14111",
    "title": "Essential-Web v1.0: 24T tokens of organized web data",
    "summary": "Essential-Web v1.0 is a 24 trillion-word annotated dataset that includes twelve categories of taxonomy labels and can obtain high-quality subsets for multiple fields through simple SQL filtering.",
    "abstract": "Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace:this https URL",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Essential AI:Andrew Hojel,Michael Pust,Tim Romanski,Yash Vanjani,Ritvik Kapila,Mohit Parmar,Adarsh Chaluvaraju,Alok Tripathy,Anil Thomas,Ashish Tanwer,Darsh J Shah,Ishaan Shah,Karl Stratos,Khoi Nguyen,Kurt Smith,Michael Callahan,Peter Rushton,Philip Monk,Platon Mazarakis,Saad Jamal,Saurabh Srivastava,Somanshu Singla,Ashish Vaswani",
    "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
    ],
    "comments": "",
    "keypoint": "- ESSENTIAL-WEB V1.0 is a 24-trillion-token dataset with expressive and extensive metadata at a document-level, enabling both general and domain-specific filtering through a single interface.\n- Simple SQL filters over the taxonomy produce datasets competitive with the best open-source, web-based baselines on math (-8.0% relative to SOTA), code (+14.3%), STEM (+24.5%), and medical (+8.6%) domains.\n- A metric suite including normalized mutual information (NMI) for category independence, annotator agreement using Cohen’s κ, and domain-recall to measure high-value domain coverage was introduced.\n- EAI-Distill-0.5b is a 0.5B-parameter classifier that labels the entire dataset in ≈90k MI300x GPU-hours while remaining within 3%, 14%, 1% of the teacher model Qwen-2.5-32B-Instruct on annotator agreement, NMI, and domain-recall respectively.\n- The average inter-category NMI for Qwen2.5-32b-Instruct is 0.079 and 0.083 for random and STEM sets, indicating very weak dependence between different categories.\n- EAI-Distill-0.5b matches Qwen2.5-32b-Instruct within ±1pp recall while preserving the same thin data footprint (<1% of crawl for math, ≈5% for code with tight filters).\n- Shrinking to a 0.5B-parameter model, removing the prompt, and condensing generations boost throughput from 1.4 to 70 requests per second per GPU.\n- Average annotator κ across random and STEM sets of EAI-Distill-0.5b falls by <3% (0.74 → 0.72) and inter-category NMI stays <0.10, preserving label quality and low redundancy.\n- On an unseen 102M-document sample of Common Crawl, math and code recall remain within 1pp of Qwen2.5-32b-Instruct while keeping the same compact filtered sets (<5% of data).",
    "date": "2025-06-20",
    "paper": "arXiv:2506.14111v1  [cs.CL]  17 Jun 2025\nESSENTIAL-WEB V1.0: 24T tokens of organized web data\nEssential AI\nSan Francisco, CA\nresearch@essential.ai\nAbstract\nData plays the most prominent role in how language models acquire skills and knowledge. The\nlack of massive, well-organized pre-training datasets results in costly and inaccessible data\npipelines. We present ESSENTIAL-WEB V1.0, a 24-trillion-token dataset in which every docu-\nment is annotated with a twelve-category taxonomy covering topic, format, content complexity,\nand quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter\nmodel that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With\nnothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0%\nrelative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). ESSENTIAL-\nWEB V1.0 is available on HuggingFace: EssentialAI/essential-web-v1.0.\nDeduplicated, Filtered CC\n100TB\nTraining High-Recall Classifier\ncuration timeline: weeks to months\ntrain base math classifier\nrun inference\n100TB\nmanually inspect output\ncurate new training data\nretrain\n>100B high recall math\nESSENTIAL-WEB V1.0\n>1B distinct document labels\n100TB\nESSENTIAL-WEB V1.0 Approach\ncuration timeline: hours to days\nsubject == math\narbitrary subject\n>100B high recall math\nhigh recall dataset\nEssential AI (2025)\nContents\n1\nIntroduction\n3\n2\nRelated Work\n4\n2.1\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3\nTaxonomy\n5\n3.1\nFormal Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nDesiderata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.3\nDevelopment Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.4\nSelected Categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nDownstream Results\n6\n4.1\nExperimental Protocol\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.2\nMath . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3\nCode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.4\nMedical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n4.5\nSTEM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5\nDeveloping Taxonomy\n11\n5.1\nMeasuring Orthogonality, Correctness, & Expressivity\n. . . . . . . . . . . . . . . . . . . . . .\n11\n5.2\nTeacher Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n6\nRunning at Scale\n15\n6.1\nPerformance Considerations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n6.2\nDistillation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n6.3\nEAI-Distill-0.5b Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n7\nConclusion\n18\n7.1\nBroader impact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n7.2\nPossible Next Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA Appendix\n25\n2\n1\nIntroduction\nAmong innovations that can accelerate the path to more intelligent AI models, data innovations frequently outpace\nothers, such as architectures or optimizers [Kaplan et al., 2020, Beck et al., 2024, Bahri et al., 2024, Sorscher et al.,\n2023, Shen et al., 2024a, Liu et al., 2025]. A careful curation of the bytes consumed by large language models\n(LLMs) enables greater control over the skills they acquire. With pre-training datasets scaling to trillions of tokens,\na detailed examination of their content can be intimidating. Moreover, open-weight models rarely disclose the\ncomposition of their datasets, and the broader ecosystem is trending toward reduced transparency as pre-training\ndatasets continue to grow. This introduces challenges in reproducibility and auditing models – challenges that\nwill grow as models become more autonomous. An accessible and interpretable open-data ecosystem is therefore\nessential for training competitive models in the open.\nTraining Tokens (Trillions)\nModel Release Date\n0\n10\n20\n30\n40\nQ1 2023\nQ2 2023\nQ3 2023\nQ4 2023\nQ1 2024\nQ2 2024\nQ3 2024\nQ4 2024\nQ1 2025\nQ2 2025\nLlama 1\nQwen 2\nDeepSeek 3\nFigure 1: LLM pre-training dataset sizes over time.\nOpen-source pre-training datasets are divided into: (1) enormous, general-purpose datasets sorted by uninter-\npretable quality classifiers (2) smaller, domain-specific datasets curated with bespoke, complex pipelines. Both\ngeneral and domain-specific datasets are unstructured, difficult to explore, and difficult to iteratively improve.\nConstructing these datasets requires significant computational resources given the scale and complexity of the\ndata pipelines. Once the datasets are publicly released, their metadata rarely enables tuning beyond modifying\nclassifier thresholds.\nWe release ESSENTIAL-WEB V1.0, a 24-trillion-token dataset with expressive and extensive metadata at a\ndocument-level. This metadata includes subject matter, web page type, content complexity, and document quality.\nPractitioners can now rapidly and inexpensively curate new datasets by writing SQL-like filters that utilize\nthese metadata columns. Suppose a researcher wants to prepare a multi-billion-token chemistry corpus using\npublicly-available web data. Today, the researcher must first train a high-recall chemistry classifier, a task hindered\nby scarce labeled data. Then, the classifier is run across hundreds of millions of documents to recall sufficient\ndata. With ESSENTIAL-WEB V1.0, a researcher can filter for chemistry, skip low-quality web pages (ads, product\nlistings), and surface reasoning-dense documents — all with a query that takes under 15 minutes to write.\nTo construct ESSENTIAL-WEB V1.0, we take advantage of powerful open-weight LLMs to synthetically la-\nbel web documents with a 12-category taxonomy. We utilize these labels to train a more efficient classifier,\nEAI-Distill-0.5b, and run inference on 23.6B documents. Inference at this scale requires ≈90k AMD MI300x\nGPU-hours.4 We expect this one-off inference cost to be amortized as the community iterates on datasets and\nmethods utilizing ESSENTIAL-WEB V1.0.\n3Llama dataset sizes sourced from Touvron et al. [2023a], Touvron et al. [2023b], Llama Team [2024], Meta AI [2025]\n3Qwen dataset sizes sourced from Bai et al. [2023], Yang et al. [2024], Qwen [2025], Yang et al. [2025]\n3DeepSeek dataset sizes sourced from DeepSeek-AI [2024b], DeepSeek-AI [2024c], and DeepSeek-AI [2025]\n4The inference job ran on 512 AMD MI300x for about 1 week.\n3\nTo demonstrate the utility of ESSENTIAL-WEB V1.0, we construct simple filters to curate high-performing\ndatasets in math, web code, STEM, and medical domains. Our math dataset performs within 8.0% of SOTA and\nour web code, STEM, and medical datasets outperform SOTA by 14.3%, 24.5%, 8.6% respectively.\n2\nRelated Work\nMost web-scale datasets rely on Common Crawl, an 18-year crawl of more than 250 billion web pages [Common\nCrawl Foundation, 2025]. Research on Common Crawl data can be naturally grouped into (1) heuristic filtering\nand de-duplication (2) monolithic, model-based filtering (3) domain-focused, model-based filtering, and (4)\ntaxonomies for data curation.\nHeuristic filtering and de-duplication.\nC4 (160B tokens, 2019) pioneered heuristic filtering for Common\nCrawl. C4 proposes a processing pipeline to deduplicate and filter low quality text using document-level statistical\nheuristics [Raffel et al., 2023]. It was followed by an explosion of large-scale, open-source datasets and pipelines\nsuch as RefinedWeb (600B tokens), SlimPajama (600B tokens), Dolma-CC (3T tokens), and RedPajama (30T\ntokens) [Penedo et al., 2023, Weber et al., 2024a, Shen et al., 2024b, Soldaini et al., 2024].\nMonolithic, model-based filtering.\nIn 2024, we see the introduction of monolithic model-based classifiers\napplied after well-tuned heuristic filtering and de-duplication. FineWeb-Edu (1.3T tokens) filters documents\nwith an education-density classifier whose labels are bootstrapped from Llama-3-70B-Instruct [Penedo et al.,\n2024]. Similarly, DCLM-baseline (3.6T tokens) filters documents using a fastText classifier trained to detect high\ninstruction density [Li et al., 2025]. Nemotron-CC ensembles several quality classifiers into one score before\nsampling Common Crawl [Su et al., 2025]. While classifier-ranked datasets excel on broad language benchmarks,\nthey lag on math and code evaluations. Their minimal metadata, often just the URL and a handful of quality\nscores, makes post-hoc domain filtering with existing metadata almost impossible. Researchers therefore build\ndomain-specific processing pipelines to compensate.\nDomain-focused, model-based filtering.\nMath, for instance, is rare (less than 0.5% of Common Crawl)5 yet\nbeneficial for reasoning skills. OpenWebMath (12B tokens) builds a LaTeX detection pipeline, develops a custom\nHTML-to-text extractor to preserve mathematical formatting, and a math-detecting classifier [Paster et al., 2023].\nDeepSeek Math (120B tokens, not publicly released) iteratively trains a classifier to maximize recall of math\ndocuments resulting in a significantly larger dataset [Shao et al., 2024]. MegaMath (264B web-based math\ntokens) builds multi-step classification, extraction, and processing pipelines for math and code [Zhou et al., 2025].\nFineMath trains an initial classifier to recall math documents, re-extracts billions of pages with a math-specific\nextractor, and re-classifies these documents using a second, more powerful math classifier [Allal et al., 2025].\nMath is not the only domain that has been targeted: OpenCoder iteratively trains a classifier to detect web code\nand TheBlueScrubs-v1 trains a classifier to detect medical documents [Huang et al., 2025, Felipe et al., 2025].\nTaxonomies for curating training data.\nWettig et al. [2025] introduce taxonomies to structure web data. The\nauthors develop a two-category taxonomy that measures topic and format, train a classifier using synthetic labels\ngenerated by Llama-3.1-Instruct models, label 200B tokens of Common Crawl, and demonstrate the utility of a\ncombining model-based quality filters and a taxonomy. The authors introduce normalized mutual information to\nmeasure label and category redundancy. Using normalized mutual information, they show that embedding-based\nclusters mostly overlap with topical labels, not format ones.\nUnlike prior multi-trillion-token datasets, our work, ESSENTIAL-WEB V1.0 attaches a 12-field taxonomy to every\ndocument, enabling both general and domain-specific filtering through a single interface.\n2.1\nContributions\nWe make four contributions:\n1. ESSENTIAL-WEB V1.0 a deduplicated, 23.6B-document (24T token) corpus drawn from Common Crawl and\ntagged with EAI-TAXONOMY, a 12-field taxonomy, covering topic, web page type, content complexity, and\nquality.\n2. Downstream validation. Simple SQL filters over the taxonomy produce datasets competitive with the best\nopen-source, web-based baselines on math (-8.0% relative to SOTA), code (+14.3%), STEM (+24.5%), and\n5Calculated using our subject matter label, FDC, to filter for Mathematics.\n4\nmedical (+8.6%). Our results demonstrate that taxonomy-curated datasets are competitive with complex,\ndomain-specific pipelines with no domain-specific training.\n3. Taxonomy evaluation toolkit. We introduce a metric suite of normalized mutual information (NMI) to gauge\ncategory independence [Wettig et al., 2025], annotator agreement using a variant of Cohen’s κ to ensure clear\ndecision boundaries between category labels, and domain-recall to measure how well we recall high-value\ndomains.\n4. Efficient annotator model. We release EAI-Distill-0.5b a 0.5B-parameter classifier produced by fine-\ntuning Qwen2.5-0.5b-Instruct using labels from Qwen-2.5-32B-Instruct [Qwen, 2025]. It labels the\nentire dataset in ≈90k MI300x GPU-hours while remaining within 3%, 14%, 1% of the teacher on annotator\nagreement, NMI, and domain-recall.\n3\nTaxonomy\n3.1\nFormal Definition\nDefinition 1. A taxonomy is a finite set T = {C1, . . . , Ck} of categories. Each category Ci has a non-empty,\nfinite label set Li.\nFor a document d, the taxonomic annotation is:\nT(d) =\n\u0000(λ1, µ1), . . . , (λk, µk)\n\u0001\n,\nλi ∈Li, µi ∈\n\u0000Li \\ {λi}\n\u0001\n∪{⊥},\nwhere λi is the primary label for category Ci, µi is an optional secondary label distinct from λi (useful when a\ndocument legitimately fits two labels), and ⊥denotes abstention. All categories and label sets are fixed a priori,\nallowing us to train a single static classifier.\n3.2\nDesiderata\nA useful metaphor for approaching taxonomy development is to think of the categories as axes in a high-\ndimensional grid space. Each document lives on a coordinate in that space, namely the category/label pairs\nassigned to it. We argue that a well-designed, web-scale taxonomy should satisfy five desiderata:\n1. Orthogonality. Every category should contribute information that is largely independent of the others —\nanalogous to orthogonal axes in Euclidean space.\n2. Expressivity. The coordinate system must be sufficiently fine-grained that unions of category/label pairs\ncan assemble a wide variety of targeted subsets.\n3. Correctness. Expressive, orthogonal axes are useless if labels are arbitrary. Annotators, human or LLM,\nmust consistently assign the same labels to a document far more often than by chance.\n4. Efficiency. Scaling to Common Crawl demands a fast and high-performing classifier.\n5. Effectiveness. Ultimately, a taxonomy matters only if it delivers in downstream performance.\n3.3\nDevelopment Methodology\n1\nTaxonomy\nDevelopment\n2\nTeacher Model\nSelection\n3\nStudent Model\nDistillation\n4\nLarge-scale\nInference\n5\nDownstream\nPerformance\nCandidates:\n- Qwen2.5-32B-Inst.\n- Qwen2.5-72B-Inst.\n- DeepSeekV3\nMetrics:\n- orthogonality\n- expressivity\n- correctness\nPerformance:\n3% relative drop in\nannotator κ\nEfficiency:\nmax throughput\nprocess 23.6B docs\nCurate datasets for:\n- math\n- web code\n- medical\n- STEM\nFigure 2: Overview of our five-stage methodology for developing and deploying EAI-TAXONOMY.\nThe development of EAI-TAXONOMY and EAI-Distill-0.5b (Figure 2) are discussed in the following sections:\n5\n• Downstream Results (Section 4, p. 6) compares datasets for math, web code, medical, and STEM curated\nusing EAI-TAXONOMY against top-performing, open-source datasets in each domain.\n• Measuring Orthogonality, Correctness, & Expressivity (Section 5.1, p. 11) defines the metrics used to\nevaluate orthogonality, correctness, and expressivity and introduces held-out evaluation sets.\n• Teacher Model Selection (Section 5.2, p. 12) evaluates top performing open-source LLMs and motivates\nthe selection of Qwen2.5-32b-Instruct as a teacher model.\n• Running at Scale (Section 6, p. 15) discusses the inference optimizations, training recipe, and performance\nevaluation of EAI-Distill-0.5b, the student model.\n3.4\nSelected Categories\nEach web page receives labels for 12 categories across 5 logical groupings, which we refer to as EAI-TAXONOMY.\nIn Table 1, we briefly introduce our categories and explain what each tries to capture, see Appendix A.3 for more\ndetails. Many of these categories are inspired by or directly taken from existing work in the open-source data\ncuration community.6 ESSENTIAL-WEB V1.0 has 14.1M unique combinations of primary labels and 1.2B unique\ncombinations of primary / secondary labels across all 23.6B docs.\nDescription\nCategories\nFDC. Free Decimal Correspondence is a public-domain\nanalogue of the Dewey Decimal System.7 Labels sub-\nject matter hierarchically.\nLevel 1: broad topic (ex: 5 - Science)\nLevel 2: fine topic (ex: 51 - Mathematics)\nLevel 3: very fine topic (ex: 512 - Algebra)\nBloom. Educational-objective taxonomy.\nCognitive Process: mental effort required\nKnowledge Domain: content abstraction\nDocument Type. Web page types.\nV1: broad types\nV2: fine types\nContent Quality. Measures rigor and target audience.\nReasoning Depth: thinking steps\nEducational Level: reader background\nTechnical Correctness: correctness\nExtraction. Crawl artifacts and text extraction errors.\nExtraction Artifacts: HTML extraction errors\nMissing Content: from scrape or extraction\nTable 1: Taxonomy categories and their descriptions.\n4\nDownstream Results\nWe show that taxonomy-based datasets with no domain-specific training are competitive with bespoke pipelines\nbuilt to target math, web code, medical and STEM data.\n4.1\nExperimental Protocol\nIn line with recent work such as DeepSeekMath, SmolLM2, and MegaMath, we anneal \"pre-trained\" models with\ndomain-specific datasets to evaluate performance [Shao et al., 2024, Allal et al., 2025, Zhou et al., 2025]. We train\ntwo 2.3B parameters transformer models for 320B tokens, 8× the Chinchilla compute-optimal ratio [Hoffmann\net al., 2022].8 To evaluate domain-specific datasets, we decay the learning rate of one of the base models to zero\nwhile training on 80B tokens of the new dataset. We also anneal on the original data mix to provide a baseline.\nThe rationale behind starting with a base model is that it improves signal on difficult benchmarks such as MMLU\n6The Free Decimal Correspondance was designed by Ockerbloom [2010]. The Bloom taxonomy was designed by Anderson\nand Krathwohl [2001]. Document Type V2 was designed by Wettig et al. [2025]. Reasoning Depth and Technical Correctness\nwere designed by Yuan et al. [2025]. Education Level was designed by Penedo et al. [2024].\n7\"Dewey,\" \"Dewey Decimal,\" \"Dewey Decimal Classification,\" and \"DDC\" are trademarks of OCLC.\n8When calculating Pareto, we ignore the size of the tied embedding/un-embedding matrix, which is 295M parameters.\n6\nLearning Rate\nTokens Consumed (B tokens)\n3 × 10−5\n3 × 10−6\n320\n400\n2\nGeneral Training\nDomain-Specific Annealing\nFigure 3: We train two base models: (1) General Base (100% web data) and (2) Code Base (50% web data, 50%\ncode data) for 320B tokens. We then evaluate all domain-specific EAI-TAXONOMY and top-performing,\nweb-curated datasets by annealing one of the two base models for 80B tokens.\nand GSM8K, as opposed to training on each dataset from scratch for 80B tokens. After annealing, each model has\nseen 400B tokens, 10× Chinchilla Pareto.\nWe vary the data composition of the two base models, while keeping all other training hyper-parameters fixed.\nThe model architecture, configuration, and training hyper-parameters can be found in Appendix A.6.\n• General-Base: DCLM-baseline [Li et al., 2025]\n• Code-Base: 50% DCLM-baseline + 50% Python from Stack v2 Dedup [Lozhkov et al., 2024]\nDCLM-baseline.\nDCLM-baseline is a 3.6T token pre-training dataset based on Common Crawl. It is de-\nduplicated, heuristically-filtered, and labeled using a model-based classifier. For classification, the authors train a\nfastText classifier on instruction-formatted data from OpenHermes 2.5 and r/ExplainLikeImFive subreddit\n[Teknium, 2023]. DCLM-baseline is curated by selecting the top 10% of documents after de-duplication and\nheuristic filters based on this classifier score [Li et al., 2025].9 The HuggingFace dataset card notes the dataset is\nnot intended for domains such as code and math.\nStack v2.\nThe Stack v2 is a dataset of 104.2M github repositories collected from Software Heritage [Software\nHeritage Foundation, 2025]. We use the Minhash LSH deduplicated version [Lozhkov et al., 2024].10 We restrict\ntraining to the Python subsect because HumanEval+ and MBPP+ are Python only [Chen et al., 2021, Austin et al.,\n2021, Liu et al., 2023].\nTable 17 and Table 24 in the Appendix contain HuggingFace links for our datasets and public datasets used in this\nsection, respectively.\nDecontamination.\nAll datasets were decontaminated with a 13-gram Bloom filter that normalizes text and\npunctuation before white-space tokenization. The following evals were decontaminated against: GSM8K, MATH,\nHumanEval, MBPP, MMLU, CareQA, MedMCQA, MedQA-USMLE, and PubMedQA.\nEvaluation framework.\nWe run evaluations using lm-eval-harness [Gao et al., 2024]. See Appendix A.4\nfor details.\n9DCLM-baseline can be found on HuggingFace: mlfoundations/dclm-baseline-1.0.\n10Stack v2 dedup can be found on HuggingFace: bigcode/the-stack-v2-dedup.\n7\n4.2\nMath\nThere has been extensive work in the open source community to develop high-quality math datasets [Paster et al.,\n2023, Allal et al., 2025, Zhou et al., 2025, Shao et al., 2024]. We compare the performance of taxonomy-based\nmath datasets with all top performing, public math datasets that are curated from Common Crawl. A list of datasets\ntested and the domain-specific steps used during preparation can be found in Table 2.11 We use General-Base\nfor the annealing experiments. To evaluate performance, we run 8-shot GSM8K, 4-shot MATH, and 5-shot\nMMLU-Math, which consists of all MMLU subtasks related to math [Hendrycks et al., 2021].12\nDataset\nSize (B tok)\nMath extract\nMath classifier\nDomain–specific curation notes\nFineMath 3+\n32\n✓\n✓\n118M math classifer on 7.1B docs\nOpenWebMath\n12\n✓\n✓\nmath fastText\nMegaMath Web (Top 10%)\n30\n✓\n✓\niteratively trained math fastText\nEAI-TAXONOMY Top Math\n29\n✗\n✗\nEAI-TAXONOMY filter\nEAI-TAXONOMY Math w/ FM\n34\n✗\n✓\nEAI-TAXONOMY filter;\nFineMath classifier on 116M docs\nTable 2: Overview of top-performing, open-source math datasets and taxonomy-based math datasets along with\nbrief summary of effort to curate. \"Math extract\" denotes a domain-specific HTML-to-plaintext extraction was\nused. \"Math classifier\" indicates a domain-specific classifier was used.\n4.2.1\nTaxonomy-Based Math Datasets\nWe prepare two taxonony-based math datasets. EAI-TAXONOMY Top Math (29B tokens) targets high-quality\nmath documents that exhibit reasoning and are technically correct, the full filter can be found in Algorithm 2 in Ap-\npendix A.7. EAI-TAXONOMY Math w/ FM (34B tokens) filters for any document labeled as 51 - Mathematics.\nWe then label all 116M recalled documents with the FineMath Classifier13 and filter for the top 34B tokens so that\nthe size is comparable to FineMath 3+ (Algorithm 3).\n4.2.2\nDownstream Math Results\nDataset\nGSM8K\nMATH\nMMLU–Math\nFineMath 3+\n26.4%±1.4\n11.7%±0.4\n32.3%±1.5\nOpenWebMath\n14.6%±1.1\n9.3%±0.4\n29.9%±1.5\nMegaMath Web (Top 10%)\n9.8%±0.9\n7.9%±0.3\n29.9%±1.5\nDCLM-baseline\n4.8%±0.7\n4.4%±0.3\n27.0%±1.4\nEAI-TAXONOMY Top Math\n21.3%±1.3\n11.0%±0.4\n30.5%±1.5\nEAI-TAXONOMY Math w/ FM\n22.4%±1.3\n11.5%±0.4\n30.9%±1.5\nTable 3: Model performance (mean ± standard error) on GSM8K, Hendrycks Math, and MMLU-Math\nbenchmarks.\nFineMath 3+ achieves the highest score in GSM8K, with EAI-TAXONOMY Top Math and EAI-TAXONOMY\nMath w/ FM datasets trailing by -4.0pp (15.2%) and -5.1pp (19.3%) respective. On MATH and MMLU-Math,\nEAI-TAXONOMY Math w/ FM performs within standard error of FineMath 3+. Other curated sets (MegaMath\nWeb, OpenWebMath) lag both FineMath and our taxonomy splits on GSM8K and MATH. The full results to the\nexperiments can be found in Table 3. FineMath, MegaMath, and OpenWebMath are domain-specific datasets\nwith complex pipelines to maximize performance on mathematics benchmarks. The FineMath classifier targets\n“high school and early undergraduate levels” of mathematics, which directly caters to GSM8K and MATH. The\n11We acknowledge that the total dataset preparation cost of the taxonomy is much higher than any of these individual datasets\nbecause we run a 500M parameter transformer to classify 23.6B documents. However, we only focus on domain-specific\ncuration costs in Table 2.\n12MMLU-Math: MMLU Abstract Algebra, MMLU College Mathematics, MMLU Elementary Mathematics, MMLU High\nSchool Mathematics, MMLU High School Statistics\n13A 118M parameter transformer-based classifier trained on synthetic labels generated by LLama3-70B-Instruct. More\ndetails can be found on HuggingFace: HuggingFaceTB/finemath-classifier.\n8\nperformance of EAI-TAXONOMY Top Math comes from a simple semantic filter. EAI-TAXONOMY Math w/\nFM takes advantage of the FineMath classifier by recalling a small, high-density subset of Common Crawl to\nreclassify.\nTo understand the composition of FineMath 3+, we annotate it using the taxonomy. We find that only 61.9% of\nFineMath 3+ documents are labeled with FDC 51 - Mathematics, as primary or secondary label. By inspecting\nthe FDC categories of the remaining documents in FineMath 3+, we find other common subject matters in the\ndataset: 53 - Physics, 33 - Economics, and 621 - Applied Physics.\n4.3\nCode\nRecent code LMs mix execution-ready code with code tutorials, documentation, and API docs recalled from web\ndata [Hui et al., 2024, DeepSeek-AI, 2024a, Huang et al., 2025]. We curate two web code datasets using the\ntaxonomy and compare them to the only openly available, large-scale web code dataset: OpenCoder FineWeb\nCode Corpus (OpenCoder FW) [Huang et al., 2025]. OpenCoder FW is a 49B token dataset curated with a\nfastText classifier iteratively trained to maximize math and code recall.14 We anneal web code datasets (Table 4)\nfrom Code-Base to get signal on code knowledge and code generation evals. During annealing, we train on a\n1:1 mixture of the dataset being evaluated and Python from the Stack-Edu [Allal et al., 2025]. To measure code\ngeneration performance, we run 0-shot HumanEval+ and 3-shot MBPP++.15 To gauge code-related knowledge,\nwe run 5-shot MMLU-CS, which consists of all MMLU subtasks related to Computer Science.16\nDataset\nSize (B tok)\nCode classifier\nDomain–specific curation notes\nOpenCoder FW\n49\n✓\niteratively-trained math and web code fastText\nEAI-TAXONOMY Top Code\n145\n✗\nEAI-TAXONOMY filter\nEAI-TAXONOMY Code w/ DCLM\n564\n✗\nEAI-TAXONOMY filter; DCLM fastText\nTable 4: Overview of existing SOTA open-source web code datasets and taxonomy-based web code datasets along\nwith brief summary of effort to curate. \"Code classifier\" indicates a domain-specific classifier was used to detect\ncode.\n4.3.1\nTaxonomy-Based Web Code Datasets\nWe prepare EAI-TAXONOMY Top Code (145B tokens) to target high-quality code documentation that is technically\ncorrect and exhibits intermediate to advanced reasoning (Algorithm 4). In addition, we construct EAI-TAXONOMY\nCode w/ DCLM (564B tokens), which combines taxonomy filters targeting code documentation and the DCLM\nclassifier, at the same threshold as DCLM-baseline, to filter for instruction-dense documents (Algorithm 5). We\nadd 51 - Mathematics to EAI-TAXONOMY Code w/ DCLM because OpenCoder FW targets both web code\nand mathematics.\n4.3.2\nDownstream Code Results\nAcross the code-generation evaluations, all datasets perform within standard error of each other (Table 5). We see\nan absolute +15.0pp (46.8%) improvement in the MMLU-CS score from DCLM-baseline to EAI-TAXONOMY\nCode w/ DCLM. Both DCLM-baseline and taxonomy-based code datasets outperform OpenCoder FW on MMLU-\nCS. However, there is a clear impact on general code knowledge when using a taxonomy-curated web code\ndataset.\n4.4\nMedical\nTheBlueScrubs-v1 (24B tokens) is the only public, curated Common Crawl medical corpus. The authors train\na logistic regression classifier to detect medical documents and use it to filter SlimPajama, a 627B web dataset\n14The classifier is trained with a strategy similar to DeepSeek Math [Shao et al., 2024]. The authors use 500,000 LLM-scored\ncode / math documents as an initial training set [Huang et al., 2025].\n15HumanEval+ and MBPP++ are variants of HumanEval and MBPP with more challenging test cases and corrected\nground-truth solutions [Chen et al., 2021, Austin et al., 2021, Liu et al., 2023]. When running 0-shot MBPP+, the model\nstruggles to properly output the termination condition (\"[DONE]\") in the default lm-eval-harness config given it isn’t lack\nof explanation in the prompt. Therefore, we run with 3-shot to provide in-context examples of the termination condition.\n16MMLU-CS: MMLU College Compute Science, MMLU High School Computer Science, MMLU Computer Security\n9\nWeb-Code Dataset\nHumanEval+\nMBPP+\nMMLU–CS\nDCLM-baseline\n28.0%±3.5\n45.5%±2.6\n32.0%±2.7\nOpenCoder FW\n26.2%±3.4\n45.8%±2.6\n27.7%±2.6\nEAI-TAXONOMY Top Code\n27.4%±3.5\n46.6%±2.6\n29.0%±2.6\nEAI-TAXONOMY Code w/ DCLM\n28.7%±3.5\n45.0%±2.6\n47.0%±2.9\nTable 5: Pass@1 accuracy (mean ± standard error) on 0-shot HumanEval+, 3-shot MBPP+, and accuracy on the\nMMLU computer-science subset.\n[Felipe et al., 2025]. We compare TheBlueScrubs-v1 against DCLM-baseline and taxonomy-based medical\ndatasets by annealing General-Base. We evaluate medical performance using 3-shot PubMedQA, 5-shot CareQA-\nen, 5-shot MedMCQA, 5-shot MedQA-USMLE-4-options, and 5-shot MMLU-Med [Jin et al., 2019, Arias-Duart\net al., 2025, Jin et al., 2020, Pal et al., 2022].17\nDataset\nSize (B tok)\nMedical classifier\nDomain–specific curation notes\nTheBlueScrubs-v1\n24\n✓\nlogistic regression to detect medical\nEAI-TAXONOMY Med\n433\n✗\nEAI-TAXONOMY filter\nEAI-TAXONOMY Med w/ DCLM\n205\n✗\nEAI-TAXONOMY filter; DCLM fastText\nTable 6: Overview of existing open-source medical datasets and taxonomy-based medical datasets along with brief\nsummary of effort to curate. \"Medical classifier\" indicates a domain-specific classifier was used to detect medical.\n4.4.1\nTaxonomy-Based Medical Dataset\nWe prepare the EAI-TAXONOMY Med (433B tokens) to target scientific medical documents that exhibit reasoning\nand are technically correct (Algorithm 6). We then apply the DCLM classifier, at the same threshold as DCLM-\nbaseline, to the EAI-TAXONOMY Med to construct EAI-TAXONOMY Med w/ DCLM (205B tokens, Algorithm 7).\n4.4.2\nDownstream Medical Results\nModel\nCareQA-en\nMedMCQA\nMedQA-USMLE\nPubMedQA\nMMLU–Med\nDCLM-baseline\n26.9%±0.6\n31.6%±0.7\n25.9%±1.2\n70.6%±2.0\n31.0%±1.5\nTheBlueScrubs-v1\n25.1%±0.6\n32.2%±0.7\n25.3%±1.2\n69.2%±2.1\n25.7%±1.4\nTaxonomy Medical\n27.7%±0.6\n32.5%±0.7\n28.1%±1.3\n67.0%±2.1\n29.5%±1.5\nTaxonomy Medical w/ DCLM\n31.5%±0.6\n32.7%±0.7\n30.1%±1.3\n68.6%±2.1\n39.2%±1.6\nTable 7: Accuracy (mean ± standard error) on four medical QA benchmarks and the MMLU medical subset.\nAcross medical evaluations, EAI-TAXONOMY Med w/ DCLM either achieves the best performance or performs\nwithin standard error of the best dataset (Table 7). Both taxonomy-based medical datasets are able to perform above\nrandom chance (≈25%) on MedQA-USMLE, where DCLM-baseline and TheBlueScrubs-v1 are unable to do so.\nTheBlueScrubs-v1 is also unable to perform above chance on CareQA-en. Across all evals, EAI-TAXONOMY\nMed w/ DCLM beats DCLM-baseline by +3.2pp (8.6%) and TheBlueScrubs-v1 by +4.9pp (13.8%).\n4.5\nSTEM\nIn addition to evaluating EAI-TAXONOMY in domains with existing, multi-billion-token baseline datasets curated\nfrom Common Crawl, we also select a domain where we wish there was a large-scale dataset available. Given\nthe importance of STEM domains for reasoning and benchmarking performance of LLMs, we curate a large,\nSTEM-specific dataset. We select two high-performing, general datasets as baselines: DCLM-baseline and\n17MMLU-Med: MMLU Anatomy, MMLU Clinical Knowledge, MMLU College Biology, MMLU College Medicine,\nMMLU Medical Genetics, MMLU Professional Medicine. These categories were grouped for medical evaluation by Singhal\net al. [2022]\n10\nFineWeb-Edu [Penedo et al., 2024]. We benchmark the performance of each dataset (Table 8) by annealing\nGeneral-Base.\nDataset\nSize (T tok.)\nDomain–specific curation notes\nDCLM-baseline\n3.65\ninstruction density classifier (DCLM fastText)\nFineWeb-Edu\n1.27\neducational quality classifier\nEAI-TAXONOMY STEM\n1.74\nEAI-TAXONOMY filter\nEAI-TAXONOMY STEM w/ DCLM\n0.91\nEAI-TAXONOMY filter, DCLM fastText\nTable 8: Overview of high-perfoming general web datasets and taxonomy-based stem dataset.\n4.5.1\nTaxonomy-Based STEM Dataset\nWe prepare the EAI-TAXONOMY STEM (1742B tokens) targeting science, engineering, medical, and computer\nscience documents. We select high quality document types per sub-topic and filter for documents that exhibit\nreasoning (Algorithm 8). We then apply the DCLM classifier, at the same threshold as DCLM-baseline, to the\nEAI-TAXONOMY STEM dataset to construct EAI-TAXONOMY w/ DCLM (912B tokens, Algorithm 9).\n4.5.2\nDownstream STEM Results\nModel\nMMLU–STEM\nDCLM-baseline\n27.7%±0.8\nFineWeb-Edu\n26.7%±0.8\nTaxonomy STEM\n29.1%±0.8\nTaxonomy STEM w/ DCLM\n34.5%±0.8\nTable 9: Accuracy (% ± standard error) on the MMLU–STEM subset.\nEAI-TAXONOMY STEM is able to outperform DCLM-baseline and FineWeb-Edu beyond standard error on\nMMLU-STEM. EAI-TAXONOMY w/ DCLM outperforms DCLM-baseline by +6.8pp (24.5%) and FineWeb-Edu\nby +7.8pp (29.2%).\n5\nDeveloping Taxonomy\n5.1\nMeasuring Orthogonality, Correctness, & Expressivity\nIn this section, we introduce methods to measure: (1) orthogonality, (2) correctness, and (3) expressivity.\n5.1.1\nOrthogonality: Category Independence (NMI)\nFor two categories Ci, Cj, let the random variables X, Y denote their empirical primary label codes. Redundancy\nis measured by the normalized mutual information\nNMI(X, Y ) =\n2 I(X; Y )\nH(X) + H(Y ),\nI(X; Y ) =\nX\nx,y\npxy log pxy\npxpy\n.\nwhere H is Shannon entropy. NMI = 0 indicates statistical independence, NMI = 1 perfect duplication. This\nmetric was proposed by Wettig et al. [2025] to evaluate taxonomies.\n5.1.2\nCorrectness: Annotator Agreement (annotator κ)\nTo gauge label clarity we compare a candidate model M with two gold annotators (GPT-4o [OpenAI, 2024] and\nClaude Sonnet-3.5 [Anthropic, 2024]) via a variant of Cohen’s κ [Cohen, 1960].\n11\nAnnotation format.\nEach annotator outputs an ordered set\nS ∈{∅, {ℓ}, {ℓ1, ℓ2}}.\nUnder the taxonomy (Definition 1) a primary label is mandatory; the empty set ∅arises only when a model’s raw\noutput is malformed and cannot be parsed. Such issues are negligible for high-capacity LLMs but more frequent\nfor small ones. Two annotations agree iff Sa ∩Sb ̸= ∅or Sa ∪Sb = ∅.\nObserved and expected agreement.\nLet Po be the empirical agreement rate over held-out documents. The\nexpected agreement Pe is computed by assuming label sets are constructed independently from each annotator’s\nempirical fertility distribution (probability of emitting 0/1/2 labels, where the 0-case captures parse failures) and\ntheir label-choice distribution (Appendix A.8.4). Then κ is computed as usual:\nκ = Po −Pe\n1 −Pe\n,\n−1 ≤κ ≤1.\nWe report the mean of the two gold-vs-M scores per category (annotator κ); a high κ implies unambiguous label\ndefinitions.\n5.1.3\nExpressivity: Domain–Recall\nSome rare domains (math, code) drive downstream performance. To measure how well a classifier surfaces such\nniche material we introduce domain–recall score inspired by recent works that iteratively train a classifier to\nmaximize recall of a specific domain [Shao et al., 2024]:\n1. Select a small set U = {u1, . . . , um} of human-vetted base URLs that are human-judged ≥90% in-domain.\nAll documents whose URL begins with any u ∈U form the domain positives D+ ⊂D.\n2. Apply a classifier (taxonomy filter, fastText classifier, etc.) that returns a subset of documents bD ⊆D.\nThe recall of the classifier on this topic is\nRecall = | bD ∩D+ |\n|D+|\n.\nA higher recall value means the strategy retrieves more of the trusted in-domain pages. We report recall alongside\nthe data kept fraction | bD|/|D| to indicate the overall data volume returned by the classifier.\n5.1.4\nHeld-out Evaluation Sets\nTo evaluate the categories in EAI-TAXONOMY (Section 3.4), we report the normalized mutual information and\nannotator κ on the following evaluation sets. Both evaluations are sampled from the deduplicated and heuristically-\nfiltered Common Crawl used to prepare ESSENTIAL-WEB V1.0 (Appendix A.2). Neither set was seen during\nfine-tuning.\n1. Random Set: 2,017 randomly sampled documents.\n2. STEM Set: 871 STEM documents 18\nIn addition, we report domain-recall for web code and math domains. We label a set of 104.6M documents with\nEAI-TAXONOMY using Qwen2.5-32b-Instruct [Qwen, 2025]. Details about the annotation can be found in\nAppendix A.12. Domain-recall is then calculated on documents from two sets of human-vetted \"gold\" URL sets\n(Appendix A.9.1):\n1. Web Code: 30 base-URLs; |D+| = 330,934 documents\n2. Math: 42 base-URLs; |D+| = 16,199 documents\n5.2\nTeacher Model Selection\nWhen selecting a teacher model, we evaluate Qwen2.5-32b-Instruct, Qwen2.5-72b-Instruct, and\nDeepSeek-V3 using NMI (orthogonality), annotator κ (correctness), and domain-recall (expressivity). We\nseek to maximize performance and inference efficiency of the teacher model.\n18See Appendix A.9.2 for details about the STEM evaluation set.\n12\n5.2.1\nAnnotator κ Results\nWe report annotator κ between powerful open source LLMs (DeepSeek-V3 [DeepSeek-AI, 2025],\nQwen2.5-72b-Instruct, and Qwen2.5-32b-Instruct) and our two gold annotators (GPT-4o, Claude\nSonnet 3.5) for all 12 categories of EAI-TAXONOMY on the random and STEM evaluation sets in Table 10. A\ndiscussion justifying the use of LLMs as gold annotators instead of humans can be found in Appendix A.8.3.\nCategory\nDeepSeek-V3\nQwen 2.5-72B-Inst.\nQwen 2.5-32B-Inst.\nRandom\nSTEM\nRandom\nSTEM\nRandom\nSTEM\nBloom Knowledge Domain\n0.69± 0.02\n0.64± 0.03\n0.46± 0.02\n0.39± 0.03\n0.62± 0.02\n0.68± 0.02\nBloom Cognitive Process\n0.76± 0.01\n0.76± 0.02\n0.67± 0.02\n0.70± 0.02\n0.73± 0.01\n0.79± 0.02\nDocument Type V1\n0.90± 0.01\n0.91± 0.01\n0.88± 0.01\n0.91± 0.01\n0.86± 0.01\n0.89± 0.01\nFree Decimal Corr. (level 1)\n0.92± 0.01\n0.95± 0.01\n0.90± 0.01\n0.92± 0.01\n0.88± 0.01\n0.92± 0.01\nFree Decimal Corr. (level 2)\n0.86± 0.01\n0.87± 0.01\n0.83± 0.01\n0.81± 0.01\n0.81± 0.01\n0.81± 0.01\nFree Decimal Corr. (level 3)\n0.71± 0.01\n0.70± 0.01\n0.67± 0.01\n0.63± 0.01\n0.64± 0.01\n0.60± 0.01\nExtraction Artifacts\n0.81± 0.02\n0.86± 0.03\n0.57± 0.02\n0.53± 0.03\n0.74± 0.01\n0.65± 0.03\nMissing Content\n0.83± 0.01\n0.85± 0.02\n0.63± 0.01\n0.65± 0.02\n0.66± 0.02\n0.65± 0.02\nDocument Type V2\n0.89± 0.01\n0.89± 0.01\n0.80± 0.01\n0.80± 0.01\n0.85± 0.01\n0.83± 0.01\nEducation Level\n0.89± 0.01\n0.86± 0.02\n0.82± 0.01\n0.81± 0.02\n0.88± 0.01\n0.85± 0.02\nReasoning Depth\n0.75± 0.01\n0.72± 0.02\n0.70± 0.01\n0.67± 0.02\n0.67± 0.02\n0.67± 0.02\nTechnical Correctness\n0.60± 0.02\n0.61± 0.02\n0.52± 0.01\n0.60± 0.02\n0.52± 0.01\n0.51± 0.02\nOverall mean\n0.80\n0.80\n0.70\n0.70\n0.74\n0.74\nTable 10: Annotator κ (± standard error) between each candidate model and two gold annotators (GPT-4o,\nClaude Sonnet-3.5) on the random (n=2,017) and STEM (n=871) evaluation sets.\nObservations.\nDeepSeek-V3 achieves the highest average pairwise Cohen’s κ for both random and STEM\nof 0.80. However, the 671B-parameter Mixture-of-Experts is expensive to serve for large-scale inference.19\nQwen2.5-32b-Instruct provides faster inference and is within 0.06 (7.5%) of DeepSeek-V3’s performance,\nachieving 0.74 for the random set and STEM set, outperforming its larger sibling Qwen2.5-72b-Instruct. We\nalso measure the performance of Qwen2.5-14B-Instruct and find that random and STEM annotator κ drop\nto 0.53 and 0.52 (see Appendix A.10.1 for full table). We select Qwen2.5-32b-Instruct to label the training\nset of EAI-Distill-0.5b given its balance of fast inference speed and high annotator κ (see Table 12 for an\nanalysis of inference performance).\n5.2.2\nInter-Category NMI Results\nIn Figure 4, we report the inter-category NMI values as a heatmap. The NMI heatmaps for DeepSeek-V3 and\nQwen2.5-72b-Instruct are in the Appendix A.10.2 and average inter-category NMIs can be found in Table 14.\nObservations.\nThe average inter-category NMI for Qwen2.5-32b-Instruct is 0.079 and 0.083 for random\nand STEM.20 This value indicates very weak dependence between different categories of EAI-TAXONOMY.\nIn addition, this value is in line with the reported inter-category NMI of 0.10 in the 2-category taxonomy\ndefined by Wettig et al. [2025]. The average NMI for Qwen2.5-32b-Instruct, Qwen2.5-72b-Instruct, and\nDeepSeek-V3 are all under 0.10 for both random and STEM. Qwen2.5-32b-Instruct has the lowest average\nNMI out of the three tested open-source models (Table 14).\n5.2.3\nDomain–Recall Results\nTable 11 shows how effectively different filtering strategies retrieve documents from two human-vetted \"gold\"\nURL sets: web code (30 base-URLs; 330,934 documents) and math (42 base-URLs; 16,199 documents) drawn\nfrom a 104M-document Common Crawl sample. To provide a baseline we train two fastText classifiers using\nhigh quality math and code documents as target sets, with random samples of Common Crawl as negative sets\n19At the time of annotation, the SGLang image for DeepSeek-V3 on AMD MI300x was much slower. See Table 12 and\nAppendix A.12.1 [Zheng et al., 2024].\n20When calculating the average inter-category NMI we omit Document Type V2, given its similar purpose to Document\nType V1, and Free Decimal Correspondence Level 1 and Level 2 to avoid double-counting FDC hierarchical splits.\n13\n(a) random NMI\n(b) STEM NMI\nFigure 4: Comparison of NMI heatmaps on random and STEM eval sets for Qwen2.5-32B-Instruct\n[Bojanowski et al., 2017]. The fastText classifiers should be considered simple baselines given we do not apply\ntechniques such as iterative recall bootstrapping or BPE tokenization to improve recall performance [Shao et al.,\n2024, Huang et al., 2025]. Details on the training of the fastText classifiers can be found in Appendix A.9.3.\nDomain\nFilter\nClassifier\nRecall (%)\nData kept (%)\nWeb Code\nFDC ∈004,005∗& Doc Type V1†\nEAI-TAXONOMY filter\n94.9\n4.80\nFDC ∈004,005∗\nEAI-TAXONOMY filter\n96.5\n7.50\nscore > 0.01\nfastText web code\n25.2\n6.40\nMath\nFDC ∈51 Mathematics\nEAI-TAXONOMY filter\n98.0\n0.50\nscore > 0.5\nfastText math\n74.6\n3.80\nscore > 0.1\nfastText math\n98.5\n47.9\nTable 11: Domain–recall for web code and math of Qwen2.5-32b-Instruct-annotated EAI-TAXONOMY in\ncomparison to basic fastText classifiers. ∗FDC codes 004 and 005 denote Computer Science and Software\nEngineering. †Document Type V1 filters for \"Code/Software\", \"Reference/Encyclopedic/Educational\", and\n\"Social/Forum\".\nCode.\nThe Free Decimal Correspondence (FDC) alone recalls 96.5% of vetted code pages while discarding\n92.5% of the web. Adding Document Type V1 constraint trims 35.9% of volume of data kept with only 1.6pp loss\nin recall of code pages. The baseline fastText model performs significantly worse recalling only 25.2% of target\nweb code documents.\nMath.\nTagging with the single FDC level 2 code 51 - Mathematics surfaces 98.0% of vetted math pages\nwhile keeping just 0.5% of the corpus—an efficient high-recall, low-volume filter. Lowering thresholds of the\nfastText can match recall (98.5%) but at the cost of retaining nearly half the crawl resulting in a much lower\ndensity of math documents in the returned set of documents.\nThe hierarchical Free Decimal Correspondence, combined with the Document Type V1 category tags, yields\nvery high recall for specialized domains at modest data volumes, outperforming simple, topic-specific fastText\nclassifiers with no need for domain-specific training.\n5.2.4\nTeacher Model Selection Summary\n1. Model choice. Given its annotator κ–speed trade-off, we adopt Qwen-2.5-32B-Instruct as the teacher for\nEAI-Taxonomy-0.5B.\n2. Low redundancy. Inter-category NMI of <0.10 on both random and STEM evaluation sets indicates that\nEAI-TAXONOMY captures largely orthogonal signals.\n14\n3. High label clarity. Qwen2.5-32b-Instruct reaches an annotator κ of 0.74 vs. two powerful gold LLM\nannotators.\n4. Efficient domain targeting. Simple EAI-TAXONOMY filters recall >96% of vetted math and code pages\nwhile retaining <5% of Common Crawl, far denser than fastText baselines.\n6\nRunning at Scale\nUsing a sample of the 104.6M Common Crawl documents labeled with Qwen2.5-32b-Instruct (Ap-\npendix A.12), we fine-tune Qwen2.5-0.5b-Instruct to perform the taxonomy classification task.\nThe\nfine-tuned classifier, EAI-Distill-0.5b, achieves 50 times faster inference speed relative to prompting\nQwen2.5-32b-Instuct while maintaining performance across NMI, annotator κ, and domain-recall for math\nand web code.\n6.1\nPerformance Considerations\nTo annotate at a billion-document scale, we optimize three levers to maximize throughput: (1) reducing model\nsize, (2) shorter generations, and (3) context distillation. Table 12 summarizes speedups.\nModel\nPrompt Tokens\nGeneration Tokens\nRPS/GPU\nSpeed ∆\nDeepSeek-V3 (old image)\nOriginal\nOriginal\n0.14\n0.10\nDeepSeek-V3\nOriginal\nOriginal\n0.54\n0.39\nQwen2.5-72B\nOriginal\nOriginal\n0.62\n0.44\nQwen2.5-32B\nOriginal\nOriginal\n1.40\n1.00\nNone\nCondensed\n6.30\n4.50\nQwen2.5-0.5B\nOriginal\nOriginal\n5.07\n3.62\nNone\nOriginal\n5.46\n3.90\nOriginal\nCondensed\n50.91\n36.36\nNone\nCondensed\n70.28\n50.20\nNone\nEmbedding\n189.23\n135.16\nTable 12: Inference performance comparison. Requests per second per GPU are measured with prefix caching\nenabled. Speed ∆is calculated relative to Qwen2.5-32B with the original prompt which is provided in\nAppendix A.13.3. The italicized row indicates the configuration used by EAI-Distill-0.5b. When we were\nannotating the sample of 104M document, the SGLang image for DeepSeek-V3 was much slower as shown in the\ntable. Embedding signifies using a classification-head instead of token generation.\nMeasuring performance.\nTo compare performance of different model sizes and prefill/generation strategies,\nwe report requests per second per GPU (RPS/GPU). Given the large batch workload, we care exclusively about\nmaximizing throughput (total number of documents processed per second). To calculate the effects of design\ndecisions on performance, we calculate the number of tokens in the shared prefix, average document, and average\ngeneration.21 We use vLLM for inference of the Qwen2.5 models and SGLang for DeepSeek-V3 [Zheng et al.,\n2024, Kwon et al., 2023].\nCondensing model size.\nWe select the smallest model in the Qwen2.5 model family. We decide to use a\npre-trained small LM to take advantage of strong priors developed during training.22. When maintaining the\noriginal prompt and generation format with prefix caching enabled, we only see a 3.6 × increase in RPS/GPU.\n21These calculation were done solely using the prompt from Appendix A.13.3 given we annotated the 104.6M Common\nCrawl documents in two passes to add additional categories before fine-tuning (Appendix A.12). Therefore, these are lower\nbounds on the true gains given the Original prompt and Original generation only encapsulate 9 categories, whereas the\nCondensed generation contains all 12 categories.\n22Recently, even smaller pre-trained LMs have been released such as SmolLM2-135M-Instruct, which could potentially\nfurther improve inference performance [Allal et al., 2025]\n15\nCondensing generation tokens.\nWe are able to extract and condense the output generated by\nQwen2.5-32b-Instruct programatically before fine-tuning. By reducing the average generation tokens from\n791 to 51, the RPS/GPU of Qwen2.5-0.5b increase by 10 ×.\nContext distillation.\nWe remove the prompt during finetuning, which increases the RPS/GPU of Qwen2.5-0.5b\nby 1.4 × [Snell et al., 2022]. This speed increase is in spite of the fact that we have prefix caching enabled during\ninference.\nWhy not use a classification head?\nAs a first iteration, we opted to condense the number of generated tokens\ninstead of relying on a classification-head. However, there are many potential benefits to using a classification head\nsuch as persisting a document-embedding that can be re-used and faster inference (2.7 × increase in RPS/GPU\nover the configuration used for EAI-Distill-0.5b). We hope to look into this in the future.\n6.2\nDistillation\nTo train the Qwen2.5-0.5b-Instruct model, we fine-tune the model on 82B tokens of documents synthetically\nlabeled by Qwen2.5-32b-Instruct. We perform context distillation and condense the number of generated\ntokens. See Appendix A.13.2 for the output format of EAI-Distill-0.5b. The loss is computed only on the\nQwen2.5-32b-Instruct’s completion tokens; the input document, chat template, and system prompt are masked\nout during loss calculation. The hyper-parameters used for fine-tuning the model can be found in Table 32,\nAppendix A.11.2. An ablation of prompt/generation formatting, learning rate, and token budget can be found in\nAppendix A.11.3. The ablations find that there is little-to-no degradation from context distillation and generation\ncondensation. We also find that we achieve similar annotator κ across categories with as little as 12B training\ntokens. EAI-Distill-0.5b is available on HuggingFace: EssentialAI/eai-distill-0.5b.\n6.3\nEAI-Distill-0.5b Performance\nWe evaluate the student model, EAI-Distill-0.5b, against its teacher, Qwen2.5-32b-Instruct, along three\nmetrics: NMI (orthogonality), annotator κ (correctness), and domain recall (expressivity).\n6.3.1\nAnnotation Consistency (κ)\nObservations.\nEAI-Distill-0.5b’s annotator κ decreases by 0.03 (4.1%) and 0.01 (1.4%) relative to\nQwen2.5-32b-Instruct (Table 13). We observe consistent performance across the two models in both Bloom\ncategories, Document Type V1/V2, and the Free Decimal Correspondence. EAI-Distill-0.5b exhibits stronger\nperformance than its teacher model on Reasoning Depth and Technical Correctness and weaker performance\non Extraction Artifacts, Missing Content, and Education Level. We provide in depth analysis of these shifts in\nAppendix A.8.2.\n6.3.2\nInter-Category NMI Results\nThe average inter-category NMI of EAI-Distill-0.5b is 0.092 and 0.093 for random and STEM, increasing by\n0.013 (16.5%) and 0.01 (12.0%) relative to Qwen2.5-32b-Instruct. However, EAI-Distill-0.5b remains\ncomparable to other powerful open-source LLMs and stays below 0.10, indicating low redundancy. See Table 14\nfor the average inter-category NMI of all models tested and Figure 5 for heatmaps with the relative change in\ninter-category NMI from Qwen2.5-32b-Instruct to EAI-Distill-0.5b.\n6.3.3\nDomain–Recall on Unseen 100M Document Slice\nWhen calculating domain-recall for EAI-Distill-0.5b, we sample a fresh set 102.6M unseen documents from\nCommon Crawl because a subset of the data used to calculate domain-recall in Section 5.2.3 was used to train\nEAI-Distill-0.5b. The size of D+ for math and web code for the new set of 102.6M documents are are:\n• Web Code: |D+| = 266,051 documents\n• Math: |D+| = 12,835 documents\n16\nCategory\nQwen2.5-32B-Inst.\nEAI-Distill-0.5b\nRandom\nSTEM\nRandom\nSTEM\nBloom Knowledge Domain\n0.62± 0.02\n0.68± 0.02\n0.63± 0.02\n0.65± 0.03\nBloom Cognitive Process\n0.73± 0.01\n0.79± 0.02\n0.70± 0.02\n0.76± 0.02\nDocument Type V1\n0.86± 0.01\n0.89± 0.01\n0.83± 0.01\n0.86± 0.01\nFree Decimal Corr. (level 1)\n0.88± 0.01\n0.92± 0.01\n0.88± 0.01\n0.93± 0.01\nFree Decimal Corr. (level 2)\n0.81± 0.01\n0.81± 0.01\n0.81± 0.01\n0.80± 0.01\nFree Decimal Corr. (level 3)\n0.64± 0.01\n0.60± 0.01\n0.63± 0.01\n0.61± 0.01\nExtraction Artifacts\n0.74± 0.01\n0.65± 0.03\n0.27± 0.02\n0.37± 0.03\nMissing Content\n0.66± 0.02\n0.65± 0.02\n0.48± 0.01\n0.57± 0.02\nDocument Type V2\n0.85± 0.01\n0.83± 0.01\n0.88± 0.01\n0.86± 0.01\nEducation Level\n0.88± 0.01\n0.85± 0.02\n0.79± 0.02\n0.86± 0.02\nReasoning Depth\n0.67± 0.02\n0.67± 0.02\n0.87± 0.01\n0.76± 0.02\nTechnical Correctness\n0.52± 0.01\n0.51± 0.02\n0.72± 0.01\n0.75± 0.02\nOverall mean\n0.74\n0.74\n0.71\n0.73\nTable 13: Annotator κ (± standard error) against gold annotators on the random (n = 2,017) and STEM\n(n = 871) evaluation sets, after fine-tuning. We did not report Qwen2.5-0.5b-Instruct because it achieves an\nannotator κ of 0.00 for both random and STEM evaluation sets.\nModel\nAverage NMI\nRandom\nSTEM\nClaude Sonnet-3.5\n0.083\n0.085\nGPT-4o\n0.063\n0.084\nDeepSeek-V3\n0.093\n0.099\nQwen2.5-72b-Instruct\n0.092\n0.103\nQwen2.5-32b-Instruct\n0.079\n0.083\nEAI-Distill-0.5b\n0.092\n0.093\nTable 14: Average inter-category NMI results for random and STEM evaluation sets. We omit Document Type\nV1, given its similar purpose to Document Type V2, and Free Decimal Correspondence Level 1 and Level 2 to\navoid double-counting FDC hierarchical splits.\n(a) Random\n(b) STEM\nFigure 5: Change in inter-category NMI per category from Qwen2.5-32b-Instruct to EAI-Distill-0.5b. A\nhigher value indicates an increase in inter-category co-occurrence.\n17\nDomain\nFilter\nModel\nRecall (%)\nData kept (%)\nWeb Code\nFDC ∈004,005 & Doc Type V1\nEAI-Distill-0.5b\n95.8\n4.91\nFDC ∈004,005 & Doc Type V1\nQwen2.5-32b-Instruct\n94.9\n4.79\nFDC ∈004,005\nEAI-Distill-0.5b\n97.1\n7.61\nFDC ∈004,005\nQwen2.5-32b-Instruct\n96.5\n7.47\nMath\nFDC ∈51 Mathematics\nEAI-Distill-0.5b\n97.6\n0.52\nFDC ∈51 Mathematics\nQwen2.5-32b-Instruct\n98.0\n0.50\nTable 15: Domain–recall for web code and math domains. FDC codes 004 and 005 correspond to Computer\nScience and Software Engineering. The Document Type V1 filters for \"Code/Software\",\n\"Reference/Encyclopedic/Educational\", and \"Social/Forum\". Qwen2.5-32b-Instruct measured on the original\n104.6M-document corpus; numbers shown for reference.\nObservations. EAI-Distill-0.5b model matches Qwen2.5-32b-Instruct within ±1pp recall while preserv-\ning the same thin data footprint (<1% of crawl for math, ≈5% for code with tight filters) as seen in Table 15.23\n6.3.4\nEAI-Distill-0.5b Performance Summary\n1. 50× faster annotation. Shrinking to a 0.5B-parameter model, removing the prompt, and condensing genera-\ntions boost throughput from 1.4 to 70 requests per second per GPU.\n2. Minimal quality loss. Average annotator κ across random and STEM sets of EAI-Distill-0.5b falls by\n<3% (0.74 →0.72) and inter-category NMI stays <0.10, preserving label quality and low redundancy.\n3. Domain coverage intact. On an unseen 102M-document sample of Common Crawl, math and code recall\nremain within 1pp of Qwen2.5-32b-Instruct while keeping the same compact filtered sets (<5% of data).\n7\nConclusion\nESSENTIAL-WEB V1.0 delivers 24 trillion tokens of web data with document-level annotations spanning subject,\npage type, content complexity, and quality. Applying simple filters on these labels produce competitive corpora\nfor mathematics, code, medicine, and STEM in minutes, not months — competing with or surpassing specialist\ndatasets while avoiding domain-specific training.\nBuilding such a corpus required three technical ingredients: (1) A principled metric suite of NMI for redundancy,\nannotator κ for label clarity, and domain-recall for domain-specific filtering to guide taxonomy design. (2) A fast\nannotator, EAI-Distill-0.5b, that retains the teacher’s quality (<3% drop in annotator κ, <1pp drop in recall\nof math and code, average inter-category NMI below 0.10) yet runs 50 × faster, making billion-page labeling\npractical. (3) A transparent release with all data and EAI-Distill-0.5b available on HuggingFace, enabling\nreproducibility and iteration.\n7.1\nBroader impact\nStructured web data transforms corpus curation from an complex, expensive processing pipeline into a search\nproblem that anyone can solve. We hope ESSENTIAL-WEB V1.0 becomes a community commons: a foundation\nothers can refine, audit, or curate in new ways, accelerating open research on LLM training data, arguably the\nmost valuable, yet least shared, asset contributing to modern LLM capabilities.\n7.2\nPossible Next Steps\nTaxonomies serve as an excellent bridge to build an interpretable coordinate system on data, despite their biases. In\naddition to using our ESSENTIAL-WEB V1.0 for organic and synthetic data curation, we see new opportunities to\nautomatically curate data to train large neural networks at scale. Whether taxonomies will remain a core element\nof state-of-the-art data pipelines or merely serve as a stepping-stone en route to fully unsupervised methods is still\nan open question.\n23Because Qwen2.5-32b-Instruct was evaluated on a different sample of Common Crawl, small drift is expected;\nnevertheless, parity holds. We did not re-evaluate Qwen2.5-32b-Instruct on the new sample given the cost to relabel\n102.6M documents.\n18\nReferences\nLoubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall,\nAndrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb\nFahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka,\nMathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big –\ndata-centric training of a small language model, 2025. URL https://arxiv.org/abs/2502.02737.\nL.W. Anderson and D.R. Krathwohl. A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom’s\nTaxonomy of Educational Objectives. Longman, 2001.\nAnthropic.\nClaude 3.5 Sonnet Model Card Addendum.\nModel card, Anthropic, June 2024.\nURL\nhttps://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_\nClaude_3_Addendum.pdf. Accessed 23 May 2025.\nAnna Arias-Duart, Pablo Agustin Martin-Torres, Daniel Hinjos, Pablo Bernabeu-Perez, Lucia Urcelay Ganzabal,\nMarta Gonzalez Mallo, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Sergio Alvarez-Napagao, and\nDario Garcia-Gasulla. Automatic evaluation of healthcare llms beyond question-answering, 2025. URL\nhttps://arxiv.org/abs/2502.06666.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021.\nURL https://arxiv.org/abs/2108.07732.\nYasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws.\nProceedings of the National Academy of Sciences, 121(27), June 2024. ISSN 1091-6490. doi: 10.1073/pnas.\n2311878121. URL http://dx.doi.org/10.1073/pnas.2311878121.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming\nLu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,\nShijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng\nYang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang,\nZhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023.\nURL https://arxiv.org/abs/2309.16609.\nMaximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp,\nGünter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory,\n2024. URL https://arxiv.org/abs/2405.04517.\nJanek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search Engine for the\nClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi, and Benjamin Piwowarski,\neditors, Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018), Lecture\nNotes in Computer Science, Berlin Heidelberg New York, March 2018. Springer.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword\ninformation, 2017. URL https://arxiv.org/abs/1607.04606.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such,\nDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William\nSaunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec\nRadford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained\non code, 2021.\nJacob Cohen. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37\n– 46, 1960. URL https://api.semanticscholar.org/CorpusID:15926286.\nYann Collet. xxhash, 2025. URL https://xxhash.com.\n19\nCommon Crawl Foundation. Common Crawl Corpus. https://commoncrawl.org, 2025. all shards uo to\nCC-MAIN-2024-38.\nDeepSeek-AI. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence, 2024a.\nURL https://arxiv.org/abs/2406.11931.\nDeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism, 2024b. URL https:\n//arxiv.org/abs/2401.02954.\nDeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024c. URL\nhttps://arxiv.org/abs/2405.04434.\nDeepSeek-AI. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437.\nAlvan R Feinstein and Domenic V Cicchetti. High agreement but low kappa: I. the problems of two paradoxes.\nJournal of clinical epidemiology, 43(6):543–549, 1990.\nLuis Felipe, Carlos Garcia, Issam El Naqa, Monique Shotande, Aakash Tripathi, Vivek Rudrapatna, Ghulam\nRasool, Danielle Bitterman, and Gilmer Valdes. Thebluescrubs-v1, a comprehensive curated medical dataset\nderived from the internet, 2025. URL https://arxiv.org/abs/2504.02874.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa,\nJason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish\nThite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL\nhttps://zenodo.org/records/12608602.\nGemma 2 Team. Gemma 2: Improving open language models at a practical size, 2024. URL https://arxiv.\norg/abs/2408.00118.\nGemma 3 Team. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring massive multitask language understanding, 2021. URL https://arxiv.org/abs/2009.03300.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland,\nKatherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén\nSimonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. An empirical analysis of compute-\noptimal large language model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and\nA. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 30016–30030. Cur-\nran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\nc1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf.\nSiming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, Jiaheng Liu, Chenchen\nZhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi,\nYinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models, 2025.\nURL https://arxiv.org/abs/2411.04905.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen\nYu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo\nMiao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin.\nQwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does\nthis patient have? a large-scale open domain question answering dataset from medical exams, 2020. URL\nhttps://arxiv.org/abs/2009.13081.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. PubMedQA: A dataset for\nbiomedical research question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567–2577, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1259. URL\nhttps://aclanthology.org/D19-1259/.\n20\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https:\n//arxiv.org/abs/2001.08361.\nAS Kolesnyk and NF Khairova. Justification for the use of cohen’s kappa statistic in experimental studies of nlp\nand text mining. Cybernetics and Systems Analysis, 58(2):280–288, 2022.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao\nZhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention,\n2023. URL https://arxiv.org/abs/2309.06180.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha,\nSedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat,\nMayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina,\nAmro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah\nPratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi\nChandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong\nOh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang,\nDirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair\nCarmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation\nof training sets for language models, 2025. URL https://arxiv.org/abs/2406.11794.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really\ncorrect? rigorous evaluation of large language models for code generation, 2023. URL https://arxiv.org/\nabs/2305.01210.\nJingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe\nLu, Junjie Yan, Yanru Chen, Huabin Zheng, Yibo Liu, Shaowei Liu, Bohong Yin, Weiran He, Han Zhu,\nYuzhi Wang, Jianzhou Wang, Mengnan Dong, Zheng Zhang, Yongsheng Kang, Hao Zhang, Xinran Xu,\nYutao Zhang, Yuxin Wu, Xinyu Zhou, and Zhilin Yang. Muon is scalable for llm training, 2025. URL\nhttps://arxiv.org/abs/2502.16982.\nLlama Team. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang,\nDmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes\nBelkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal,\nJia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß,\nNaman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru\nTang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu,\nBinyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu,\nTorsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa\nPatwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas\nWolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the stack v2: The next generation,\n2024. URL https://arxiv.org/abs/2402.19173.\nMeta AI. The llama 4 herd: The beginning of a new era of natively multimodal ai innovation, April 2025. URL\nhttps://ai.meta.com/blog/llama-4-multimodal-intelligence/. Accessed: 2025-06-15.\nJohn Mark Ockerbloom.\nFree decimal correspondence.\nhttps://everybodyslibraries.com/\nfree-decimal-correspondence/, August 2010. Released 19 August 2010; dedicated to the public domain\n(CC0).\nOpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject\nmulti-choice dataset for medical domain question answering. In Gerardo Flores, George H Chen, Tom Pollard,\nJoyce C Ho, and Tristan Naumann, editors, Proceedings of the Conference on Health, Inference, and Learning,\nvolume 174 of Proceedings of Machine Learning Research, pages 248–260. PMLR, 07–08 Apr 2022. URL\nhttps://proceedings.mlr.press/v174/pal22a.html.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of\nhigh-quality mathematical web text, 2023. URL https://arxiv.org/abs/2310.06786.\n21\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming\ncurated corpora with web data, and web data only, 2023. URL https://arxiv.org/abs/2306.01116.\nGuilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Lean-\ndro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale,\n2024. URL https://arxiv.org/abs/2406.17557.\nQwen. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides,\nSarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin\nCassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang,\nAmelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina\nHiggins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lor-\nraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun\nTerzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James\nBradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart,\nSimon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett,\nDemis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &\ninsights from training gopher, 2022. URL https://arxiv.org/abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023. URL\nhttps://arxiv.org/abs/1910.10683.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nY. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language\nmodels, 2024. URL https://arxiv.org/abs/2402.03300.\nXuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. Scaling laws for linear complexity\nlanguage models, 2024a. URL https://arxiv.org/abs/2406.16690.\nZhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel\nHestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations\nfor llm training, 2024b. URL https://arxiv.org/abs/2309.10818.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay\nTanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly,\nNathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S.\nCorrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle\nBarral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode\nclinical knowledge, 2022. URL https://arxiv.org/abs/2212.13138.\nCharlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context, 2022. URL https://arxiv.org/\nabs/2209.15189.\nSoftware Heritage Foundation. Software Heritage Archive. https://archive.softwareheritage.org, 2025.\nSnapshot accessed 10 June 2025.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin,\nKhyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy,\nXinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal\nNam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant\nSubramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy,\nDirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model\npretraining research, 2024. URL https://arxiv.org/abs/2402.00159.\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural scaling laws:\nbeating power law scaling via data pruning, 2023. URL https://arxiv.org/abs/2206.14486.\n22\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad\nShoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into a refined long-horizon\npretraining dataset, 2025. URL https://arxiv.org/abs/2412.02595.\nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL https:\n//huggingface.co/datasets/teknium/OpenHermes-2.5.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a. URL https:\n//arxiv.org/abs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya\nChen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,\nVedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.\nLlama 2: Open foundation and fine-tuned chat models, 2023b. URL https://arxiv.org/abs/2307.09288.\nMaurice Weber, Daniel Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong\nLyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max\nRyabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open dataset for\ntraining large language models, 2024a. URL https://arxiv.org/abs/2411.12372.\nMaurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong\nLyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max\nRyabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and Ce Zhang. Redpajama: an open dataset for\ntraining large language models. NeurIPS Datasets and Benchmarks Track, 2024b.\nAlexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. Organize the\nweb: Constructing domains enhances pre-training data curation, 2025. URL https://arxiv.org/abs/2502.\n10341.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang\nLin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang,\nRu Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu\nLiu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren,\nXuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nZhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\nHuang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan\nLin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou,\nJunyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li,\nPei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang,\nWenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang\nZhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388.\nWeizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong\nTian, Jason E Weston, and Xian Li. Naturalreasoning: Reasoning in the wild with 2.8m challenging questions,\n2025. URL https://arxiv.org/abs/2502.13124.\nLianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos\nKozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of\nstructured language model programs, 2024. URL https://arxiv.org/abs/2312.07104.\n23\nFan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong Liu, and Eric P.\nXing. Megamath: Pushing the limits of open math corpora, 2025. URL https://arxiv.org/abs/2504.\n02807.\n24\nA\nAppendix\nA.1\nContributions and Acknowledgments\nCore Contributors\nAndrew Hojel∗\nMichael Pust∗\nTim Romanski\nYash Vanjani\nRitvik Kapila\nMohit Parmar\nAshish Vaswani\nContributors\nAdarsh Chaluvaraju\nAlok Tripathy\nAnil Thomas\nAshish Tanwer\nDarsh J Shah\nIshaan Shah\nKarl Stratos\nKhoi Nguyen\nKurt Smith\nMichael Callahan\nPeter Rushton\nPhilip Monk\nPlaton Mazarakis\nSaad Jamal\nSaurabh Srivastava\nSomanshu Singla\nA.2\nESSENTIAL-WEB V1.0: Dataset Overview\nWe begin with DCLM Pool (89 resiliparse-extracted Common Crawl WARC snapshots from CC-MAIN-2013-20\nto CC-MAIN-2022-49 [Common Crawl Foundation, 2025, Li et al., 2025]. We then extract 12 additional snapshots\nfrom CC-MAIN-2023-06 to CC-MAIN-2024-38 using resiliparse [Bevendorff et al., 2018].\nPost-extraction, we process the data using the following steps:\n1. Generate document ids using xxhash.xxh3_64_intdigest(document) [Collet, 2025]\n2. Globally deduplicate all 101 snapshots of Common Crawl using the hash-based document identifier.\n3. Minhash LSH de-duplication at a snapshot-level. We run Minhash LSH with a target Jaccard threshold\nof 0.7, using 14 bands and 9 rows per band. Running at a snapshot-level was inspired by Penedo et al.\n[2024] and DeepSeek-AI [2024b].\n4. Annotate every document with statistical and model-based quality signals using a variant of the\nRedPajama-Data-V2 processing pipeline [Weber et al., 2024b]. The model-based signals include\nthe DCLM-baseline fastText classifier [Li et al., 2025].\n5. Filter out low quality documents and only keep English, while maximally allowing math and code, using\nmanually tuned quality signal filters (see Algorithm 1). The filters and starting values for tuning were\ninspired by Penedo et al. [2024], Weber et al. [2024b], and Rae et al. [2022].24\n6. Label every document with EAI-TAXONOMY using EAI-Distill-0.5b (Section 6).\nSee Table 16 for the stage-by-stage removal rates.\n*Equal contribution.\n24In the future, we hope to remove this step or automatically tune the quality signal thresholds to minimize the human-in-\nthe-loop.\n25\nAlgorithm 1: Quality Signal Filter Rules\n# Rule 1: Initial Quality Filters (applied to ALL documents)\nRULE_1_CONDITIONS = [\nword_count < 50,\nfrac_chars_top_2gram > 0.20,\nfrac_chars_top_3gram > 0.18,\nfrac_chars_dupe_10grams > 0.50,\nfrac_chars_dupe_9grams > 0.52,\nfrac_chars_dupe_8grams > 0.54,\nfrac_chars_dupe_7grams > 0.56,\nfrac_chars_dupe_6grams > 0.58,\nfrac_chars_dupe_5grams > 0.60\n]\n# Rule 2: Bypass Conditions (if ANY is true, skip Rule 3)\nRULE_2_CONDITIONS = [\nml_math_score > 0.3,\nml_web_code_score > 0.3\n]\n# Rule 3: Additional Filters (only if Rule 2 bypass fails)\nRULE_3_CONDITIONS = [\nfrac_unique_words > 0.95,\nfrac_no_alph_words > 0.6,\nldnoobw_words > 10,\nml_english_score < 0.6\n]\n# Quality Signals Filter Pipeline\nfor each document d in corpus D:\n# Step 1: Apply initial quality filters\nif ANY condition in RULE_1_CONDITIONS is true:\nREJECT document d\n# Step 2: Check for bypass conditions\nif ANY condition in RULE_2_CONDITIONS is true:\nACCEPT document d\n# Skip Rule 3\n# Step 3: Apply additional filters\nif ANY condition in RULE_3_CONDITIONS is true:\nREJECT document d\nelse:\nACCEPT document d\nProcessing step\n% removed\nCumulative % removed\nDocs (B)\n101 raw CC snapshots\n–\n–\n248.4\nSnapshot exact-dedup\n32.1%\n32.1%\n168.7\nGlobal exact-dedup\n45.6%\n63.1%\n91.7\nSnapshot LSH dedup\n23.1%\n71.6%\n70.5\nLanguage filter\n50.8%\n86.0%\n34.8\nDocument quality filters\n32.1%\n90.5%\n23.6\nTable 16: Common-Crawl processing pipeline. \"% removed” is the drop at that stage; \"Cumulative % removed” is\nwith respect to the original 248.4 B documents. Some steps are run at the same time or in parallel, so we estimate\nthe step-level removal rate on a subset of documents.\nSee Table 17 for the datasets used and released in this work.\n26\nDataset Type\nHugging Face Repository\nESSENTIAL-WEB V1.0\nEssentialAI/essential-web-v1.0\nESSENTIAL-WEB V1.0 1T FDC L2\nEssentialAI/essential-web-1t-sample-fdc-partitioned\nEAI-TAXONOMY w/ FM\nEssentialAI/eai-taxonomy-math-w-fm\nEAI-TAXONOMY w/ DCLM\nEssentialAI/eai-taxonomy-code-w-dclm\nEAI-TAXONOMY Med w/ DCLM\nEssentialAI/eai-taxonomy-med-w-dclm\nEAI-TAXONOMY STEM w/ DCLM\nEssentialAI/eai-taxonomy-stem-w-dclm\nTable 17: Datasets we release in this work.\nA.3\nDescriptions of Taxonomy Categories\nIn Section 3.4 we briefly introduce the categories in our taxonomy. What follows is an in-depth description of\nthose categories and their labels.\nA.3.1\nFDC\nThe Free Decimal Correspondence [Ockerbloom, 2010] is a set of decimal numbers, each corresponding to a\ngroup of subjects or disciplines. It is intended to be compatible with the Dewey Decimal System 25, popularly\nused to catalog libraries. We use the FDC to provide three nested categories – Level 1,2,3 – each successive level\nbeing a refinement of its parent. Enumerating all codes and their corresponding labels would be impractical, but\nTable 18 lists the codes and labels for Level 1.\nCode\nLabel\nLevel 1\n0\nGeneral works, books and libraries, information sciences\n1\nPhilosophy and psychology\n2\nReligion\n3\nSocial Sciences\n4\nPhilology and Laguage and languages\n5\nScience and Natural history\n6\nIndustrial arts and Technology and Engineering\n7\nArts and recreation\n8\nLiterature\n9\nHistory and Geography\nLevel 2\n00 - 99\nSub-divisions of Level 1 categories\nLevel 3\n000 - 999\nSub-divisions of Level 2 categories\nTable 18: FDC category codes and labels. Level 2 and Level 3 labels omitted for brevity. A full set of label/code\ncorrespondences can be found at https://github.com/JohnMarkOckerbloom/fdc/blob/master/fdc.txt. A web viewer\ncan be found at https://www.librarything.com/mds.\nA.3.2\nBloom\nBloom’s Taxonomy of Educational Objectives has had many updates since its introduction in 1948. We use\ntwo categories and their labels from Anderson and Krathwohl’s 2001 revision of the taxonomy Anderson and\nKrathwohl [2001]. Table 19 describes the categories and labels used, but note that we did not provide our teacher\nLLM model with the label descriptions, instead relying on its internal knowledge of Bloom’s taxonomy.\n25\"Dewey,\" \"Dewey Decimal,\" \"Dewey Decimal Classification\", and \"DDC\" are trademarks of OCLC.\n27\nLabel\nDescription\nKnowledge Domain\nAuthor demonstrates use of...\nFactual\nbasic elements to learn or solve problems in the discipline\nConceptual\ninterrelationships between basic elements within a larger context\nProcedural\nmethods in the discipline\nMetacognitive\nawareness of how learning works in relation to one’s self\nCognitive Processing Level\nAuthor demonstrates ability to...\nRemember\nretrieve relevant knowledge from memory\nUnderstand\ndetermine the meaning of instructional messages\nApply\nuse a procedure in a given situation\nAnalyze\nbreak materials into components and determine how they work together\nEvaluate\nmake judgments based on criteria and standards\nCreate\ncreate a new or original work\nTable 19: Bloom categories and label descriptions.\nA.3.3\nDocument Type\nOur taxonomy includes two collections of labels that categorize by common web document types. The two\ncollections have a good degree of overlap. Version 1 was created in-house. Version 2 is a replication of the\ndocument type category in WebOrganizer [Wettig et al., 2025]. The labels for both versions are found in Table\n20. In addition to the labels described, each category also has a label option Unclassified, for documents that resist\nclassification.\nA.3.4\nContent Quality\nThe categories in this group are designed to assess the sophistication of material discussed in a document. They\nare inspired by two other efforts to categorize web data by information quality: NaturalReasoning [Yuan et al.,\n2025], and FineWeb [Penedo et al., 2024]. Table 21 describes the categories and labels we developed based on\ntheir work. In addition to the labels described, each category also has a label option Indeterminate, if a document\ndoes not have enough context to make a judgement for that category.\nA.3.5\nExtraction\nWe provide two sets of labels to flag issues that make a document difficult to read, likely from errors converting\nstructured text formats such as HTML to plain text: Extraction Artifacts and Missing Content. Their labels are\nfound in Table 22. Documents could potentially be flagged with multiple errors, but only the most notable errors\nare allowed to be flagged for primary / secondary labels. Either category may label a document Indeterminate if\nthe document does not have enough content to pass judgment on it.\n28\nLabel\nExamples\nLabel\nExamples\nV1\nNews / Editorial\nCNN articles, opinion\ncolumns\nAcademic / Research\nArXiv papers, articles\nReference / Encyclopedic /\nEducational\nFAQs, Wikipedia\nCode / Software\nGithub repos, code\nexamples\nSocial / Forum\nconversation threads, Q&A\nboards\nPromotional /\nAdvertisement\nproduct pages, calls to\naction\nSearch / Directory /\nBibliography\nlink pages, search results\nAdult / Pornographic\nJustice Potter Stewart\nquotes\nPersonal / Misc\nblogs, user profiles\nMachine-Generated\n\"lorem ipsum\", garbled text\nLegal / Regulatory\ncontracts, terms of service\nGovernment / Political\nlegislation proposals, press\nreleases\nLiterary / Creative\npoems, short stories\nReviews / Criticism\nfilm critiques, product\nreviews\nE-Commerce / Marketplace\neBay listings, Amazon\nproduct pages\nImages / Videos / Audio\nYouTube video, Imgur page\nV2\nAbout (Org.)\norg. self-description\nAbout (Personal)\npersonal profile or intro\nAcademic Writing\nresearch paper, abstract\nAudio Transcript\ninterview or court transcript,\ncaptions\nComment Section\nReddit, comment sections\nContent Listing\nsite maps, product catalogs\nCreative Writing\nsong lyrics, Novel exerpts\nDocumentation\nAPI docs, README files\nFAQ\nquestion / answer lists\nKnowledge Article\nWikipedia, Britannica\nLegal Notices\nprivacy policy, license\nagreement\nListicle\nBuzzfeed-style articles\nNews (Org.)\ngovernment blog posts\nNews Article\nnewspaper digital article,\nCNN article\nNonfiction Writing\neditorials, obituaries,\nmemoirs\nPersonal Blog\nindividual’s daily journal\nProduct Page\nproduct promotion, course\ndescription\nQ&A Forum\nQuora, Stack Exchange\nSpam / Ads\nspam content, SEO\nkeyword stuffing\nStructured Data\ndatasheets, glossaries,\nJSON files\nCustomer Support\ntroubleshooting guides\nTruncated\npay-walled site, Image\ngalleries\nTutorial\ncooking recipies, WikiHow\npage\nUserReview\nYelp or TripAdvisor review\nTable 20: Document Type categories and labels / examples\n29\nLabel\nDescription\nReasoning Depth\nDocument contains...\nNo Reasoning\nfacts, but no evidence of reasoning about facts\nBasic Reasoning\nbasic analysis, with minimal explanation and summarization\nIntermediate Reasoning\nsome logical steps that connect ideas, and structured thinking, but is missing deep examples of\neither\nAdvanced Reasoning\nmulti-step reasoning, and thorough analysis with well-developed explanations\nExceptional Reasoning\nnovel abstractions, theoretical frameworks, long chain-of-thought, original insights, or proofs\nTechnical Correctness\nDocument demonstrates...\nTechnically Flawed\nsignificant errors, inaccuracies, undermining validity of contents\nPartially Correct\nsome technical correctness, but contains flaws, omissions, errors in calculation or terminology.\ngenerally understandable\nMostly Correct\ntechnical correctness, with minor flaws, omissions, incomplete explanations, or other small\nissues\nHighly Correct\nhigh technical correctness, precise definitions, accurate and clear explanations, and strong\ncommand of technical material with at most minimal errors\nExceptionally Correct\nexceptional technical correctness, formal proofs, flawless technical content, precise calculations,\nand mastery of material\nEducation Level\nContent...\nGeneral Audience\nis accessible to anyone with basic literacy. Uses simple terms most can understand\nHigh School Level\nrequires high school level education to fully comprehend. Contains specialized terminology\nexplained for non-experts. Assumes basic background knowledge\nUndergraduate Level\nrequires college-level education. Uses specialized terminology/concepts, and assumes signifi-\ncant subject-area background knowledge\nGraduate/Expert Level\nrequires graduate-level education or domain expertise. Assumes deep background knowledge\nand specialized training to comprehend\nTable 21: Content Quality categories and label descriptions.\nLabel\nDescription\nExtraction Artifacts\nDocument has...\nNo Artifacts\nno leftover HTML or irrelevant elements. Text is clean\nLeftover HTML\nHTML/code artifacts remaining after extraction\nText Extraction Errors\nbroken math expressions, encoding errors, improperly parsed tables\nIrrelevant Content\nheaders, footers, nav menus, side-bars, or non-core sections extracted by mistake\nMissing Content\nDocument exhibits...\nNo Missing Content\nno signs of missing content. text seems complete/coherent\nTruncated Snippets\nobvious \"...\", incomplete paragraphs, or cut-off text\nClick Here References\n\"Download here\", \"Click here\", or references to content not present in text\nIncoherent Flow\nsigns of unreadable or illogical flow due to missing key context\nMissing Images or Figures\nplaceholders or references to images/figures not included in text\nMissing Referenced Data\nsigns that data/tables/datasets are not included. e.g. \"See Table 3\", but it’s absent\nTable 22: Extraction categories and label descriptions.\n30\nA.4\nDownstream Eval Details\nEval\n# shots\nTask Name\nConfig\nMMLU\n5\nmmlu\nmmlu/default/_mmlu.yaml\nGSM8K\n8\ngsm8k\ngsm8k/gsm8k.yaml\nMATH\n4\nhendrycks_math\nhendrycks_math/hendrycks_math.yaml\nMBPP+\n3\nmbpp_plus\nmbpp/mbpp_plus.yaml\nHumanEval+\n0\nhumaneval_plus\nhumaneval/humaneval_plus.yaml\nCareQA_en\n5\ncareqa_en\ncareqa/careqa_en.yaml\nMedMCQA\n5\nmedmcqa\nmedmcqa/medmcqa.yaml\nMedQA–USMLE\n5\nmedqa_4options\nmedqa/medqa.yaml\nPubMedQA\n3\npubmedqa\npubmedqa/pubmedqa.yaml\nTable 23: Number of shots and configuration files used for each evaluation. Files refer to commit d09e03d of the\nEleutherAI/lm-evaluation-harness repository; prepend lm_eval/tasks/ to each path for the full\nlocation.\nA.5\nEvaluated Open-Source Datasets\nDataset Type\nHugging Face Repository\nMath\nHuggingFaceTB/finemath\nMath\nopen-web-math/open-web-math\nMath\nLLM360/MegaMath\nGeneral\nmlfoundations/dclm-baseline-1.0\nGeneral\nHuggingFaceFW/fineweb-edu\nCode\nOpenCoder-LLM/opc-fineweb-code-corpus\nCode\nbigcode/the-stack-v2-dedup\nCode\nHuggingFaceTB/stack-edu\nMedical\nTheBlueScrubs/TheBlueScrubs-v1\nTable 24: Overview of all the open-source datasets used in Section 4.\nA.6\nModel Config and Hyperparameters\nA.6.1\nArchitecture\nWe use a modern decoder-only transformer model based on Gemma 3, which replaces the soft-capping method in\nGemma 2 with QK-norm for attention stability [Gemma 3 Team, 2025, Gemma 2 Team, 2024]. We use a local\nattention sliding window size of 4096.\nA.6.2\n2.3B Model Config\nFor the 2.3B parameter model, we use Llama3’s 128k tokenizer, an embedding dimension of 2304, an MLP\ndimension of 9216, and 26 layers [Llama Team, 2024]. Given that the embedding and un-embedding matrices are\ntied, the total embedding parameters is 294.9M parameters.\nA.6.3\n2.3B Training Configurations\n31\nHyperparameter\nValue\nOptimizer\nAdamW\nβ1, β2\n0.9, 0.95\nWeight decay\n0.1\nGlobal batch size\n2M tokens\nPeak LR\n3.5 × 10−5\nLR warm-up\n2B tokens\nCosine decay\n3.5 × 10−5 →3.5 × 10−6\nTotal training tokens\n320B\nSequence length\n8,192\n(a) Base-model training config.\nHyperparameter\nValue\nPeak LR\n3.5 × 10−6\nLinear decay\n3.5 × 10−6 →0\nTotal annealing tokens\n80B\n(b) Annealing training config. Load the parameters and\noptimizer state from base model checkpoint at 320B tokens.\nTable 25: Training and annealing configs. Hyper-parameters not listed for the annealing phase inherit the values\non the left.\nA.7\nTaxonomy-Based Dataset Filters\nAlgorithm 2: Semantic filter used to curate Taxonomy Top Math dataset\nDOC_TYPE_V1 = [\n\"Reference/Encyclopedic/Educational\", \"Code/Software\", \"Social/Forum\", \"Personal/Misc\"\n]\nDOC_TYPE_V2 = [\n\"Comment Section\", \"Documentation\", \"FAQ\", \"Knowledge Article\", \"Nonfiction Writing\",\n\"Personal Blog\", \"Q&A Forum\", \"Structured Data\", \"Tutorial\"\n]\nREASONING_DEPTH = [\n\"Basic Reasoning\", \"Intermediate Reasoning\", \"Advanced Reasoning\", \"Exceptional Reasoning\"\n]\nTECH_CORRECTNESS = [\"Highly Correct\", \"Exceptionally Correct\"]\n# Free Decimal Correspondence: 51 = Mathematics\nFDC_KEEP = [\"51\"]\n# D: Essential Common Crawl (23.6B documents)\n# R: Taxonomy Top Math (19.8M documents)\nR = {\nd for d in D\nif prefix(d.fdc.primary) in FDC_KEEP\nand d.doc_type_v1.primary in DOC_TYPE_V1\nand d.doc_type_v2.primary in DOC_TYPE_V2\nand d.reasoning_depth.primary in REASONING_DEPTH\nand d.technical_correctness.primary in TECH_CORRECTNESS\n}\n32\nAlgorithm 3: Semantic filter used to curate Taxonomy Math w/ FM dataset\n# Free Decimal Correspondence: 51 = Mathematics\nFDC_KEEP = [\"51\"]\n# D: Essential Common Crawl (23.6B documents)\n# R: Taxonomy Math w/ FM (21.6M documents)\nR = {\nd for d in D\nif (prefix(d.fdc.primary) in FDC_KEEP or prefix(d.fdc.secondary) in FDC_KEEP)\nand d.finemath_score >= 3.25\n}\nAlgorithm 4: Filter used to curate Taxonomy Code dataset\nDOC_TYPE_V1 = [\n\"Reference/Encyclopedic/Educational\", \"Social/Forum\"\n]\nDOC_TYPE_V2 = [\n\"Comment Section\", \"Documentation\", \"Knowledge Article\", \"Tutorial\", \"Personal Blog\",\n\"Q&A Forum\"\n]\nREASONING_DEPTH = [\n\"Intermediate Reasoning\", \"Advanced Reasoning\", \"Exceptional Reasoning\"\n]\nTECH_CORRECTNESS = [\"Highly Correct\"]\n# Free Decimal Correspondence: 005.1 = Programming, 005.3 = Systems Programming\nFDC_KEEP = [\"005.1\", \"005.3\"]\n# D: Essential Common Crawl (23.6B documents)\n# R: Taxonomy Code (42.7M documents)\nR = {\nd for d in D\nif prefix(d.fdc.primary) in FDC_KEEP\nand d.doc_type_v1.primary in DOC_TYPE_V1\nand d.doc_type_v2.primary in DOC_TYPE_V2\nand d.reasoning_depth.primary in REASONING_DEPTH\nand d.technical_correctness.primary in TECH_CORRECTNESS\n}\n33\nAlgorithm 5: Filter used to curate Taxonomy Code w/ DCLM dataset\nDOC_TYPE_V2 = [\n\"Personal Blog\", \"Knowledge Article\", \"Comment Section\", \"Documentation\", \"Tutorial\", \"Q&A\nForum\"\n]\nREASONING_DEPTH = [\n\"Basic Reasoning\", \"Intermediate Reasoning\", \"Advanced Reasoning\", \"Exceptional Reasoning\"\n]\n# threshold used to filter DCLM Pool -> DCLM-baseline\nDCLM_baseline_thresh = 0.01811\n# Free Decimal Correspondence: 004 = Computer Science, 005 = Software Development,\n#\n51 = Mathematics\nFDC_KEEP = [\"004\", \"005\", \"51\"]\n# D: Essential Common Crawl (23.6B documents)\n# R: Taxonomy Code w/ DCLM (274M documents)\nR = {\nd for d in D\nif prefix(d.fdc.primary) in FDC_KEEP\nand d.doc_type_v2.primary in DOC_TYPE_V2\nand d.reasoning_depth.primary in REASONING_DEPTH\nand d.quality_signals.rps_doc_ml_eli5_score > DCLM_baseline_thresh\n}\n34\nAlgorithm 6: Filter filter used to curate Taxonomy Medical\nDOC_TYPE_V1 = [\n\"Academic/Research\", \"Reference/Encyclopedic/Educational\"\n]\nDOC_TYPE_V2 = [\n\"Academic Writing\", \"Documentation\", \"Knowledge Article\", \"Q&A Forum\"\n]\nREASONING_DEPTH = [\n\"Basic Reasoning\", \"Intermediate Reasoning\", \"Advanced Reasoning\", \"Exceptional Reasoning\"\n]\nTECH_CORRECTNESS = [\"Highly Correct\", \"Exceptionally Correct\"]\n# Hard-science Free Decimal Correspondence Prefixes\nSCIENCE_CODES = [\"50\", \"51\", \"54\", \"57\", \"58\", \"59\", \"61\"]\n# Free Decimal Correspondence: 61 = Medicine\nFDC_KEEP = [\"61\"]\n# Document-type blacklists\nDOC_TYPE_V1_BLACKLIST = [\n\"News/Editorial\", \"Code/Software\", \"Social/Forum\", \"Promotional/Advertisement\", \"Adult/Pornographic\",\n\"Personal/Misc\", \"Machine-Generated\", \"E-Commerce/Marketplace\", \"Images/Videos/Audio\"\n]\nDOC_TYPE_V2_BLACKLIST = [\n\"About (Org.)\", \"About (Personal)\", \"Audio Transcript\", \"Comment Section\", \"Content Listing\", \"Creative Writing\",\n\"Legal Notices\", \"Listicle\", \"News (Org.)\", \"News Article\", \"Personal Blog\", \"Product Page\", \"Spam / Ads\",\n\"Structured Data\", \"Truncated\", \"Tutorial\", \"User Review\"\n]\n# D: Essential Common Crawl (23.6B documents)\n# S: Taxonomy Medical (235M documents)\nS = {\nd for d in D\n# FDC pairing: Medicine (61) w/ another science code\nif (\n(prefix(d.dds.primary) in FDC_KEEP and prefix(d.dds.secondary) in SCIENCE_CODES) or\n(prefix(d.dds.secondary) in FDC_KEEP and prefix(d.dds.primary) in SCIENCE_CODES)\n)\n# Document-type whitelist & blacklist checks\nand (\nd.doc_type_v1.primary in DOC_TYPE_V1 or\nd.doc_type_v2.primary in DOC_TYPE_V2\n)\nand d.doc_type_v1.primary not in DOC_TYPE_V1_BLACKLIST\nand d.doc_type_v2.primary not in DOC_TYPE_V2_BLACKLIST\n# Quality\nand d.reasoning_depth.primary in REASONING_DEPTH\nand d.technical_correctness.primary in TECH_CORRECTNESS\n}\nAlgorithm 7: Filter to curate the Taxonomy Medical w/ DCLM. Refer to Algorithm 6 for Taxonomy Medical.\n# threshold used to filter DCLM Pool -> DCLM-baseline\nDCLM_baseline_thresh = 0.01811\n# D: Taxonomy Medical (235M documents)\n# R: Taxonomy Medical w/ DCLM (81.2M documents)\nR = {\nd for d in D\nif d.quality_signals.rps_doc_ml_eli5_score > DCLM_baseline_thresh\n}\n35\nAlgorithm 8: Filter used to curate the Taxonomy STEM corpus\n# Document-types\nCODE_DOC_TYPE_V1 = [\"Academic/Research\", \"Reference/Encyclopedic/Educational\", \"Code/Software\", \"Social/Forum\"]\nCODE_DOC_TYPE_V2 = [\"Academic Writing\", \"Comment Section\", \"Documentation\", \"Knowledge Article\", \"Personal Blog\",\n\"Q&A Forum\", \"Tutorial\"]\nMED_DOC_TYPE_V1\n= [\"Academic/Research\", \"Reference/Encyclopedic/Educational\", \"Code/Software\", \"Legal/Regulatory\"]\nMED_DOC_TYPE_V2\n= [\"Academic Writing\", \"Documentation\", \"FAQ\", \"Knowledge Article\", \"News Article\", \"Tutorial\"]\nENG_DOC_TYPE_V1\n= [\"Academic/Research\", \"Reference/Encyclopedic/Educational\", \"Personal/Misc\", \"Legal/Regulatory\"]\nENG_DOC_TYPE_V2\n= [\"Academic Writing\", \"Audio Transcript\", \"Documentation\", \"FAQ\", \"Knowledge Article\", \"News Article\",\n\"Tutorial\"]\nDEF_DOC_TYPE_V1\n= [\"Academic/Research\", \"Reference/Encyclopedic/Educational\"]\nDEF_DOC_TYPE_V2\n= [\"Academic Writing\", \"Knowledge Article\", \"News Article\"]\n# Reasoning depth\nREASONING_DEPTH_BAD = [\"Abstain\", \"Indeterminate\"]\n# Free Decimal Correspondence prefixes\nSCIENCE_FDC = [\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\"]\nTECH_FDC\n= [\"60\",\"61\",\"62\",\"66\",\"00\"]\n# 00 = Computer science\nVALID_FDC\n= SCIENCE_FDC + TECH_FDC\nPROG_FDC\n= [\"005.1\",\"005.4\"]\n# programming & systems prog.\n# D: Essential Common Crawl (23.6 B docs)\n# R: Taxonomy-STEM (1B docs)\nR = {\nd for d in D\nif prefix(d.fdc.primary) in VALID_FDC\nand prefix(d.fdc.secondary) in VALID_FDC\nand d.reasoning_depth.primary not in REASONING_DEPTH_BAD\nand d.reasoning_depth.primary is not None\nand (\n# Code\n((prefix(d.fdc.primary,5) in PROG_FDC or prefix(d.fdc.secondary,5) in PROG_FDC)\nand d.doc_type_v1.primary in CODE_DOC_TYPE_V1\nand d.doc_type_v2.primary in CODE_DOC_TYPE_V2)\nor # Medical\n((prefix(d.fdc.primary)==\"61\" or prefix(d.fdc.secondary)==\"61\")\nand d.doc_type_v1.primary in MED_DOC_TYPE_V1\nand d.doc_type_v2.primary in MED_DOC_TYPE_V2)\nor # Engineering\n((prefix(d.fdc.primary)==\"62\" or prefix(d.fdc.secondary)==\"62\")\nand d.doc_type_v1.primary in ENG_DOC_TYPE_V1\nand d.doc_type_v2.primary in ENG_DOC_TYPE_V2)\nor # Default science/tech\n(d.doc_type_v1.primary in DEF_DOC_TYPE_V1\nand d.doc_type_v2.primary in DEF_DOC_TYPE_V2)\n)\n}\nAlgorithm 9: Filter to curate the Taxonomy STEM w/ DCLM. Refer to Algorithm 8 for Taxonomy STEM.\n# threshold used to filter DCLM Pool -> DCLM-baseline\nDCLM_baseline_thresh = 0.01811\n# D: Taxonomy STEM (1B documents)\n# R: Taxonomy STEM w/ DCLM (342M documents)\nR = {\nd for d in D\nif d.quality_signals.rps_doc_ml_eli5_score > DCLM_baseline_thresh\n}\n36\nA.8\nAgreement Metric Discussion\nA.8.1\nMotivating Our Variant of Cohen’s Kappa\nCohen’s κ is a popular choice for measuring inter-annotator agreement across a number of fields. Its robustness\nagainst annotators guessing with an imbalanced label distribution has been both criticized [Feinstein and Cicchetti,\n1990] and lauded [Kolesnyk and Khairova, 2022]. For our part, we are attempting to measure agreement across\nmultiple categories, and there are several points in our analysis where it is helpful to aggregate scores. While not\nfool-proof, we believe we are on firmer ground doing so using a kappa metric, than raw accuracy, since a kappa’s\nnormalization term can make comparing categories with different numbers of labels easier.\nStandard Cohen’s κ (and discrete agreement metrics in general) is most appropriate when the decision boundaries\nbetween class labels are clear. In cases where there is potential for fuzzy agreement (for example \"assign a quality\nscore from 1 to 10”), other agreement metrics like Kendall’s W are preferred. For our tasks, however, we believe\nthere is fuzzyness in some of our class labels that cannot be solved by sorting the label space. For example,\na document can match multiple document types (eg. a document can be both a \"Tutorial\" and \"FAQ\"), or its\nsubject-matter can cross multiple FDC labels.\nTo this end we developed a variant of Cohen’s κ that works with overlapping agreement. We allow our annotators\nto output up to two labels for each category. Overlaps between annotation label sets x and y can either be scored\nas 0 or 1\ns(x, y) = 1(x∩y̸=∅)∨(x∪y=∅)\nor weighted by proportion overlap\ns(x, y) =\n(\n1\nif x ∩y = ∅\n|x∩y|\n|x∪y|\notherwise\nwith appropriate Pe for each.\nA.8.2\nOn wild swings of κ.\nThe κ function is normally described as\nκ = Po −Pe\n1 −Pe\n0 ≤Po ≤1,\n0 ≤Pe < 1,\n−1 ≤κ ≤1\nbut its not necessary for Po and Pe to take on values over their whole range for κ to do so. In fact, for any ϵ > 0\n1 −ϵ ≤Po ≤1,\n1 −ϵ ≤Pe < 1 =⇒−1 ≤κ ≤1\nThis implies that as Po, Pe →1, κ must become increasingly sensitive to small changes in its inputs.\nIn Table 13, there are big swings of κ in a few categories, the largest occurring to the Extraction Artifacts category.\nIf we include Pe and Po when looking at κ values (Table 26), we can see the effect of small changes to κ’s inputs.\nWhile the Extraction Artifacts category’s κ falls almost 50pp, the Po’s are within a margin of 15pp, and the Pe’s\nare within 4pp. In the Education Level category on the random evaluation set, Po and κ disagree on whether\nQwen2.5-32b-Instruct or EAI-Distill-0.5b performs better.\nCategory\nQwen2.5-32B-Inst.\nEAI-Distill-0.5b\nPo\nPe\nκ\nPo\nPe\nκ\nExtraction Artifacts (Random)\n0.93\n0.75\n0.74\n0.79\n0.71\n0.27\nReasoning Depth (Random)\n0.90\n0.69\n0.67\n0.98\n0.89\n0.87\nTechnical Correctness (STEM)\n0.77\n0.55\n0.51\n0.94\n0.78\n0.75\nEducation Level (Random)\n0.96\n0.72\n0.88\n0.98\n0.91\n0.79\nTable 26: Po and Pe components help to explain whether a rise or drop in κ is real, or an artifact of the metric. In\nthe first row, κ exaggerates Po. In the second two, Po and κ are roughly in line with relative improvement. The\nlast row illustrates an ordering disagreement between Po and κ\nThere are at least two sources for simultaneously large Po and Pe:\n37\nAnnotators\nA\nB\nC\nD\nE\nF\nClaude Sonnet 3.5\nDeepSeek-V3\nA\n0.41\n0.50\n0.55\n0.59\nB\n0.41\n0.54\n0.68\n0.71\nC\n0.50\n0.54\n0.72\n0.74\nD\n0.38\n0.42\n0.48\n0.56\nE\n0.38\n0.51\n0.65\n0.64\nF\n0.42\n0.51\n0.66\n0.70\nClaude Sonnet 3.5\n0.55\n0.68\n0.72\n0.48\n0.65\n0.66\n0.81\nDeepSeek-V3\n0.59\n0.71\n0.74\n0.56\n0.64\n0.70\n0.81\nTable 27: Cohen κ between human (A, B, C, D, E, and F) and LLM annotators, averaged across four taxonomy\ncategories. All human annotators have higher agreement with LLM annotators than they do with other human\nannotators\n• Unbalanced distribution of labels, as discussed in Feinstein and Cicchetti [1990].\n• An agreement score that is too relaxed. To that end, we created a version of our κ that weights agreement\nvia degree of overlap (Appendix A.8.4). This version reduced ordering discrepancies and big swings in κ,\nbut since the unweighted version of our κ did not affect any decisions after averaging across categories,\nwe do not report this version of our κ scores.\nA.8.3\nMotivating the Use of LLMs as Reference Annotators\nWhile assessing the quality of the different versions of our labeler, we’ve compared agreement with labeling\ncarried out by a set of reference LLMs. A fair question would be whether we are missing something by not\nevaluating against humans. To that end, we enlisted six people to annotate 1150 documents with four of our\ntaxonomy categories: FDC, Document Type V2, Reasoning Depth, and Education Level. Three annotators (A, B,\nand C in Table 27) annotated 575 documents, and the other three annotators (D, E, and F) labeled the remaining\n575 documents. We then compared the human annotators against each other, and against two strong LLMs, using\naverage Cohen’s κ across the labeled categories. For every human annotator, we found that the annotator had\nhigher average κ with the LLMs than they did with other human annotators.\nA.8.4\nDeriving Pe Estimator for Annotator κ\nFor annotators An, n = 1, 2 let fn,k, k = 0, 1, 2 be the probability An generates k labels for a document.\nAssuming k > 0, let wn,x be the probability of An first labeling the document with label x. Assume the second\nlabel is picked with the same wn, without replacement, so that if k = 2, then\nwn,y\n1−wn,x is the probability An picks\nlabel y given that x was picked first.\nDenote pj,k the probability of an agreement between A1 and A2’s annotations, given A1’s annotation is length j\nand A2’s annotation is length k, as discussed in 5.1.2. Then\nPe = f1,0f2,0 +\n2\nX\nj=1\n2\nX\nk=1\nf1,jf2,kpj,k\nSo we need to calculate pj,k for j = 1, 2, k = 1, 2\nj = 1, k = 1\nThis is just the same calculation for Cohen’s κ\np1,1 =\nX\nx\nX\ny\n1x=yw1,xw2,ys([x], [y])\n= s1,1\nX\nx\nw1,xw2,x\ns([x], [y]) is the weighted or unweighted matching score, and in both the weighted and unweighted case, s1,1 = 1\n(in subsequent calculations, si,j is always 1 in the unweighted case)\n38\nj = 1, k = 2\nBegin calculating all possible matches of [x] and [y, z]\np1,2 =\nX\nx\nX\ny\nX\nz\n1y̸=z∧(x=y∨x=z)w1,xw2,y\nw2,z\n1 −w2,y\ns([x], [y, z])\nwe can break the calculation into two conditions: x = y and x = z. The conditions are disjoint, since both being\ntrue would imply y = z\np1,2 = s1,2\nX\nx\nX\nz\n1x̸=zw1,xw2,x\nw2,z\n1 −w2,x\n+ s1,2\nX\nx\nX\ny\n1y̸=xw1,xw2,y\nw2,x\n1 −w2,y\np1,2 = s1,2\nX\nx\nw1,xw2,x\nX\nz\n1x̸=z\nw2,z\n1 −w2,x\n+ s1,2\nX\nx\nw1,xw2,x\nX\ny\n1y̸=x\nw2,y\n1 −w2,y\nIn the weighted case, s1,2 = 1/2 . If we denote\nrn,y =\nX\nx\n1x̸=y\nwn,x\n1 −wn,x\n(1)\nAnd note the identity\nX\nx\n1y̸=x\nwn,x\n1 −wn,y\n= 1\n(2)\nthen\np1,2 = s1,2\nX\nx\nw1,xw2,x(1 + r2,x)\n(3)\nj = 2, k = 1\nThe same calculation leading to equation 3 implies\np2,1 = s2,1\nX\nx\nw1,xw2,x(1 + r1,x)\nwhere s2,1 = s1,2\nj = 2, k = 2\nBegin by calculating all possible matches of [x, y] and [z, v].\np2,2 =\nX\nx\nX\ny\nX\nz\nX\nv\n1x̸=y∧z̸=v∧(x=z∨x=v∨y=z∨y=v)\nw1,xw1,y\n1 −w1,x\nw2,zw2,v\n1 −w2,z\ns([x, y], [z, v])\nWe can calculate the four scenarios x = z, x = v, y = z, and y = v separately, but unlike case j = 1,k = 2,\nthere are intersections we will end up double-counting, which will require subtractions (or weight adjustment\nin the weighted case). Thinking about the implications of intersections of scenarios in x ̸= y ∧z ̸= v leads to\ndetermining that the only non-empty intersections are x = z ∧y = w and x = w ∧y = z. So, p2,2 can be\nrewritten:\np2,2 = s2,2\nX\nx\nX\ny\nX\nv\n1x̸=y∧x̸=v\nw1,xw1,y\n1 −w1,x\nw2,xw2,v\n1 −w2,x\n(x = z)\n+ s2,2\nX\nx\nX\ny\nX\nz\n1x̸=y∧x̸=z\nw1,xw1,y\n1 −w1,x\nw2,zw2,x\n1 −w2,z\n(x = v)\n+ s2,2\nX\nx\nX\ny\nX\nv\n1x̸=y∧y̸=v\nw1,xw1,y\n1 −w1,x\nw2,yw2,v\n1 −w2,y\n(y = z)\n+ s2,2\nX\nx\nX\ny\nX\nz\n1x̸=y∧z̸=y\nw1,xw1,y\n1 −w1,x\nw2,zw2,y\n1 −w2,z\n(y = v)\n+ a\nX\nx\nX\ny\n1x̸=y\nw1,xw1,y\n1 −w1,x\nw2,xw2,y\n1 −w2,x\n(x = z ∧y = v)\n+ a\nX\nx\nX\ny\n1x̸=y\nw1,xw1,y\n1 −w1,x\nw2,yw2,x\n1 −w2,y\n(x = v ∧y = z)\n39\nWhere s2,2 = 1/3 in the weighted case, and a is an over-count adjustment. a = −1 in the unweighted case, since\nwe have counted matches of the form [x, y], [x, y] and [x, y], [y, x] twice. a = +1/3 in the weighted case, because\nthe true weighted score for such matches is 1, not 1/3, and they were counted twice. After a little rearrangement\non the four scenarios, and then substituting in equations 1 and 2\np2,2 = s2,2\nX\nx\nw1,xw2,x\nX\ny\n1x̸=y\nw1,y\n1 −w1,x\nX\nv\n1x̸=v\nw2,v\n1 −w2,x\n+ s2,2\nX\nx\nw1,xw2,x\nX\ny\n1x̸=y\nw1,y\n1 −w1,x\nX\nz\n1x̸=z\nw2,z\n1 −w2,z\n+ s2,2\nX\ny\nw1,yw2,y\nX\nx\n1x̸=y\nw1,x\n1 −w1,x\nX\nv\n1y̸=v\nw2,v\n1 −w2,y\n+ s2,2\nX\ny\nw1,yw2,y\nX\nx\n1x̸=y\nw1,x\n1 −w1,x\nX\nz\n1y̸=z\nw2,z\n1 −w2,z\n+ a\nX\nx\nX\ny\n1x̸=y\nw1,xw1,y\n1 −w1,x\nw2,xw2,y\n1 −w2,x\n+ a\nX\nx\nX\ny\n1x̸=y\nw1,xw1,y\n1 −w1,x\nw2,yw2,x\n1 −w2,y\np2,2 = s2,2\nX\nx\nw1,xw2,x(1 + r2,x + r1,x + r1,xr2,x)\n+ a\nX\nx\nX\ny\n1x̸=y\nw1,xw1,y\n1 −w1,x\nw2,xw2,y\n1 −w2,x\n+ a\nX\nx\nX\ny\n1x̸=y\nw1,xw1,y\n1 −w1,x\nw2,yw2,x\n1 −w2,y\nA.9\nEvaluation Sets\nA.9.1\nDomain-recall Gold URL Sets\nFor the human-vetted \"gold\" URL sets, we manually inspect and collect a set of high quality base URLs for the\nmath and web code domains. Our goal is to ensure that these vetted URLs have a very high density of documents\nfrom their respective domain. For example, when visiting https://dlmf.nist.gov/ it is clear that the vast majority of\nthe website should be designated as a mathematics. The math URLs can be found at Algorithm 10 and the code\ncan be found at Algorithm 11.\nA.9.2\nSampling STEM Evaluation Set\nThe STEM evaluation set was created by up-sampling STEM categories as labeled by Qwen2.5-32b-Instruct.\nA breakdown of the Free Decimal Classification counts of the STEM evaluation set can be found in Table 28.\nAlgorithm 10: Vetted Math Base URLS\nvetted_math_urls =\n[\"https://dlmf.nist.gov/\", \"https://tutorial.math.lamar.edu/\", \"https://math24.net\", \"https://\nwhitman.edu/mathematics/calculus_online\", \"https://sfu.ca/math-coursenotes\", \"https://www2.clarku.edu/faculty/\ndjoyce\", \"https://clarku.edu/faculty/djoyce/trig\", \"https://math.libretexts.org\", \"https://sophisticatedprimate.com\n\", \"https://mathisfun.com\", \"https://chilimath.com\", \"https://encyclopediaofmath.org/wiki\", \"https://\nbetterexplained.com/articles\", \"https://settheory.net/\", \"https://math24.net\", \"https://www.opentextbookstore.com/\nbuscalc/buscalc/\", \"https://oeis.org/\", \"https://yutsumura.com\", \"https://math.mit.edu/~djk/calculus_beginners\", \"\nhttps://bookdown.org/slcodia/Stat_136\", \"https://bookdown.org/huckley/Physical_Processes_In_Ecosystems/\", \"https://\nstacks.math.columbia.edu/tag\", \"https://golem.ph.utexas.edu/\", \"https://johncarlosbaez.wordpress.com/\", \"https://\nqchu.wordpress.com/\", \"https://cornellmath.wordpress.com/\", \"https://ncatlab.org\", \"https://proofwiki.org/wiki/\", \"\nhttps://mathoverflow.net/questions\", \"https://math.stackexchange.com/questions\", \"https://projecteuler.net\", \"https\n://aperiodical.com/\", \"https://unapologetic.wordpress.com/\", \"https://www.jeremykun.com/\", \"https://terrytao.\nwordpress.com/\", \"https://11011110.github.io/blog/\", \"https://planetmath.org/\", \"https://alexsisto.wordpress.com/\",\n\"https://cstheory.stackexchange.com/questions\", \"https://web.ma.utexas.edu/mediawiki\", \"http://theoremoftheweek.\nwordpress.com/\", \"http://tqft.net/mlp\"]\n40\nAlgorithm 11: Vetted Web Code Base URLS\nvetted = [\"github.com\", \"www.geeksforgeeks.org\", \"stackoverflow.com/questions\", \"www.experts-exchange.com/questions\", \"\nmail.python.org/pipermail\", \"www.linuxquestions.org/questions\", \"www.tomshardware.com/forum\", \"www.coderanch.com/t\"\n, \"superuser.com/questions\", \"community.spiceworks.com/topic\", \"sourceforge.net/p\", \"sourceforge.net/directory\", \"\ndocs.microsoft.com/en-us\", \"serverfault.com/questions\", \"askubuntu.com/questions\", \"www.bleepingcomputer.com/forums\n\", \"msdn.microsoft.com/en-us\", \"coderanch.com/t\", \"learn.microsoft.com/en-us\", \"unix.stackexchange.com/questions\",\n\"forums.unrealengine.com/t\", \"community.filemaker.com/thread\", \"www.geekstogo.com/forum\", \"forum.arduino.cc/t\", \"\nwww.daniweb.com/programming\", \"sharepoint.stackexchange.com/questions\", \"www.construct.net/en/forum\", \"apple.\nstackexchange.com/questions\", \"forums.developer.nvidia.com/t\", \"techcommunity.microsoft.com/t5\"]\nClass\nFDC top-level category\nDocuments\n0\nInformation sciences\n54\n1\nPhilosophy & psychology\n156\n3\nSocial sciences\n130\n4\nLanguages & linguistics\n46\n5\nScience & natural history\n237\n6\nTechnology & applied sciences\n232\n7\nArts & recreation\n9\n8\nLiterature\n4\n9\nHistory & geography\n3\nTotal\n871\nTable 28: Document counts by Free Decimal Classification level 1 of the STEM evaluation set as labeled by\nQwen2.5-32b-Instruct.\nA.9.3\nfastText Classifier Details\nThe fastText classifier for math and web code were both trained on 500k positive examples and 500k negative\nexamples. The negative samples were documents randomly sampled from Common Crawl [Shao et al., 2024].\nThe hyper-parameters used to train the classifiers can be found in Table 29. The positive sets used to train the\nclassifiers we use in the domain-recall experiments are:\n• Math: sample from OpenWebMath [Paster et al., 2023]\n• Web Code: documentation from package manager platforms (such as npm, PyPI, etc.)\nHyperparameter\nValue\nTokenizer\nwhitespace\nWord n-grams\n4\nEmbedding dimension\n128\nHash bucket size\n1,000,000\nMin count\n10\nEpochs\n2\nTable 29: Hyperparameters use to train math and web code fastText classifiers.\nThe fastText classifiers did not utilize BPE tokenization or iterative training, which have both been shown to\nimprove performance on domains such as math and code [Shao et al., 2024, Huang et al., 2025].\nA.10\nDetailed Taxonomy Evaluation Results\nA.10.1\nAnnotator κ including Qwen2.5-14b-Instruct\nSee Table 30 for annotator κ of all open-source models tested.\n41\nCategory\nDeepSeek-V3\nQwen 2.5-72B-Inst.\nQwen 2.5-32B-Inst.\nQwen 2.5-14B-Inst.\nRandom\nSTEM\nRandom\nSTEM\nRandom\nSTEM\nRandom\nSTEM\nKnowledge Domain\n0.69± 0.02\n0.64± 0.03\n0.46± 0.02\n0.39± 0.03\n0.62± 0.02\n0.68± 0.02\n0.20± 0.01\n0.24± 0.02\nCognitive Process\n0.76± 0.01\n0.76± 0.02\n0.67± 0.02\n0.70± 0.02\n0.73± 0.01\n0.79± 0.02\n0.30± 0.01\n0.40± 0.02\nDocument Type V1\n0.90± 0.01\n0.91± 0.01\n0.88± 0.01\n0.91± 0.01\n0.86± 0.01\n0.89± 0.01\n0.60± 0.01\n0.62± 0.02\nFDC (level 1)\n0.92± 0.01\n0.95± 0.01\n0.90± 0.01\n0.92± 0.01\n0.88± 0.01\n0.92± 0.01\n0.88± 0.01\n0.91± 0.01\nFDC (level 2)\n0.86± 0.01\n0.87± 0.01\n0.83± 0.01\n0.81± 0.01\n0.81± 0.01\n0.81± 0.01\n0.78± 0.01\n0.76± 0.01\nFDC (level 3)\n0.71± 0.01\n0.70± 0.01\n0.67± 0.01\n0.63± 0.01\n0.64± 0.01\n0.60± 0.01\n0.57± 0.01\n0.55± 0.01\nExtraction Artifacts\n0.81± 0.02\n0.86± 0.03\n0.57± 0.02\n0.53± 0.03\n0.74± 0.01\n0.65± 0.03\n0.27± 0.02\n0.30± 0.02\nMissing Content\n0.83± 0.01\n0.85± 0.02\n0.63± 0.01\n0.65± 0.02\n0.66± 0.02\n0.65± 0.02\n0.43± 0.01\n0.45± 0.02\nDocument Type V2\n0.89± 0.01\n0.89± 0.01\n0.80± 0.01\n0.80± 0.01\n0.85± 0.01\n0.83± 0.01\n0.71± 0.01\n0.69± 0.02\nEducation Level\n0.89± 0.01\n0.86± 0.02\n0.82± 0.01\n0.81± 0.02\n0.88± 0.01\n0.85± 0.02\n0.69± 0.02\n0.71± 0.02\nReasoning Depth\n0.75± 0.01\n0.72± 0.02\n0.70± 0.01\n0.67± 0.02\n0.67± 0.02\n0.67± 0.02\n0.47± 0.01\n0.50± 0.02\nTechnical Corr.\n0.60± 0.02\n0.61± 0.02\n0.52± 0.01\n0.60± 0.02\n0.52± 0.01\n0.51± 0.02\n0.30± 0.01\n0.25± 0.02\nOverall mean\n0.80\n0.80\n0.70\n0.70\n0.74\n0.74\n0.52\n0.53\nTable 30: Annotator κ (± s.e.) between each candidate model and the two gold annotators on the random\n(n = 2,017) and STEM (n = 871) evaluation sets.\nA.10.2\nInter-Category NMI\nAdditional NMI Heatmaps\nAdditional inter-category NMI heatmaps can be found in Figure 6.\n(a) DeepSeek-V3 random NMI\n(b) DeepSeek-V3 STEM NMI\n(c) Qwen2.5-72b-Instructrandom NMI\n(d) Qwen2.5-72b-Instruct STEM NMI\nFigure 6: NMI heatmaps on random and STEM evaluation sets for DeepSeek-V3 and Qwen2.5-72b-Instruct\n42\nA.11\nEAI-Distill-0.5b Training Details & Ablations\nA.11.1\nComparison of prefill and generation tokens of Qwen2.5-32b-Instruct vs EAI-Distill-0.5b\nGiven we annotated the 104.6M sample of Common Crawl in two passes as explained in Apprendix A.12, we decide\nto use the Prompt 1 (Appendix A.13.3) when calculating performance deltas from Qwen2.5-32b-Instruct to\nEAI-Distill-0.5b. This is a lower bound on performance impact given Prompt 1 just annotates a document for\n9 categories (not all 12). However, EAI-Distill-0.5b annotates a document with all 12 categories.\nAverage Input & Output Tokens.\nWe sample 1,024,83 documents annotated by both Qwen2.5-32b-Instruct\nand EAI-Distill-0.5b. We then calculate and report the average number of tokens in the prompt and generated\noutput. We ignore the cached prefix of EAI-Distill-0.5b given it is only 11 tokens before the document is\nprovided (these tokens come from applying the chat template and the short system prompt). The average prompt\nand generated tokens can be found in Table 31.\nPrompt\nOutput\nShared prefix\nAvg. prompt tok.\nAvg. gen. tok.\nPrompt 1\nPrompt 1 Output\n2104\n3037\n791\nNo Prompt\nFinal Output\n0\n934\n51\nTable 31: Average token statistics across 1,024,836 documents. Prompt 1 Output is produced by\nQwen2.5-32B-Instruct; Final Output by EAI-Distill-0.5b.\nA.11.2\nTraining Hyperparameters\nThe final hyperparameters used to fine-tuned Qwen2.5-0.5b-Instruct can be found in Table 32.\nHyperparameter\nValue\nNotes\nOptimizer type\nAdamW\nβ1, β2\n0.9, 0.95\nWeight decay\n0.1\nGlobal batch size\n2M tokens\nPeak learning rate\n1 × 10−4\nLR warm-up\n2B tokens\nlinear to peak LR\nCosine decay phase\n1 × 10−4 →1 × 10−5\nfollows warm-up\nLinear anneal phase\n1 × 10−5 →0\nfinal 2B tokens\nTotal fine-tuning tokens\n82 B\nsynthetic labels\nSequence Length\n16,384\nTable 32: Hyper-parameters used to fine-tune Qwen2.5-0.5b-Instruct.\nA.11.3\nEAI-Distill-0.5b Fine-Tuning Ablations\nFine-tuning prompt / generation format ablation.\nWe ablate the affects on annotator κ of context distillation\nand shortening generation tokens when fine-tuning Qwen2.5-0.5b-Instruct. We compare training on Prompt\n1 (Appendix A.13.3) with its standard generation format against no prompt and a highly condensed output format\n(Algorithm 13).26 Both models are trained for 200B tokens, have a learning rate of 2 × 10−6, and a max sequence\nlength of 8192. Other than that all hyper-parameters match Table 32. Unfortunately, one run crashed at around\n80B tokens seen, so we take the latest shared checkpoint of both models for comparison.27 We see that annotator\nκ of the two variants are very similar (Table 33).\nFine-tuning learning rate ablation.\nHaving selected no prompt and the condensed generation format (Al-\ngorithm 13), we run ablations to determine the best learning rate to use during training.\nFor these ex-\nperiments, we fine-tune Qwen2.5-0.5b-Instruct for 12B tokens and test the following learning rates:\n26We used an abridged version of the condensed output format that only outputs the 9 categories in Prompt 1.\n27The model with no prompt, condensed generation format saw more examples because more examples can be packed into a\ngiven sequence.\n43\nCategory\nPrompt 1 Input & Format\nNo Prompt, Condensed Generation\nRandom\nSTEM\nRandom\nSTEM\nKnowledge Domain\n0.47± 0.03\n0.52± 0.03\n0.54± 0.02\n0.58± 0.03\nCognitive Process\n0.56± 0.02\n0.62± 0.03\n0.68± 0.02\n0.74± 0.02\nDocument Type V1\n0.82± 0.01\n0.84± 0.01\n0.82± 0.01\n0.83± 0.01\nFree Decimal Corr. (level 1)\n0.83± 0.01\n0.86± 0.01\n0.86± 0.01\n0.91± 0.01\nFree Decimal Corr. (level 2)\n0.74± 0.01\n0.65± 0.01\n0.79± 0.01\n0.78± 0.01\nFree Decimal Corr. (level 3)\n0.54± 0.01\n0.48± 0.02\n0.62± 0.01\n0.58± 0.01\nExtraction Artifacts\n0.31± 0.03\n0.48± 0.04\n0.20± 0.03\n0.21± 0.04\nMissing Content\n0.64± 0.02\n0.74± 0.02\n0.50± 0.02\n0.55± 0.03\nOverall mean\n0.62\n0.65\n0.63\n0.65\nTable 33: Annotator κ (± standard error) against gold annotators on the random (n = 2,017) and STEM\n(n = 871) evaluation sets for prompt / generation format ablation.\n2 × 10−6, 1 × 10−5, 1 × 10−4, and 1 × 10−3. All other hyper-parameters match Table 32. We can see\nthat performance drops off at a learning rate of 1 × 10−3. We select 1 × 10−4 given its the highest learning rate\nbefore the drop. The results for this ablation can be found in Table 34.\nCategory\n2 × 10−6\n1 × 10−5\n1 × 10−4\n1 × 10−3\nRand.\nSTEM\nRand.\nSTEM\nRand.\nSTEM\nRand.\nSTEM\nKnowledge Domain\n0.63±0.02\n0.61±0.03\n0.63±0.02\n0.65±0.03\n0.64±0.02\n0.64±0.03\n0.58±0.02\n0.61±0.03\nCognitive Process\n0.68±0.02\n0.76±0.02\n0.69±0.02\n0.76±0.02\n0.69±0.02\n0.75±0.02\n0.67±0.02\n0.73±0.02\nDocument Type V1\n0.82±0.01\n0.84±0.01\n0.84±0.01\n0.86±0.01\n0.85±0.01\n0.86±0.01\n0.83±0.01\n0.85±0.01\nFree Decimal Corr. (level 1)\n0.87±0.01\n0.89±0.01\n0.88±0.01\n0.92±0.01\n0.88±0.01\n0.92±0.01\n0.86±0.01\n0.87±0.01\nFree Decimal Corr. (level 2)\n0.79±0.01\n0.70±0.01\n0.80±0.01\n0.78±0.01\n0.81±0.01\n0.80±0.01\n0.78±0.01\n0.73±0.01\nFree Decimal Corr. (level 3)\n0.61±0.01\n0.52±0.02\n0.64±0.01\n0.58±0.01\n0.64±0.01\n0.62±0.01\n0.62±0.01\n0.54±0.01\nExtraction Artifacts\n0.28±0.02\n0.39±0.04\n0.26±0.02\n0.43±0.03\n0.28±0.02\n0.42±0.04\n0.23±0.03\n0.38±0.03\nMissing Content\n0.52±0.02\n0.60±0.02\n0.50±0.02\n0.60±0.02\n0.50±0.01\n0.60±0.02\n0.50±0.02\n0.59±0.03\nDocument Type V2\n0.86±0.01\n0.86±0.01\n0.88±0.01\n0.87±0.01\n0.88±0.01\n0.87±0.01\n0.87±0.01\n0.83±0.01\nEducational Level\n0.75±0.03\n0.87±0.02\n0.77±0.02\n0.86±0.02\n0.78±0.03\n0.85±0.02\n0.74±0.03\n0.83±0.02\nReasoning Depth\n0.86±0.02\n0.75±0.02\n0.86±0.02\n0.75±0.02\n0.86±0.02\n0.75±0.03\n0.86±0.02\n0.73±0.03\nTechnical Correctness\n0.70±0.02\n0.74±0.03\n0.71±0.01\n0.74±0.03\n0.72±0.02\n0.73±0.02\n0.71±0.01\n0.70±0.03\nOverall mean\n0.70\n0.71\n0.71\n0.73\n0.71\n0.74\n0.69\n0.70\nTable 34: Annotator κ (mean ± standard error) of Qwen2.5-0.5b-Instruct finetuned for 12B tokens with four\nlearning rates. Each pair of columns shows Random (n=2,017) and STEM (n=871) subsets; headers indicate the\nlearning rate.\nFine-tuning token budget ablation.\nHaving selected a learning rate of 1 × 10−4, we experiment with the token\nbudget used for fine-tuning Qwen2.5-0.5b-Instruct. All hyper-parameters are set to the values in Table 32\nand we vary the token budget as follows: 12B, 22B, 42B, 82B. Annotator κ does not change as we fine-tune\nfor longer token budgets. Given we trained all 4 models, we select the model trained on the most tokens for\nEAI-Distill-0.5b. However, it is clear that similar performance can be achieved with a smaller token budget.\nThis has important implications for determining the number of synthetic annotations needed from a powerful LLM\nto adequately fine-tune a much smaller LM for classification / data labeling tasks like the taxonomic annotation.\nResults of this ablation can be found in Table 35.\nA.12\nQwen2.5-32b-Instruct Large Scale Annotation\nFor the purposes of fine-tuning EAI-Distill-0.5b and getting a large sample for experimentation, we\nlabel 104.6M documents randomly sampled from our processed Common Crawl (Appendix A.2) with\nQwen2.5-32b-Instruct. This annotation was done in two phases given we added additional categories to\nthe taxonomy before fine-tuning. The two prompts used for annotation can be found in Appendix A.13.3 and\nAppendix A.13.3. Both phases used the system prompt found in Appendix A.13.3.\n44\nCategory\n12 B tokens\n22 B tokens\n42 B tokens\n82 B tokens\nRand.\nSTEM\nRand.\nSTEM\nRand.\nSTEM\nRand.\nSTEM\nKnowledge Domain\n0.63± 0.02\n0.64± 0.03\n0.62± 0.02\n0.65± 0.03\n0.61± 0.02\n0.66± 0.02\n0.63± 0.02\n0.65± 0.03\nCognitive Process\n0.69± 0.01\n0.75± 0.02\n0.70± 0.01\n0.76± 0.02\n0.69± 0.01\n0.76± 0.02\n0.70± 0.02\n0.76± 0.02\nDocument Type V1\n0.85± 0.01\n0.87± 0.01\n0.84± 0.01\n0.86± 0.01\n0.85± 0.01\n0.87± 0.01\n0.83± 0.01\n0.86± 0.01\nFree Decimal Corr. (level 1)\n0.88± 0.01\n0.92± 0.01\n0.88± 0.01\n0.93± 0.01\n0.88± 0.01\n0.93± 0.01\n0.88± 0.01\n0.93± 0.01\nFree Decimal Corr. (level 2)\n0.81± 0.01\n0.80± 0.01\n0.81± 0.01\n0.80± 0.01\n0.81± 0.01\n0.80± 0.01\n0.81± 0.01\n0.80± 0.01\nFree Decimal Corr. (level 3)\n0.64± 0.01\n0.62± 0.01\n0.64± 0.01\n0.61± 0.01\n0.65± 0.01\n0.61± 0.01\n0.63± 0.01\n0.61± 0.01\nExtraction Artifacts\n0.28± 0.02\n0.42± 0.03\n0.28± 0.02\n0.39± 0.03\n0.28± 0.02\n0.39± 0.03\n0.27± 0.02\n0.37± 0.03\nMissing Content\n0.50± 0.01\n0.60± 0.02\n0.50± 0.01\n0.59± 0.02\n0.50± 0.01\n0.58± 0.02\n0.48± 0.01\n0.57± 0.02\nDocument Type V2\n0.88± 0.01\n0.87± 0.01\n0.88± 0.01\n0.87± 0.01\n0.88± 0.01\n0.86± 0.01\n0.88± 0.01\n0.86± 0.01\nEducation Level\n0.78± 0.02\n0.85± 0.02\n0.78± 0.02\n0.85± 0.02\n0.80± 0.03\n0.86± 0.02\n0.79± 0.02\n0.86± 0.02\nReasoning Depth\n0.86± 0.02\n0.74± 0.03\n0.87± 0.01\n0.76± 0.03\n0.86± 0.01\n0.76± 0.02\n0.87± 0.01\n0.76± 0.02\nTechnical Correctness\n0.72± 0.01\n0.73± 0.03\n0.72± 0.01\n0.73± 0.02\n0.72± 0.02\n0.75± 0.03\n0.72± 0.01\n0.75± 0.02\nOverall mean\n0.71\n0.74\n0.71\n0.73\n0.71\n0.74\n0.71\n0.73\nTable 35: Annotator κ (mean ± s.e.) for Qwen2.5-0.5b-Instruct fine-tuned on 12, 22, 42, and 82 billion\ntraining tokens. Each pair of columns shows Random (n=2,017) and STEM (n=871) subsets.\nAlgorithm 12: Function to subsample documents with more than 30,000 characters\ndef chunk_text(text, max_char_per_doc):\nif len(text) <= max_char_per_doc:\nreturn text\nchunk_size = max_char_per_doc // 3\nstart = text[:chunk_size]\n# Calculate valid range for middle section\nmiddle_start = chunk_size\nmiddle_end = len(text) - chunk_size\n# Randomly select middle point within valid range\nmid_point = random.randint(middle_start + chunk_size//2, middle_end - chunk_size//2)\nmiddle = text[mid_point - chunk_size//2:mid_point + chunk_size//2]\nend = text[-chunk_size:]\nreturn f\"[beginning]\\n{start}\\n[middle]\\n{middle}\\n[end]\\n{end}\"\nA.12.1\nDeepSeek-V3 SGLang Image\nAt the time we ran this large scale annotation, the SGLang image for AMD MI300x was much slower. The old\nimage mentioned in Table 12 refers to v0.4.1.post3 and the newer image benchmarked refers to v0.4.3.post4.\nA.13\nPrompts, Generation Formats, & Examples\nA.13.1\nDocument Sampling\nWhen running inference with Qwen2.5-32b-Instruct and EAI-Distill-0.5b, we subsample documents with\nmore than 30,000 characters. We do this to improve inference speed. We use the function in Algorithm 12 to take\nthe beginning, random sample of the middle, and end of any document with over 30,000 characters. Assuming\nthere are 4 bytes per token, the maximum document should be about 7500 tokens. We decide not to tokenize\nwhen sub-sampling to avoid the CPU-cost of tokenizing all document before inference. The training data for\nEAI-Distill-0.5b is prepared using subsampling to ensure that the training and inference distributions match.\n45\nAlgorithm 13: Template for EAI-Distill-0.5b Condensed Model Output Format\n{FDC primary classification},{FDC secondary classification or skip}\n{Bloom cognitive process primary (1-6)},{Bloom cognitive process secondary (1-6) or skip}\n{Bloom knowledge domain primary (1-4)},{Bloom knowledge domain secondary (1-4) or skip}\n{Document type v1 primary (1-17)},{Document type v1 secondary (1-17) or skip}\n{Extraction artifacts primary (0-4)},{Extraction artifacts secondary (0-4) or skip}\n{Missing content primary (0-6)},{Missing content secondary (0-6) or skip}\n{Document type v2 primary (1-25)},{Document type v2 secondary (1-25) or skip}\n{Reasoning depth primary (1-6)},{Reasoning depth secondary (1-6) or skip}\n{Technical correctness primary (1-6)},{Technical correctness secondary (1-6) or skip}\n{Educational level primary (1-5)},{Educational level secondary (1-5) or skip}\nA.13.2\nEAI-Distill-0.5b Generation Templates\nDuring fine-tuning we update the generation to a highly-condensed format. We do so by programatically extracting\nthe response codes from Generations 1 and 2. The generation template for EAI-Distill-0.5b can be found in\nAlgorithm 13.\nA.13.3\nPrompts\nEAI-Distill-0.5b System Prompt\nGitHub: EAI-Distill-0.5b System Prompt\nQwen2.5-32b-Instruct System Prompt\nGitHub: System Prompt\nQwen2.5-32b-Instruct Prompt 1\nGitHub: Prompt 1\nQwen2.5-32b-Instruct Prompt 2\nGitHub: Prompt 2\n46\n",
    "content": "### 1. What are the core contents and main contributions of this paper?\n\nThe core content of this paper introduces ESSENTIAL-WEB V1.0, a web dataset containing 24 trillion tokens. Each document in the dataset is annotated with a twelve-category taxonomy covering topics, format, content complexity, and quality. Using simple SQL-style filters, users can quickly obtain competitive web-curated datasets in various fields such as mathematics (-8.0% relatively better than state-of-the-art), web code (+14.3%), STEM (+24.5%), and medicine (+8.6%).\n\n**Main Contributions:**\n1. **ESSENTIAL-WEB V1.0**: A 24-trillion-token dataset where each document comes with detailed metadata.\n2. **Downstream Validation**: Datasets generated using simple SQL filters perform well across multiple domains (mathematics, code, STEM, medicine).\n3. **Taxonomy Evaluation Toolkit**: Introduced a set of metrics to assess the orthogonality, accuracy, and expressiveness of the taxonomy.\n4. **Efficient Annotation Model**: Released EAI-Distill-0.5b, a 0.5B-parameter classifier fine-tuned from Qwen2.5-0.5b-Instruct. It can annotate the entire dataset within approximately 90k MI300x GPU hours while maintaining performance close to the teacher model.\n\n---\n\n### 2. What breakthroughs or innovations does this paper introduce?\n\n**Breakthroughs and Innovations:**\n\n1. **Efficient Construction of Large-Scale Datasets**:\n   - Proposed a taxonomy-based framework for rapidly extracting high-value data from Common Crawl.\n   - Leveraged powerful open-source large language models (e.g., Qwen2.5-32B-Instruct) to generate synthetic labels and trained more efficient classifiers like EAI-Distill-0.5b for inference.\n\n2. **Taxonomy Design**:\n   - Designed a 12-field taxonomy (EAI-TAXONOMY) that covers topics, webpage types, content complexity, and quality.\n   - Introduced metrics for orthogonality, correctness, and expressiveness to optimize taxonomy design.\n\n3. **High Performance Across Domains**:\n   - Datasets generated through simple filters outperform or match existing state-of-the-art open-source datasets in mathematics, code, STEM, and medicine.\n   - Avoided the complex pipelines traditionally required for domain-specific datasets, significantly simplifying the data preparation process.\n\n4. **Efficient Model Distillation**:\n   - EAI-Distill-0.5b is 50 times faster than its teacher model while maintaining similar performance.\n   - Context distillation and generation compression techniques were used to significantly improve inference efficiency.\n\n5. **Openness and Transparency**:\n   - Both the dataset and the EAI-Distill-0.5b model are publicly available on HuggingFace, promoting reproducibility and community iteration.\n\n---\n\n### 3. Based on the content of this paper, what are some good startup ideas?\n\nHere are some potential startup ideas based on the core content of this paper:\n\n#### (1) **Smart Data Filtering Platform**\n- **Project Description**: Develop an intelligent data filtering platform based on ESSENTIAL-WEB V1.0 and EAI-TAXONOMY, allowing users to quickly retrieve high-quality data in specific domains via simple SQL queries.\n- **Use Cases**: For training machine learning models, generating domain-specific knowledge bases, or supporting customized large language model development.\n- **Advantages**: Eliminates the need for complex preprocessing workflows, reducing data preparation costs.\n\n#### (2) **Domain-Specific Large Language Model Training Service**\n- **Project Description**: Provide an on-demand service for generating domain-specific large language models using high-quality datasets from ESSENTIAL-WEB V1.0.\n- **Use Cases**: For example, offer customized language models for education, healthcare, programming, and other industries.\n- **Advantages**: Users only need to define their requirements, and the platform automatically handles data filtering and model training.\n\n#### (3) **Automated Data Curation Tool for Code and Mathematics**\n- **Project Description**: Focus on developing an automated data curation tool for mathematics and code, helping developers quickly generate high-quality training data.\n- **Use Cases**: Suitable for program synthesis, code completion, and mathematical problem-solving tasks.\n- **Advantages**: Compared to traditional manual curation methods, it significantly improves efficiency and data quality.\n\n#### (4) **Academic Research Support Platform**\n- **Project Description**: Provide a platform based on ESSENTIAL-WEB V1.0 to help academic researchers quickly access high-quality literature data in specific disciplines.\n- **Use Cases**: Support interdisciplinary research, scientific literature analysis, and educational material generation.\n- **Advantages**: Rich metadata and flexible filtering capabilities make it easy for researchers to locate target data.\n\n#### (5) **Big Data Solutions for Medicine and Health**\n- **Project Description**: Use the medical subset of ESSENTIAL-WEB V1.0 to develop big data solutions tailored for the healthcare industry.\n- **Use Cases**: Support medical question-answering systems, disease diagnosis models, and health data analytics.\n- **Advantages**: Provide validated high-quality medical data, accelerating the development of medical AI models.\n\n#### (6) **Open Source Dataset Marketplace**\n- **Project Description**: Create an open-source dataset marketplace based on ESSENTIAL-WEB V1.0, enabling users to obtain customized datasets through simple queries.\n- **Use Cases**: Serve AI developers and researchers who require large amounts of training data.\n- **Advantages**: Lower the threshold for data acquisition, promoting the普及and innovation of AI technology.\n\nThese projects can leverage the powerful features of ESSENTIAL-WEB V1.0 to provide users with efficient and flexible data solutions while driving the application and development of AI technology in various fields.",
    "github": "https://huggingface.co/datasets/EssentialAI/essential-web-v1.0",
    "hf": ""
}