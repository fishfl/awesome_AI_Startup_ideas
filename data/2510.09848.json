{
    "id": "2510.09848",
    "title": "Cell Instance Segmentation: The Devil Is in the Boundaries",
    "summary": "This article proposes a new pixel clustering-based cell instance segmentation method called Ceb, which utilizes cell boundary features and labels to segment foreground pixels into cell instances.",
    "abstract": "State-of-the-art (SOTA) methods for cell instance segmentation are based on deep learning (DL) semantic segmentation approaches, focusing on distinguishing foreground pixels from background pixels. In order to identify cell instances from foreground pixels (e.g., pixel clustering), most methods decompose instance information into pixel-wise objectives, such as distances to foreground-background boundaries (distance maps), heat gradients with the center point as heat source (heat diffusion maps), and distances from the center point to foreground-background boundaries with fixed angles (star-shaped polygons). However, pixel-wise objectives may lose significant geometric properties of the cell instances, such as shape, curvature, and convexity, which require a collection of pixels to represent. To address this challenge, we present a novel pixel clustering method, called Ceb (for Cell boundaries), to leverage cell boundary features and labels to divide foreground pixels into cell instances. Starting with probability maps generated from semantic segmentation, Ceb first extracts potential foreground-foreground boundaries with a revised Watershed algorithm. For each boundary candidate, a boundary feature representation (called boundary signature) is constructed by sampling pixels from the current foreground-foreground boundary as well as the neighboring background-foreground boundaries. Next, a boundary classifier is used to predict its binary boundary label based on the corresponding boundary signature. Finally, cell instances are obtained by dividing or merging neighboring regions based on the predicted boundary labels. Extensive experiments on six datasets demonstrate that Ceb outperforms existing pixel clustering methods on semantic segmentation probability maps. Moreover, Ceb achieves highly competitive performance compared to SOTA cell instance segmentation methods.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Peixian Liang,Yifan Ding,Yizhe Zhang,Jianxu Chen,Hao Zheng,Hongxiao Wang,Yejia Zhang,Guangyu Meng,Tim Weninger,Michael Niemier,X. Sharon Hu,Danny Z Chen",
    "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "comments": "Comments:Accepted at IEEE Transactions On Medical Imaging (TMI)",
    "keypoint": "Ceb is a novel pixel clustering method that leverages cell boundary features and labels to divide foreground pixels into cell instances.\nCeb uses a revised Watershed algorithm to extract potential foreground-foreground boundaries.\nA boundary feature representation called boundary signature is constructed by sampling pixels from the current foreground-foreground boundary and neighboring background-foreground boundaries.\nA lightweight boundary classifier predicts binary boundary labels based on boundary signatures.\nCell instances are obtained by merging or dividing regions based on predicted boundary labels.\nCeb outperforms existing pixel clustering methods on semantic segmentation probability maps across six datasets.\nCeb achieves highly competitive performance compared to state-of-the-art cell instance segmentation methods.\nThe method incorporates temporal instance consistency for 2D temporal datasets, improving segmentation performance.\nCeb preserves geometric properties of cell instances such as shape, curvature, and convexity better than pixel-wise approaches.\nAn optimized instance matching model is used to assign true/false labels to generated boundaries during training.",
    "date": "2025-10-15",
    "paper": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\n1\nCell Instance Segmentation:\nThe Devil Is in the Boundaries\nPeixian Liang, Yifan Ding, Yizhe Zhang, Jianxu Chen, Hao Zheng, Hongxiao Wang, Yejia Zhang,\nGuangyu Meng, Tim Weninger, Michael Niemier, X. Sharon Hu, Danny Z Chen\nAbstract— State-of-the-art (SOTA) methods for cell in-\nstance segmentation are based on deep learning (DL)\nsemantic segmentation approaches, focusing on distin-\nguishing foreground pixels from background pixels. In or-\nder to identify cell instances from foreground pixels (e.g.,\npixel clustering), most methods decompose instance in-\nformation into pixel-wise objectives, such as distances to\nforeground-background boundaries (distance maps), heat\ngradients with the center point as heat source (heat dif-\nfusion maps), and distances from the center point to\nforeground-background boundaries with fixed angles (star-\nshaped polygons). However, pixel-wise objectives may lose\nsignificant geometric properties of the cell instances, such\nas shape, curvature, and convexity, which require a collec-\ntion of pixels to represent. To address this challenge, we\npresent a novel pixel clustering method, called Ceb (for Cell\nboundaries), to leverage cell boundary features and labels\nto divide foreground pixels into cell instances. Starting\nwith probability maps generated from semantic segmen-\ntation, Ceb first extracts potential foreground-foreground\nboundaries (i.e., boundary candidates) with a revised\nWatershed algorithm. For each boundary candidate, a\nboundary feature representation (called boundary signa-\nture) is constructed by sampling pixels from the current\nforeground-foreground boundary as well as the neighbor-\ning background-foreground boundaries. Next, a lightweight\nboundary classifier is used to predict its binary boundary\nlabel based on the corresponding boundary signature. Fi-\nnally, cell instances are obtained by dividing or merging\nneighboring regions based on the predicted boundary la-\nbels. Extensive experiments on six datasets demonstrate\nthat Ceb outperforms existing pixel clustering methods\non semantic segmentation probability maps. Moreover,\nCeb achieves highly competitive performance compared to\nstate-of-the-art cell instance segmentation methods. The\ncode is available at: https://github.com/pxliang/Ceb.\nThis research was supported in part by NSF grant 2212239, DARPA\ncontracts HR001121C0168 and HR00112290106, and the Louisiana\nBoard of Regents under Contract Number LEQSF(2025-28)-RD-A-\n21. The work of J.C. was partially supported by the “Ministerium f¨ur\nKultur und Wissenschaft des Landes Nordrhein-Westfalen” and “Der\nRegierende B¨urgermeister von Berlin, Senatskanzlei Wissenschaft und\nForschung”, and by the Bundesministerium f¨ur Forschung, Technologie\nund Raumfahrt, BMFTR under the funding reference 161L0272.\nPeixian Liang, Yifan Ding, Yizhe Zhang, Hongxiao Wang, Yejia Zhang,\nGuangyu Meng, Tim Weninger, Michael Niemier, X. Sharon Hu, and\nDanny Z. Chen are with the Department of Computer Science and\nEngineering, University of Notre Dame, Notre Dame, IN 46556, USA (e-\nmail: {pliang, yding4, yzhang29, hwang21, yzhang46, gmeng, tweninge,\nmniemier, shu, dchen}@nd.edu).\nJianxu\nChen\nis\nwith\nLeibniz-Institut\nf¨ur\nAnalytische\nWis-\nsenschaften–ISAS–e.V.,\nDortmund\n44139,\nGermany\n(e-mail:\njianxu.chen@isas.de).\nHao Zheng is with the School of Computing and Informatics, Uni-\nversity of Louisiana at Lafayette, Lafayette, LA 70503, USA (email:\nhao.zheng@louisiana.edu).\nIndex Terms— Cell instance segmentation, Optimal in-\nstance matching, Boundary classification, Temporal in-\nstance consistency\nI. INTRODUCTION\nC\nELL instance segmentation is a fundamental problem in\nquantitative cell biology research. It provides accurate\nand detailed information about individual cells, including their\npositions, morphology, and life cycle. This level of granularity\nis crucial for understanding cellular behaviors, dynamics, and\ninteractions [1]. Unlike other instance segmentation tasks\nsuch as those in natural scenes (e.g., the COCO dataset [2])\nand multi-class organs (e.g., heart [3] and chest [4]), cell\ninstance segmentation presents distinguished characteristics\nand challenges. For example, cell instances typically exhibit a\nroughly convex shape (e.g., resembling a star-shaped polygon\nwhose entire boundary is visible from an interior point). A\nsingle image of cells can contain hundreds or even thousands\nof individual cells. In many scenarios, cells are tightly packed\ntogether. Cells of the same type can have either similar or\nquite different sizes and different textual characteristics.\nDeep learning (DL) semantic segmentation methods have\nexhibited remarkable performance in biomedical image seg-\nmentation tasks. Existing state-of-the-art (SOTA) methods for\ncell instance segmentation rely on semantic segmentation\nto distinguish foreground pixels from background pixels. In\norder to identify accurate cell instances among foreground\npixels (e.g., pixel clustering), most current methods use\npixel-wise objectives. Hover-Net [5] used shortest distances\nto foreground-background boundaries (distance maps); Cell-\nPose [6] used heat gradients with the center point as heat\nsource (heat diffusion maps); StarDist [7] used multiple dis-\ntances from inner point to foreground-background boundaries\nwith fixed angles (star-shaped polygons); pixel-embedding\nmethods [8]–[11] used learnable vectors to indicate pair-\nwise similarities (pixel embeddings). However, these pixel-\nwise objectives ignore significant geometric properties of the\noriginal cell instances, such as shape, curvature, and convexity,\nwhich require a structured collection of pixels to represent.\nIn this paper, we propose a new approach for clustering\nforeground pixels into cell instances, called Ceb (for Cell\nboundaries), based on boundary-wise features and labels.\nUnlike pixel-wise objectives, boundary-wise features and ob-\njectives are based on a structured group of key pixels, and\nthus instance geometric properties can be better preserved.\nSpecifically, our Ceb framework work as follows. Given a\narXiv:2510.09848v1  [cs.CV]  10 Oct 2025\n2\nIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nstep (1) \nraw image\nseeds\nprobability map\nCNN\nstep (4)\nstep (2)\nboundaries+regions\nboundary signatures\nclassiﬁer predictions\nstep (5) \nﬁnal results\nprediction\nboundary labels\n ground truth\nstep (3) \nregion nodes\nboundary edges\nsupervision\nclassifer\n1.0\n0.0\n0.9\n0.2\nFig. 1: An overview of our Ceb framework. An input image is fed to a CNN network (e.g., U-Net) to produce a semantic\nprobability map. Step (1) seed generation generates seeds from the probability map. Step (2) boundary generation uses these\nseeds and the probability map to produce possible boundaries and the divided regions. Step (3) boundary label assignment\nmatches ground truth instance masks and the divided regions to attain true regions and their corresponding boundaries (as true\nboundaries). Step (4) boundary signature extraction generates boundary-based feature representations, boundary signatures,\nfor all possible boundaries. During training, these boundary signatures, along with their corresponding true/false boundary\nlabels obtained in Step (3), are fed to Step (5) boundary classification to train a boundary classifier. During inference, the\nboundary classifier predicts a true/false label for each possible boundary based on its boundary signature. The final instance\nresults are obtained by merging connected regions divided by false boundaries.\nprobability map produced by a DL semantic segmentation\nmodel (e.g., U-Net [12]) for an input image, we first com-\npute instance seeds from the probability map, and generate\nall potential cell boundaries with a revised Watershed algo-\nrithm [13]. In the training stage, labels (true or false) of the\nbinary boundaries thus generated are obtained by computing\noptimal matching between ground truth cell instances and all\nthe possible instance candidates produced by enumerating all\npossible binary boundary labels. For each boundary candi-\ndate, we then extract a novel boundary feature representation\n(called boundary signature) by sampling key pixels from\nthe current foreground-foreground boundary as well as the\nneighboring background-foreground boundaries. Based on the\nextracted boundary signatures, we use a lightweight binary\nimage classifier (a convolutional neural network (CNN)) to\ndistinguish true and false boundaries. In the inference stage,\npositive boundaries are kept, negative boundaries are removed,\nand all neighboring regions divided by predicted negative\nboundaries are merged to form cell instances. On 2D temporal\n(video) datasets, our method can further incorporate temporal\nconsistency information [14] to better segment cell instances.\nWe conduct experiments on six cell instance segmentation\ndatasets (four video datasets and two 2D datasets). Our bound-\nary classifier shows an excellent ability to distinguish true/false\nboundaries based on novel boundary-based features. Ceb out-\nperforms all the compared foreground pixel clustering methods\non semantic probability maps across different datasets. Com-\npared to SOTA instance segmentation methods, Ceb also yields\ncompetitive performances. In video settings, our temporal-\nconsistency based method further improves the performance.\nIn summary, our main contributions are as follows.\n• We propose Ceb, a novel method for clustering fore-\nground pixels into cell instances. Ceb tackles the pixel\nclustering problem with a DL boundary classifier based\non boundary-level features, which can better preserve\ninstance geometric properties compared to existing pixel-\nwise objectives.\n• We develop a novel boundary-level feature representation,\ncalled boundary signature, by sampling pixels from each\npotential foreground-foreground boundary and its neigh-\nboring background-foreground boundaries. Boundary sig-\nnatures can effectively reflect geometric properties which\nare essential for distinguishing true and false boundaries.\n• We revise the Watershed algorithm to generate all po-\ntential foreground-foreground boundaries, and present an\noptimized instance matching method to assign labels to\nthe generated boundaries.\n• We propose a novel matching method to incorporate\ntemporal instance consistency into the Ceb framework,\nfurther improving instance segmentation performance on\n2D temporal datasets.\nII. RELATED WORK\nA. Cell Instance Segmentation\nRecent SOTA methods for cell instance segmentation can\nbe broadly categorized into two types: semantic segmentation-\nbased approaches and region-based approaches, with the vast\nmajority of SOTA methods belonging to the former type.\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING\n3\n1) Semantic Segmentation-based Approaches:\nSemantic\nsegmentation-based methods first distinguish foreground pix-\nels and background pixels, and then cluster foreground pixels\ninto individual instances [12], [15]–[19]. The most recent\nwork adopted pixel-wise objectives to distinguish individual\ninstances. In the training stage, cell instance labels are trans-\nformed to pixel-wise labels. In the inference stage, pixel-\nwise labels are predicted and further processed to produce\nfinal instances. For example, Hover [5] and CellViT [20]\nused distance maps considering the distances from each inner\npixel to the nearest instance boundaries. StarDist [7] extended\ndistance maps to radial maps with 32 fixed angles. CellPose [6]\nintroduced heat diffusion maps with the center point as the heat\nsource, and the heat gradients of each point were considered as\npixel-wise labels. Another kind of popular methods is based\non pixel-embedding, using contrastive learning to represent\npixel-pixel similarities [8]–[11], [21]. Pixels with similar em-\nbeddings are clustered together to form the final instance seg-\nmentation results. For example, InstanSeg [22] predicted seed\npoints that represent instance centers and learned pixel-wise\nembeddings which were used to cluster pixels into instances\nbased on their similarity to seed embeddings. Despite their\nflexibility and generalizability, these pixel-wise objectives may\nstill lose significant geometric properties of original cell in-\nstances, such as shape, curvature, and convexity, which require\na structured collection of pixels to represent. In this work, we\npresent Ceb, aiming to preserve the structure properties of cell\ninstances with boundary-level features by selecting pixels from\nforeground-foreground boundaries and background-foreground\nboundaries.\n2) Region-based Approaches: In region-based models, in-\nstances are assigned to grids or anchors within an image,\nallowing for region-wise classification to detect and segment\ninstances [23]–[28]. For example, CelloType [29] employed\na Transformer-based detector (DINO) to generate bounding\nboxes and extract latent features, which are then processed by\nMaskDINO for joint optimization of detection, segmentation,\nand classification. However, such region-based representations\nmay not be a good fit for cell instances. For example, bounding\nboxes can be suppressed by nearby instances, especially in\ncrowded scenes. Pre-defined bounding boxes can also suffer\nfrom the imbalance problem of false/true instances.\nB. Boundary Generation Methods\nInstance boundaries play a vital role in image segmentation.\nTraditional boundary generation methods generate instance\nboundaries based on local pixel features. Two well-known\nmethods are Watershed [13] and Active Contours [30]. The\nWatershed algorithm considers an image as a heat map based\non pixel intensities. Instance boundaries are determined as lo-\ncal minimum lines/curves between instances. Subsequent work\nincorporated the Watershed algorithm with DL methods [31],\n[32]. For example, DIC [31] predicted cell seeds first and used\nWatershed as a foreground pixel clustering step to obtain final\ncell instances. In [33], the Watershed algorithm is transformed\ninto a learnable model to consider altitudes of pixels as well\nas the corresponding region assignment. Active contours are\nenergy-minimizing curves that deform and converge to the\nboundaries of the regions of interest (RoIs) in an image [34]–\n[36]. In [37], parameter maps/initial contours for Sobolev\nactive contours were predicted by CNN models, and Sobolev\nactive contours were applied as a foreground pixel clustering\nstep to obtain final predictions. Subsequent work proposed\nactive contour inspired losses (e.g., Mumford-Shah loss [38],\nactive contour without edge (ACWE) loss [39]–[42], and sneak\nactive contour loss [43]). Boundary-based approaches are also\na rising focus for point cloud segmentation. For example,\nCBL [44] proposed a boundary contrastive loss for point cloud\nsegmentation. Unlike most existing boundary-based methods\nwhich utilize solely pixel-level features to generate bound-\naries, Ceb employs boundary-level features to classify binary\nboundary labels on top of foreground pixels. Consequently,\nCeb retains the advantages of semantic segmentation compared\nto the other boundary generation methods while still being\nable to preserve instance structure properties as the known\nboundary generation methods.\nC. Segmentation Trees\nAnother line of related work is segmentation tree based\nmethods [45]–[51], which utilize tree-based structures to solve\ninstance segmentation problems. These methods generate over-\nsegmented components, and then perform final instance seg-\nmentation by clustering the components. Unlike our method,\nsuch methods often produce tree-like structures without DL\nnetworks. For example, a tree can be generated from super-\npixels [45], [46]; after the “leaf” regions of the initial over-\nsegmented candidate regions are obtained, a tree structure\nis built by iteratively merging similar super-pixels, until a\npre-specified stopping criterion is met. GP-S3Net [52] used\ndensity-based spatial clustering of applications with noise\n(DBSCAN) [53] to produce over-segmented candidates, and\na graph neural network (GNN) model was used to predict the\nlabel of each edge. Note that these methods make final deci-\nsions based on node features, which make modeling geometric\nfeatures of instance regions difficult. In contrast, our method\ndirectly utilizes boundary features for boundary classification,\nwhich are generally easier to identify and classify.\nIII. METHODOLOGY\nIn this section, we present our Ceb framework for cell\ninstance segmentation. Fig. 1 gives an overview of our frame-\nwork, which consists of five main steps. (1) Seed generation\n(Section III-A): We first generate instance seeds from seman-\ntic segmentation probability maps. (2) Boundary generation\n(Section III-B): Given the generated seeds and probability\nmaps, we employ a revised Watershed algorithm to generate\npossible cell boundaries and the regions enclosed by these\nboundaries. Thus, the cell instance segmentation problem\nbecomes a boundary selection problem. (3) Boundary label\nassignment (Section III-C): To obtain boundary labels (true or\nfalse) for the training stage, we build an optimized matching\nmodel between ground truth instance masks and the divided\nregions to attain true regions. The corresponding boundaries\n4\nIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nAlgorithm 1: A Revised Watershed Algorithm to\nGenerate Possible Cell Boundaries and Regions\n1 Input Foreground Pixels F = {(xf, yf)}; Seeds\nS = {(xs, ys)} ∈F; Probability map p,\np(xf, yf) ∈[0, 1], ∀(xf, yf) ∈F;\n2 Output Regions R, Boundaries B # Regions and Boundaries are\nboth Hashmaps, representing index and corresponding pixels.\n3 R ←∅, B ←∅\n4 # status mapping function f: −1 represents unvisited\n(UNVISITED), −2 represents in the queue to be assigned\n(INQE), −3 represents a place holder to be placed in the queue\nlater (MASK), 0 represents a Watershed boundary (WSHD),\npositive integer represents the region index.\n5 for (x, y) ∈F do\n6\nf[(x, y)] ←−1\n7 region index ←0\n8 for (x, y) ∈S do\n9\nregion index ←region index + 1\n10\nf[(x, y)] ←region index\n11\nR[f[(x, y)]].add((x, y))\n12 Initialize Queue ←∅\n13 Initialize Hashmap ←∅# key represents probability map value,\nvalue is a list of pixels with the corresponding probability\n14 for (x, y) ∈F \\ S do\n15\nHashmap[p(x, y)].add((x, y))\n16 for key ∈reverse sorted(Hashmap.keys) do\n17\nfor (x, y) ∈Hashmap[key] do\n18\nf[(x, y)] ←MASK\n19\nfor (xn, yn) ∈Neighbors(x, y) do\n20\nif f[(xn, yn)] > 0 then\n21\nEnqueue(Queue, (x, y)) f[(x, y)] ←INQE\n22\nbreak\n23\nwhile Queue ̸= ∅do\n24\n(x, y) ←Dequeue(Queue);\n25\nfor (xn, yn) ∈Neighbors(x, y) do\n26\nif f[(xn, yn)] > 0 then\n27\nif f[(x, y)] = INQE then\n28\nf[(x, y)] ←f[(xn, yn)]\nR[f[(xn, yn)]].add((x, y))\n29\nelse if f[(x, y)] > 0 and f[(x, y)] ̸= f[(xn, yn)]\nthen\n30\nB[f[(x, y)], f[(xn, yn)]].add((x, y))\nf[(x, y)] ←WSHD\n31\nelse if f[(xn, yn)] = WSHD then\n32\nif f[(x, y)] = INQE then\n33\nB[B.find key((xn, yn))].add((x, y))\nf[(x, y)] ←WSHD\n34\nelse if f[(xn, yn)] = MASK then\n35\nf[(xn, yn)] ←INQE\nEnqueue(Queue, (xn, yn))\n36 return B, R\nof true regions are true boundaries while the other bound-\naries are false boundaries. (4) Boundary signature extraction\n(Section III-D): A novel type of boundary features, called\nboundary signature, is extracted for each boundary to capture\nits geometric characteristics. Boundary signatures serve as\ninput for the subsequent step of boundary classification. (5)\nBoundary classification (Section III-E): We build a lightweight\nbinary boundary classifier based on the extracted boundary\nsignatures and the corresponding boundary labels. The final\ncell instances are obtained by keeping true boundaries and\nmerging connected regions separated by false boundaries.\nFor cell video datasets that have extra properties of temporal\ninstance consistency, we further incorporate such instance\nconsistency information into our method. Specifically, we\nincorporate both temporal instance consistency and boundary\nprobability scores by the boundary classifier to produce final\ninstance segmentation using a matching and selection method.\nA. Seed Generation\nIn this step, we generate seeds using the probability map,\nwhich are then fed to the next boundary generation step (see\nstep (1) in Fig. 1). The seeds are generated from an instance\ncandidate forest (ICF) [14]. The process is as follows: Given\nan input image x with a foreground probability map p from a\npixel-wise classification model, the probability value of each\npixel ranges from 0 to 1. All the probability values of p are\nsorted into a list V = {v1, v2, . . . , vH} in increasing order\nafter removing duplicated values and merging highly similar\nvalues. Each value vh ∈V as a threshold value determines the\nconnected components in x whose pixels’ probability values\nare all ≥vh. Thus, vh induces an instance candidate set Cvh,\nwhich is a collection of mutually disjoint regions in x. The\npixels of instance candidates in the set Cvh are a subset of the\npixels of instance candidates in the set Cvh−1 for h > 1. Thus,\nwe have a forest structure F = (C, H), where C is the node\nset of all the candidate regions and H is the parent function\nfor each node in C. We then select the local maximal values\nof all the leaf nodes in F as the set S of seeds.\nB. Boundary Generation\nGiven the probability map p and the set S of seeds, we seek\nto generate all possible cell boundaries and the corresponding\nregions for the connected components of foreground pixels\n(see step (2) in Fig. 1). One can expect that each boundary\nis adjacent to two different regions. Let B be the set of\nboundaries and R be the set of regions thus obtained.\nWe revised the Watershed algorithm [13] to generate possi-\nble boundaries (called region-region boundaries in Section III-\nD). In a high-level view, Watershed is a seed-growth method\nbased on pixel intensities. First, each seed is initialized as an\nindividual instance. Then, all the other pixels are ordered by\ntheir intensities and assigned labels by the neighboring pixels.\nIf a pixel is attached only to pixels of a single instance, it is\nlabeled as the same instance. If a pixel is attached to multiple\ninstances, it is labeled as a boundary (Watershed line) and\nindexed by the labels of the attached instances. If a pixel is\nattached only to one pixel of a boundary, it is labeled as the\nsame boundary. See Algorithm 1 for more details.\nAs illustrated in step (2) of Fig. 1, the regions divided\nby the possible boundaries can be modeled as an undirected\ngraph, G = (R, B), by considering each individual region\nas a node and each generated boundary as an edge between\ntwo attached regions. If a subgraph Gs ⊆G is a connected\ngraph, its corresponding node set Is can form a possible cell\ninstance. All the possible cell instances in G constitute an\ninstance candidate set I = {Is} (see Fig. 2).\nC. Boundary Label Assignment\nGiven the generated boundaries, we aim to assign them true\nor false labels for the training stage. Note that a boundary is\ngenerated from the probability map p, and thus is quite likely\ndifferent from the boundaries of the ground truth masks. To\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING\n5\n(a) regions + boundaries (b) graph\n(c) cell instance candidates\nFig. 2: The process of generating cell instance candidates from\nthe possible regions and boundaries. Given the boundaries\nand regions (a), an undirected graph is built (b), in which\nthe regions are represented as nodes and boundaries as edges.\nBy enumerating all possible connected subgraphs, all instance\ncandidates are obtained (c).\nassign true/false boundary labels, we formulate it as a problem\nof computing optimal instance matching between ground truth\ninstances and all the possible valid instances of the prediction.\nBoundary Label Assignment Matching Model. This match-\ning model aims to find an optimal matching flow between\nground truth instances G and the instance candidate set I.\nGI-matching(G, I) = max\nf\nX\ni∈G\nX\nj∈I\nMi,jfi,j\n(1)\nX\nj∈I\nfi,j ≤1, ∀i ∈G,\n(2)\nX\ni∈G\nX\nk∈K(r)\nfi,k ≤1, ∀r ∈R,\n(3)\nfi,j ∈{0, 1}, ∀i ∈G, ∀j ∈I.\n(4)\nThe objective of the above matching model is to maximize\nthe sum of all the matching scores Mi,j ∈M multiplied by\nthe flow variables fi,j ∈{0, 1}, where i ∈G represents a\nground truth (GT) instance mask and j ∈I is a possible\ncell instance. We use the measure of Intersection-over-Union\n(IoU) for matching scores in M. We require each GT mask\nto match with at most one instance candidate (see Eq. (2)).\nFurther, each region r ∈R can be selected at most once. This\nconstraint is enforced for each region r ∈R by considering\nall possible instance candidates that contain r, that is, K(r) is\nthe collection of all possible instance candidates that contain\nr. For each r ∈R, we require that the sum of all the\nflows to the regions in K(r) be less than or equal to 1 (see\nEq. (3)). This optimal matching model is solved by integer\nlinear programming (ILP). Then for each instance candidate\nj ∈I (P\ni∈G fi,j = 1) which is selected by the model, all its\ninternal boundaries are taken as false boundaries, and all the\nother boundaries are taken as true boundaries (see Fig. 3).\nAlgorithm 2: Boundary Signature Extraction\n1 Input: Regions R, Boundaries B.\n2 Output: Boundary Signature BS(b).\n3 Obtain foreground-background boundaries ˆ\nB; ˆ\nB = B ∪ˆ\nB;\n4 for b ∈B do\n5\n(n1, n2) ←find endpoints(b);\n6\n(ˆb11,ˆb12) ←nearest boundaries(n1, ˆ\nB \\ b);\n7\n(ˆb21,ˆb22) ←nearest boundaries(n2, ˆ\nB \\ b);\n8\nBS(b) = sample points(b,ˆb11,ˆb12,ˆb21,ˆb22, n1, n2);\n9 return BS(b).\n(a) ground \ntruth\n(b) ground truth       \ninstances\n(c) cell instance candidates\n(e) boundaries \nand regions\n(f) boundary \nlabels\n(d) connected subgraphs\nlabel \nassignment\nfalse\ntrue\nmatching \nmodel\nFig. 3: The optimal instance matching model for boundary\nlabel assignment. We consider instance-level matching be-\ntween ground truth (GT) instance masks (a) and the generated\nboundaries and regions (e). The GT labels are decomposed into\nindividual ground truth instances (b); possible cell instance\ncandidates (c) are generated by considering all possible con-\nnected subgraphs (d) in the graph (e). Dashed lines indicate the\nmatching results (two instances are selected). The boundaries\ninside the matched instances are assigned false labels, and\nthose enclosing the matched instances are assigned true labels.\nD. Boundary Signature Extraction\nThe boundary signature extraction step extracts boundary-\nbased features to represent some geometric properties of each\nboundary. As shown in Fig. 4 and Algorithm 2, our strategy\nseeks to build a binary image for each boundary by sampling\npixels around the two endpoints of the boundary.\nFirst, we apply the border following algorithm in [54]\nto obtain foreground-background boundaries ˆ\nB. Each pixel\nof the foreground-background boundaries is indexed by its\nbackground and neighborhood regions. Hence, the foreground-\nbackground boundaries are divided into different segments\n(see the red, green, and blue lines in Fig. 4(b)). The divided\nforeground-background boundaries, along with the region-\nregion boundaries (e.g., see Fig. 4(c)) obtained by Water-\nshed [13], form the boundary set, called the boundary code-\nbook. Next, for each region-region boundary, we create a\nweighted undirected graph, in which every pixel is a node\nand each neighboring pixel pair forms an edge with the\ndistance between them as the edge weight. We apply the\nFloyd–Warshall algorithm [55] to obtain the shortest path\nbetween every pixel pair in the graph. Then the endpoints of\nthe boundary are the pixel pair with the largest-valued shortest\npath (e.g., see Fig. 4(d)). We observe that there is a fork among\nthe boundaries around each endpoint. Specifically, given two\nneighboring regions R1, R2 (e.g., the red and green regions in\nFig. 4(a)) with the corresponding region-region boundary B1\n(the yellow boundary in Fig. 4(a)), each endpoint of the region-\nregion boundary is also attached to two other neighboring\nboundaries (the red and green boundaries in Fig. 4(b)). We can\n6\nIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\n(b) boundary \ncodebook\n(a) boundaries+ \nregions\n(c) region-region boundaries\n(e) boundary \nsignatures\nA\nB\nC\nD\nsample \npoints\n(d) endpoints\nA\nB\nD\nC\nFig. 4: The process of extracting boundary signatures. Given\nextracted boundaries and divided regions (a), we produce\nregion-region boundaries (c), and obtain all possible bound-\naries, called the boundary codebook (b) which includes region-\nregion boundaries and foreground-background boundaries. For\neach region-region boundary, we locate its two endpoints (d)\nand select a fork road around each endpoint including the\ncorresponding boundary and two neighboring boundaries from\nthe boundary codebook (b). We then sample pixels from the\ntwo fork roads and transform them into a binary mask to obtain\na boundary signature (two boundary signatures are in (e)).\nquery the neighboring boundaries in the boundary codebook to\ncreate the fork road around each endpoint. Finally, a bound-\nary signature is obtained by sampling nearby pixels to the\nendpoints on both the fork roads and transforming them into\na binary mask (e.g., see Fig. 4(e)). Each boundary signature\n(as a binary mask) is associated with a true or false label\n(corresponding to that boundary, obtained in Section III-C).\nThe boundaries and their labels will be input to the boundary\nclassifier (Section III-E) for training the model.\nE. Boundary Classification\nWith the extracted boundary signature and corresponding\nlabel for each boundary, we conduct binary boundary classi-\nfication. Specifically, each boundary signature (an individual\nbinary image, in the same format as MNIST [56]) with its label\nis used as a training sample for the classifier. The associated\nboundary label (true or false) acts as ground truth. We utilize a\nResidual Network (ResNet-18) [57] as the classifier backbone,\nwhich predicts a boundary probability score Prob(b) ∈[0, 1]\nfor a boundary b; a higher score indicates a higher probability\nof the boundary being a true boundary. Focal loss [58] is used\nas the objective function.\nIn our experiments, we observe a large distribution shift\nbetween the training probability map and the testing prob-\nability map (e.g., in the ratio of true boundaries and false\nboundaries and in the foreground-background boundary cor-\nrectness). To deal with such distribution shift issues, we apply\nfive-fold cross-validation on the training data, and use only the\nvalidation probability map as training data for our boundary\nclassifier. In the inference stage, we apply the commonly-used\nthreshold value of 0.5 to identify true/false boundaries. Only\ntrue boundaries are preserved, and neighboring regions are\nmerged after removing false boundaries. Fig. 1 presents an\nexample of the inference process.\nF. Instance Temporal Setting\nIn 2D temporal cell instance segmentation datasets, corre-\nsponding cell instances often exhibit both structural and distri-\nbutional consistency across consecutive frames. For example,\nthe same cell typically retains a stable position, size, and shape\nin a short time interval. These characteristics, known as tem-\nporal consistency, have been leveraged in prior research [14]\nto enhance instance segmentation. We develop an iterative\nmatching and selection method, called Ceb+temporal, to\nincorporate temporal consistency. Fig. 5 shows an example\nto illustrate our Ceb+temporal algorithm. Given the regions\nand region-region boundaries (represented by a graph Gw =\n(Rw, Bw), as in Section III-B) for a frame w and the boundary\nprobability scores (generated by the boundary classifier, as\nin Section III-E), our goal is to produce a set of final\nsegmentation instances U w in frame w from Gw. We first\napply the boundary classifier to identify high-confidence false\nand true boundaries in Bw, which allow us to find easy-to-\nidentify cell instances. These instances are selected to form an\ninitial state U w\n0 , and the corresponding selected nodes (with\ntheir adjacent edges) are removed from graph Gw to obtain a\nreduced graph ¯Gw\n0 (Creating an Initial State). This reduced\ngraph induces a new set of not-yet-selected possible instances\n¯Iw\n0 . Next, the selected instances U w−1\nt\nin frame w −1 and\nU w+1\nt\nin frame w + 1 are matched with the not-yet-selected\ninstances ¯Iw\nt in frame w to select additional instances, forming\nan updated state U w\nt+1 of w, for t = 0, 1, . . . , T −1. The newly\nselected instances are removed from graph ¯Gw\nt , resulting in a\nreduced graph ¯Gw\nt+1. The not-yet-selected possible instances,\n¯Iw\nt+1, are also determined. This matching and selection process\nis repeated for T iterations to update the state (Iterative\nMatching and Selection). Finally, the remaining possible\ninstances in ¯Iw\nT are selected using a Final Selection method.\nThe instances selected with the Final Selection, Pw, combined\nwith the instances already in the state U w\nT , form the final\nsegmentation instances U w in frame w. Below we present\nthese steps in detail.\n1) Creating an Initial State: For each frame w in a video\nX, we represent its region-region boundaries and regions\nby an undirected graph Gw = (Rw, Bw). We obtain the\nboundary probability score for each boundary b\n∈\nBw\n(see Section III-E). The boundaries with probability scores\nlower than a threshold σ1 are labeled as false, Bw\nF alse =\n{b | Prob(b) < σ1, b ∈Bw}; the boundaries with probability\nscores larger than another threshold σ2 are labeled as true,\nBw\nT rue = {b | Prob(b) > σ2, b ∈Bw}; the remaining\nboundaries are marked as uncertain, Bw\nUC = {b | σ1 ⩽\nProb(b) ⩽σ2, b ∈Bw}. The regions separated only by false\nboundaries are merged together, resulting in a new region\nset ¯Rw. False boundaries and true boundaries are removed\nfrom the boundary set Bw, keeping only the uncertain bound-\naries. Consequently, the resulted regions ¯Rw and uncertain\nboundaries Bw\nUC form a new graph,\n¯Gw\n= ( ¯Rw, Bw\nUC).\nWe then determine the easy-to-identify instances as isolated\nnodes connecting to no other nodes in graph ¯Gw (i.e., their\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING\n7\n0.94\n0.65\n0.58\nCeb\nInitial State\nIterative Matching \nmatching flow\nunselected instances\nboundaries\nselected instances\nframe w\nframe w+1\nframe w-1\nCeb\nCeb\ntrue\ntrue\ntrue\niter 0\niter 1\niter 2\nFig. 5: An example illustrating our Ceb+Temporal method applied to three consecutive frames, w −1, w, and w + 1. First,\nthe Ceb method generates regions, boundaries, and the associated boundary probability scores (shown as numerical values).\nNext, high-confidence boundaries (e.g., with scores ≥0.9) are selected and the corresponding cell instances (attached only\nwith high-confidence boundaries) are selected to form the initial state (marked in green at iter 0). In the 1st iteration (iter 1),\nthe selected instances from frame w + 1 are propagated to frame w, allowing two previously unselected instances in frame w\nto be chosen. In the 2nd iteration (iter 2), instances from frame w are propagated to frame w −1, enabling the selection of\ntwo additional instances in that frame. All the selected instance candidates constitute the final instance segmentation results.\ncorresponding regions contain no uncertain boundaries). We\nselect such regions as cell instances in the initial state:\nU w\n0 = {r | E(r) = ∅, r ∈¯Rw},\n(5)\nwhere E(r) is the set of edges adjacent to a node r in ¯Gw. The\nselected regions are then removed from graph ¯Gw, resulting\nin a new graph ¯Gw\n0 = ( ¯Rw\n0 , Bw\n0 ), with ¯Rw\n0 = ¯Rw \\U w\n0 , Bw\n0 =\nBw\nUC. The not-yet-selected possible instances ¯Iw\n0 contained in\n¯Rw\n0 are obtained by enumerating all possible instances in ¯Gw\n0\n(this process is described in Section III-B).\n2) Iterative Matching and Selection: Given the selected in-\nstances in the state U w\nt\nand not-yet-selected instances ¯Iw\nt\nof\nframe w obtained in iteration t, we use the selected instances\nU w−1\nt\nof frame w −1 and U w+1\nt\nof frame w + 1 to match\nwith the not-yet-selected instances ¯Iw\nt of frame w in iteration\nt + 1. The matching and selection process is repeated for T\niterations. In each iteration, we perform three major substeps:\nselected-selected matching, selected-unselected matching, and\nstate update, as follows.\nSelected-selected Matching (SSM): For any two consecutive\nframes (w, w+1), we first perform a matching between all their\nselected instances (i.e., between U w\nt and U w+1\nt\n). An instance\nin U w+1\nt\nthat is involved in any matched pair with an instance\nin U w\nt\nis marked as “occupied”, and thus should not be used\nto further match with any not-yet-selected instance in ¯Iw\nt .\nWe use the following matching model SSM(U w+1\nt\n, U w\nt )\nto compute an optimal matching between U w+1\nt\nand U w\nt\n(SSM(U w−1\nt\n, U w\nt ) is for the U w−1\nt\nand U w\nt\nmatching):\nSSM(U w+1\nt\n, U w\nt ) = max\nf\nX\ni∈U w+1\nt\nX\nj∈Uw\nt\nMi,jfi,j\n(6)\nX\ni∈U w+1\nt\nfi,j ≤1, ∀j ∈U w\nt ,\n(7)\nX\nj∈Uw\nt\nfi,j ≤1, ∀i ∈U w+1\nt\n,\n(8)\nfi,j ∈{0, 1}, ∀i ∈U w+1\nt\n, ∀j ∈U w\nt .\n(9)\nWe utilize Intersection over Union (IoU) as the measure for\nthe matching score Mi,j between each pair of instances in\nthe two frames. We solve this matching problem by integer\nlinear programming (ILP) to obtain the optimal matching\nresult (flows) fi,j. Then, the matched instances in U w+1\nt\nare\nremoved to form the set U w+1→w\nt\nof unmatched selected\ninstances of frame w+1 with respect to frame w: U w+1→w\nt\n=\nU w+1\nt\n\\ {i | P\nj∈Uw\nt fi,j = 1, i ∈U w+1\nt\n}. Similarly, SSM\nis applied for the matching between frames w −1 and w:\nU w−1→w\nt\n= U w−1\nt\n\\ {i | P\nj∈Uw\nt fi,j = 1, i ∈U w−1\nt\n}.\nSelected-Unselected Matching (SUM): Next, we use the\nunmatched but selected instances obtained with SSM (i.e.,\nU w+1→w\nt\nand U w−1→w\nt\n) to match with the not-yet-selected\npossible instances in ¯Iw\nt\nof frame w. We define SUM from\nframe w + 1 to frame w as SUM(U w+1→w\nt\n, ¯Iw\nt ) (and that\nfrom frame w −1 to frame w as SUM(U w−1→w\nt\n, ¯Iw\nt )):\nSUM(U w+1→w\nt\n, ¯Iw\nt ) = max\nf\nX\ni∈U w+1→w\nt\nX\nj∈¯Iw\nt\nMi,jfi,j\n(10)\nX\nj∈¯Iw\nt\nfi,j ≤1, ∀i ∈U w+1→w\nt\n,\n(11)\nX\ni∈U w+1→w\nt\nX\nk∈K(j)\nfi,k ≤1, ∀j ∈¯Iw\nt ,\n(12)\nfi,j ∈{0, 1}, ∀i ∈U w+1→w\nt\n, ∀j ∈¯Iw\nt ,\n(13)\nwhere Mi,j denotes the matching score for each pair of\nconsidered regions based on the Intersection over Union (IoU)\nmeasure. Note that Eq. (12) enforces that each region r ∈¯Rw\nt\ncan be matched at most once, where K(r) denotes the set of\nall possible instance candidates that contain r.\n8\nIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nState Update: Note that for each frame w in a video X\nexcept the first and last frames, ¯Iw\nt\ncan receive matching\nresults from both frame w −1 and frame w + 1. This gives\nrise to two sets, Sw−1→w\nt\nand Sw+1→w\nt\n, of matched instance\ncandidates in ¯Iw\nt . However, there may be inconsistencies\nbetween Sw−1→w\nt\nand Sw+1→w\nt\n. Such inconsistencies in the\nmatching results of Sw−1→w\nt\nand Sw+1→w\nt\nmust be resolved.\nFor this, we apply the following method. We call a maximal\nconnected subgraph C in an undirected graph G (i.e., C is\nnot a subgraph of any larger connected subgraph in G) as a\ncomponent of G [59]. Let Gw\ncc be the set of all the components\nin the graph ¯Gw\nt . For each component Gw\ncci ∈Gw\ncc, if the sum\nof the matching scores on Gw\ncci from frame w −1, f w−1→w\nGw\ncci\n,\nis larger than or equal to the sum of the matching scores on\nGw\ncci from frame w + 1, f w+1→w\nGw\ncci\n, then we use the matching\nresults from w−1 as the matching results for Gw\ncci; otherwise,\nwe use the matching results from w + 1 for Gw\ncci. That is,\nF w\nt =\n\n\n\n\n\n{f w+1→w}\nw = 1,\n{f w−1→w}\nw = |X|,\n{max{f w+1→w\nGw\ncci\n, f w−1→w\nGw\ncci\n} | Gw\ncci ∈Gw\ncc}\notherwise,\n(14)\nwhere {f w+1→w} and {f w−1→w} are the sets of matching\nscores from frames w + 1 and w −1, respectively. Let ∆U w\nt\nbe the set of matched instance candidates in ¯Iw\nt corresponding\nto the matching scores in F w\nt . Then the state is updated as:\nU w\nt+1 = U w\nt ∪∆U w\nt .\n(15)\nThe graph is updated by removing the selected nodes and\ntheir adjacent edges: ¯Gw\nt+1 = ( ¯Rw\nt+1, Bw\nt+1), where ¯Rw\nt+1 =\n¯Rw\nt \\∆U w\nt+1, Bw\nt+1 = Bw\nt \\E(∆U w\nt+1), and E(∆U w\nt+1) denotes\nthe set of edges in ¯Gw\nt adjacent to any node in ∆U w\nt+1. The\nnot-yet-selected instances, ¯Iw\nt+1, are then obtained from ¯Gw\nt+1.\n3) Final Selection: After T iterations of the above matching\nand selection process, some instance candidates in ¯Iw\nT may\nremain unselected, which may still be selected as instances.\nThus, we apply a “final selection” process that assigns false\nor true labels to boundaries using a threshold value of 0.5 on\ntheir boundary probability scores, obtaining a selected instance\nset Pw.\nThe final instance set obtained in frame w is the union of\nall the instances selected throughout, as:\nU w = U w\nT ∪Pw.\n(16)\nU = {U 1, U 2, . . . , U |X|} is taken as the final instance\nsegmentation results of the input image video X.\nIV. EXPERIMENTS\nThe experiments evaluate our Ceb approach for cell instance\nsegmentation by applying Ceb on top of probability maps pro-\nduced by three representative semantic segmentation models.\nA. Datasets\nWe evaluate our framework on four cell video datasets from\nthe Cell Tracking Challenge [60] (Fluo-N2DL-HeLa, Fluo-\nN2DH-SIM+, PhC-C2DH-U373, and DIC-C2DH-HeLa) and\nTABLE I: Instance segmentation results on four video datasets.\nA bold score marks the best performance on the correspond-\ning dataset. An underline score denotes the performance of\nthe best foreground pixel clustering method with a specific\nbackbone. “–” denotes that either KTH-SE is not applicable\nto the DIC-HeLa and PhC-U373 datasets, or CellViT, UN-\nSAM, and GAC do not perform well on the corresponding\ndatasets.\nDIC-HeLa\nFluo-HeLa\nPhC-U373\nFluo-SIM+\nF1\nAJI\nF1\nAJI\nF1\nAJI\nF1\nAJI\nKTH-SE [60]\n–\n–\n96.3\n90.0\n–\n–\n97.9\n87.8\nCellPose [61]\n95.4\n84.4\n96.2\n91.0\n93.3\n89.4\n96.8\n78.9\nMask R-CNN [23]\n93.7\n71.6\n90.7\n77.0\n67.6\n62.5\n90.4\n76.9\nStarDist [7]\n96.4\n80.1\n96.7\n91.1\n93.4\n82.2\n96.2\n79.1\nKIT-Sch-GE [62]\n77.3\n58.1\n96.6\n92.6\n93.2\n79.6\n95.7\n81.5\nnnU-Net [18]\n91.1\n78.7\n93.1\n83.7\n92.3\n86.3\n97.9\n83.9\nInstanSeg [22]\n92.8\n77.4\n96.3\n92.2\n93.7\n87.9\n94.6\n80.3\nCellViT [20]\n–\n–\n88.0\n84.6\n66.8\n70.6\n81.6\n70.4\nFCIS [63]\n44.6\n32.3\n88.6\n80.1\n84.3\n75.8\n90.4\n71.7\nUN-SAM [64]\n–\n–\n88.5\n82.9\n89.0\n87.4\n98.1\n85.5\nCelloType [65]\n94.2\n82.6\n51.8\n31.4\n85.7\n70.2\n89.9\n71.2\nU-Net\n[12]\n0.5-Th\n73.5\n57.6\n93.4\n84.8\n92.4\n88.5\n92.9\n75.5\nOtsu’s\n72.8\n56.9\n93.2\n84.4\n91.3\n89.0\n92.6\n74.9\nDenseCRF\n70.6\n55.4\n91.4\n80.0\n92.5\n89.0\n91.6\n73.1\nMaxValue\n73.3\n57.2\n93.3\n84.5\n91.6\n88.9\n91.4\n72.1\nH-EMD\n87.2\n72.0\n96.4\n92.4\n93.5\n89.4\n97.6\n84.3\nGAC\n–\n–\n82.6\n61.4\n89.4\n79.8\n89.1\n73.2\nACWE\n61.6\n51.4\n88.5\n72.9\n93.0\n87.6\n84.7\n64.7\nWatershed\n93.0\n83.9\n95.7\n91.5\n91.6\n89.4\n97.6\n83.8\nCeb w/o cls\n93.5\n83.6\n95.7\n91.3\n80.0\n73.5\n97.0\n83.8\nCeb\n96.1\n84.6\n96.4\n92.4\n93.2\n88.7\n97.7\n84.5\nCeb+temporal\n97.1\n85.0\n96.6\n92.9\n93.5\n89.5\n97.8\n84.7\nDCAN\n[15]\n0.5-Th\n62.9\n50.0\n92.3\n82.8\n89.6\n87.6\n92.6\n76.2\nOtsu’s\n64.5\n49.5\n92.1\n82.5\n89.4\n88.1\n92.5\n75.6\nDenseCRF\n62.6\n45.8\n91.2\n80.0\n91.0\n88.1\n91.7\n74.0\nMaxValue\n62.5\n47.7\n92.6\n83.5\n89.0\n88.0\n91.5\n73.3\nH-EMD\n82.2\n66.6\n96.1\n91.6\n93.3\n88.7\n98.0\n85.2\nGAC\n–\n–\n82.2\n61.4\n89.7\n80.0\n89.0\n72.1\nACWE\n42.5\n31.8\n88.4\n72.9\n92.9\n86.9\n84.6\n63.6\nWatershed\n87.9\n80.1\n95.5\n91.4\n91.2\n88.4\n97.7\n83.7\nCeb w/o cls\n88.6\n81.1\n96.0\n92.0\n93.3\n89.3\n98.0\n84.4\nCeb\n91.3\n81.3\n96.3\n92.2\n94.0\n89.6\n98.3\n85.5\nCeb+temporal\n93.0\n83.9\n96.6\n92.4\n94.1\n89.4\n98.3\n85.7\nRes2Net\n[66]\n0.5-Th\n59.7\n45.5\n91.1\n80.8\n87.9\n86.4\n85.4\n56.4\nOtsu’s\n59.6\n45.0\n91.0\n80.6\n87.4\n86.3\n85.0\n55.1\nDenseCRF\n57.2\n43.2\n90.9\n79.7\n90.8\n86.3\n83.4\n53.5\nMaxValue\n56.3\n41.5\n90.9\n80.6\n87.0\n86.7\n82.6\n51.4\nH-EMD\n88.7\n77.3\n95.8\n90.6\n92.2\n86.9\n96.2\n73.3\nGAC\n–\n–\n82.2\n61.1\n88.5\n79.1\n78.1\n56.1\nACWE\n47.4\n33.6\n88.4\n73.0\n91.4\n85.4\n71.7\n40.2\nWatershed\n92.8\n83.6\n95.5\n91.5\n84.6\n83.8\n96.2\n73.2\nCeb w/o cls\n84.7\n77.4\n95.2\n91.1\n90.8\n85.8\n95.7\n73.3\nCeb\n93.5\n83.7\n96.0\n91.6\n92.4\n86.6\n96.5\n73.9\nCeb+temporal\n97.6\n86.5\n96.4\n92.2\n93.6\n87.9\n96.3\n73.6\ntwo 2D cell datasets (BBBC039 [67] and TissueNet [68]).\nEach cell video dataset contains two sequences with annotated\nlabels. We use one sequence as training data and the other\nsequence as test data. Specifically, Fluo-N2DH-SIM+ uses the\nsecond video for training and the first video for testing, and\nvice versa for all the other datasets. The BBBC039 dataset\ncontains 100 training and 50 test images of nuclei of U2OS\ncells collected with fluorescent microscopy. The TissueNet\ndataset contains 2, 601 training and 1, 249 test images of six\ndifferent tissue types collected with fluorescent microscopy;\neach image has manual segmentations of cells and nuclei. We\nuse the nuclei segmentation labels in this study.\nB. Compared Methods\nWe compare our Ceb framework with two major categories\nof the known cell instance segmentation methods.\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING\n9\n(a) raw images\n(b) GT\n(c) StarDist\n(d) CellPose\n(g) Ceb w/o cls (k) Ceb (ours)\nBBBC039\nTissueNet\nPhC-U373\nFluo-SIM+\nFluo-HeLa\nDIC-HeLa\n(e) CellViT\n(f) InstanSeg\n(j) FCIS\n(h) UN-SAM\n(i) CelloType\nFig. 6: Visual examples of cell instance segmentation results. GT denotes ground truth. Orange arrows point to some over-\nsegmentation errors corrected by Ceb. Cyan arrows point to some under-segmentation errors corrected by Ceb. White arrows\npoint to false negative errors corrected by Ceb. Green arrows point to false positive errors corrected by Ceb.\n1) Semantic Segmentation: These methods were built on\ndifferent image encoders (i.e., backbones) with semantic seg-\nmentation objectives (i.e., pixel-wise classification) and dif-\nferent foreground pixel clustering strategies. Specifically, we\nuse three widely-used backbones: U-Net [12], DCAN [15],\nand Res2Net [66]. We then apply 8 different foreground pixel\nclustering methods, which are categorized into the following\ntwo types.\nBoundary-generation based methods:\n• Geodesic Active Contour (GAC) [30]: It evolves a con-\ntour toward object boundaries by optimizing an energy\nfunction that depends on image gradients.\n• Active Contours without Edges (ACWE) [35]: It seg-\nments images by optimizing an energy function based on\nthe differences in intensity between inside and outside\nregions of the contour.\n• Watershed [13]: It takes probability maps as input to\ngenerate cell segmentation results. To mitigate over-\nsegmentation issues, we smooth the probability maps\nfollowing previous work [14].\nThe other methods:\n• 0.5 thresholding (0.5-Th) [17]: The probability maps are\nbinarized using the threshold value of 0.50, and connected\ncomponents are computed as the final instances.\n• Otsu’s [69]: Otsu’s algorithm is a global thresholding\nmethod that automatically determines the optimal thresh-\nold by maximizing the between-class variance of pixel\nintensities in a probability map.\n• DenseCRF [70]: Both the raw images and their prob-\nability maps are fed to the DenseCRF model. Pixels\nwith similar features (e.g., color and probability) are\nassigned to the same semantic class (e.g., foreground or\nbackground).\n• MaxValue [14]: Each pixel is assigned to one of three\nclasses (background, boundary, and foreground) with the\nmaximum probability.\n• H-EMD [14]: Given probability maps across a sequence\nof cell images (e.g., in a video), instance candidates are\nfirst generated from the probability maps, and temporal\nconsistency is leveraged to select an optimal subset of\ninstance candidates as the final instance segmentation\nresults.\n2) SOTA Cell Instance Segmentation Methods: Note that\nsome cell instance segmentation methods do not necessarily\nproduce semantic segmentation probability maps. We consider\nthe following SOTA methods:\n• KTH-SE [60]: It uses a bandpass filtering based seg-\nmentation algorithm [71] to segment cells and applies\nthe Viterbi tracking algorithm [72] to correct potential\nsegmentation errors.\n• CellPose [61]: It predicts a spatial gradient vector field\npointing from pixels within each cell toward its centroid,\nand uses this field to reconstruct individual cell instances.\n• StarDist [7]: It predicts a set of radial distances (32 in\nthe experiments) along fixed angles from each pixel to\nthe object boundaries.\n10\nIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nTABLE II: Instance segmentation results on two 2D datasets.\nA bold score marks the best performance on the correspond-\ning dataset. An underline score denotes the performance of\nthe best foreground pixel clustering method with a specific\nbackbone. “–” denotes that either KIT-Sch-GE and nnU-Net\nare not applicable to the corresponding dataset, or GAC does\nnot perform well on the corresponding dataset.\nBBBC039\nTissueNet\nF1 (%)\nAJI (%)\nAP (%)\nF1 (%)\nAJI (%)\nAP (%)\nCellPose\n95.0\n89.2\n90.0\n94.3\n82.1\n88.0\nMask R-CNN\n94.3\n86.0\n88.8\n94.0\n82.3\n87.0\nStarDist\n94.5\n88.3\n89.5\n92.9\n82.2\n86.0\nKIT-Sch-GE\n89.6\n79.0\n80.5\n–\n–\n–\nnnU-Net\n94.2\n85.8\n89.2\n–\n–\n–\nInstanSeg\n95.0\n89.4\n90.4\n94.4\n84.3\n88.6\nCellViT\n85.2\n80.5\n75.6\n93.5\n84.2\n87.2\nFCIS\n88.2\n81.0\n77.0\n83.4\n75.2\n69.5\nUN-SAM\n89.7\n76.3\n79.6\n75.7\n60.0\n60.3\nCelloType\n80.9\n61.1\n66.8\n90.2\n69.8\n80.2\nU-Net\n0.5\n88.2\n75.9\n76.8\n66.9\n40.4\n50.3\nOtsu’s\n87.9\n75.5\n76.3\n64.4\n37.2\n47.5\nDenseCRF\n86.8\n73.7\n74.3\n57.9\n31.1\n41.1\nMaxValue\n88.0\n75.1\n75.2\n67.1\n40.9\n51.1\nGAC\n76.6\n59.3\n58.7\n–\n–\n–\nACWE\n79.3\n64.0\n62.9\n42.6\n32.6\n30.2\nWatershed\n94.5\n87.8\n89.2\n91.4\n82.7\n82.4\nCeb w/o cls\n93.5\n88.4\n87.1\n91.2\n80.0\n82.0\nCeb (ours)\n95.0\n89.5\n90.4\n94.5\n83.2\n88.5\nDCAN\n0.5-Th\n87.2\n73.8\n74.9\n67.0\n40.3\n50.8\nOtsu’s\n87.1\n73.4\n74.9\n64.3\n37.0\n47.8\nDenseCRF\n86.1\n72.0\n73.0\n57.1\n30.3\n40.6\nMaxValue\n87.4\n73.8\n75.4\n67.2\n40.5\n50.9\nGAC\n76.6\n59.7\n58.6\n–\n–\n–\nACWE\n79.7\n65.3\n63.7\n40.1\n31.0\n29.8\nWatershed\n94.6\n87.4\n88.6\n92.0\n84.1\n83.5\nCeb w/o cls\n94.8\n89.4\n90.0\n92.3\n81.8\n84.0\nCeb (ours)\n95.2\n89.6\n90.6\n94.7\n84.3\n88.8\nRes2Net\n0.5-Th\n87.0\n74.0\n74.6\n63.8\n36.5\n47.1\nOtsu’s\n86.9\n73.8\n74.4\n62.4\n35.0\n45.5\nDenseCRF\n86.0\n72.3\n72.9\n55.9\n28.9\n39.3\nMaxValue\n86.7\n73.4\n74.3\n63.1\n36.1\n46.4\nGAC\n76.0\n58.6\n57.9\n–\n–\n–\nACWE\n79.3\n64.3\n63.0\n39.8\n29.8\n28.3\nWatershed\n92.9\n86.2\n85.7\n91.8\n84.6\n83.1\nCeb w/o cls\n90.0\n84.3\n80.9\n93.1\n83.1\n85.6\nCeb (ours)\n94.1\n88.0\n88.8\n95.3\n85.3\n90.0\n• KIT-Sch-GE [62]: It predicts cell distance maps, to which\nthe watershed algorithm is applied to generate the final\ninstance segmentation results.\n• nnU-Net [18]: It is an automatic self-configured U-Net-\nbased model including pre-processing, network architec-\nture, training, and post-processing for biomedical image\nsegmentation.\n• Mask R-CNN [23]: It first generates instance proposals\nand then predicts segmentation masks within each pro-\nposed region.\n• InstanSeg [11]: It predicts seed points that represent\ninstance centers and learns pixel-wise embeddings; pixels\nare then clustered into instances based on their similarity\nto the seed embeddings.\n• CellViT [20]: It employs a distance-map based approach\nand leverages Vision Transformer (ViT) [73] encoders.\n• FCIS [63]: It encodes foreground pixels by assigning\nidentical values to pixels within the same instance while\nensuring neighboring instances receiving different values.\n• UN-SAM [64]: It fine-tunes the Segment Anything Model\n(SAM) [74] to adapt it for cell instance segmentation.\n• CelloType [65]: It employs a Transformer-based detector\n(DINO) [75] to generate cell proposals, followed by\nsegmentation of cell instances.\nNote that for KTH-SE, we are able to evaluate the method\non the Fluo-N2DL-HeLa and Fluo-N2DH-SIM+ datasets, for\nwhich KTH-SE was originally designed. For H-EMD, it was\ndesigned for cell instance segmentation in videos, and is not\ndirectly suitable for general 2D cell instance segmentation\ntasks. We use the publicly released codes of the corresponding\nmethods, and train their models in the same train/test split\nsettings.\nC. Evaluation Metrics\nWe utilize two widely-used cell instance segmentation\nevaluation metrics: F1-score [17] and Average Jaccard Index\n(AJI) [17] for video datasets. For 2D cell datasets, we addi-\ntionally use the Average Precision (AP) metric [6], a standard\nmeasure for these datasets.\nD. Implementation Details\nTable III summarizes the implementation details of the three\nsemantic segmentation backbones we use (U-Net, DCAN,\nand Res2Net) and the boundary classifier we use (ResNet-\n18). All the training and inference procedures are performed\non an NVIDIA Tesla V100 GPU with 32 GB of memory.\nThe semantic segmentation task is formulated as a three-class\nclassification problem consisting of foreground, boundary,\nand background regions. The boundary class is generated by\napplying a three-pixel dilation to the ground truth masks.\nOur Ceb framework involves a handful of hyperparameters.\nIn the Seed Generation stage, to filter out noisy instance\ncandidates, we use only threshold values > 0.50 to generate\ninstance candidate forests (ICFs). In the Boundary Classifica-\ntion stage, the threshold value for the boundary classifier is\nset to 0.50. The Integral Linear Programming (ILP) problems\nare solved using the GLPK solver1, in which the matching\nscores are defined by the Intersection over Union (IoU) metric.\nFor the temporal experimental setting, the hyperparameter T,\nrepresenting the number of iterations, is set to 10.\nE. Results\n1) Cell Video Dataset Results: Table I shows the instance\nsegmentation results on the four cell video datasets. We\nconsider three settings for our method: Ceb represents our\nfull model, and Ceb w/o cls represents our method without\nboundary classification (cls is short for classification). In\nother words, Ceb w/o cls takes all the extracted region-region\nboundaries as true boundaries. Ceb + Temporal represents our\nmethod in the temporal setting, which incorporates instance\ntemporal consistency to determine the final instance results.\nBased on the results in Table I, we draw three conclu-\nsions. First, compared to Ceb w/o cls, Ceb shows consistent\nimprovements across all the datasets with all the semantic\nsegmentation backbone models. Second, compared to other\nforeground pixel clustering methods that extract instances from\n1https://www.gnu.org/software/glpk/\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING\n11\nTABLE III: Summary of implementation details of the DL backbones that we use.\nFunctionality\nModel\nFramework Initialization Optimizer Learning Rate\nAugmentation\nBatch\nLoss\nInput size\nSegmentation\nU-Net\nTensorFlow\nGaussian\nAdam\n5e-4\nRotate + Flip\n8\nCross-Entropy\n192\nSegmentation\nDCAN\nTensorFlow\nGaussian\nAdam\n5e-4\nRotate + Flip\n8\nCross-Entropy\n192\nSegmentation\nRes2Net\nPyTorch\nGaussian\nAdam\n5e-4\nRotate + Flip\n8\nCross-Entropy\n192\nBoundary Classifier ResNet-18\nPyTorch\nPre-trained\nSGD\n1e-3\nRotate + Crop + Flip\n8\nFocal Loss\n224\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nFluo-HeLa, area=0.825\nFluo-SIM+, area=0.981\nPhC-U373, area=0.548\nDIC-HeLa, area=0.975\nBBBC039, area=0.897\nTissueNet, area=0.926\n(a)\nFluo-SIM+\nFluo-HeLa\nBBBC039\n(b)\nFig. 7: (a) Receiver operating characteristic (ROC) curves of the boundary classifier on the six datasets based on a U-Net\nbackbone. (b) True and false boundary signature samples in three different datasets.\n: false samples;\n: true samples.\n(a) raw images\n(b) GT\n(c) PM\n(d) GAC\n(e) ACWE\n(f) Watershed\n(g) Ceb w/o CLS\nFluo-HeLa\nPhC-U373\nFluo-SIM+\nFig. 8: Visual comparisons of different boundary generation methods. GT denotes ground truth; PM denotes probability\nmap; GAC denotes Geodesic Active Contours; ACWE denotes Active Contours without Edges. Orange arrows indicate over-\nsegmentation errors; cyan arrows indicate under-segmentation errors; white arrows indicate visual boundary errors.\nprobability maps, Ceb yields the best performances across all\nthe datasets. Compared to the best foreground pixel clustering\nperformances, Ceb improves the F1 score by 3.4% and AJI\nby 1.0% on the DIC-C2DH-HeLa dataset with the DCAN\nbackbone. We also notice that even though H-EMD already\neffectively utilizes temporal information in videos, Ceb still\noutperforms H-EMD in most of the cases without using\nany temporal information. Third, Ceb + Temporal further\nimproves performance compared to Ceb by incorporating\ninstance temporal consistency. Ceb + Temporal outperforms\nthe SOTA instance segmentation methods in most the metrics.\nWe observe that CellViT does not yield good performance on\nthe video datasets, due to the limited training data available,\nas ViT-based architectures typically require large-scale training\ndata sets. However, this limitation is common in cell datasets,\nwhere acquiring extensive training data can be both time-\nconsuming and expensive.\n2) 2D Dataset Results: On the two general 2D cell datasets,\nBBBC039 and TissueNet, Table II shows the results. One can\nsee that Ceb consistently boosts instance segmentation perfor-\nmances. Compared to the other foreground pixel clustering\nmethods, Ceb yields the best scores of all the backbones.\nOn the BBBC039 dataset, Ceb improves F1 by 1.2%, AJI by\n1.8%, and AP by 3.1% with a Res2Net backbone compared\nto the best results of the known foreground pixel clustering\nmethods. On the TissueNet, Ceb improves F1 by 3.5%, AJI by\n0.7%, and AP by 6.9% with a Res2Net backbone compared to\nthe best results of the known foreground pixel clustering meth-\nods. Compared to the SOTA instance segmentation methods,\nCeb still achieves the best results across all the metrics.\nFig. 6 shows some visual results of cell instance segmenta-\ntion on four datasets. One can see that for instances that are\nover-segmented or under-segmented by other methods, Ceb\ncan attain correct instance-level segmentation results.\n12\nIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\nTABLE IV: Tracking performance evaluation using the TRA\nmetric on the video datasets.\nDIC-HeLa\nFluo-HeLa\nPhC-U373\nFluo-SIM+\nCeb\n0.871\n0.969\n0.917\n0.990\nCeb+temporal\n0.889\n0.977\n0.934\n0.991\n3) Cell Tracking Evaluation on Video Datasets: To evaluate\nthe effectiveness of incorporating temporal consistency by\nour Ceb+temporal method, we conduct additional tracking\nexperiments. Specifically, we apply the EMD-based tracking\nmodel in [76] to the cell instance segmentation results obtained\nby Ceb and Ceb+temporal (both using the U-Net backbone).\nWe use the Tracking Accuracy (TRA) metric [60] to measure\nhow accurately cell instances are tracked across frames. Ta-\nble IV presents the results, showing that leveraging temporal\nconsistency leads to improved tracking performance.\nV. ANALYSIS\nA. Boundary Classifier Analysis\nFig. 7a shows the receiver operating characteristic (ROC)\ncurves for binary boundary classification with a U-Net back-\nbone on the six datasets. The boundary classifier yields\noutstanding effects on the Fluo-N2DH-SIM+, DIC-C2DH-\nHeLa, TissueNet, BBBC039, and Fluo-D2DL-HeLa datasets.\nOn PhC-C2DH-U373, the boundary classifier obtains mild\naccuracy (close to 0.55) in the AUC-ROC metric. We observe\nthat the irregular cell shapes of this dataset impact the feature\nquality of the boundary signatures.\nB. Boundary Signature Visualization\nFig. 7b shows some true and false boundary signature ex-\namples. We observe that several significant boundary signature\npatterns can be used to distinguish true and false boundaries.\nFirst, we find that true boundary signatures tend to have an\nX shape (with some arcs). In comparison, false boundary\nsignatures tend to have a T shape or an H shape with some\nnearly right angles. Second, true boundaries tend to align well\nbetween both parts of a boundary signature, while many false\nboundary signatures have misalignment or a larger distance\nbetween the two parts.\nC. Boundary Generation Methods Visualization\nFig. 8 shows some visual results by different boundary\ngeneration methods. One can see that our method effec-\ntively mitigates over-segmentation, under-segmentation, and\nboundary shape errors in the outputs of the other boundary\ngeneration methods.\nD. Ablation Studies\nTo examine the effects of the key components in our\napproach, we conduct ablation studies using four cell video\ndatasets with a U-Net backbone.\nTABLE V: Ablation study results.\nDIC-HeLa\nFluo-HeLa\nPhC-U373\nFluo-SIM+\nF1\nAJI\nF1\nAJI\nF1\nAJI\nF1\nAJI\nCeb (WS seed)\n93.6\n84.0\n95.6\n92.0\n92.8\n87.4\n97.4\n84.2\nCeb\n96.1\n84.6\n96.4\n92.4\n93.2\n88.7\n97.7\n84.5\nPM + temporal\n96.4\n84.5\n96.4\n92.5\n93.4\n89.3\n97.4\n84.2\nCeb + NMS\n94.4\n83.6\n96.4\n92.4\n93.4\n89.0\n97.5\n84.3\nCeb + temporal\n97.1\n85.0\n96.6\n92.9\n93.5\n89.5\n97.8\n84.7\n(a) raw images\n(b) GT\n(c) PM\nPhC-U373\nFluo-SIM+\nFluo-HeLa\nDIC-HeLa\n(d) Ceb (WS seed) (e) Ceb (ours)\nFig. 9: Visual comparison between the Ceb version with\nthe original Watershed seed generation (Ceb (WS seed)) and\nour Ceb method (Ceb (ours)). GT denotes ground truth.\nPM denotes probability map. Orange arrows indicate over-\nsegmentation errors. Cyan arrows indicate under-segmentation\nerrors.\n1) The Influence of the Seed Generation Method: We replace\nour seed generation method (see Section III-A) with the\noriginal Watershed seed generation algorithm [13], denoted as\nCeb (WS seed) in Table V. As shown in the results, our method\nCeb consistently outperforms Ceb (WS seed), demonstrating\nthe effectiveness of our proposed seed generation method in\ncapturing all possible cell instances. Fig. 9 shows a visual\ncomparison between our Ceb method and Ceb (WS seed).\n2) The Effect of the Boundary Classifier: In the temporal\nsetting, we employ the boundary classifier to determine easy-\nto-identify instances and create the initial state (see Section III-\nF). We replace our boundary classifier by the method used\nin H-EMD [14] to create an initial set of selected instances,\nwhich selects connected regions from the probability maps\n(PM) with a threshold of 0.5 such that these regions do not\nsplit when the threshold value gets larger, treating such regions\nas easy-to-identify instances (see PM + temporal in Table V).\nOne can see that our method, Ceb + temporal, outperforms\nPM + temporal, demonstrating that our boundary classifier is\nmore effective in determining easy-to-identify cell instances.\n3) The Effect of the SUM Matching Model: We replace\nour proposed SUM matching model (see Section III-F) by\nthe commonly-used Non-Maximum Suppression (NMS) [23]\nselection method. The NMS method selects matching instances\nin a frame w among the not-yet-selected possible instances\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING\n13\nin frame w based on their objectness scores using a greedy\nstrategy (see Ceb + NMS in Table V). The objectness score of\na not-yet-selected possible instance c in frame w is defined\nas the maximum IoU score that c obtains with respect to\nall selected instances in a considered neighboring frame (say,\nframe w + 1) that can possibly form a match with c. One\ncan see that our method, Ceb + temporal, outperforms Ceb\n+ NMS, suggesting that our SUM matching model is more\neffective in determining optimal matching instance pairs.\nVI. LIMITATIONS\nOur Ceb method is not without limitations. Note that Ceb\ndepends on a full semantic segmentation model (e.g., U-\nNet) and has five additional components: Seed Generation,\nBoundary Generation, Boundary Label Assignment (training\nonly), Boundary Signature Extraction, and Boundary Clas-\nsification. The pipeline of our framework with these five\nextra components introduces more computational complexity\nand inherits the limitations associated with probability maps\ngenerated by semantic segmentation models.\nA. Computational Complexity\nTable VI presents a runtime breakdown of the main com-\nponents in Ceb on the PhC-C2DH-U373 dataset. One can see\nthat Seed Generation and Boundary Signature Extraction take\nmuch more time (4.5s and 4.8s) than Semantic Segmentation\n(1.2s) and Boundary Classification (0.3s). In the current imple-\nmentation, we use the Python language with loops to process\neach instance individually in every image, which is time-\nconsuming. However, these computations are independent and\ncan be parallelized for different instances concurrently without\ninterference, thus allowing significant speedups if rewritten in\nC/C++ and integrated with Python via Cython, hence further\naccelerated on GPUs using CUDA kernels. One example of\nsuch implementations can be found in ROIAlign2 and NMS3\nof Mask R-CNN [23], and is beyond the scope of this work.\nB. Dependency on Semantic Segmentation\nOur Ceb method relies on a semantic segmentation back-\nbone (e.g., U-Net), and thus depends on the performance\nof semantic segmentation (i.e., the probability maps). Note\nthat some SOTA methods such as StarDist and CellPose also\ndepend on semantic segmentation for foreground-background\npixel distinction, and have the same limitation. Nevertheless,\nsemantic segmentation commonly outperforms other methods\nsuch as bounding-box based methods (e.g., Mask R-CNN [23])\nfor cell instance segmentation tasks. Improving semantic seg-\nmentation will improve our method, which focuses on address-\ning the challenge of distinguishing and clustering background\npixels and foreground pixels based on probability maps.\n2https://github.com/multimodallearning/pytorch-mask-\nrcnn/tree/master/roialign/roi align/src\n3https://github.com/multimodallearning/pytorch-mask-\nrcnn/tree/master/nms/src\nTABLE VI: Breakdown of computational time of Ceb on a\nsingle image of the PhC-C2DH-U373 dataset.\nComponent\nIncluded in inference\nLatency (sec.)\nSemantic Segmentation (U-Net [12])\n✓\n1.2\nSeed Generation\n✓\n4.5\nBoundary Generation\n✓\n1.1\nBoundary Label Assignment\n0.6\nBoundary Signature Extraction\n✓\n4.8\nBoundary Classification\n✓\n0.3\nC. Dependency on Watershed\nOur method utilizes a revised Watershed algorithm to gen-\nerate all potential instance-instance boundaries, and hence\ndepends on the capability of Watershed. During instance-wise\nevaluation, we observed that the potential boundaries thus\ngenerated have high recall to cover most ground-truth instance-\ninstance boundaries and, therefore, are effective.\nVII. CONCLUSIONS\nKnown state-of-the-art cell instance segmentation methods\nare mainly based on semantic segmentation to distinguish\nforeground pixels from background pixels. To capture precise\ncell instances, pixel-wise objectives are commonly used to rep-\nresent cell instances. However, such pixel-wise representations\nmay overlook geometric properties of cell instances, which\nmay need a structured group of pixels to represent. In this\nwork, we presented a new approach, Ceb, which utilizes cell\nboundaries in the foreground pixel clustering process. Built on\ntop of existing semantic segmentation backbone models, Ceb\ntransforms the clustering of foreground pixels into a binary\nboundary classification problem. The boundary classifier is a\nlightweight CNN based on a novel type of boundary-based\nfeatures, by sampling pixels from the current foreground-\nforeground boundary as well as the neighboring background-\nforeground boundaries. Evaluated on six public cell instance\nsegmentation datasets, Ceb consistently outperforms all the\nknown foreground pixel clustering methods on top of semantic\nsegmentation probability maps. Compared to state-of-the-art\ncell instance segmentation methods, Ceb obtains comparable\nor better performances. By incorporating instance temporal\nconsistency in cell videos, our Ceb + temporal method further\nimproves the cell instance segmentation performance.\nREFERENCES\n[1] C. S. Madukoma, P. Liang, A. Dimkovikj, J. Chen, S. W. Lee, D. Z.\nChen, and J. D. Shrout, “Single cells exhibit differing behavioral phases\nduring early stages of Pseudomonas aeruginosa swarming,” Journal of\nBacteriology, vol. 201, no. 19, pp. e00184–19, 2019.\n[2] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick, “Microsoft COCO: Common objects in\ncontext,” in ECCV 2014, Part V 13, pp. 740–755, Springer, 2014.\n[3] X. Zhuang and J. Shen, “Multi-scale patch and multi-modality atlases\nfor whole heart segmentation of MRI,” Medical Image Analysis, vol. 31,\npp. 77–87, 2016.\n[4] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto, T. Kobayashi, K.-\ni. Komatsu, M. Matsui, H. Fujita, Y. Kodera, and K. Doi, “Development\nof a digital image database for chest radiographs with and without a\nlung nodule: Receiver operating characteristic analysis of radiologists’\ndetection of pulmonary nodules,” American Journal of Roentgenology,\nvol. 174, no. 1, pp. 71–74, 2000.\n14\nIEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2020\n[5] S. Graham, Q. D. Vu, S. E. A. Raza, A. Azam, Y. W. Tsang, J. T.\nKwak, and N. Rajpoot, “Hover-Net: Simultaneous segmentation and\nclassification of nuclei in multi-tissue histology images,” Medical Image\nAnalysis, vol. 58, p. 101563, 2019.\n[6] C. Stringer, T. Wang, M. Michaelos, and M. Pachitariu, “Cellpose: A\ngeneralist algorithm for cellular segmentation,” Nature Methods, vol. 18,\nno. 1, pp. 100–106, 2021.\n[7] U. Schmidt, M. Weigert, C. Broaddus, and G. Myers, “Cell detection\nwith star-convex polygons,” in MICCAI, pp. 265–273, 2018.\n[8] C. Payer, D. ˇStern, M. Feiner, H. Bischof, and M. Urschler, “Segmenting\nand tracking cell instances with cosine embeddings and recurrent hour-\nglass networks,” Medical Image Analysis, vol. 57, pp. 106–119, 2019.\n[9] M. Zhao, A. Jha, Q. Liu, B. A. Millis, A. Mahadevan-Jansen, L. Lu,\nB. A. Landman, M. J. Tyska, and Y. Huo, “Faster Mean-shift: GPU-\naccelerated clustering for cosine embedding-based cell segmentation and\ntracking,” Medical Image Analysis, vol. 71, p. 102048, 2021.\n[10] M. Lalit, P. Tomancak, and F. Jug, “EmbedSeg: Embedding-based\ninstance segmentation for biomedical microscopy data,” Medical Image\nAnalysis, vol. 81, p. 102523, 2022.\n[11] T. Goldsborough, B. Philps, A. O’Callaghan, F. Inglis, L. Leplat,\nA. Filby, H. Bilen, and P. Bankhead, “InstanSeg: An embedding-based\ninstance segmentation algorithm optimized for accurate, efficient and\nportable cell segmentation,” arXiv preprint arXiv:2408.15954, 2024.\n[12] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” in MICCAI, pp. 234–241,\n2015.\n[13] F. Meyer, “Topographic distance and watershed lines,” Signal Process-\ning, vol. 38, no. 1, pp. 113–125, 1994.\n[14] P. Liang, Y. Zhang, Y. Ding, J. Chen, C. S. Madukoma, T. Weninger,\nJ. D. Shrout, and D. Z. Chen, “H-EMD: A hierarchical Earth Mover’s\ndistance method for instance segmentation,” IEEE Transactions on\nMedical Imaging, vol. 41, no. 10, pp. 2582–2597, 2022.\n[15] H. Chen, X. Qi, L. Yu, and P.-A. Heng, “DCAN: Deep contour-aware\nnetworks for accurate gland segmentation,” in IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 2487–2496, 2016.\n[16] S. Graham, H. Chen, J. Gamper, Q. Dou, P.-A. Heng, D. Snead, Y. W.\nTsang, and N. Rajpoot, “MILD-Net: Minimal information loss dilated\nnetwork for gland instance segmentation in colon histology images,”\nMedical Image Analysis, vol. 52, pp. 199–211, 2019.\n[17] Y. Zhou, O. F. Onder, Q. Dou, E. Tsougenis, H. Chen, and P.-A.\nHeng, “CIA-Net: Robust nuclei instance segmentation with contour-\naware information aggregation,” in IPMI, pp. 682–693, 2019.\n[18] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-\nHein, “nnU-Net: A self-configuring method for deep learning-based\nbiomedical image segmentation,” Nature Methods, vol. 18, 2021.\n[19] P. Liang, J. Chen, H. Zheng, L. Yang, Y. Zhang, and D. Z. Chen,\n“Cascade decoder: A universal decoding method for biomedical image\nsegmentation,” in IEEE 16th International Symposium on Biomedical\nImaging, pp. 339–342, IEEE, 2019.\n[20] F. H¨orst, M. Rempe, L. Heine, C. Seibold, J. Keyl, G. Baldini, S. Ugurel,\nJ. Siveke, B. Gr¨unwald, J. Egger, et al., “CellViT: Vision Transformers\nfor precise cell segmentation and classification,” Medical Image Analy-\nsis, vol. 94, p. 103143, 2024.\n[21] J. Wang, “Mudslide: A universal nuclear instance segmentation method,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 11673–11682, 2024.\n[22] T. Goldsborough, A. O’Callaghan, F. Inglis, L. Leplat, A. Filby, H. Bilen,\nand P. Bankhead, “A novel channel invariant architecture for the seg-\nmentation of cells and nuclei in multiplexed images using InstanSeg,”\nbioRxiv, pp. 2024–09, 2024.\n[23] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask R-CNN,” in IEEE\nInternational Conference on Computer Vision, pp. 2961–2969, 2017.\n[24] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for\ninstance segmentation,” in CVPR, pp. 8759–8768, 2018.\n[25] K. Chen, J. Pang, J. Wang, Y. Xiong, X. Li, S. Sun, W. Feng,\nZ. Liu, J. Shi, W. Ouyang, et al., “Hybrid task cascade for instance\nsegmentation,” in CVPR, pp. 4974–4983, 2019.\n[26] Z. Huang, L. Huang, Y. Gong, C. Huang, and X. Wang, “Mask scoring\nR-CNN,” in CVPR, pp. 6409–6418, 2019.\n[27] Z. Cai and N. Vasconcelos, “Cascade R-CNN: High quality object\ndetection and instance segmentation,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 43, no. 5, pp. 1483–1498, 2019.\n[28] C. Tang, H. Chen, X. Li, J. Li, Z. Zhang, and X. Hu, “Look closer to\nsegment better: Boundary patch refinement for instance segmentation,”\nin CVPR, pp. 13926–13935, 2021.\n[29] M. Pang, T. K. Roy, X. Wu, and K. Tan, “CelloType: A unified model\nfor segmentation and classification of tissue images,” Nature Methods,\npp. 1–10, 2024.\n[30] V. Caselles, R. Kimmel, and G. Sapiro, “Geodesic active contours,”\nInternational Journal of Computer Vision, vol. 22, no. 1, p. 61, 1997.\n[31] F. Lux and P. Matula, “DIC image segmentation of dense cell pop-\nulations by combining deep learning and watershed,” in 2019 IEEE\nInternational Symposium on Biomedical Imaging, pp. 236–239, 2019.\n[32] D. Eschweiler, T. V. Spina, R. C. Choudhury, E. Meyerowitz, A. Cunha,\nand J. Stegmaier, “CNN-based preprocessing to optimize watershed-\nbased cell segmentation in 3D confocal microscopy images,” in IEEE\n16th International Symp. on Biomedical Imaging, pp. 223–227, 2019.\n[33] S. Wolf, L. Schott, U. Kothe, and F. Hamprecht, “Learned watershed:\nEnd-to-end learning of seeded segmentation,” in IEEE International\nConference on Computer Vision, pp. 2011–2019, 2017.\n[34] M. Kass, A. Witkin, and D. Terzopoulos, “Snakes: Active contour\nmodels,” International Journal of Computer Vision, vol. 1, no. 4,\npp. 321–331, 1988.\n[35] T. F. Chan and L. A. Vese, “Active contours without edges,” IEEE\nTransactions on Image Processing, vol. 10, no. 2, pp. 266–277, 2001.\n[36] G. Sundaramoorthi, A. Yezzi, and A. C. Mennucci, “Sobolev active\ncontours,” International Journal of Computer Vision, vol. 73, pp. 345–\n366, 2007.\n[37] C. Rupprecht, E. Huaroc, M. Baust, and N. Navab, “Deep active\ncontours,” arXiv preprint arXiv:1607.05074, 2016.\n[38] B. Kim and J. C. Ye, “Mumford–Shah loss functional for image seg-\nmentation with deep learning,” IEEE Transactions on Image Processing,\nvol. 29, pp. 1856–1866, 2019.\n[39] X. Chen, B. M. Williams, S. R. Vallabhaneni, G. Czanner, R. Williams,\nand Y. Zheng, “Learning active contour models for medical image\nsegmentation,” in CVPR, pp. 11632–11640, 2019.\n[40] M. Zhang, B. Dong, and Q. Li, “Deep active contour network for medical\nimage segmentation,” in MICCAI, pp. 321–331, 2020.\n[41] Y. Kim, S. Kim, T. Kim, and C. Kim, “CNN-based semantic segmen-\ntation using level set loss,” in WACV, pp. 1752–1760, 2019.\n[42] S. Gur, L. Wolf, L. Golgher, and P. Blinder, “Unsupervised microvas-\ncular image segmentation using an active contours mimicking neural\nnetwork,” in ICCV, pp. 10722–10731, 2019.\n[43] D. Marcos, D. Tuia, B. Kellenberger, L. Zhang, M. Bai, R. Liao, and\nR. Urtasun, “Learning deep structured active contours end-to-end,” in\nCVPR, pp. 8877–8885, 2018.\n[44] L. Tang, Y. Zhan, Z. Chen, B. Yu, and D. Tao, “Contrastive boundary\nlearning for point cloud segmentation,” in CVPR, pp. 8489–8499, 2022.\n[45] N. Silberman, D. Sontag, and R. Fergus, “Instance segmentation of\nindoor scenes using a coverage loss,” in ECCV, pp. 616–631, 2014.\n[46] J. Funke, C. Zhang, T. Pietzsch, M. A. G. Ballester, and S. Saalfeld,\n“The candidate multi-cut for cell segmentation,” in IEEE International\nSymposium on Biomedical Imaging, pp. 649–653, 2018.\n[47] H. Fehri, A. Gooya, Y. Lu, E. Meijering, S. A. Johnston, and A. F.\nFrangi, “Bayesian polytrees with learned deep features for multi-class\ncell segmentation,” IEEE Transactions on Image Processing, vol. 28,\nno. 7, pp. 3246–3260, 2019.\n[48] R. Souza, L. Tavares, L. Rittner, and R. Lotufo, “An overview of\nmax-tree principles, algorithms and applications,” in 2016 SIBGRAPI\nConference on Graphics, Patterns and Images Tutorials, pp. 15–23.\n[49] R. Jones, “Connected filtering and segmentation using component trees,”\nComputer Vision and Image Understanding, vol. 75, no. 3, pp. 215–228,\n1999.\n[50] S. U. Akram, J. Kannala, M. Kaakinen, L. Eklund, and J. Heikkil¨a, “Seg-\nmentation of cells from spinning disk confocal images using a multi-\nstage approach,” in Asian Conference on Computer Vision, pp. 300–314,\n2014.\n[51] S. U. Akram, J. Kannala, L. Eklund, and J. Heikkil¨a, “Cell segmen-\ntation and tracking using cell proposals,” in IEEE 13th International\nSymposium on Biomedical Imaging, pp. 920–924, 2016.\n[52] R. Razani, R. Cheng, E. Li, E. Taghavi, Y. Ren, and L. Bingbing, “GP-\nS3Net: Graph-based panoptic sparse semantic segmentation network,”\nin ICCV, pp. 16076–16085, 2021.\n[53] M. Ester, H.-P. Kriegel, J. Sander, X. Xu, et al., “A density-based\nalgorithm for discovering clusters in large spatial databases with noise,”\nin KDD, vol. 96, pp. 226–231, 1996.\n[54] S. Suzuki et al., “Topological structural analysis of digitized binary\nimages by border following,” Computer Vision, Graphics, and Image\nProcessing, vol. 30, no. 1, pp. 32–46, 1985.\n[55] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction\nto Algorithms. MIT Press, 2022.\nAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING\n15\n[56] Y. LeCun, “The MNIST database of handwritten digits,” http://yann.\nlecun. com/exdb/mnist/, 1998.\n[57] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in CVPR, pp. 770–778, 2016.\n[58] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss\nfor dense object detection,” in Proceedings of the IEEE International\nConference on Computer Vision, pp. 2980–2988, 2017.\n[59] “Component\n(graph\ntheory)\nwikipedia,\nthe\nfree\nencyclopedia:\nhttps://en.wikipedia.org/wiki/component (graph theory),” 2024.\n[60] V. Ulman, M. Maˇska, K. E. Magnusson, O. Ronneberger, C. Haubold,\nN. Harder, P. Matula, P. Matula, D. Svoboda, M. Radojevic, et al.,\n“An objective comparison of cell-tracking algorithms,” Nature Methods,\nvol. 14, no. 12, pp. 1141–1152, 2017.\n[61] M. Pachitariu and C. Stringer, “Cellpose 2.0: How to train your own\nmodel,” Nature Methods, pp. 1–8, 2022.\n[62] T. Scherr, K. L¨offler, O. Neumann, and R. Mikut, “On improving\nan already competitive segmentation algorithm for the cell tracking\nchallenge-lessons learned,” bioRxiv, 2021.\n[63] Y. Zhang, Y. Zhou, Y. Wang, J. Xiao, Z. Wang, Y. Zhang, and J. Chen,\n“The four color theorem for cell instance segmentation,” in International\nConference on Machine Learning (ICML), 2025.\n[64] Z. Chen, Q. Xu, X. Liu, and Y. Yuan, “UN-SAM: Domain-adaptive\nself-prompt segmentation for universal nuclei images,” Medical Image\nAnalysis, p. 103607, 2025.\n[65] M. Pang, T. K. Roy, X. Wu, and K. Tan, “CelloType: A unified model\nfor segmentation and classification of tissue images,” Nature Methods,\nvol. 22, no. 2, pp. 348–357, 2025.\n[66] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and\nP. Torr, “Res2Net: A new multi-scale backbone architecture,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 43,\nno. 2, pp. 652–662, 2019.\n[67] J. C. Caicedo, J. Roth, A. Goodman, T. Becker, K. W. Karhohs,\nM. Broisin, C. Molnar, C. McQuin, S. Singh, F. J. Theis, et al.,\n“Evaluation of deep learning strategies for nucleus segmentation in\nfluorescence images,” Cytometry Part A, vol. 95, pp. 952–965, 2019.\n[68] N. F. Greenwald, G. Miller, E. Moen, A. Kong, A. Kagel, T. Dougherty,\nC. C. Fullaway, B. J. McIntosh, K. X. Leow, M. S. Schwartz, et al.,\n“Whole-cell segmentation of tissue images with human-level perfor-\nmance using large-scale data annotation and deep learning,” Nature\nBiotechnology, vol. 40, no. 4, pp. 555–565, 2022.\n[69] N. Otsu, “A threshold selection method from gray-level histograms,”\nIEEE Transactions on Systems, Man, and Cybernetics, vol. 9, no. 1,\npp. 62–66, 1979.\n[70] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,\nD. K. Menon, D. Rueckert, and B. Glocker, “Efficient multi-scale 3D\nCNN with fully connected CRF for accurate brain lesion segmentation,”\nMedical Image Analysis, vol. 36, pp. 61–78, 2017.\n[71] M. Maˇska, V. Ulman, D. Svoboda, P. Matula, P. Matula, C. Ederra,\nA. Urbiola, T. Espa˜na, S. Venkatesan, D. M. Balak, et al., “A benchmark\nfor comparison of cell tracking algorithms,” Bioinformatics, vol. 30,\nno. 11, pp. 1609–1617, 2014.\n[72] K. E. Magnusson, J. Jald´en, P. M. Gilbert, and H. M. Blau, “Global\nlinking of cell tracks using the Viterbi algorithm,” IEEE Transactions\non Medical Imaging, vol. 34, no. 4, pp. 911–929, 2014.\n[73] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\n[74] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\nT. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., “Segment anything,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 4015–4026, 2023.\n[75] M. Caron, H. Touvron, I. Misra, H. J´egou, J. Mairal, P. Bojanowski, and\nA. Joulin, “Emerging properties in self-supervised Vision Transformers,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, pp. 9650–9660, 2021.\n[76] J. Chen, Y. Cai, C. Wei, L. Yang, M. S. Alber, and D. Z. Chen,\n“Segmentation and tracking of Pseudomonas aeruginosa for cell dy-\nnamics analysis in time-lapse images,” in 2016 IEEE 13th International\nSymposium on Biomedical Imaging (ISBI), pp. 968–971, IEEE, 2016.\n",
    "content": "# Paper Interpretation: Cell Instance Segmentation: The Devil Is in the Boundaries\n\n## 1. Core Content and Key Contributions\n\nThis paper proposes a novel cell instance segmentation method called **Ceb** (Cell boundaries), designed to address the issue that existing deep learning approaches lose critical geometric information when processing cellular images due to their reliance on pixel-level objective functions.\n\n### Central Idea\nTraditional methods typically decompose cell instance segmentation into semantic segmentation tasks (foreground vs. background) and then use pixel-level features—such as distance maps or heat diffusion maps—to cluster pixels and identify individual cells. However, these approaches often overlook structural geometric properties like cell shape, curvature, and convexity. The innovation of Ceb lies in shifting from **\"pixel-level\" to \"boundary-level\" modeling**: instead of segmenting cells via pixel clustering, it reframes the problem as a classification task over potential inter-cell boundaries.\n\n### Main Contributions\n- **Proposes the Ceb framework**: A new boundary-centric pipeline for cell instance segmentation that effectively preserves geometric structure.\n- **Introduces Boundary Signature**: A novel representation for boundary features, constructed by sampling around foreground-foreground candidate boundaries and adjacent background-foreground transitions into binary images, capturing differences between true and false boundaries.\n- **Improves Watershed algorithm**: Used to generate all possible inter-cell candidate boundaries; combined with an optimal matching model to assign ground-truth labels (true/false) during training.\n- **Incorporates temporal consistency (Ceb+temporal)**: For video data, leverages inter-frame instance stability to improve segmentation performance.\n- Experimental results show Ceb outperforms existing pixel-clustering baselines and is competitive with state-of-the-art (SOTA) holistic instance segmentation methods across multiple public datasets.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### (1) Paradigm Shift: From \"Pixel Clustering\" to \"Boundary Decision\"\nThis is the most significant innovation. Unlike mainstream methods such as StarDist, CellPose, or Hover-Net—which rely on pixel embeddings or distance fields for clustering—Ceb redefines the task as a **binary classification problem**: determining whether each watershed-generated candidate boundary is a real cell-cell interface. This \"boundary-first\" strategy aligns more closely with how biologists visually inspect cells and better preserves geometric priors.\n\n> ✅ Innovation Value: Solves the fundamental limitation of pixel-level loss functions in expressing complex geometric attributes.\n\n### (2) Insightful Design of Boundary Signature\nThe feature extraction mechanism cleverly captures key geometric cues:\n- **True boundaries** often form “X”-shaped junctions where two cells meet, with continuous and aligned edges on both sides;\n- **False boundaries** (artifacts from over-segmentation) tend to appear as “T” or “H” shapes, exhibiting discontinuities or misalignments.\n\nBy extracting local topological structures at boundary endpoints and encoding them into fixed-size binary images, even lightweight CNN classifiers can efficiently distinguish real from spurious boundaries.\n\n> ✅ Innovation Value: Provides a learnable and interpretable boundary representation, significantly improving classification accuracy.\n\n### (3) Optimal Matching Mechanism for Boundary Labeling\nSince automatically generated candidate boundaries do not perfectly align with ground-truth masks, the authors introduce an Integer Linear Programming (ILP)-based optimal matching model. By maximizing the total IoU between predicted and true instances, this model infers correct labels for each candidate boundary (internal merging → false; external enclosing → true). This ensures high-quality supervision signals without manual annotation.\n\n> ✅ Innovation Value: Enables accurate boundary labeling under weak supervision, avoiding costly manual boundary annotations.\n\n### (4) Temporal Consistency Fusion Strategy (Ceb+temporal)\nFor video sequences, previously identified cell instances are used as \"anchors\" to propagate stable predictions across frames through iterative matching, reducing false segmentations. This approach proves more robust than simple post-processing.\n\n> ✅ Innovation Value: Demonstrates how spatiotemporal context can be seamlessly integrated into non-end-to-end frameworks, enhancing robustness in dynamic scenarios.\n\n---\n\n## 3. Entrepreneurial Opportunities Based on the Research\n\nLeveraging the core technical strength of this work—**high-precision, low-over-segmentation-risk cell boundary detection**—here are several commercially promising startup directions:\n\n---\n\n### 🌟 Project 1: AI-Powered Pathology Diagnostic Platform — “CellBoundary Dx”\n\n#### Concept\nDevelop an AI software system specialized for digital pathology slide analysis, focusing on **cell density, atypia, and spatial arrangement patterns** in tumor tissues to assist pathologists in assessing cancer grades (e.g., Gleason Score for Prostate Cancer, Nottingham Grade for Breast Cancer).\n\n#### Technical Advantages\n- Uses Ceb to deliver precise single-cell contours, avoiding missed detections or incorrect merges caused by cell adhesion in traditional methods.\n- Quantifies cellular atypia using geometric metrics such as boundary curvature and concavity depth.\n- Supports multi-scale analysis: from single-cell morphology → cell clusters → tissue microenvironment.\n\n#### Business Model\n- SaaS subscription sold directly to hospital pathology departments.\n- Pre-installation partnerships with digital slide scanner manufacturers.\n- Offer both research-use and clinically certified versions (FDA/CE clearance pathway).\n\n#### Differentiation\n> “Not just another segmentation tool—but the first pathology AI that truly understands the meaning of ‘cell boundaries.’”\n\n---\n\n### 🌟 Project 2: Real-Time Feedback System for Smart Microscopes — “LiveCell Guard”\n\n#### Concept\nIntegrate the Ceb algorithm into live-cell imaging microscopes to enable real-time monitoring and alerts for cell behavior. Ideal for applications in drug screening, stem cell culture, and immune co-culture experiments.\n\n#### Key Features\n- Real-time detection of excessive confluence (>80%), abnormal mitosis, necrosis, or cell detachment.\n- Automatically flag suspicious regions and trigger alarms or image capture.\n- Dynamically track cell count trends and generate growth kinetics reports.\n\n#### Implementation\n- Deploy lightweight version of Ceb (fine-tuned ResNet-18) on edge devices.\n- Support plug-and-play USB microscope integration to lower entry barriers.\n- Provide API connectivity to LIMS (Laboratory Information Management Systems).\n\n#### Target Customers\n- Biopharmaceutical companies (drug discovery units)\n- University life science labs\n- Stem cell therapy startups\n\n#### Market Opportunity\n> Global live-cell imaging market projected to reach $15B by 2027—strong demand for automation, but current products lack intelligent capabilities.\n\n---\n\n### 🌟 Project 3: Cloud-Based AI Phenotypic Screening Platform — “PhenoScreen Cloud”\n\n#### Concept\nBuild a cloud service platform targeting pharmaceutical and gene-editing companies. Users upload fluorescently labeled cell images, and the system automatically:\n1. Precisely segments nuclei and cytoplasm;\n2. Extracts hundreds of morphological features (area, perimeter, texture, eccentricity, boundary irregularity, etc.);\n3. Constructs phenotypic fingerprint profiles for drug response classification or gene function annotation.\n\n#### Core Competitive Edge\n- Ceb provides high-quality segmentation, ensuring reliable downstream feature extraction.\n- Standardizes data across different instruments and staining protocols.\n- Built-in ML modules for automatic clustering and interactive visualization.\n\n#### Revenue Model\n- Pay-per-image cloud processing plans.\n- Custom analysis services (contract research).\n- SDK licensing for private deployment by enterprise clients.\n\n#### Solving Industry Pain Points\n> Current phenotypic screening is severely limited by poor segmentation quality—Ceb can become the \"gold standard\" engine.\n\n---\n\n### 💡 Summary: Comparison of Three Startup Directions\n\n| Project | Technical Core | Target Customers | Commercial Potential | Entry Difficulty |\n|--------|----------------|------------------|------------------------|--------------------|\n| CellBoundary Dx | High-precision pathological segmentation | Hospitals, diagnostic labs | ⭐⭐⭐⭐☆ (Medical necessity) | ⭐⭐⭐⭐☆ (Regulatory hurdles) |\n| LiveCell Guard | Real-time boundary recognition | Research institutes, pharma | ⭐⭐⭐☆☆ (Niche instrument add-on) | ⭐⭐☆☆☆ (Hardware integration) |\n| PhenoScreen Cloud | High-dimensional phenotypic analysis | Pharma, CROs | ⭐⭐⭐⭐☆ (R&D productivity tool) | ⭐⭐☆☆☆ (Scalable SaaS model) |\n\n---\n\n> 🔚 **One-Sentence Entrepreneurial Advice**:  \n> Productize the idea that **“boundaries are knowledge”**, and build next-generation biomedical AI infrastructure centered on **precise, interpretable, and quantifiable geometric analysis of cells**.",
    "github": "",
    "hf": ""
}