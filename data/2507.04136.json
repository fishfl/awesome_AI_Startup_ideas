{
    "id": "2507.04136",
    "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models",
    "summary": "This paper reviews the integration of reinforcement learning (RL) with large language models (LLMs), providing a detailed introduction to various RL algorithms and methods, and analyzing their applications and challenges in different fields.",
    "abstract": "Reinforcement Learning (RL) has emerged as a transformative approach for aligning and enhancing Large Language Models (LLMs), addressing critical challenges in instruction following, ethical alignment, and reasoning capabilities. This survey offers a comprehensive foundation on the integration of RL with language models, highlighting prominent algorithms such as Proximal Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally, it provides an extensive technical overview of RL techniques specifically tailored for LLMs, including foundational methods like Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced strategies such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO). We systematically analyze their applications across domains, i.e., from code generation to tool-augmented reasoning. We also present a comparative taxonomy based on reward modeling, feedback mechanisms, and optimization strategies. Our evaluation highlights key trends. RLHF remains dominant for alignment, and outcome-based RL such as RLVR significantly improves stepwise reasoning. However, persistent challenges such as reward hacking, computational costs, and scalable feedback collection underscore the need for continued innovation. We further discuss emerging directions, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks. This survey serves as a roadmap for researchers advancing RL-driven LLM development, balancing capability enhancement with safety and scalability.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Saksham Sahai Srivastava,Vaneet Aggarwal",
    "subjects": [
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:24 pages, LaTeX source",
    "keypoint": "- Reinforcement Learning (RL) has become a transformative approach for aligning and enhancing Large Language Models (LLMs).\n- Prominent RL algorithms like Proximal Policy Optimization (PPO), Q-Learning, and Actor-Critic methods are foundational in integrating RL with LLMs.\n- Techniques such as Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF) are highlighted as foundational methods.\n- Advanced strategies like Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) are detailed for their effectiveness in alignment and reasoning.\n- Applications of RL techniques span domains including code generation and tool-augmented reasoning.\n- A comparative taxonomy based on reward modeling, feedback mechanisms, and optimization strategies is presented.\n- RLHF remains dominant for alignment, while outcome-based RL significantly improves stepwise reasoning.\n- Challenges identified include reward hacking, computational costs, and scalable feedback collection.\n- Emerging directions involve hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks.",
    "date": "2025-07-10",
    "paper": "arXiv:2507.04136v1  [cs.AI]  5 Jul 2025\nA Technical Survey of Reinforcement Learning\nTechniques for Large Language Models\nSaksham Sahai Srivastava\nUniversity of Colorado Boulder\nBoulder, CO, USA\nsaksham.srivastava@colorado.edu\nVaneet Aggarwal\nPurdue University\nWest Lafayette, IN, USA\nvaneet@purdue.edu\nAbstract—Reinforcement Learning (RL) has emerged as a\ntransformative approach for aligning and enhancing Large\nLanguage Models (LLMs), addressing critical challenges in\ninstruction following, ethical alignment, and reasoning capa-\nbilities. This survey offers a comprehensive foundation on the\nintegration of RL with language models, highlighting prominent\nalgorithms such as Proximal Policy Optimization (PPO), Q-\nLearning, and Actor-Critic methods. Additionally, it provides an\nextensive technical overview of RL techniques specifically tailored\nfor LLMs, including foundational methods like Reinforcement\nLearning from Human Feedback (RLHF) and AI Feedback\n(RLAIF), as well as advanced strategies such as Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization\n(GRPO). We systematically analyze their applications across\ndomains, i.e., from code generation to tool-augmented reasoning.\nWe also present a comparative taxonomy based on reward\nmodeling, feedback mechanisms, and optimization strategies. Our\nevaluation highlights key trends. RLHF remains dominant for\nalignment, and outcome-based RL such as RLVR significantly\nimproves stepwise reasoning. However, persistent challenges such\nas reward hacking, computational costs, and scalable feedback\ncollection underscore the need for continued innovation. We\nfurther discuss emerging directions, including hybrid RL algo-\nrithms, verifier-guided training, and multi-objective alignment\nframeworks. This survey serves as a roadmap for researchers\nadvancing RL-driven LLM development, balancing capability\nenhancement with safety and scalability.\nIndex Terms—reinforcement learning, large language models,\nRLHF, alignment, reasoning, natural language processing, arti-\nficial intelligence\nI. INTRODUCTION\nLarge Language Models (LLMs) have emerged as trans-\nformative technologies in artificial intelligence, demonstrating\nremarkable capabilities in understanding and generating hu-\nman language. From GPT-3’s 175 billion parameters [1] to\nmore recent architectures like LLaMA 3.1 with 405 billion\nparameters [2] and DeepSeek-V3 with 671 billion parameters\n[3], these models have progressively expanded in scale and\ncapability. Despite their impressive performance across various\ntasks, LLMs still struggle with alignment, so ensuring their\noutputs consistently reflect human values, preferences, and\nintentions remains a significant challenge. These models often\nstruggle with hallucinations [4], exhibit vulnerability to gen-\nerating harmful content [5]–[7], and frequently fail to follow\ncomplex instructions precisely [8].\nReinforcement Learning (RL) is a paradigm where agents\nlearn through trial and error by interacting with an environ-\nment. It has emerged as a powerful framework for address-\ning these alignment challenges. Unlike traditional supervised\nlearning methods that rely on labeled examples, it offers\nmechanisms to incorporate non-differentiable feedback signals\nand optimizes for complex, multi-faceted objectives. The inte-\ngration of RL with LLMs represents a significant advancement\nin AI alignment research, enabling models to learn from\nhuman preferences, improve reasoning capabilities, and better\nadhere to ethical guidelines. This technical survey provides\na comprehensive examination of RL techniques applied to\nLLMs, focusing on both alignment with human values and\nenhancement of reasoning capabilities.\nThe application of RL to LLMs presents unique challenges\nthat distinguish it from traditional RL domains. The state\nspace in LLM contexts typically comprises input prompts\nor conversation histories. The action space encompasses the\nmodel’s entire vocabulary of the LLM. This results in an\nextremely large and discrete set of possible actions. The high-\ndimensional action space necessitates specialized approaches\nthat differ substantially from those employed in more con-\nventional RL applications such as robotics or game playing.\nFurthermore, the reward signals in LLM alignment often\nderive from complex human judgments about text quality,\nhelpfulness, harmlessness, and honesty. These attributes are\ninherently subjective and challenging to quantify.\nReinforcement Learning from Human Feedback (RLHF) [9]\nhas become the de facto standard for aligning LLMs with\nhuman preferences. This approach typically follows a three-\nstage process: supervised fine-tuning on high-quality demon-\nstrations, training a reward model from human preference\ndata, and optimizing the policy using algorithms like Proximal\nPolicy Optimization (PPO) [10]. RLHF has demonstrated\nremarkable effectiveness in improving instruction-following\ncapabilities and reducing harmful outputs, as evidenced by\nOpenAI’s InstructGPT [9].\nHowever, the scalability limitations of human annotation\nhave motivated the development of alternative approaches.\nReinforcement Learning from AI Feedback (RLAIF) [11]\nreplaces or augments human feedback with evaluations from\nother AI systems, significantly reducing annotation costs while\nmaintaining comparable performance. Constitutional AI [12]\nrepresents a specialized form of RLAIF where models critique\nand revise their own outputs based on predefined principles,\nparticularly effective for harmlessness alignment. More recent\ninnovations have focused on simplifying the RLHF pipeline.\nDirect Preference Optimization (DPO) [13] bypasses explicit\nreward modeling by directly optimizing the policy using\npreference pairs, offering improved computational efficiency\nand training stability. Empirical evaluations have shown that\nDPO can match or exceed the performance of PPO [10]-based\nRLHF on tasks like sentiment control and summarization with\nsubstantially reduced complexity.\nBeyond alignment with human preferences, RL techniques\nhave increasingly been applied to enhance reasoning capa-\nbilities in LLMs. Outcome-Based Reinforcement Learning\napproaches [14] reward models for generating correct final\nanswers, even when intermediate reasoning steps are not\nexplicitly supervised. More sophisticated methods like Re-\ninforcement Learning with Verifiable Rewards (RLVR) [15]\nprovide step-wise feedback on reasoning processes, signifi-\ncantly improving performance on mathematical and logical\nreasoning tasks. For instance, RLVR boosted GPT-3.5’s ac-\ncuracy on the GSM8K mathematical reasoning benchmark\nfrom 56.8% to 72.5% with minimal training examples. Despite\nthese advances, significant challenges persist in applying RL\nto LLMs. One such challenge is reward hacking [16], [17],\nwhere models exploit loopholes in the reward functions instead\nof genuinely improving their behavior. The computational\ncosts associated with RL training, particularly for models\nwith billions of parameters, also present practical limitations\nfor widespread adoption. Additionally, ensuring the quality\nand representativeness of feedback [18], [19], whether from\nhumans or AI systems, continues to be a complex problem.\nThis survey makes several key contributions to the field.\nFirst, we provide a comprehensive technical overview of\nRL techniques applied to LLMs. We cover foundational\nmethods such as RLHF and RLAIF, as well as advanced\napproaches like Direct Preference Optimization (DPO) and\nGroup Relative Policy Optimization (GRPO). Second, we\nsystematically analyze applications across various domains.\nThese domains include code generation and tool-augmented\nreasoning, demonstrating RL’s versatility and effectiveness.\nThird, we present a comparative taxonomy based on reward\nmodeling strategies, feedback mechanisms, and optimization\napproaches. This taxonomy provides a structured framework to\nunderstand the landscape of RL techniques for LLMs. Finally,\nwe identify emerging research directions such as hybrid RL\nalgorithms, verifier-guided training, and multi-objective align-\nment frameworks.\nThe remainder of this paper is organized as follows: Section\n2 establishes the foundational concepts of LLMs and RL;\nSection 3 details specific RL algorithms adapted for LLMs;\nSection 4 explores RL techniques for alignment and reasoning\nenhancement; Section 5 presents applications across various\ndomains; Section 6 provides a comparative analysis and eval-\nuation; Section 7 discusses challenges and limitations; Section\n8 talks about future research directions; and Section 9 presents\nthe conclusion of this survey. We aim to provide researchers\nand practitioners with a roadmap for advancing RL-driven\nLLM development through this comprehensive examination.\nThis roadmap seeks to balance capability enhancement with\nsafety and scalability considerations.\nII. BACKGROUND & FOUNDATIONS\nThis section establishes the foundational concepts necessary\nfor understanding reinforcement learning techniques as applied\nto large language models. We begin with an overview of large\nlanguage models, followed by the fundamentals of reinforce-\nment learning, and conclude with the intersection of these two\ndomains.\nA. Large Language Models\nLarge Language Models (LLMs) are neural network archi-\ntectures, typically based on the transformer architecture [20],\nthat have been trained on vast corpora of text data. These\nmodels have revolutionized natural language processing by\ndemonstrating remarkable capabilities in understanding and\ngenerating human language.\n1) Architecture and Training: Modern large language mod-\nels overwhelmingly employ the decoder-only branch of the\nTransformer introduced by Vaswani et al. [20], stacking\nself-attention blocks to generate text autoregressively, token\nby token. This design exploits multi-head self-attention to\nprocess all positions in parallel, giving the models the ca-\npacity to capture long-range dependencies far more efficiently\nthan recurrent networks. The training of LLMs is framed\nas maximum-likelihood next-token prediction. Therefore, to-\nday’s LLMs essentially represent large conditional probability\nmodels that forecast the most likely subsequent token given\nthe context. The scale of large language models (LLMs) has\nexpanded dramatically in recent years. OpenAI’s GPT-2 [21]\ninitially featured 1.5 billion parameters, a number significantly\nsurpassed by GPT-3’s [1] 175 billion parameters. Google’s\nPaLM further scaled this capacity to 540 billion parameters.\nThen, Meta’s LLaMA 3.1 [2] had 405 billion parameters, and\nDeepSeek-V3 [3], a mixture-of-experts model contained 671\nbillion parameters.\nThe training of LLMs typically follows a two-phase ap-\nproach. The first phase is Pre-training. In this phase, the model\nis trained on a diverse corpus of text using self-supervised\nlearning objectives, such as predicting masked tokens or next-\ntoken prediction. This phase enables the model to learn general\nlanguage patterns, world knowledge, and reasoning capabili-\nties. The second phase is fine-tuning. In this phase, the pre-\ntrained model is further trained on specific datasets to adapt\nit for particular tasks or to align it with human preferences.\n2) Capabilities and Limitations: Pre-trained LLMs demon-\nstrate impressive capabilities across various tasks, including\ntext completion, summarization, translation, question answer-\ning, and even complex reasoning. Despite these advancements,\nthese models still suffer from notable limitations. One critical\nissue is hallucinations [4], where LLMs generate plausible\nyet factually incorrect information, leading to potentially\nmisleading outputs. Additionally, without proper safeguards,\nthese models can produce harmful, biased, or toxic content\n[22], reflecting and amplifying societal biases [23] present\nin their training data. LLMs [8] also frequently struggle\nto precisely follow complex or multi-step user instructions,\nlimiting their practical utility in structured tasks. Furthermore,\nmodels trained primarily on predictive objectives often strug-\ngle to align effectively with human values, preferences, or\nintentions, underscoring the necessity of specialized align-\nment techniques. Although prompting-based methods such\nas MathPrompter [24] and MathDivide [25] have enhanced\nmathematical reasoning within narrow domains by employing\nstrategies like multi-path validation and problem decomposi-\ntion, they remain constrained by their dependency on fixed\nprompting heuristics and their inability to generalize or incor-\nporate broader interactive feedback. These limitations high-\nlight the need for additional training methodologies beyond\nconventional supervised learning. Such methods are crucial to\nensure that large language models remain consistently helpful,\nharmless, and honest, which is a central challenge known as\nthe alignment problem.\nB. Reinforcement Learning Fundamentals\nReinforcement\nLearning\n(RL)\nis\na\nmachine\nlearning\nparadigm in which an agent learns optimal decision-making\nby interacting with an environment and receiving feedback\nthrough rewards or penalties. The RL framework is typically\nformalized as a Markov Decision Process (MDP), defined by\nstates (S) representing possible situations, actions (A) indi-\ncating available decisions, a transition function (P) specifying\nprobabilities of moving between states given actions, and a\nreward function (R) providing immediate feedback on actions.\nCentral to RL are policies (π) which are strategies mapping\nstates to actions, and value functions (V or Q), estimating\nexpected cumulative future rewards. A discount factor (γ)\nbalances immediate versus future rewards. Ultimately, the\nobjective in RL is to discover the optimal policy (π∗) that\nmaximizes long-term expected cumulative rewards.\nSeveral families of RL algorithms have been developed to\neffectively learn optimal decision-making. Value-based meth-\nods, such as Q-learning [26] and Deep Q-Networks (DQN)\n[27], focus on estimating value functions to determine the ex-\npected cumulative reward from each state or state-action pair.\nIn contrast, policy gradient methods, including REINFORCE\n[28] and Proximal Policy Optimization (PPO) [10], directly\noptimize the policy by computing gradients of expected returns\nwith respect to policy parameters. Actor-critic methods, such\nas Advantage Actor-Critic (A2C) [29] and Soft Actor-Critic\n(SAC) [30], combine these approaches by simultaneously\nlearning a value function (critic) and a policy (actor).\nDespite their effectiveness, RL algorithms face inherent\nchallenges that complicate their practical deployment. The\nexploration-exploitation tradeoff [31] requires balancing the\nneed to discover new beneficial actions with exploiting known\nsuccessful behaviors. Credit assignment poses difficulties in\nattributing rewards accurately to specific actions within se-\nquences, especially when outcomes are delayed. Sample ef-\nficiency is another critical concern, as RL algorithms often\nrequire extensive interactions with the environment to learn\neffectively. Finally, ensuring stability during learning, par-\nticularly when employing complex function approximations\nlike neural networks, is essential to prevent divergence and\nmaintain reliable performance.\nC. Intersection of RL and LLMs\nThe application of reinforcement learning (RL) to large\nlanguage models (LLMs) represents a promising strategy for\naddressing the alignment problem and also for enhancing the\nreasoning capability of models. RL allows models to utilize\nfeedback signals that are inherently non-differentiable, such\nas subjective human judgments. This way, instead of relying\nsolely on absolute quality scores, RL facilitates preference-\nbased learning [9], enabling the model to improve by compar-\ning different outputs and selecting those preferred by humans.\nSimilarly, RL can strengthen model’s step-by-step reasoning\nby rewarding chains of thought that lead to verifiably correct\nanswers. Recent studies [32], [33] show that even a handful\nof RL updates with logic-based rewards measurably improve\nperformance on multi-step math and planning tasks.\nTo effectively apply RL to LLMs, several adaptations to\nthe standard framework are required. The state (S) in LLMs\nis typically represented by the input prompt or conversation\nhistory, which provides the necessary context for generat-\ning coherent responses. The action space (A) comprises the\nmodel’s entire vocabulary of tokens, resulting in an extremely\nlarge and discrete set of possible actions. Direct human eval-\nuation of outputs is infeasible at scale, so rewards are derived\nfrom trained models that approximate human preferences. For\nreasoning tasks, these reward models are further supplemented\nwith automated verification tools, such as equation solvers or\nprogram interpreters, to validate intermediate reasoning steps.\nPolicy updates are guided by this composite reward signal.\nAlgorithms like Proximal Policy Optimization (PPO) [10]\nare adapted to accommodate long token sequences and large\nbatch sizes. A KL-divergence penalty, which measures how\nmuch the RL policy deviates from the supervised pre-training\nbaseline, ensures that the RL-tuned policy remains anchored\nand does not stray too far from the original distribution of\noutputs. Meanwhile, entropy regularization encourages explo-\nration by maintaining a level of uncertainty in the model’s\ntoken predictions, preventing it from prematurely settling on\nnarrow patterns. Together, these mechanisms are critical for\nfostering coherent and robust multistep reasoning in LLMs.\nThese innovations have spurred specialized RL methodologies\nfor LLMs. Reinforcement Learning from Human Feedback\n(RLHF) [9] remains the cornerstone for aligning models\nwith human values. Emerging techniques, exemplified by\nframeworks like DeepSeek-R1 [34], further prioritize explicit\nrewards for logical stepwise reasoning, sharpening LLMs’\nability to solve complex problems through structured, veri-\nfiable thought processes.\nIII. REINFORCEMENT LEARNING ALGORITHMS FOR\nLLMS\nThis section reviews prominent reinforcement learning (RL)\nalgorithms specifically tailored for aligning large language\nmodels (LLMs) with human preferences and enhancing their\nreasoning capabilities. We first examine Proximal Policy Op-\ntimization [10] (PPO), highlighting its stability and wide\nadoption in Reinforcement Learning from Human Feed-\nback (RLHF) frameworks. Next, we explore Q-learning and\nother Off-Policy RL methods, such as Implicit Language Q-\nLearning (ILQL) [35] and VerifierQ [36], focusing on their\neffectiveness in leveraging offline datasets and verifier-based\nreasoning enhancement. Finally, we discuss advanced method-\nologies like Group Relative Policy Optimization (GRPO) [34],\nwhich improves training efficiency and robustness by adopt-\ning relative advantage estimation within grouped candidate\nresponses.\nA. Proximal Policy Optimization:\nProximal Policy Optimization (PPO) is a stable and effective\non-policy, policy-gradient reinforcement learning algorithm.\nIt performs well in high-dimensional action spaces, making\nit particularly suitable for large language models (LLMs).\nDue to these favorable properties, PPO has emerged as the\nde facto standard algorithm for fine-tuning LLMs in Rein-\nforcement Learning from Human Feedback (RLHF) scenarios.\nNotably, OpenAI [10] popularized the use of PPO through\ntheir RLHF implementation in the InstructGPT model [9].\nHere, PPO was employed to align the model with human\ninstructions. Within the RLHF framework, PPO iteratively\nupdates the language model policy by maximizing rewards\nprovided by a learned reward model. It also simultaneously\nconstrains policy changes relative to a reference model to\nmaintain stable updates. Zheng et al. [37] highlighted that PPO\nworks well because it uses policy-constraint mechanisms like\nclipped probability ratio updates or KL divergence penalties.\nThese constraints keep the updates stable and reliable during\ntraining. They also introduced a new variant called PPO-Max,\nwhich makes the training process even more stable. PPO’s\nbalanced optimization properties have established it as the\npredominant method for training aligned LLM policies using\nRLHF. Mathematically, the primary objective of PPO, called\nthe clipped surrogate objective, is formulated as follows:\nLPPO(θ) = Et\nh\nmin\n\u0010\nrt(θ) ˆAt, clip(rt(θ), 1 −ϵ, 1 + ϵ) ˆAt\n\u0011i\nwhere rt(θ) =\nπθ(at|st)\nπθold(at|st) (probability ratio), ˆAt = estimated\nadvantage at time step t, ϵ = clip parameter (typically 0.1\nor 0.2), πθ =new policy, πθold = policy before the update.\nIts adaptability and proven effectiveness make PPO a corner-\nstone of modern reinforcement learning techniques for LLM\nalignment.\nB. Q-Learning and Off-Policy RL:\nQ-learning and other off-policy RL methods, though less\nprevalent compared to PPO, have shown promise when applied\nto large language models, particularly when offline datasets\nof human feedback are available. A notable approach in this\ndomain is Implicit Language Q-Learning (ILQL), introduced\nby Snell et al. [35]. ILQL is an offline RL algorithm that\nleverages Q-learning on static datasets containing state-action-\nreward tuples, such as dialogue responses annotated with\npreference scores. By integrating the utility-maximization ca-\npabilities of reinforcement learning with the stability afforded\nby supervised learning, ILQL constrains learned Q-values to\nremain close to the original behavior policy defined by the\ndataset. Empirical evaluations showed that ILQL is effective\nin optimizing specific objectives, like reducing toxic dialogue\noutputs. It significantly outperformed traditional supervised\nfine-tuning in these tasks. The ILQL training objective is\ndefined as:\nLILQL(θ) = LTD(θ) + α · Lconservatism(θ),\nwhere\nLTD(θ)\ndenotes\nthe\ntemporal\ndifference\nloss,\nLconservatism(θ)\nrepresents\nthe\nconservatism\nregularization\nterm, and α is a hyperparameter balancing these two\ncomponents.\nRecent research has explored off-policy Q-learning frame-\nworks to improve verifier models. These models assess the\nquality of reasoning steps generated by LLMs. VerifierQ [36]\nemploys offline Q-learning to train a verifier that improves\nguidance for multi-step reasoning processes. This method\ntreats the verifier as a value-based critic and evaluates the\noutputs from an LLM generator to refine its reasoning steps.\nThe VerifierQ loss function is defined as:\nLVerifierQ(θ) = LBellman(θ) + β · LCQL(θ),\nwhere LBellman(θ) is a modified Bellman error term, LCQL(θ)\nrepresents a conservative Q-learning regularization term, and\nβ is a hyperparameter controlling the balance between these\ntwo losses.\nThese developments illustrate that off-policy RL techniques,\ntraditionally used in discrete-action settings like games [38],\ncan effectively be adapted to language generation scenarios.\nSuch adaptations frequently incorporate conservative strategies\nlike Conservative Q-Learning (CQL) [39] to stabilize training\nand mitigate distributional shift. The general formulation for\nCQL is expressed as:\nLCQL(θ) = LBellman(θ) + α ·\n\u0000E(s,a)∼µ[Qθ(s, a)]\n−E(s,a)∼D[Qθ(s, a)]\n\u0001\nwhere µ is typically a uniform or policy-driven distribution\nover state-action pairs, D represents the dataset distribution,\nand the hyperparameter α regulates the strength of the con-\nservative regularization. These advances highlight the growing\nrole of off-policy methods in complementing traditional RL\napproaches for language models, particularly in scenarios that\ndemand nuanced and verifiable reasoning.\nC. Group Relative Policy Optimization\nGroup Relative Policy Optimization (GRPO) is an advanced\nreinforcement learning algorithm specifically designed to fine-\ntune large language models (LLMs). It places particular focus\non enhancing the models’ reasoning capabilities. Introduced\nby DeepSeek AI in their DeepSeekMath project [40], GRPO\novercomes certain limitations inherent to traditional methods\nlike Proximal Policy Optimization (PPO) [10]. Specifically,\nGRPO eliminates the need to maintain a separate value func-\ntion estimator. This change significantly reduces computational\noverhead and enhances training efficiency. Unlike PPO, which\nrelies on absolute reward signals, GRPO adopts a group-\nbased relative advantage estimation. For each prompt, multiple\ncandidate responses are generated, and their corresponding\nrewards are normalized within each group. This normalization\nmitigates the impact of noisy or sparse reward signals, making\nGRPO especially effective for complex reasoning tasks such as\nmathematical problem-solving. Formally, GRPO adapts PPO’s\nclipped surrogate objective by replacing the traditional advan-\ntage estimator with a group-normalized advantage, defined as\nfollows:\nLGRPO(θ) = E(s,{ai})∼πθold\n\"\n1\nG\nG\nX\ni=1\nmin\n\u0010\nri(θ) ˆAGR\ni ,\nclip(ri(θ), 1 −ϵ, 1 + ϵ) ˆAGR\ni\n\u0011#\nwhere ri(θ) represents the probability ratio, computed as\nri(θ) =\nπθ(ai|s)\nπθold(ai|s) and the group-normalized advantage ˆAGR\ni\nis\ncalculated by ˆAGR\ni\n= r(ai)−µ\nσ\nwhere µ and σ denote the mean\nand standard deviation of the rewards within the response\ngroup {ai}G\ni=1. GRPO has demonstrated strong empirical\nperformance, notably in the training of models like DeepSeek-\nR1 [34], showcasing its practical applicability and robustness\nfor advanced LLM alignment and reasoning tasks.\nIV. REINFORCEMENT LEARNING TECHNIQUES FOR LLMS\nThis section explores reinforcement learning (RL) tech-\nniques specifically developed to enhance large language mod-\nels (LLMs). Our discussion centers around two primary ob-\njectives. First, we examine methods designed to align LLM\noutputs closely with human values and preferences, including\nReinforcement Learning from Human Feedback (RLHF) [9],\nReinforcement Learning from AI Feedback (RLAIF) [11],\nConstitutional AI [12], and Direct Preference Optimization\n(DPO) [13]. Second, we focus on techniques that enhance the\nreasoning and problem-solving capabilities of LLMs, such as\nOutcome-Based Reinforcement Learning (OB-RL), Chain-of-\nThought Reward Optimization (CoT-RO), Verifier-Guided RL,\nDebate and Self-Play Reinforcement Learning, Hierarchical\nRL for tool-augmented reasoning, and Program-Synthesis RL\nfor code reasoning. Collectively, these methods illustrate the\nevolving and crucial role reinforcement learning plays in\nproducing more aligned, robust, and capable language models.\nA. Reinforcement learning from Human Feedback (RLHF)\nReinforcement Learning from Human Feedback (RLHF)\n[9] has emerged as the standard approach for aligning LLMs\nwith human preferences. The RLHF pipeline typically consists\nof three main stages: supervised fine-tuning, reward model\ntraining, and reinforcement learning optimization. The process\nbegins with supervised fine-tuning of a pre-trained LLM on a\ndataset of human-written demonstrations of desired behavior.\nThis stage adapts the model to the target task and provides\na starting point for further optimization. The supervised fine-\ntuning (SFT) model is trained to maximize the likelihood of\nproducing the desired output given the input prompt. This\ntraining objective is formalized as a loss function, defined as:\nLSFT(θ) = −E(x,y)∼DSFT [log pθ(y|x)]\nwhere θ represents the model parameters, x is the input\nprompt, y is the desired output, and DSFT is the dataset of\nhuman demonstrations. In the second stage, a reward model is\ntrained to predict human preferences between different model\noutputs. Human annotators are presented with a prompt and\ntwo possible responses, and they indicate which response they\nprefer. This preference data is used to train a reward model\nrϕ that assigns a scalar value to a given prompt-response pair.\nThe reward model rϕ is optimized to correctly reflect these\npreferences by minimizing the following loss:\nLRM(ϕ) = −E(x,yw,yl)∼Dpref [log σ(rϕ(x, yw) −rϕ(x, yl))]\nwhere ϕ represents the reward model parameters, yw is the\npreferred (winning) response, yl is the less preferred (los-\ning) response, and Dpref is the dataset of human preference\njudgments. In the final stage, the SFT model (now called the\npolicy model) is fine-tuned using reinforcement learning to\nmaximize the reward predicted by the reward model. This is\ntypically done using the Proximal Policy Optimization (PPO)\n[10] algorithm, which we discussed in the previous section.\nTo prevent the policy from deviating too far from the original\nSFT model, a Kullback-Leibler (KL) divergence penalty is\noften added to the objective. The KL divergence, if denoted as\nDKL(P ∥Q), quantifies the difference between two probability\ndistributions P and Q and is mathematically defined as:\nDKL(P ∥Q) =\nX\nx\nP(x) log\n\u0012P(x)\nQ(x)\n\u0013\nThe resulting loss function incorporates both the PPO objective\nand the KL penalty and is given by:\nLtotal(θ) = LPPO(θ) −βDKL(pθ(·|x)||pSFT(·|x))\nwhere β is a hyperparameter that controls the strength of\nthe KL penalty, and pSFT is the SFT model’s distribution.\nThis three-stage pipeline has proven to be a robust founda-\ntion for fine-tuning LLMs, balancing human alignment with\nstable policy updates. Recent work by Chakraborty et al.\n[41] enhanced the RLHF framework by introducing a bi-\nlevel optimization approach. In this formulation, the upper-\nlevel alignment objective (reward design) is parameterized\nby the optimal policy derived from the lower-level problem.\nThe lower-level optimization aims to maximize the reward by\nadjusting the policy. Subsequently, the upper-level optimiza-\ntion refines the reward model to align the resulting policy\nmore closely with human preferences. This nested structure\nexplicitly accounts for the downstream effects of reward design\non policy behavior. The bi-level formulation demonstrated\nimproved performance compared to methods such as Pebble\n[42] and PEBBLE+SURF [43]. Additionally, the sample com-\nplexity of bi-level approaches has been theoretically examined\nby Gaur et al. [44].\nB. Reinforcement learning from AI Feedback (RLAIF)\nAlthough Reinforcement Learning from Human Feedback\n(RLHF) [9] has demonstrated strong effectiveness for aligning\nlarge language models (LLMs), it faces significant scalability\nlimitations due to the substantial time and cost associated with\ngathering human annotations. Reinforcement Learning from\nAI Feedback (RLAIF) [11] provides an alternative solution\nto this bottleneck by employing AI-based evaluators instead\nof human annotators. The RLAIF pipeline closely mirrors\nRLHF but replaces the human preference collection stage\nwith AI-driven evaluations. Initially, the model undergoes\nsupervised fine-tuning using human-authored demonstrations.\nSubsequently, AI systems assess the generated outputs. These\nAI systems include general-purpose models (e.g., GPT-4 [45]),\nspecialized classifiers targeting toxicity, bias, or factual inac-\ncuracies, and ensembles of multiple specialized models. These\nAI evaluations produce preference scores or judgments, which\nare then used to train a reward model. This reward model\nlearns to predict the scores given by the AI evaluators, es-\nsentially mimicking how human preferences were modeled in\nRLHF. The final stage involves policy optimization, typically\nemploying Proximal Policy Optimization (PPO) [10] to refine\nthe model policy based on feedback from the AI-generated\nreward model.\nRLAIF offers several compelling advantages compared to\nRLHF. It significantly enhances scalability, as AI-generated\nfeedback can be produced in far greater volumes and at lower\ncosts than human annotations. Additionally, Lee et al. [46]\nshowed that AI evaluators typically provide more consistent\nassessments than human annotators, whose judgments can\nvary. Specialized models can also be fine-tuned to evalu-\nate specific aspects of model behavior, such as relevance,\nhelpfulness, or harmfulness. However, the RLAIF approach\nintroduces its own set of challenges. Primarily, AI evaluators\nmight not fully capture nuanced human values and preferences\nand might potentially diminish the feedback quality. Further-\nmore, Sharma et al. [19] demonstrated that these evaluators\nmay propagate biases inherent in their own training data or\ndesign choices. Lastly, using AI systems to align other AI\nmodels can create a recursive alignment issue. If the AI\nevaluators themselves are misaligned or biased, their flaws\nmay be inherited and potentially amplified by the models\nthey supervise. This can lead to feedback loops where biases\nare reinforced over successive training cycles, making them\nharder to identify and correct. Therefore, ensuring that these\nevaluators themselves remain faithfully aligned with human\nvalues is a complex and unresolved challenge.\nC. Constitutional AI\nConstitutional AI [12] represents a specialized approach\nwithin Reinforcement Learning from AI Feedback (RLAIF)\n[11], in which models are explicitly guided to critique and\nrevise their own outputs according to a predefined set of ethical\nprinciples, known as a “constitution”. The Constitutional AI\nprocess starts by establishing a clear set of principles or rules\nthat guide the model’s behavior. These principles cover aspects\nlike helpfulness, harmlessness, honesty, and respect for human\nautonomy. After an initial response is generated for a prompt,\nthe model engages in self-critique. It then evaluates its output\nagainst these constitutional principles and identifies potential\nviolations or areas that need improvement. Subsequently, the\nmodel revises its response based on this critique, often iterating\nmultiple times to progressively refine its alignment with the\ndefined constitution. The revised responses then serve as\npositive examples in reinforcement learning. They are incor-\nporated either through conventional Reinforcement Learning\nfrom Human Feedback (RLHF) [9] methods or by training\nspecialized reward models directly from these constitutional\nstandards.\nA critical component of Constitutional AI is the practice of\n“red-teaming”, proposed by Perez et al. [47]. In this practice,\nthe model is intentionally prompted to generate potentially\nharmful or problematic outputs. These outputs are systemati-\ncally critiqued and revised in alignment with the established\nconstitutional principles. Through this deliberate challenge-\nand-response process, potential failure modes of the model\nare proactively uncovered and mitigated. This significantly\nstrengthens the model’s robustness against adversarial inputs\nand aligns its behavior more closely with desired ethical\nguidelines.\nD. Direct Preference Optimization (DPO)\nDirect Preference Optimization (DPO) [13] is a recent\ndevelopment aimed at simplifying the Reinforcement Learning\nfrom Human Feedback (RLHF) pipeline by removing the\nnecessity for explicit reward modeling and reinforcement\nlearning. Rather than training a separate reward model, DPO\ndirectly optimizes the policy to align with human preferences.\nThe core insight behind DPO is that the optimal policy under\na given reward function can be directly expressed in terms of\na reference policy. This reference policy is typically obtained\nthrough supervised fine-tuning and combined with the under-\nlying reward function. In this formulation, the input prompt\nx represents the context or question to which the model must\nrespond, while the output y represents a candidate response to\nthat prompt. The relationship between these elements and the\nreward function is captured in the following equation:\np∗\nθ(y|x) ∝pref(y|x) exp(βr(x, y))\nwhere pref denotes the reference policy, r(x, y) represents the\nreward function, and β is a temperature parameter. By rear-\nranging this relationship, DPO expresses the reward function\nexplicitly in terms of the optimal and reference policies as\nfollows:\nr(x, y) = 1\nβ log p∗\nθ(y|x)\npref(y|x) + Z(x)\nHere, Z(x) is a normalization term dependent solely on x.\nLeveraging this relationship, DPO formulates a direct loss\nfunction optimized for aligning model outputs with human\npreference data which is given as:\nLDPO(θ) = −E(x,yw,yl)∼Dpref\n\"\nlog σ\n \nβ log pθ(yw|x)\npref(yw|x)\n−β log pθ(yl|x)\npref(yl|x)\n!#\nwhere yw and yl represent the preferred and less-preferred\nresponses, respectively.\nDPO offers several notable advantages over traditional\nRLHF [9] approaches. Since it eliminates the need for sep-\narate reward modeling and reinforcement learning stages, this\napproach offers improved computational efficiency. Therefore,\nit reduces the overall resource demands required for training.\nMoreover, Xu et al. [48] showed that DPO tends to exhibit\ngreater training stability compared to algorithms like PPO,\nwhich can be sensitive to hyperparameter settings. Neverthe-\nless, DPO is not without limitations. Due to its direct opti-\nmization nature, it may be less effective at exploring diverse\noutputs compared to traditional reinforcement learning-based\nmethods. Additionally, the quality of the resulting policy re-\nmains highly dependent on the accuracy and representativeness\nof the underlying human preference data.\nE. Outcome-Based Reinforcement Learning for Reasoning\n(OB-RL)\nUesato et al. [49] proposed Outcome-Based Reinforcement\nLearning for Reasoning (OB-RL). It is a sparse-reward frame-\nwork designed to boost a language model’s reasoning ability\nby focusing only on the correctness of the final answer. Unlike\nmethods that supervise every intermediate step, OB-RL re-\nwards the model only when it generates a correct final answer.\nIn the OB-RL setup, a prompt x is mapped to a full reasoning\ntrajectory τ = (y1, . . . , yT ) produced autoregressively by the\nmodel πθ. Only the terminal token (or a verifier’s binary\njudgement of the terminal answer) supplies the scalar reward\nR(τ), leaving the internal reasoning path unconstrained. This\nformulation mirrors goal-conditioned reinforcement learning\nin classical control, where the agent does not need to imitate\nhuman proofs token by token. Instead, it focuses on reaching\na verifiably correct final state. Formally, the learning objective\nis\nLOB-RL(θ) = −E(x,τ)∼πθ\n\u0002\nR(τ) log πθ(τ | x)\n\u0003\nwhich corresponds to the REINFORCE [28] gradient with\na sparse, outcome-level reward. In practice, R(τ) can be\nprovided by an automated verifier, such as a mathematical\nproof checker, program executor, or fact-checking model.\nThis approach allows for large-scale, low-cost feedback on\nreasoning tasks like theorem proving or code generation.\nOB-RL offers several advantages over process-supervised\nor preference-based methods. First, it does not require human\nannotation of intermediate reasoning steps. The learning signal\nis automatically generated based on success or failure, which\ngreatly improves scalability. Second, it encourages exploration\nof diverse reasoning paths. Any trajectory that yields a correct\nterminal answer receives the same positive reward, thereby\nfostering creativity and robustness. However, OB-RL inherits\nthe high-variance gradients typical of sparse-reward settings.\nIt may converge slowly without additional techniques such as\nvalue-function baselines, curriculum learning, or hybrid train-\ning that mixes outcome-level rewards with chain-of-thought\nimitation. Moreover, because only the final answer is verified,\nthe model can still produce opaque or convoluted rationales.\nF. Chain-of-Thought Reward Optimization (CoT-RO)\nLightman et al. [50] first proposed Chain-of-Thought Re-\nward Optimization (CoT-RO). It is a dense-reward framework\nthat strengthens a language model’s reasoning by scoring each\nintermediate step in its chain of thought, not just the final\nanswer. Given a prompt x, the policy πθ autoregressively\nproduces a reasoning trajectory τ = (y1, . . . , yT ). After every\ntoken or logical step yt, a lightweight evaluator such as a\nsymbolic verifier, unit-test harness, or learned critic assigns\nan immediate reward rt = r(x, y≤t). The objective is to\nmaximise the discounted return over the entire trajectory.\nMathematically, this objective is captured by the following\nloss function:\nLCoT-RO(θ) = −E(x,τ)∼πθ\nh T\nX\nt=1\nγ t−1rt log πθ(yt | x, y<t)\ni\nwhere γ ∈(0, 1] balances early versus late rewards. Because\nfeedback is provided at every step, CoT-RO supplies a rich\nlearning signal. This encourages the model to build logically\nsound, self-consistent chains instead of leaping directly to an\nanswer. In practice, rewards can check local validity (e.g.,\nalgebraic correctness), global coherence (e.g., absence of\ncontradictions), or stylistic constraints, allowing fine-grained\ncontrol over the reasoning process.\nCoT-RO brings several benefits relative to outcome-only\nmethods. The dense reward accelerates convergence because\nthe model does not have to wait until the end of a long\ntrajectory to learn whether it is on the right track. It also\npinpoints specific failure steps, making it easier to create\ntargeted curriculum schedules and corrections. Furthermore,\nby rewarding transparency at every stage, CoT-RO produces\nexplanations that are easier to inspect and debug. The chief\ndrawbacks are computational because step-level evaluation can\nbe expensive. Additionally, dense feedback may encourage\nverbose yet shallow reasoning unless it is tempered with\nbrevity or entropy penalties.\nG. Verifier-Guided Reinforcement Learning\nVerifier-Guided Reinforcement Learning (V-RL) augments\na language model’s policy with an external verifier. This\nverifier continuously evaluates candidate outputs and supplies\nthe reward signal. Given a prompt x, the policy πθ generates\neither a full chain of thought τ = (y1, . . . , yT ) or a single-\nshot answer y. A separate model Vϕ is trained to judge\nlogical validity, factual accuracy, or task-specific success. It\nassigns a scalar reward R = Vϕ(x, τ). The training process\nuses a policy-gradient loss function to update the model.\nMathematically, it is expressed as:\nLV-RL(θ) = −E(x,τ)∼πθ\n\u0002\nVϕ(x, τ) log πθ(τ | x)\n\u0003\n,\nIn an actor–critic variant, the verifier’s score is treated as the\ntarget value for a learned critic that reduces variance in the\ngradient estimate. Because Vϕ can operate at any level of\ngranularity, V-RL can adaptively use dense step-wise feedback\nto evaluate each intermediate state. It can also rely on sparse\noutcome feedback by judging only the final answer. The\nverifier may be a frozen specialist model, an ensemble of\nheuristics, or a parameter-sharing sibling that co-evolves with\nthe policy.\nV-RL combines several desirable properties. First, it de-\ncouples generation from evaluation, allowing the policy to\nexplore creative reasoning paths. At the same time, the verifier\nenforces correctness, thereby lowering the risk of reward\nhacking seen in hand-designed metrics. Second, the verifier\ncan be updated or replaced without retraining the policy from\nscratch, enabling rapid iteration on new evaluation criteria.\nFinally, V-RL supplies richer signals than binary correctness,\nsuch as graded scores for partial progress. This typically\nleads to faster convergence compared to purely outcome-based\nmethods. Nonetheless, its effectiveness hinges on verifier\nquality. A weak or biased verifier may mis-reward spurious\nsolutions, while an overly strict one can stifle exploration.\nComputational cost is another concern, as every policy sample\nmust be scored by Vϕ.\nH. Debate and Self-Play Reinforcement Learning\nDebate and Self-Play Reinforcement Learning (DSP-RL)\nadvances\nverifier-guided\nconcepts\nby\ninvolving\nmultiple\nagents. These agents either compete or collaborate to un-\ncover errors in each other’s arguments before producing a\nfinal answer. In the debate variant, there are two policies:\nproponent and opponent. Both of these policies alternately\npresent concise arguments about a question. A judge, either\na human or an automated scoring model, determines which\nside provided the most truthful and valuable information. A\nwin provides a positive reward to the debater who can be\npersuasive while remaining truthful. This encourages both\nagents to present evidence, challenge flawed reasoning, and\nreach verifiable conclusions. In self-play fine-tuning, a single\nmodel is iteratively distilled into stronger versions by playing\nboth roles. It generates a candidate answer, critiques it from the\nperspective of an adversary, then revises the original response,\nreceiving a reward from an internal judge. Training proceeds\nwith policy-gradient updates or KL-constrained objectives that\nencourage each new generation to outperform its predecessor\nwhile remaining close enough for stable learning.\nDSP-RL carries distinct strengths. First, the adversarial\nsetting incentivises models to reveal hidden flaws, reducing\nhallucinations and improving factual accuracy without large\nhuman-labelled datasets. Second, self-play naturally creates\na continually evolving curriculum. As agents improve, they\ngenerate increasingly difficult counter-arguments, steadily rais-\ning the bar for reasoning depth and robustness. However,\nthe approach also introduces challenges. Reward hacking can\nhappen if debaters learn to exploit the judge’s weaknesses\ninstead of seeking objective truth. Additionally, training be-\ncomes computationally expensive as each prompt involves\nmultiple turns of interaction among several models. Moreover,\nensuring that the judging mechanism itself remains unbiased\nand aligned is critical; otherwise the system can reinforce\npersuasive but incorrect rhetoric.\nI. Hierarchical RL for Tool-Augmented Reasoning\nHierarchical Reinforcement Learning for Tool-Augmented\nReasoning (HRL-TAR) provides a two-tier control structure\nfor language models. It has a high-level policy that decides\nwhen and which external tool to use, like a calculator, code\ninterpreter, or web search API. It also includes a low-level\npolicy that generates the token-level arguments or natural-\nlanguage rationale needed to call the chosen tool and integrate\nits result into the ongoing chain of thought. So, given a\nprompt x, the high-level policy πhi\nθ produces a sequence of\nsub-goals or tool actions g1, . . . , gK. Conditioned on each gk,\nthe low-level policy πlo\nθ yields a sub-trajectory τk comprising\nthe concrete API call and any accompanying prose. After\nexecuting the tool and receiving an observation ok, control\nreturns to the high-level policy for the next decision. During\ntraining, the model learns to maximize task performance at\nboth levels, combining high-level goal selection and low-\nlevel execution into a single optimization objective. The loss\nfunction capturing this hierarchical reinforcement learning\nframework is expressed as follows:\nLHRL-TAR(θ) = −Ex,g1:K,τ1:K\n\" K\nX\nk=1\nRhi(gk, ok)\n+\nX\nt∈τk\nγ t−1Rlo(yt, ok)\n#\nwhere Rhi measures task-level progress, such as the cor-\nrectness of the final answer or the efficiency of tool use.\nMeanwhile Rlo provides rewards for well-formed API calls\nand the accurate integration of tool outputs into the overall\nresponse. In practice, off-policy value learning (for πhi) is\npaired with on-policy token-level updates (for πlo) to yield\nsample-efficient training across long horizons.\nHRL-TAR confers several benefits over flat token-level\noptimisation. By separating planning (tool selection) from\nexecution (argument generation), it achieves superior credit\nassignment on multi-step tasks and reduces the search space\nfaced by the low-level policy. The hierarchical design also\nenables compositional generalisation. Once a low-level skill\nfor a specific tool is learned, the high-level policy can reuse\nit across domains, fostering rapid adaptation. Furthermore,\nreward signals can be customized for each level. Dense\nfunctional tests can be used for tool calls, while sparse end-\ntask metrics can measure overall success, leading to faster\nconvergence compared to outcome-only schemes. Nonetheless,\nHRL-TAR introduces some challenges. Both policies must be\ncarefully synchronized. If the high-level policy makes poor\ndecisions, it can deprive the low-level learner of valuable\nlearning signals. Training stability further depends on accurate\nsimulators or verifiers to generate reliable hierarchical rewards.\nMis-specified tool APIs can lead to unexpected solutions or\npotential exploitation of the system.\nJ. Program-Synthesis RL for Code Reasoning\nProgram-Synthesis Reinforcement Learning (PS-RL) takes\nLLM training into executable code tasks. It rewards the model\nfor generating programs that pass hidden unit tests or static\nanalyzers instead of simply mimicking reference snippets\ntoken by token. So, given a natural-language specification x\n(e.g., a leet-code style problem), the policy πθ autoregressively\nemits a candidate program c. The program is then compiled\nand executed against a test suite, returning a scalar reward\nR(c) = pass_rate(c) ∈[0, 1]. The loss adopts a standard\npolicy-gradient form and is given as follows:\nLPS-RL(θ) = −E(x,c)∼πθ\n\u0002\nR(c) log πθ(c | x)\n\u0003\nIt can optionally include an entropy bonus to encourage explo-\nration. Some frameworks adopt a curriculum of increasingly\ndifficult programs, such as filling function bodies, then loops,\nand finally complete solutions. This helps mitigate sparse\nrewards and the challenges of long credit-assignment chains.\nOthers maintain an online buffer of failing cases to focus\nlearning on tricky edge conditions.\nPS-RL offers several advantages over purely supervised\nfine-tuning. First, the reward signal is functional. This ap-\nproach means that any syntactically diverse program passing\nall tests is considered correct. It allows the model to create\nnovel implementations beyond those seen in the training set.\nSecond, automatic evaluation via compilers and unit tests\nscales cheaply to millions of prompts thereby bypassing the\nneed for human labels. Third, dense metrics such as static-\nquality scores, code smells or complexity reductions can be\ncombined with pass-rate to guide style and efficiency.\nThe framework also has drawbacks. Reward sparsity re-\nmains a challenge. Many candidate programs fail to compile or\npass zero tests. This produces high-variance gradients. Tech-\nniques such as value baselines, mutation-guided exploration, or\nhierarchical decomposition into smaller completion subtasks\nare often required. Execution-based evaluation can be slow and\nbrittle, especially for resource-intensive benchmarks or non-\ndeterministic environments. Finally, because tests are finite,\nmodels may learn to exploit loopholes and overfit to the exact\ntest suite or game static metrics. Therefore, continual red-team\ntests and broader benchmarks are essential for ensuring robust\ngeneralization.\nK. Other RL techniques for LLMs\nBeyond the alignment and reasoning methods already dis-\ncussed, a growing body of work [51] explores offline or batch\nreinforcement-learning algorithms for language models. These\napproaches, including Implicit Language Q-Learning [35],\nConservative Q-Learning [39], and other value-based variants,\nrely entirely on pre-collected logs of human interactions or\nsynthetic conversations. This design helps avoid the safety\nconcerns and high costs associated with live environment\nrollouts. Constrained or KL-regularised objectives are often\nlayered on top of these offline methods to prevent the policy\nfrom drifting toward distributional outliers. Another branch\ninvestigates safe or constrained RL, where the learning objec-\ntive incorporates explicit penalty terms for toxicity, privacy\nleakage, or policy-side effects. In a similar spirit, multi-\nobjective reinforcement learning formulations balance mul-\ntiple reward signals such as helpfulness, harmlessness, and\nefficiency. These objectives are combined using scalarisation\nor Pareto front optimisation, allowing practitioners to adjust\nalignment trade-offs after training.\nTo support advanced reasoning and tool use, researchers\nsuch as Ye at al. [52] are exploring planning-guided rein-\nforcement learning approaches. These methods involve un-\nrolling a symbolic or differentiable planner over candidate\nchains of thought and scoring entire action trees instead of\njust linear sequences. Complementary research on retriever-\npolicy reinforcement learning by Li et al. [53] focuses on\ntraining models to determine when to retrieve external doc-\numents or code snippets during reasoning. These information-\nseeking actions are rewarded if they lead to improved answer\naccuracy. Curriculum-based reinforcement learning systems,\nsuch as DeepSeek-Prover-V2 [54], have demonstrated strong\nperformance on tasks like mathematical theorem proving and\ncode generation. In these systems, task difficulty increases\nprogressively as the model succeeds at earlier stages, enabling\nmore effective learning over complex reasoning benchmarks.\nFinally, parameter-efficient methods such as LoRA-PPO [55]\nand adapter-based Q-learning [56] make reinforcement learn-\ning fine-tuning feasible on consumer-grade hardware. These\napproaches update only small, task-specific matrices, signif-\nicantly reducing computational costs and expanding access\nto reinforcement-tuned language models. Collectively, these\nemerging techniques illustrate the breadth of the RL toolbox\nand its potential to further refine language-model capability,\nsafety, and efficiency.\nV. APPLICATIONS OF RL FOR LLMS\nThis section maps the landscape of reinforcement-learning\napplications that advance large language models along two\naxes of alignment and capability. We begin with Instruc-\ntion Following, where RL helps tighten adherence to user\ndirectives. Next is Code Generation, where outcome-based\nrewards guide the model to produce syntactically correct\nand test-passing programs. Then comes Ethical Alignment,\nwhich constrains harmful or biased outputs. After that, Tool\nUse explores hierarchical and retrieval-aware policies. These\npolicies determine when and how to call external resources. Fi-\nnally, Reasoning Capabilities details dense and sparse-reward\nschemes that cultivate transparent, step-by-step problem solv-\ning.\nA. Instruction following\nLarge-scale instruction tuning now pairs supervised demon-\nstrations with reinforcement-based refinements and therefore\nlifts models from “pattern parrots” to cooperative assistants.\nOpenAI’s InstructGPT [9] first showed that RLHF could\nturn a GPT-3 policy into one that follows user directives\nfaithfully and safely. The same three-stage recipe underlies\ntoday’s GPT-4o [57]. It adds a stronger KL constraint and\nsafety-oriented rewards to preserve adherence on intricate,\nmulti-step prompts while reducing refusal errors. Later, Lu\net al. [58] showed that Google’s Gemini 2.0 Flash follows a\nsimilar path, but augments the reward model with schema-\nvalidators so the system can emit perfectly formatted JSON\nand tables on demand. Anthropic’s Claude-3.5 Sonnet [59]\nlayers Constitutional-AI penalties on top of RLHF to refuse\ndisallowed requests without losing helpfulness, while Meta’s\nLlama-3 Instruct [2] variants combine LoRA-PPO updates\nwith extensive red-teaming to boost compliance across dozens\nof languages. Collectively, these pipelines demonstrate that\nmodern instruction-first models rely on reinforcement learning\nto align with user intents. This includes methods such as full\nPPO fine-tuning, Direct Preference Optimization, and KL-\nregularized adapters, all of which help ensure tight format\nadherence and reduce hallucinations.\nResearch is now pushing instruction-following further with\ncheaper or richer feedback. Direct Preference Optimization\n[13] replaces the reward model entirely, aligning policies\nthrough a simple classification loss yet matching PPO-grade\nquality on instruction benchmarks. Self-Rewarding LLMs [60]\ncut annotation costs by letting a frozen copy of the model act\nas judge during preference collection, then fine-tune on those\nauto-labelled pairs for additional gains in directive accuracy.\nOpenAI’s o1 model [61] refines its own chain-of-thought via\noutcome-based RL. It produces step-by-step answers that out-\nperform earlier GPT-4 [45] variants on complex, instruction-\nheavy tasks. Finally, retrieval-aware instruction tuning trains\na policy to determine when to consult external documents.\nThis approach rewards citation-backed, grounded responses\nand underpins production systems like Gemini Flash and GPT-\n4o [57] in search applications. Each new flagship model,\nsuch as GPT-4o [57], Gemini 2.0 Flash [62], Llama-3 In-\nstruct [2], and Claude-3.5 [59], relies on increasingly refined\nreinforcement learning signals to follow directions, maintain\nproper formatting, and provide well-justified responses with\nexceptional reliability.\nB. Code Generation\nReinforcement learning now sits at the core of state-of-the-\nart code models, turning raw text generators into dependable\nsoftware engineers. OpenAI first demonstrated this approach\nwith Codex [63]. In that work, reinforcement learning from\nhuman feedback (RLHF) was used to improve code generation\nbased on how often the generated programs passed hidden unit\ntests. This training method more than doubled the model’s\naccuracy on the HumanEval [64] benchmark compared to\nusing supervised learning alone. The same strategy, combined\nwith stricter KL constraints and security-focused penalties,\nnow powers the Code Interpreter mode in GPT-4o. This\nversion achieves higher compile success rates and enforces\ntighter resource limits than earlier GPT-4 releases. DeepMind’s\nAlphaCode [65] pushes the code generation paradigm a step\nfurther. It uses an evolutionary self-play loop to filter thou-\nsands of candidate programs. RL fine-tunes the model on\nthe top-performing programs, enabling it to rank in the top\nhalf of human participants on Codeforces-style programming\nchallenges. Google’s Gemini 2.0 Flash [62] layers schema-\nvalidation rewards atop functional tests so that JSON or proto-\nbuf outputs remain parsable while still passing runtime checks.\nAnthropic’s Claude-3.5 Sonnet [59] applies Constitutional RL\nto penalise unsafe APIs and data-leak patterns during genera-\ntion, yielding code that meets enterprise security baselines out\nof the box. Meanwhile, Meta’s Llama-3 Instruct [2] models\nuse lightweight LoRA-PPO updates plus massive red-teaming\nto align open-source code generation with community style\nguides and efficiency hints.\nNew feedback channels are expanding what “good code”\nmeans. GitHub Copilot [66] now scores suggestions with a\nstatic-analysis critic that flags complexity and vulnerability\nsinks. Those signals feed an offline Conservative Q-Learning\npass that biases future completions toward cleaner patterns.\nSelf-rewarding frameworks [67] let a frozen copy of the model\njudge its own snippets, cutting annotation cost while still\ndriving gains on compilation-based benchmarks. OpenAI’s o1\n[61] and o3 [68] reasoning models combine outcome-based RL\nwith chain-of-thought distillation to solve multi-file refactor\ntasks that stumped GPT-4 [45]. Liu et al. [69] extended these\nideas to optimise energy usage or memory footprint directly\nvia simulator rewards. Collectively, these advances show that\nmodern code language models such as GPT-4o [57], Gemini\nFlash [62], Claude-3.5 [59], and Llama-3 [2] rely on a broad\nrange of reinforcement learning signals. These signals go\nbeyond functional correctness to also reward efficiency, read-\nability, and security, moving automated programming closer\nto production-ready reliability.\nC. Ethical Alignment\nReinforcement-learning pipelines now anchor the safety\nstrategies of the newest frontier models. For example, GPT-\n4o [57] was released with a RLHF setup enhanced by a\nsafety-focused head that penalizes behaviors like persuasion,\nmisinformation, and privacy leakage. Its training included\nred-teaming rounds and KL-regularized updates to maintain\ntoxicity at a “medium-risk or lower” level across various\ndomains. Anthropic’s Claude 3.5 Sonnet [70] extends the\nConstitutional AI approach by using a set of principles inspired\nby human-rights charters to guide a reward model. This reward\nmodel is optimized using PPO, enabling the system to refuse\n95% of harmful jailbreak attempts while maintaining a high\nlevel of helpfulness. Google’s Gemini 2.0 Flash [62] applies\nRLHF alongside schema-checking and post-deployment live\nalignment audits to reduce bias and misinformation across\nlanguages. Meanwhile, Meta’s Llama-3 [2] series used LoRA-\nPPO fine-tuning and broad community red-teaming to achieve\nlower toxicity without compromising fluency. Together, these\nsystems reflect a convergence on multi-stage reinforcement\nlearning pipelines. They combine supervised instruction tun-\ning, preference-based reward models, and safety-driven penal-\nties to embed ethical norms directly into model behavior.\nResearch is rapidly refining those ingredients. Automatic\nred-teaming [47] frameworks generate adversarial prompts that\nexpose loopholes, then re-optimise the policy with negative\nrewards to close them. SafeDPO [71] folds safety regularisa-\ntion into a single-stage, direct-preference loss that rivals PPO\nwhile slashing compute and hyperparameter tuning overhead.\nOffline value-based methods such as KTO-S [72] use con-\nservative Q-learning on large toxic corpora to achieve 99%\ntoxicity reduction in low-resource languages. Finally, verifier-\nguided approaches allow a fixed critic to evaluate bias, factual\naccuracy, and manipulation risks during policy updates. This\ntechnique was explored in OpenAI’s internal o1/o3 research\ntrack and in successor models to Bard [73], aiming to enforce\ngreater transparency and epistemic humility. These advances\nshow that reinforcement learning is no longer just a basic\nalignment method. It is now a powerful tool for building\nfairness, honesty, and safety into the most advanced language\nmodels.\nD. Tool Use\nReinforcement learning now powers agentic language mod-\nels that can make decisions about when and how to use\nexternal tools. These models go beyond static responses and\ninteract with APIs, web browsers, or local runtimes as part\nof their reasoning process. OpenAI’s GPT-4o [57] trains a\nreward head using thousands of developer-provided function-\ncalling traces. The model is then fine-tuned with reinforcement\nlearning from human feedback to select the correct tool\nschema and arguments. This process significantly improved\nperformance on the Function-Calling leaderboard. Google’s\nGemini 2.0 Flash [62] follows a similar reinforcement learning\nrecipe but introduces a native “action-capabilities” layer that\ngoverns tool use. Here, the RL rewards are designed to\nfavour tool sequences that minimise latency while maintaining\nfactual accuracy. This integrated approach enables the model\nto achieve competitive scores on tool-augmented question-\nanswering benchmarks. Anthropic’s Claude-3.5 Sonnet [59],\nreleased with a “computer-use” beta, uses policy-gradient\nupdates to enable interactive capabilities. The model can\nnavigate GUIs, execute shell commands, and edit files within\nan isolated virtual machine. This allows it to solve 64%\nof agentic coding tasks that earlier Claude versions were\nunable to complete. Meta’s Llama-3 Instruct [2] series applies\nlightweight LoRA-PPO fine-tuning to open-source models.\nThis training helps the models learn safe API usage and\ndatabase querying practices. As a result, community-built\nagents can retrieve live information like news or stock prices\nwithout hallucinating endpoints. Across these systems, red-\nteam stress tests and security-focused penalties are integrated\ninto the reward signal. This helps prevent prompt injection and\nunauthorized file access, ensuring that greater tool autonomy\ndoes not compromise safety guarantees.\nResearch is rapidly extending this capability set. Hier-\narchical reinforcement learning architectures [74] separate\ndecision-making into two levels. A high-level planner selects\nwhich tool to invoke, while a low-level decoder generates\nthe precise API call. Recent studies [75] show that these\ntwo-tier policies outperform flat baselines on multi-API math\nproblems. Offline Conservative Q-learning [39] pipelines train\ntool-selection critics entirely from logs. This reduces costly\nonline exploration while still discovering novel tool chains\nfor complex analytics workloads. Self-rewarding frameworks\n[67], which were tested in OpenAI’s o1/o3 and Gemini Flash\nmodels, use a frozen copy of the model to evaluate whether a\ntool invocation contributed meaningfully to task completion.\nThis approach reduced the need for human auditing by half.\nLooking ahead, the convergence of richer simulators, schema-\naware rewards, and parameter-efficient fine-tuning points to a\nshift in how models handle tools. Future systems will treat\ntool orchestration as a core skill, not a scripted add-on. This\nevolution will enable more reliable, secure, and context-aware\nAI assistants.\nE. Reasoning Capabilities\nRecent research has shifted from sheer parameter scaling\nto purpose-built reasoning models trained almost entirely with\nreinforcement signals. OpenAI launched this trend with o1\n[61], a 40-b model that learns step-by-step proofs through\noutcome-based RL. Following the release of o1, OpenAI\nintroduced o3 [68], a more advanced reasoning model trained\nwith large-scale reinforcement learning on chains of thought.\nThis approach enabled o3 to outperform its predecessor on\ncomplex tasks, including coding, mathematics, and science.\nContinuing this trend, DeepSeek-R1-Zero [34] was trained\nentirely through large-scale reinforcement learning without\nany supervised fine-tuning. Building on this, DeepSeek-R1\n[34] incorporated supervised fine-tuning as a foundation before\nRL. It achieved o1-level performance on math and logic\ntasks while cutting overall training costs by half. Qwen’s\nQwQ-32B [76] and Moonshot’s Kimi k1.5 [77] apply the\nsame reinforcement learning strategy at smaller scales. Both\ncombine RLHF with self-consistency rewards to close much\nof the gap to GPT-4 [45] on GSM-Hard and MATH, all\nwithout relying on proprietary data. Google’s experimental\nGemini 2.5 Pro [78] incorporates a planning head trained\nwith reinforcement learning to determine when to branch\nsub-goals. This design improves science-reasoning accuracy\nbeyond Claude 3 Opus [79]. Meanwhile, Anthropic’s Claude-\n3.5 Sonnet [59] combines Constitutional AI penalties with a\nPPO-trained chain-of-thought policy, achieving state-of-the-art\nresults on ARC-Challenge and GPQA.\nAlongside these flagship releases, a wave of open-literature\nalgorithms targets specific reasoning pain points. RL with\nVerifiable Rewards (RLVR) [15] shows that rewarding only\nproofs accepted by a symbolic checker doubles math accuracy\nwith just one verified trajectory per problem. VerifierQ [36]\ntrains a Q-learning critic offline to evaluate the model’s own\nreasoning steps. This approach removes hallucinated or invalid\nsteps without requiring any online rollouts. Outcome-reward\nmaximizers such as OREAL [14] use entropy bonuses and\ncurriculum schedules to make sparse-reward training more\nstable. With these techniques, a 7 billion-parameter model\nreaches 94 percent pass-@1 accuracy on the MATH-500\nbenchmark [80], matching much larger systems. DeepSeek’s\nR1 [34] showed that such RL-only models can be distilled\ninto dense into smaller, efficient versions without signifi-\ncant loss in reasoning capabilities. These distilled models,\nincluding DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-\nDistill-Llama-70B, maintain strong performance on reasoning\nbenchmarks. They perform well on MATH-500 [80] and\nAIME 2024 [81], enabling advanced capabilities on more\naffordable hardware. Together, these advances mark a clear\nshift in LLM development. The most advanced models of 2025\nsuch as GPT-o3, Gemini 2.5, Claude 3.5, Llama-3.3-Reasoner,\nDeepSeek-R1, QwQ-32B, and Kimi k1.5, no longer rely solely\non scale. Instead, they gain much of their strength from\nreinforcement learning curricula. These curricula are designed\nto reward transparent and verifiable chains of thought.\nF. Domain-Specific Applications\nBeyond the general applications discussed above, RL has\nbeen applied to enhance LLMs for specific domains:\n1) Healthcare: Reinforcement learning has enabled large\nlanguage models to become safer and more effective tools\nin healthcare settings by aligning their outputs with estab-\nlished medical guidelines and ethical principles. Models like\nGoogle’s Med-PaLM 2 [82] and GPT-4o [57] use reinforce-\nment learning to improve the accuracy of their medical advice.\nThey are also trained to highlight uncertainty and consistently\nrecommend consulting a medical professional. This approach\nreduces risks associated with misinformation, ensuring that\nmedical interactions remain responsible and aligned with\nclinical standards. Additionally, reinforcement learning has\nhelped models like Anthropic’s Claude-3 [79] series develop\nmore appropriate bedside manners. This training enables the\nmodel to communicate with empathy and adapt its responses\nto the emotional and contextual needs of patient interac-\ntions. Furthermore, reinforcement learning has significantly\nimproved clinical decision-support applications by aligning\nmodel recommendations with evidence-based medicine. This\nhelps ensure that outputs remain consistent with current clin-\nical guidelines, enhancing reliability and trust in high-stakes\ndecisions.\n2) Education: In educational contexts, RL has greatly en-\nhanced the effectiveness of language models as personalized\ntutors and learning companions. Advanced models such as\nOpenAI’s GPT-4o [57] and Google’s Gemini 2.0 Flash [62]\nuse reinforcement learning to personalize explanations based\non the learner’s level of understanding. They dynamically\nadjust the complexity and amount of detail in their responses.\nThis helps optimize both comprehension and long-term reten-\ntion. Furthermore, reinforcement learning encourages models\nto adopt a Socratic teaching style. So, instead of giving direct\nanswers, the model guides students through discovery and\ncritical thinking. Models like Anthropic Claude-3.5 [59] and\nDeepSeek [34] use reward functions trained with reinforce-\nment learning to promote interactive questioning techniques.\nThis approach helps deepen student engagement and enhance\nconceptual understanding. Additionally, reinforcement learn-\ning greatly enhances the quality and relevance of feedback on\nstudent assignments. It trains models to deliver constructive,\nspecific, and actionable suggestions that support continuous\nlearning and improvement.\n3) Legal and Financial Services: Within legal and financial\nservice domains, RL has been extensively applied to enhance\nlanguage models’ adherence to stringent compliance and accu-\nracy requirements. Models such as GPT-4o [57], Gemini 2.0\nFlash [62], and Claude-3.5 [59] have been fine-tuned using\nreinforcement learning methods. A recent case study by Sina\net al. [83] on tax-preparation software shows that even a\nsingle misclassification can trigger costly regulatory penalties,\nunderscoring the need for RL-driven safeguards in high-\nstakes settings. The RL approaches help ensure the models\nconsistently align with complex regulatory requirements and\nlegal norms. As a result, the risk of generating non-compliant\nor misleading content is significantly reduced. Moreover,\nreinforcement learning helps these models deliver balanced\nand transparent assessments of risks and uncertainties. This\ncapability is essential for supporting sound decision-making\nin financial advisory settings. For instance, Claude-3.5 [59]\nuses Constitutional AI frameworks combined with targeted\nreinforcement learning rewards to enforce strict disclosure\nnorms. This approach helps ensure that the model includes\nnecessary disclaimers and avoids inadvertent misrepresenta-\ntions. Consequently, reinforcement learning ensures that LLMs\noperating in sensitive sectors like legal and finance reliably up-\nhold ethical and professional standards, maintaining accuracy,\ntransparency, and trustworthiness.\nVI. COMPARATIVE ANALYSIS AND TAXONOMIES\nThis section presents a comprehensive comparative analysis\nof reinforcement learning techniques applied to large language\nmodels. It also introduces a taxonomy that highlights how\nthese methods improve alignment and reasoning capabilities.\nThe rapid evolution of this field has produced a diverse\narray of approaches, each with distinct mechanisms, strengths,\nand limitations. A systematic comparison is essential for\nunderstanding their relative merits and guiding future research\ndirections in this dynamic landscape.\nWe can categorize RL techniques for LLMs along several\nkey dimensions, drawing inspiration from recent comprehen-\nsive surveys [84], [85]. These dimensions include the nature of\nthe reward model, the type of feedback utilized, the underlying\nRL algorithm, and the optimization strategy. The primary axes\nof this taxonomy are:\n• Reward Model Strategy: This dimension separates\nmethods based on how the reward is defined. Some\nuse an explicit reward model (RM), which is a separate\nmodel trained to predict human preferences. This is\ntypical in traditional RLHF. Others rely on an implicit\nreward model, where the reward is built directly into the\npolicy’s optimization objective, as in DPO [13]. Further\ndistinctions depend on the granularity and form of the\nreward signal. Rewards may be applied at the response\nlevel or token level, and may take the form of pointwise\nscores or preference probabilities.\n• Feedback Mechanism: This pertains to the source and\nnature of the feedback signal. The signal can come from\nhumans, as in RLHF [9], or from AI models, as in RLAIF\n[11], [12]. Feedback formats vary and include pairwise\ncomparisons, listwise rankings, or binary signals, such as\nthose used in KTO [86].\n• Reinforcement Learning Paradigm: This includes the\nuse of a reference model, which is common in PPO-\nbased RLHF to avoid policy collapse. Some methods\ninstead adopt reference-free RL. It also distinguishes\nbetween on-policy algorithms, like PPO [10], and off-\npolicy approaches. Other important aspects are length\ncontrol during generation and the choice of divergence\nmeasure for regularization, such as KL divergence.\n• Optimization Approach: This dimension differentiates\nbetween online or iterative optimization and offline or\nnon-iterative optimization. In online optimization, the\npolicy is updated continuously with new feedback. In\noffline settings, learning happens from a fixed dataset\nof preferences. It also considers whether supervised fine-\ntuning (SFT) and alignment are merged into a single stage\nor kept as separate stages. For example, ORPO (Odds\nRatio Preference Optimization) [87] merges these stages.\nTable I presents a detailed comparative analysis of promi-\nnent RL techniques for LLMs based on these taxonomic\ndimensions. This table serves as a foundational reference point\nfor understanding the diverse landscape of RL approaches in\nLLM alignment and enhancement.\nExamining Table I in detail reveals several important pat-\nterns and distinctions among RL techniques for LLMs. The\ntable categorizes 13 prominent methods across 12 dimensions,\nproviding a comprehensive view of the design choices in each\napproach. Traditional RLHF implementations, like Instruct-\nGPT [9] and Anthropic’s approach [88], use explicit reward\nmodels trained on human preferences. These models produce\npointwise scores at the response level based on pairwise\ncomparison data. These methods employ reference models\nwith KL divergence regularization to prevent policy collapse\nduring training. In contrast, newer methods like DPO [13], IPO\n(Identity Preference Optimization) [89], and KTO (Kahneman-\nTversky Optimization) [86] employ implicit reward modeling.\nHere, the reward function is directly incorporated into the\npolicy optimization objective. This eliminates the need for\na separate reward model, potentially simplifying the train-\ning pipeline. ORPO [87], RRHF (Rank Responses to Align\nLanguage Models with Human Feedback) [90], and PRO\n(Preference Ranking Optimization) [91] take this a step further\nby merging the SFT and alignment stages, creating a more\nstreamlined training process.\nThe table also highlights the emergence of more advanced\napproaches such as DeepSeek-R1 [34]. This method uses\na mixed feedback strategy, drawing signals from multiple\nsources. It also integrates length control during generation,\na capability that most other methods do not offer. RLVR\n[15] stands out for its step-wise reward modeling approach.\nIt uses verifier-based binary feedback to enhance reasoning\ncapabilities. This represents a shift toward more granular\nreinforcement signals that can target specific aspects of model\nbehavior, particularly reasoning.\nAs discussed previously, RLHF, which was introduced by\nOpenAI [9] and later adopted by Anthropic [88], relies on\nthe PPO algorithm as its underlying reinforcement learning\nmethod. The PPO algorithm in RLHF aims to maximize the\nexpected reward from the RM while regularizing the policy\nupdate with a KL divergence term against the SFT model. This\nregularization is crucial for maintaining generation quality and\npreventing catastrophic forgetting or policy collapse. Instruct-\nGPT [9] demonstrated significant improvements in following\ninstructions and reducing harmful outputs compared to its base\nmodel, GPT-3 [1]. Their implementation used a 6B parameter\nreward model initialized from the SFT model, and human\nlabelers provided comparisons on a dataset of approximately\n33,000 prompts. The PPO fine-tuning employed a KL per-\ntoken penalty of β = 0.2 to balance reward maximization with\npolicy stability. Similarly, Anthropic’s early work on helpful\nand harmless models employed PPO with a KL penalty against\nan initial policy. They used preference data collected from\nhuman contractors.\nNumerical evaluations of RLHF have shown impressive\nresults. For example, Ouyang et al. [9] reported strong perfor-\nmance gains for their 175B RLHF model. It was preferred over\noutputs from the 175B GPT-3 [1] model in 85% ± 3% of cases\non their prompt distribution. It was also rated significantly\nbetter on overall quality, with a 71% ± 4% preference. These\nsubstantial improvements demonstrate the effectiveness of the\nRLHF approach in aligning model outputs with human prefer-\nences. However, the success of RLHF is highly dependent on\nthe quality and diversity of human feedback and the design of\nthe reward model. Challenges include the high cost of human\nannotation, potential biases in feedback collection, and the\ncomplexity of the multi-stage training pipeline.\nRLAIF emerged as a strategic innovation to scale up the\nTable I\nCOMPARATIVE ANALYSIS OF RL TECHNIQUES FOR LLM ALIGNMENT AND ENHANCEMENT\nPaper/Method\nRM Type\nRM Output\nRM Level\nFeedback\nFeedback Src.\nFeedback Fmt.\nRef. Model\nLength Ctrl.\nDivergence\nRL Policy\nOptimization\nSFT/Align Stage\nInstructGPT [9]\nExplicit\nPoint\nResponse\nPreference\nHuman\nPair\nYes\nNo\nKL\nOn\nOffline\nSeparate\nRLHF (Anthropic) [88]\nExplicit\nPoint\nResponse\nPreference\nHuman\nPair\nYes\nNo\nKL\nOff\nHybrid\nSeparate\nPPO (Online RLHF) [37]\nExplicit\nPoint\nResponse\nPreference\nHuman\nPair\nYes\nNo\nKL\nOff\nOnline\nSeparate\nRLAIF (Anthropic) [12]\nExplicit\nPoint\nResponse\nPreference\nAI\nPair\nYes\nNo\nKL\nOn\nOffline\nSeparate\nRLAIF (Google) [11]\nExplicit\nPoint\nResponse\nPreference\nAI\nPair\nYes\nNo\nKL\nOff\nOffline\nSeparate\nDPO [13]\nImplicit\nPoint\nResponse\nPreference\nHuman\nPair\nYes\nNo\nKL\nOff\nOffline\nSeparate\nIPO [89]\nImplicit\nPreference\nResponse\nPreference\nHuman\nPair\nYes\nNo\nKL\nOff\nOffline\nSeparate\nKTO [86]\nImplicit\nPoint\nResponse\nBinary\nHuman\n-\nYes\nNo\nKL\nOff\nOffline\nSeparate\nORPO [87]\nImplicit\nPreference\nResponse\nPreference\nHuman\nPair\nNo\nNo\n-\nOff\nOffline\nMerge\nRRHF [90]\nImplicit\nPreference\nResponse\nPreference\nHuman\nList\nNo\nNo\n-\nOff\nOffline\nMerge\nPRO [91]\nExplicit\nPoint\nResponse\nPreference\nHuman\nList\nNo\nNo\n-\nOff\nOffline\nMerge\nDeepSeek-R1 [34]\nExplicit\nPoint\nResponse\nMixed\nMixed\nMixed\nYes\nYes\nKL\nOn\nOnline\nSeparate\nRLVR [15]\nExplicit\nPoint\nStep-wise\nMixed\nVerifier\nBinary\nNo\nN/A\n-\nOn\nOnline\nSeparate\nfeedback process by replacing or augmenting human feedback\nwith AI-generated feedback. Lee et al. [11] demonstrated that\nRLAIF can achieve comparable or even superior performance\nto RLHF on tasks like summarization and helpful dialogue\ngeneration, while being substantially more scalable. Their\nempirical results are compelling. On summarization tasks, the\nRLAIF-trained PaLM 2-S [92] model achieved a 53% win rate\nagainst an RLHF-trained baseline. On helpful dialogue tasks, it\nreached a 50% win rate. These results suggest that AI feedback\ncan be as effective as human feedback for certain applications,\nwhile dramatically reducing the resource requirements for\ncollecting preference data.\nDPO and its variants represent a significant paradigm shift\nby bypassing the explicit reward modeling stage entirely.\nRafailov et al. [13] provided empirical evidence that DPO\ncan match or exceed the performance of PPO-based RLHF\non tasks like sentiment control and summarization with sub-\nstantially less complexity. Their results are particularly strik-\ning. On the IMDb sentiment generation task, DPO achieved\na reward of 0.72, while PPO-RLHF scored 0.53. On the\nTL;DR summarization dataset, DPO reached a reward of -0.20,\ncompared to PPO-RLHF’s -0.26. In both tasks, higher scores\nindicate better performance. These quantitative improvements,\ncoupled with the simplified training pipeline, have made DPO\nand its variants increasingly popular for LLM alignment.\nSubsequent methods like IPO (Identity Preference Opti-\nmization) [89] and KTO (Kahneman-Tversky Optimization)\n[86] build upon the DPO framework. They offer different\nloss functions or incorporating different aspects of human\npreference. For example, KTO uses binary feedback to indicate\nwhether an output is desirable or undesirable. This approach\ncan be more intuitive than pairwise comparisons in certain\napplications. These methods generally offer improved stability\nand reduced hyperparameter tuning compared to PPO-based\napproaches. Therefore, it makes them attractive alternatives\nfor practical deployment.\nBeyond alignment with preferences for style and safety,\nRL is increasingly being applied to improve the multi-step\nreasoning capabilities of LLMs. Techniques like RLVR [15],\nOpenAI’s o1 [61] and o3 [68], and DeepSeek-R1 [34] focus\non improving step-by-step reasoning in language models. They\ndo this by assigning rewards based on the correctness of\nintermediate reasoning steps. These steps are often verified\nusing external tools or smaller, specialized models. Wang et\nal. [15] showed that RLVR can significantly improve math-\nematical reasoning performance by reinforcing correct step-\nby-step logic. With just a single positive example, GPT-3.5’s\naccuracy on GSM8K increased from 56.8% to 72.5%. This\nsubstantial improvement highlights the potential of targeted re-\ninforcement learning to enhance specific cognitive capabilities\nin LLMs. Similarly, DeepSeek-R1 [34] achieves strong perfor-\nmance on reasoning benchmarks by incorporating automatic\nrewards based on logical correctness and consistency. These\napproaches often require careful design of the reward function\nto accurately reflect reasoning quality and avoid rewarding\nsuperficial or incorrect reasoning paths.\nA key insight from this comparative analysis is that the\nchoice of RL technique depends heavily on the specific goals,\navailable data, and computational resources. While PPO-based\nRLHF [9] remains a powerful and widely adopted method,\nparticularly for complex alignment tasks, newer methods like\nDPO [13] offer compelling alternatives with better efficiency\nand stability for certain scenarios.\nTables II and III present a comprehensive performance\ncomparison of various offline alignment methods, including\nDPO, KTO, and different UNA (Unified Alignment) variants.\nUNA [93] is a unified alignment framework that demonstrates\nRLHF with PPO, DPO, and KTO all optimize the same gener-\nalized implicit reward function. This perspective allows these\nmethods to be cast as a single supervised objective capable of\nhandling pairwise, binary, and scalar feedback. As a result, it\nsimplifies and stabilizes the policy fine-tuning process. There\nare three variants of UNA (Unified Alignment). Each of them\ncorrespond to a different type of feedback signal used during\npolicy optimization. UNA-pairwise leverages pairwise prefer-\nence feedback and is implemented using a Direct Preference\nOptimization (DPO-style) objective. UNA-binary uses binary\nreward signals that indicate whether a response is acceptable\nor not. It is trained with either Mean Squared Error (MSE)\nor Binary Cross Entropy (BCE) loss functions. Finally, UNA-\nscore utilizes scalar reward feedback, such as numerical scores\nfrom a reward model or verifier, and optimizes the policy\nusing Mean Squared Error loss. These variants demonstrate\nthe flexibility of UNA in unifying alignment methods under a\nsingle supervised learning framework.\nThe alignment methods are evaluated against a Mistral\n[94] baseline model. This is because the original Mistral 7B\nmodel that is trained purely with next-token prediction and\nTable II\nPERFORMANCE OF OFFLINE ALIGNMENT METHODS (DPO, KTO, UNA VARIANTS) ON NEW OPEN LLM LEADERBOARD\nMethod\nBBH\nGPQA\nMMLU-Pro\nMUSR\nIFEval\nMATH-Hard\nAverage\nMistral (Baseline)\n44.11\n29.53\n30.11\n41.79\n23.22\n2.92\n28.61\nDPO (UNA-pairwise)\n44.50\n28.48\n30.41\n39.25\n26.30\n2.25\n28.53\nKTO\n44.46\n29.51\n30.43\n40.45\n24.18\n2.34\n28.56\nUNA-binary (MSE)\n44.32\n29.86\n30.54\n39.11\n26.10\n3.32\n28.88\nUNA-binary (BCE)\n44.43\n29.42\n30.73\n39.51\n26.49\n2.99\n28.93\nUNA-score (MSE)\n43.53\n30.25\n29.72\n42.01\n37.25\n2.77\n30.92\nTable III\nPERFORMANCE OF OFFLINE ALIGNMENT METHODS (DPO, KTO, UNA VARIANTS) ON OLD OPEN LLM LEADERBOARD\nMethod\nGSM8K\nTruthfulQA\nWinograde\nARC\nHellaSwag\nMMLU\nAverage\nMistral (Baseline)\n38.02\n42.58\n77.58\n61.43\n83.44\n62.51\n60.93\nDPO (UNA-pairwise)\n40.22\n44.75\n79.16\n62.88\n84.42\n62.15\n62.26\nKTO\n41.63\n47.72\n78.14\n62.29\n84.21\n62.46\n62.74\nUNA-binary (MSE)\n40.87\n48.23\n79.48\n63.23\n84.57\n62.34\n63.12\nUNA-binary (BCE)\n40.41\n48.33\n79.40\n63.14\n84.60\n62.48\n63.06\nUNA-score (MSE)\n40.41\n55.09\n80.27\n63.23\n84.52\n62.56\n64.35\ninstruction SFT but does not employ any preference-based\nor reinforcement-learning alignment. Table II presents the\nperformance comparison on the New Open LLM Leaderboard,\nwhile Table III reports results on the Old Open LLM Leader-\nboard. The key distinction between the two is that the new\nleaderboard incorporates significantly more challenging and\ndiverse benchmarks and therefore provides a more rigorous\nevaluation of model alignment performance.\nTable II focusing on the new HuggingFace Open LLM\nLeaderboard includes challenging tasks such as Big-Bench\nHard (BBH), Grade-School Physics Questions Annotated\n(GPQA), MMLU-Pro, Multi-turn Summarization and Reason-\ning (MUSR), Instruction Following Evaluation (IFEval), and\nMATH-Hard. The results reveal that the UNA-score (MSE)\nmethod achieves the highest average score of 30.92, substan-\ntially outperforming the baseline Mistral model (28.61), as\nwell as other approaches like DPO (28.53) and KTO (28.56).\nA particularly striking result is UNA-score (MSE)’s perfor-\nmance on the IFEval benchmark, where it achieves a score of\n37.25 compared to the baseline’s score of 23.22. It is a remark-\nable improvement of 60.4%. This suggests that the score-based\nUNA approach is especially effective at enhancing instruction-\nfollowing capabilities. Interestingly, while UNA-score (MSE)\nshows the best overall performance, it actually underperforms\nthe baseline on BBH (43.53 vs. 44.11) and MMLU-Pro (29.72\nvs. 30.11). This indicates that different alignment methods may\nhave varying strengths and weaknesses across different task\ntypes.\nSimilarly, Table III examining performance on the old\nOpen LLM Leaderboard includes benchmarks such as GSM8K\n(mathematical reasoning), TruthfulQA (factual accuracy),\nWinograde (commonsense reasoning), ARC (science knowl-\nedge), HellaSwag (commonsense inference), and MMLU\n(multitask knowledge). Here again, UNA-score (MSE) demon-\nstrates superior overall performance with an average score of\n64.35, compared to the Mistral baseline’s 60.93, DPO’s 62.26,\nand KTO’s 62.74.\nThe most dramatic improvement is observed on Truth-\nfulQA, where UNA-score (MSE) achieves a score of 55.09.\nThis represents a substantial 29.4% gain over the baseline\nscore of 42.58. This suggests that the UNA-score approach\nis particularly effective at enhancing factual accuracy and\nreducing hallucinations. UNA-score (MSE) also shows notable\nimprovements on Winograde (80.27 vs. baseline’s 77.58) and\nARC (63.23 vs. baseline’s 61.43), indicating enhanced reason-\ning and knowledge capabilities.\nThese results highlight the strong potential of UNA, par-\nticularly the score-based variant trained with MSE loss. It\nsignificantly improves LLM performance across diverse eval-\nuation metrics in offline alignment settings. The consistent\noutperformance across diverse benchmarks suggests that this\napproach may offer a more robust and generalizable alignment\nstrategy compared to other methods.\nTables IV and V shift the focus to online alignment methods.\nWe compare traditional PPO-based RLHF with the online\nvariant of UNA, using a Mistral-INST [94] model as the\nbaseline. Mistral-INST serves as the baseline because it is an\ninstruction-tuned version of the Mistral 7B model that has not\nbeen further aligned using preference-based or reinforcement\nlearning methods. The tables IV and V offer insight into how\ndifferent alignment techniques perform in an online learning\nsetup, where the policy is continuously refined with new\nfeedback.\nTable IV examines performance on the new Open LLM\nLeaderboard. In this setting, the online UNA method attains\na marginally higher average score of 29.15, edging out RLHF\nTable IV\nPERFORMANCE OF ONLINE ALIGNMENT METHODS (RLHF VS. UNA) ON NEW OPEN LLM LEADERBOARD\nMethod\nBBH\nGPQA\nMMLU-Pro\nMUSR\nIFEval\nMATH-Hard\nAverage\nMistral-INST (Baseline)\n42.46\n29.05\n24.53\n38.30\n38.46\n2.02\n29.14\nRLHF\n42.50\n28.99\n24.60\n38.29\n38.53\n1.79\n29.12\nUNA (Online)\n42.78\n28.32\n24.87\n38.03\n39.17\n1.75\n29.15\nTable V\nPERFORMANCE OF ONLINE ALIGNMENT METHODS (RLHF VS. UNA) ON OLD OPEN LLM LEADERBOARD\nMethod\nGSM8K\nTruthfulQA\nWinograde\nARC\nHellaSwag\nMMLU\nAverage\nMistral-INST (Baseline)\n35.14\n55.94\n73.72\n55.29\n75.99\n53.94\n58.34\nRLHF\n34.42\n55.88\n73.56\n55.20\n76.03\n54.03\n58.19\nUNA (Online)\n35.67\n55.88\n74.03\n55.20\n76.61\n54.02\n58.57\nwhich has a score of 29.12 and the Mistral-INST baseline\nwhich has a score 29.14. While the overall improvement is\nmodest, online UNA demonstrates notable gains on certain\nbenchmarks. It achieves 42.78 on BBH (vs. RLHF’s 42.50),\n24.87 on MMLU-Pro (vs. RLHF’s 24.60), and 39.17 on IFEval\n(vs. RLHF’s 38.53). These results suggest that UNA may be\nmore effective at enhancing instruction-following capabilities\n(IFEval) and certain types of reasoning tasks (BBH, MMLU-\nPro).\nSimilarly, Table V compares performance on the old Open\nLLM Leaderboard. Here again, online UNA marginally out-\nperforms RLHF with an average score of 58.57 compared to\nRLHF’s score of 58.19 and the baseline’s score of 58.34. On-\nline UNA demonstrates better performance on GSM8K (35.67\nvs. RLHF’s 34.42), Winograde (74.03 vs. RLHF’s 73.56), and\nHellaSwag (76.61 vs. RLHF’s 76.03). The improvement on\nGSM8K is particularly noteworthy, as it suggests enhanced\nmathematical reasoning capabilities.\nWhile the performance improvements in these online learn-\ning experiments are modest, the UNA framework offers poten-\ntial advantages beyond raw performance metrics. As discussed\nby Wang et al. [93], UNA can simplify the RLHF pipeline\nby transforming it into a supervised learning problem. Such\na formulation can reduce memory consumption and shorten\ntraining time. This operational efficiency, combined with com-\npetitive performance, makes UNA a promising approach for\nonline alignment scenarios.\nTable VI provides a comprehensive overview of how promi-\nnent open-weight LLMs perform on core reasoning and knowl-\nedge tasks, along with the specific RL techniques employed\nin their training. The models, along with their respective\nreinforcement learning techniques, are evaluated on three key\nbenchmarks: 5-shot MMLU, Maj@8 GSM8K, and 25-shot\nARC-Challenge. Here, MMLU, GSM8K, and ARC-Challenge\nare datasets, and 5-shot, Maj@8, and 25-shot refer to the evalu-\nation protocols applied to them. The 5-shot evaluation protocol\nmeans that the model is given five example question–answer\npairs (shots) as context before answering each test question.\nSimilarly, 25-shot ARC-Challenge presents the model with\n25 such examples. For GSM8K, Maj@8 refers to majority\nvoting over eight sampled completions, where the final answer\nis chosen as the most frequently occurring prediction among\nthose eight.\nTable VI reveals several interesting patterns. Llama 3.1\n405B Instruct, aligned with traditional RLHF (PPO), demon-\nstrates exceptional performance across all three benchmarks.\nIt achieves 85.2% on MMLU, 96.4% on GSM8K, and 95.3%\non ARC-Challenge. This suggests that traditional RLHF, when\napplied to very large models, can yield outstanding results. The\nsmaller Llama 3.1 70B Instruct, which uses a combination of\nRLHF (PPO) and DPO, also performs admirably. It scores\n79.5% on MMLU, 89.1% on GSM8K, and 93.0% on ARC-\nChallenge.\nDeepSeek-V2 [95], which employs Group Relative Policy\nOptimization (GRPO), achieves the highest MMLU score\nof 86.4% among all models listed. This suggests that this\nRL technique may be particularly effective for enhancing\nknowledge-intensive capabilities. However, its GSM8K perfor-\nmance of 84.0% is lower than both Llama models, indicating\npotential trade-offs in mathematical reasoning.\nInterestingly, Phi-3-mini-4k Instruct [96], despite being the\nsmallest model in the table with just 3.8 billion parameters,\nachieves impressive results on GSM8K with a score of 85.7%\nand on ARC-Challenge with a score of 86.3% using the\nBreak-Fix RL approach. Break-Fix RL is a reinforcement\nlearning approach where the model is penalized for harmful or\nincorrect behaviors (“breaks”) and rewarded for corrected or\nsafe behaviors (“fixes”), enabling iterative safety and reliabil-\nity improvements. These results demonstrate that specialized\nalignment techniques can enable smaller models to compete\neffectively on targeted reasoning and problem-solving tasks.\nMixtral 8×22B Instruct [97], aligned solely with DPO,\nshows relatively lower performance compared to PPO-based\nmodels, with scores of 77.8% on MMLU, 74.1% on GSM8K,\nand 70.5% on ARC-Challenge. This might suggest that while\nDPO offers training efficiency advantages, it may not match\nthe performance of PPO-based approaches for these specific\nreasoning tasks.\nQwen2-72B Instruct [98], which uses a combination of\nRLHF (PPO) and RLAIF, performs strongly on MMLU with\nTable VI\nPERFORMANCE OF PROMINENT OPEN-WEIGHT LLMS ON CORE REASONING/KNOWLEDGE TASKS, WITH THEIR POST-TRAINING RL ALIGNMENT METHOD\nModel (size)\nReinforcement Learning Technique\nMMLU\nGSM8K\nARC-Ch.\nLlama 3.1 70B Instruct\nRLHF (PPO + DPO)\n79.5\n89.1\n93.0\nLlama 3.1 405B Instruct\nRLHF (PPO)\n85.2\n96.4\n95.3\nMixtral 8×22B Instruct\nDPO\n77.8\n74.1\n70.5\nDeepSeek-V2\nGRPO\n86.4\n84.0\n92.4\nQwen2-72B Instruct\nRLHF (PPO / RLAIF)\n84.0\n88.3\n71.6\nGemma 2 27B Instruct\nRLHF (PPO)\n75.2\n74.0\n71.4\nPhi-3-mini-4k Instruct\nBreak-Fix RL\n70.9\n85.7\n86.3\na score of 84.0% and on GSM8K with 88.3%. However,\nits performance on ARC-Challenge is comparatively lower at\n71.6%. This pattern suggests that the combination of human\nand AI feedback may be particularly effective for enhancing\ncertain types of reasoning capabilities.\nOverall, this table illustrates the complex relationship be-\ntween model size, alignment technique, and performance\nacross different reasoning tasks. It shows that although larger\nmodels generally perform better, the choice of alignment\nmethod can significantly influence the specific strengths and\nweaknesses of a model.\nTable VII shifts the focus to coding capabilities, presenting\nthe performance of popular open-weight code-capable LLMs\non two standard benchmarks: HumanEval [63] and MBPP\n(Mostly Basic Python Problems) [99]. The metric reported is\npass@1, which measures the percentage of problems correctly\nsolved by the model on the first attempt. This table provides\nvaluable insights into how different reinforcement learning\ntechniques influence coding proficiency and generalization\nacross programming tasks.\nLlama 3.3 70B Versatile [100], which employs a combina-\ntion of PPO-based RLHF and DPO, demonstrates exceptional\ncoding capabilities, achieving 88.4% pass@1 on HumanEval\nand 87.6% on MBPP. This suggests that the hybrid approach\nof combining PPO and DPO may be particularly effective for\nenhancing coding abilities. Qwen2-72B Instruct [98], using\nRLHF (PPO/RLAIF), also performs admirably with 86.0%\non HumanEval and 80.2% on MBPP. It indicates that the\ncombination of human and AI feedback can yield strong\ncoding performance.\nWizardCoder-Python-34B V1.1 is trained using Reinforce-\nment Learning from Evol-Instruct Feedback (RLEIF) [101],\na technique that combines evolutionary instruction refinement\nwith reward-based learning to improve coding performance.\nThis method enables the model to iteratively learn from\nenhanced prompts, reinforcing effective code generation strate-\ngies. As a result, WizardCoder-Python-34B V1.1 achieves\nstrong performance, scoring 79.9% on HumanEval and 78.9%\non MBPP. This evolutionary instruction tuning approach, com-\nbined with reinforcement learning from evolved instruction\nfeedback, appears to be effective for coding tasks despite the\nmodel being smaller than Llama 3.3 [100] and Qwen2 [98].\nIn contrast, DeepSeek-Coder-V2 [102], which employs\nGRPO, shows relatively lower performance with 57.3% on\nHumanEval and 45.8% on MBPP. This is particularly in-\nteresting given that DeepSeek-V2 [95] performed very well\non reasoning tasks (as seen in Table VI), suggesting that\nGRPO may be more effective for general reasoning than for\nspecialized coding tasks.\nStarCoder2-15B Instruct V0.1, which uses DPO with Self-\nCodeAlign [103], achieves respectable results with 72.6% on\nHumanEval and 75.2% on MBPP, despite being the smallest\nmodel in the table. SelfCodeAlign is a fully transparent, self-\nalignment pipeline that enhances code models without human\nlabels by having the model generate and validate its own\ninstruction–response pairs using test suites. This highlights\nthe potential of specialized alignment techniques tailored for\ncoding tasks.\nGemma 2 27B Instruct [104], aligned with RLHF using\nPPO, scores 51.8% on HumanEval and 62.6% on MBPP,\nsuggesting that the model may struggle with certain types\nof coding problems. Similarly, Mixtral 8×22B Instruct [97],\naligned with DPO, achieves 76.2% on HumanEval and 64.3%\non MBPP, indicating a performance gap in the opposite\ndirection.\nThese results collectively illustrate that coding proficiency\nis influenced by both model architecture and alignment tech-\nnique. Models specifically designed for code generation (like\nWizardCoder and StarCoder) can achieve competitive perfor-\nmance even at smaller scales when aligned with appropriate\ntechniques. Table VII also suggests that hybrid approaches\ncombining multiple alignment methods (like PPO+DPO) may\noffer advantages for enhancing coding capabilities.\nTable VIII focuses specifically on truthfulness alignment,\npresenting the performance of large open-weight LLMs on\nthe TruthfulQA [105] benchmark. This benchmark evaluates a\nmodel’s ability to avoid generating misleading or factually in-\ncorrect statements. The reported scores use a 0-shot evaluation\nsetting, meaning the models respond to each prompt without\nany task-specific examples or prior fine-tuning, thereby assess-\ning their default ability to generate truthful outputs.\nThe results reveal significant variations in truthfulness\nacross different models and alignment techniques. Qwen2-72B\nInstruct [98], which employs a combination of RLHF (PPO)\nand RLAIF, achieves the highest TruthfulQA score of 67.0%.\nThis suggests that incorporating AI feedback alongside human\nTable VII\nPERFORMANCE OF POPULAR OPEN-WEIGHT CODE-CAPABLE LLMS ON STANDARD CODING BENCHMARKS, WITH THEIR RL TECHNIQUE\nModel (size)\nReinforcement Learning Technique\nHumanEval (pass@1)\nMBPP (pass@1)\nLlama 3.3 70B Versatile\nRLHF (PPO + DPO)\n88.4\n87.6\nQwen2-72B Instruct\nRLHF (PPO / RLAIF)\n86.0\n80.2\nWizardCoder-Python-34B V1.1\nEvol-Instruct (RLEIF)\n79.9\n78.9\nDeepSeek-Coder-V2\nGRPO\n57.3\n45.8\nStarCoder2-15B Instruct V0.1\nDPO (SelfCodeAlign)\n72.6\n75.2\nGemma 2 27B Instruct\nRLHF (PPO)\n51.8\n62.6\nMixtral 8×22B Instruct\nDPO\n76.2\n64.3\nTable VIII\nTRUTHFULNESS PERFORMANCE OF LARGE OPEN-WEIGHT LLMS ON THE TRUTHFULQA BENCHMARK, WITH THEIR RL TECHNIQUE\nModel (size)\nRL Technique\nTruthfulQA 0-shot (%)\nMeta Llama 3.1 70B Instruct\nRLHF (PPO + DPO)\n62.9\nQwen2-72B Instruct\nRLHF (PPO) + RLAIF\n67.0\nMixtral 8×22B Instruct\nDirect Preference Optimisation (DPO)\n51.1\nDeepSeek-V2 Chat (RL)\nGroup Relative Policy Optimisation (GRPO)\n57.7\nGemma 2 27B Instruction-tuned\nRLHF (PPO)\n51.6\nPhi-3-mini-4k Instruct\n”Break-Fix” safety RL cycle\n38.5\nT ¨ULU 3 70B\nTrust-Region DPO + RL-VR\n63.8\nfeedback may be particularly effective for enhancing factual\naccuracy and reducing hallucinations. The strong performance\nof RLAIF in this context may be due to the ability of AI\nsystems to systematically verify factual claims against large\nknowledge bases.\nT¨ulu 3-70B model [106] uses a two-stage alignment process.\nFirst, the Llama-3.1-70B base model is aligned using length-\nnormalized Direct Preference Optimization (DPO). This is a\nKL-constrained method that keeps the new policy close to\nthe reference model. Then, the DPO-aligned model is further\nrefined using Reinforcement Learning with Verifiable Rewards\n(RLVR). RLVR gives positive rewards only when task-specific\nverifiers approve the output. It achieves the second-highest\nscore of 63.8%. The multi-stage alignment approach used for\nthis model appears to be effective for enhancing truthfulness,\npotentially because the verifiable rewards component directly\nincentivizes factual accuracy.\nMeta Llama 3.1 70B Instruct, aligned with a combination of\nRLHF (PPO) and DPO, achieves a respectable score of 62.9%.\nThis hybrid approach seems to strike a good balance between\ninstruction following and maintaining factual accuracy.\nDeepSeek-V2 Chat (RL) [95], which employs Group Rela-\ntive Policy Optimisation (GRPO), achieves a moderate score\nof 57.7%. While this is lower than the top-performing models,\nit still represents a substantial improvement over completely\nunaligned models (which typically score much lower on Truth-\nfulQA).\nMixtral 8×22B Instruct [97] and Gemma 2 27B Instruction-\ntuned [104] achieve similar scores of 51.1% and 51.6% re-\nspectively, despite using different alignment techniques (DPO\nvs. RLHF with PPO). The similarity in performance implies\nthat, for truthfulness, the specific implementation details and\ntraining data may be as important as the choice of alignment\nalgorithm.\nPhi-3-mini-4k Instruct [96], which uses the “Break-Fix”\nsafety RL cycle, achieves the lowest score of 38.5%. This may\nbe due to the model’s smaller size or to the specific focus of\nthe Break-Fix approach on safety rather than factual accuracy.\nThese results highlight the complex relationship between\nalignment techniques and truthfulness. While all alignment\nmethods aim to improve model behavior, their effectiveness\nfor enhancing factual accuracy varies considerably. The table\nsuggests that hybrid approaches combining multiple align-\nment methods, particularly those incorporating AI feedback\nor verifiable rewards, may be most effective for enhancing\ntruthfulness.\nTable IX evaluates instruction-following quality across two\nwidely used benchmarks: AlpacaEval 2.0 and MT-Bench. Al-\npacaEval 2.0 [107] evaluates a model’s single-turn helpfulness\nby comparing its responses using GPT-4 as an automated\njudge. The scores are expressed as win rates, where higher\nvalues reflect better performance. MT-Bench [108] assesses\nmulti-turn conversational ability, assigning a score between 0\nand 10 based on criteria like coherence, helpfulness, and con-\nsistency. Together, these benchmarks provide a comprehensive\nview of how different RL alignment techniques influence a\nmodel’s capacity to follow instructions and sustain effective\ndialogue.\nThe results reveal some surprising patterns. Gemma 2\n27B Instruction-tuned [104], which uses traditional PPO-\nbased RLHF, achieves the highest AlpacaEval 2.0 win rate of\n57.5%, substantially outperforming larger models. This sug-\ngests that Google’s implementation of RLHF for Gemma may\nbe particularly effective for enhancing instruction-following\nTable IX\nINSTRUCTION-FOLLOWING PERFORMANCE OF POPULAR OPEN-WEIGHT LLMS ON STANDARD DIALOGUE BENCHMARKS, WITH THEIR RL TECHNIQUE\nModel (size)\nRL Technique\nAlpacaEval 2.0 LC Win Rate (%)\nMT-Bench (0–10)\nMeta Llama 3.1 70B Instruct\nRLHF (PPO + DPO)\n34.4\n8.15\nQwen2-72B Instruct\nRLHF (PPO / RLAIF)\n36.6\n9.10\nMixtral 8×22B Instruct\nDPO\n30.9\n8.66\nDeepSeek-V2 Chat (RL)\nGRPO\n38.9\n8.97\nGemma 2 27B Instruction-tuned\nRLHF (PPO)\n57.5\n8.62\nPhi-3-mini-4k Instruct\nBreak-Fix RL\n23.1\n8.12\nT ¨ULU 3 70B\nTrust-Region DPO + RL-VR\n49.8\n8.60\ncapabilities, or that the model’s architecture is especially well-\nsuited for this task.\nT ¨ULU 3 70B [106], with its multi-stage alignment approach\n(Trust-Region DPO followed by RL-VR), achieves the second-\nhighest AlpacaEval win rate of 49.8%. This strong perfor-\nmance aligns with its high truthfulness score (as seen in Table\nVIII). It suggests that this sophisticated alignment approach\nenhances multiple aspects of model behavior simultaneously.\nDeepSeek-V2 Chat (RL) [95], which employs GRPO,\nachieves a respectable AlpacaEval win rate of 38.9% and the\nsecond-highest MT-Bench score of 8.97. This indicates that\nGRPO may be particularly effective for enhancing conversa-\ntional capabilities and instruction-following.\nQwen2-72B Instruct [98], which uses a combination of\nPPO-based RLHF and RLAIF, achieves the highest MT-Bench\nscore of 9.10 and a solid AlpacaEval win rate of 36.6%.\nThis suggests that incorporating AI feedback alongside hu-\nman feedback may be beneficial for enhancing conversational\ncapabilities.\nMeta Llama 3.1 70B Instruct, aligned with a combination of\nPPO-based RLHF and DPO, achieves a moderate AlpacaEval\nwin rate of 34.4% and an MT-Bench score of 8.15. While these\nscores are respectable, they are lower than might be expected\ngiven the model’s strong performance on reasoning tasks (as\nseen in Table VI). This indicates potential trade-offs between\nreasoning capabilities and instruction-following behavior.\nMixtral 8×22B Instruct [97], aligned solely with DPO,\nachieves an AlpacaEval win rate of 30.9% and an MT-Bench\nscore of 8.66. The relatively high MT-Bench score suggests\nthat DPO may be effective for enhancing conversational ca-\npabilities, even if it doesn’t match the instruction-following\nperformance of more complex alignment approaches.\nPhi-3-mini-4k Instruct [96], which employs the Break-\nFix RL approach, records the lowest AlpacaEval win rate\nat 23.1% but achieves a respectable MT-Bench score of\n8.12. This indicates that, while the model may struggle with\ncomplex single-turn instructions, the Break-Fix RL technique\ncontributes to its ability to sustain coherent and effective multi-\nturn conversations despite its smaller size.\nThese results highlight the complex relationship between\nalignment techniques and instruction-following capabilities.\nDifferent\nbenchmarks\nmay\ncapture\ndifferent\naspects\nof\ninstruction-following, and models may excel in some areas\nwhile underperforming in others. Finally, Table IX suggests\nthat sophisticated multi-stage alignment approaches and hybrid\nmethods incorporating multiple feedback sources may offer\nadvantages for enhancing instruction-following capabilities.\nThe comparative analysis presented underscores several key\ninsights into how RL techniques shape the performance of\nLLMs across diverse tasks. Firstly, the unified alignment\nframework (UNA), particularly its score-based variant trained\nwith Mean Squared Error (MSE), consistently demonstrates\nrobust improvements across multiple benchmarks in offline\nscenarios. This approach notably enhances factual accuracy\n(TruthfulQA) and instruction-following capabilities (IFEval),\noutperforming traditional baselines like DPO and KTO. In\nonline alignment settings, UNA maintains competitive per-\nformance with traditional PPO-based RLHF, delivering in-\ncremental improvements in reasoning benchmarks such as\nGSM8K and Big-Bench Hard (BBH). In short, based on the\ncomparative study, UNA seems to be an attractive alignment\nmethod for practical applications requiring real-time updates\nand resource efficiency.\nSecondly, the effectiveness of specific RL methods varies\nconsiderably based on the targeted task and model size. This\nhighlights the nuanced interplay between alignment strate-\ngies and desired capabilities. For reasoning and knowledge-\nintensive tasks, PPO-based RLHF and hybrid methods com-\nbining PPO with DPO consistently achieve strong perfor-\nmance, particularly in larger models like Llama 3.1 and\nQwen2-72B [98]. For specialized coding tasks, tailored\nalignment approaches like evolutionary instruction feedback\n(RLEIF) and self-alignment techniques (SelfCodeAlign) of-\nfer notable advantages. These methods enable even rela-\ntively smaller models to perform competitively by reinforcing\ndomain-specific competencies. Instruction-following evalua-\ntions further reveal that complex multi-stage alignment meth-\nods, such as Trust-Region DPO followed by RL-VR, de-\nliver well-rounded improvements across conversational bench-\nmarks. Notably, specialized alignment methods like Break-\nFix RL can empower smaller models to achieve impressive\nconversational performance, underscoring the importance of\nalignment strategy selection based on model constraints and\napplication goals. Collectively, these findings emphasize that\nno single RL technique universally dominates, and optimal\nperformance across diverse tasks typically emerges from\nthoughtful alignment of model architecture, RL technique, and\nspecific operational objectives.\nVII. CHALLENGES AND LIMITATIONS\nDespite the significant progress in applying Reinforcement\nLearning (RL) to Large Language Models (LLMs), several\ncritical challenges and limitations persist, which hinder the full\nrealization of their potential. These can be broadly categorized\ninto research bottlenecks, technical limitations, and overar-\nching challenges in deployment and evaluation. Addressing\nthese issues is paramount for the continued advancement and\nresponsible application of RL-enhanced LLMs.\nCurrent research bottlenecks primarily revolve around the\nscalability and quality of feedback, and the complexity of\nreward modeling. While RLHF is effective, its dependence\non human feedback makes it expensive and time-consuming.\nThis reliance also poses scalability challenges, especially when\naligning models across a broad range of behaviors and nuanced\ntasks. For RLAIF, as previously discussed, Sharma et al. [19]\nhighlighted the concern that AI-generated feedback can inherit\nor even amplify biases from the supervising model. This may\nlead to the emergence of behaviors that, while appearing\naligned, ultimately diverge from genuine human values. Fur-\nthermore, Denison et al. [16] and Fu et al. [17] demonstrated\nthat designing reward models capable of faithfully capturing\ncomplex human preferences across diverse contexts is inher-\nently challenging. These models are often vulnerable to reward\nhacking, a phenomenon where language models learn to ex-\nploit the reward function to maximize scores without genuinely\ncompleting the intended task. Such vulnerabilities highlight the\ndifficulty of aligning models through reward-based methods\nalone, as even small imperfections in the reward specification\ncan lead to unintended behaviors. The development of more\nsophisticated reward modeling techniques, robust evaluation\nmetrics for alignment, and efficient methods for eliciting and\naggregating diverse human (or AI) preferences are active areas\nof research crucial for overcoming these bottlenecks.\nFrom a technical standpoint, applying reinforcement learn-\ning to large language models comes with several inherent\nlimitations. One major challenge is the substantial computa-\ntional cost involved in training these models, especially when\nusing on-policy algorithms like PPO. Such training demands\nextensive hardware resources and long runtimes. As a result,\nthe process can be prohibitively expensive and difficult to\naccess for many research groups and organizations. Sample\nefficiency is another major concern. RL algorithms often\nrequire a vast number of interactions or feedback instances\nto learn effectively, which is exacerbated by the high dimen-\nsionality of the action space (i.e., text generation) in LLMs.\nMoreover, the stability of reinforcement learning training can\nbe difficult to maintain. Models may sometimes suffer from\ncatastrophic forgetting, where previously learned capabilities\nare lost during further training. In other cases, they can expe-\nrience policy collapse, leading to a sharp decline in generation\nquality. Ensuring stable and efficient training, along with the\ndevelopment of more sample-efficient RL algorithms tailored\nto language tasks, remains a major technical hurdle. Equally\nimportant is the creation of robust policy update techniques to\nenhance the practicality and reliability of RL for LLMs.\nBeyond specific research and technical hurdles, broader\nchallenges exist in the evaluation, safety, and ethical deploy-\nment of RL-aligned LLMs. Evaluating the true alignment\nand safety of large language models remains a formidable\nchallenge, as existing benchmarks often fail to capture the\nfull spectrum of failure modes and adversarial behaviors.\nThis concern has been underscored by Abeysinghe et al.\n[109] and Lee et al. [46], who emphasize the limitations of\ncurrent evaluation frameworks in reliably measuring model\nrobustness and safety. Ensuring that models are not only\nhelpful and harmless on average but also robust against misuse,\nmanipulation, or the generation of subtle misinformation is\nan ongoing struggle. The problem of “alignment faking” or\nsycophancy, as discussed by Wang et al. [110] and Greenblatt\net al. [111], adds another layer of complexity to evaluation.\nIn such cases, models may outwardly appear aligned while\nconcealing underlying misaligned behaviors. Ethical concerns\nin alignment involve the values instilled during training and the\npotential for biased feedback to produce inequitable models.\nThese risks raise serious questions about fairness, inclusivity,\nand unintended harm. Additionally, the broader societal impact\nof deploying powerful, RL-aligned LLMs calls for robust\ngovernance frameworks and oversight. These multifaceted\nchallenges underscore the need for interdisciplinary collab-\noration and a continued focus on building trustworthy and\nbeneficial AI systems.\nVIII. EMERGING TRENDS AND FUTURE DIRECTIONS\nThe field of Reinforcement Learning (RL) for Large Lan-\nguage Models (LLMs) is rapidly advancing, with several\nemerging trends poised to shape its future trajectory. One\nsignificant trend is the shift toward more advanced and effi-\ncient RL algorithms beyond PPO. This includes the adoption\nof offline RL methods, which reduce the need for costly\nonline data collection. Another development is the integration\nof alignment techniques more closely with the LLM archi-\ntecture, such as Direct Preference Optimization (DPO) and\nits variants [13], [33], [35]. These approaches aim to reduce\nthe computational burden and sample complexity associated\nwith traditional RLHF, making alignment more accessible and\nefficient. Another key direction is the increasing sophistica-\ntion of AI-driven feedback (RLAIF) and self-improvement\nmechanisms, where models learn to critique and refine their\nown outputs or learn from other AI systems. Furthermore,\nthere is an increasing emphasis on enhancing the reasoning\ncapabilities of LLMs through RL, moving beyond simple\npreference alignment to instill complex, multi-step problem-\nsolving skills, often involving verifiable rewards or process-\nbased supervision. This includes developing RL techniques\nthat can explicitly train models to generate coherent interme-\ndiate reasoning steps, crucial for tasks requiring deep logical\ninference and planning.\nLooking ahead, future research is likely to focus on more\nrobust and interpretable alignment techniques. One direction\nis moving beyond black-box reward models. Researchers aim\nto understand what values and preferences are being learned.\nThey also want to know how these influence model behav-\nior. Another area of interest is multi-objective reinforcement\nlearning. This helps balance conflicting goals like helpfulness,\nharmlessness, honesty, and fairness. Personalized alignment is\nalso a growing focus. Here, LLMs would adapt to individual\nuser preferences in a safe and controlled way. The integra-\ntion of reinforcement learning with other machine learning\nparadigms presents exciting avenues. Causal inference can\nhelp better understand model behavior. Unsupervised and self-\nsupervised methods may help discover reward signals without\nhuman labels. As LLMs gain the ability to interact with\nexternal tools and environments, multi-agent RL will become\nincreasingly important. These frameworks can enable training\nfor collaboration and social awareness. Ultimately, the grand\nchallenge is to build LLMs that are not only powerful but also\nsafe and aligned with human values. This will require ongoing\nadvances in RL techniques, evaluation strategies, and ethical\nsafeguards.\nIX. CONCLUSION\nThis survey provides a comprehensive exploration of re-\ninforcement learning techniques for large language models.\nIt shows how RL has grown from a simple fine-tuning\nmethod to a central approach in LLM development. The field\nhas progressed from Reinforcement Learning from Human\nFeedback (RLHF) to advanced methods like RLAIF, DPO,\nand GRPO. Each technique brings its own strengths and\nlimitations. Our analysis highlights key trade-offs between\nalignment methods. Traditional PPO-based RLHF performs\nexceptionally well, especially with very large models. In\ncontrast, newer approaches like UNA-score excel at improving\nfactual accuracy and instruction-following. RL techniques also\nshow strong potential in boosting reasoning abilities. Methods\nlike OB-RL, CoT-RO, and Verifier-Guided RL help improve\nmulti-step reasoning and logical consistency. Despite these\nadvancements, several challenges still remain. These include\nreward hacking, high computational costs, limited scalability\nof collecting high-quality feedback, and the risk of AI feed-\nback systems reinforcing their own biases. Looking forward,\nseveral promising research directions emerge. These include\ndeveloping more efficient algorithms and creating hybrid ap-\nproaches that combine the strengths of different methods.\nResearchers are also exploring multi-objective RL to balance\ncompeting goals. Integrating RL with other learning paradigms\nis another important path. Advancing hierarchical RL methods\nfor tool use and external resource integration is equally vital.\nTogether, these efforts aim to create language models that are\nmore helpful, harmless, and honest in serving human needs.\nREFERENCES\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D.\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners.\nAdvances in Neural Information Processing Systems, 33:1877–1901,\n2020.\n[2] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur,\nAlan Schelten, and Alex Vaughan. The llama 3 herd of models. arXiv\npreprint arXiv:2407.21783, 2024.\n[3] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda\nLu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, and Chong Ruan.\nDeepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.\n[4] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli.\nHallucination is\ninevitable: An innate limitation of large language models.\narXiv\npreprint arXiv:2401.11817, 2024.\n[5] Federico Bianchi and James Zou. Large language models are vulner-\nable to bait-and-switch attacks for generating harmful content. arXiv\npreprint arXiv:2402.13926, 2024.\n[6] Yubin Ge, Neeraja Kirtane, Hao Peng, and Dilek Hakkani-T¨ur. Llms\nare vulnerable to malicious prompts disguised as scientific language.\narXiv preprint arXiv:2501.14073, 2025.\n[7] Jingwei Yi, Rui Ye, Qisi Chen, Bin Zhu, Siheng Chen, Defu Lian,\nGuangzhong Sun, Xing Xie, and Fangzhao Wu. On the vulnerability\nof safety alignment in open-access llms. In Findings of the Association\nfor Computational Linguistics ACL 2024, pages 9236–9260, 2024.\n[8] Rudra Murthy, Praveen Venkateswaran, Prince Kumar, and Danish\nContractor. Evaluating the instruction-following abilities of language\nmodels using knowledge tasks. arXiv preprint arXiv:2410.12972, 2024.\n[9] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.\nWainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Kata-\nrina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe.\nTraining language models\nto follow instructions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730–27744, 2022.\n[10] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and\nOleg Klimov. Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347, 2017.\n[11] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard,\nJohan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune,\nAbhinav Rastogi, and Sushant Prakash. Rlaif vs. rlhf: scaling reinforce-\nment learning from human feedback with ai feedback. In Proceedings\nof the 41st International Conference on Machine Learning, ICML’24.\nJMLR.org, 2024.\n[12] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jack-\nson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini,\nCameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah,\nDanny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-\nJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael\nSellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston,\nShauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham,\nTimothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume,\nSamuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.\nConstitutional ai: Harmlessness from ai feedback.\narXiv preprint\narXiv:2212.08073, 2022.\n[13] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning,\nStefano Ermon, and Chelsea Finn.\nDirect preference optimization:\nYour language model is secretly a reward model. Advances in Neural\nInformation Processing Systems, 36:53728–53741, 2023.\n[14] Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao,\nKuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan\nCao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua\nLin, and Kai Chen. Exploring the limit of outcome reward for learning\nmathematical reasoning. arXiv preprint arXiv:2502.06781, 2025.\n[15] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu,\nBaolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao,\nWeizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen.\nReinforcement learning for reasoning in large language models with\none training example. arXiv preprint arXiv:2504.20571, 2025.\n[16] Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud,\nShauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski,\nAlex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman,\nEthan Perez, and Evan Hubinger. Sycophancy to subterfuge: Inves-\ntigating reward-tampering in large language models.\narXiv preprint\narXiv:2406.10162, 2024.\n[17] Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and\nYanghua Xiao. Reward shaping to mitigate reward hacking in rlhf.\narXiv preprint arXiv:2502.18770, 2025.\n[18] Min-Hsuan Yeh, Leitian Tao, Jeffrey Wang, Xuefeng Du, and Yixuan\nLi.\nHow reliable is human feedback for aligning large language\nmodels? arXiv preprint arXiv:2410.01957, 2024.\n[19] Archit Sharma, Sedrick Scott Keh, Eric Mitchell, Chelsea Finn, Kushal\nArora, and Thomas Kollar. A critical evaluation of ai feedback for\naligning large language models.\nAdvances in Neural Information\nProcessing Systems, 37:29166–29190, 2024.\n[20] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information processing systems,\n30, 2017.\n[21] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,\nand Ilya Sutskever.\nLanguage models are unsupervised multitask\nlearners. OpenAI blog, 1(8):9, 2019.\n[22] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and\nNoah A. Smith. RealToxicityPrompts: Evaluating neural toxic degen-\neration in language models.\nIn Trevor Cohn, Yulan He, and Yang\nLiu, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2020, pages 3356–3369, Online, November 2020. Association\nfor Computational Linguistics.\n[23] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and\nShmargaret Shmitchell.\nOn the dangers of stochastic parrots: Can\nlanguage models be too big?\nIn Proceedings of the 2021 ACM\nconference on fairness, accountability, and transparency, pages 610–\n623, 2021.\n[24] Shima Imani, Liang Du, and Harsh Shrivastava.\nMathprompter:\nMathematical reasoning using large language models. arXiv preprint\narXiv:2303.05398, 2023.\n[25] Saksham Sahai Srivastava and Ashutosh Gandhi. Mathdivide: Improved\nmathematical reasoning by large language models.\narXiv preprint\narXiv:2405.13004, 2024.\n[26] Christopher JCH Watkins and Peter Dayan.\nQ-learning.\nMachine\nlearning, 8:279–292, 1992.\n[27] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu,\nJoel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller,\nAndreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie,\nAmir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,\nDaan Wierstra, Shane Legg, and Demis Hassabis. Human-level control\nthrough deep reinforcement learning.\nNature, 518(7540):529–533,\n2015.\n[28] Ronald J Williams. Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning. Machine learning, 8:229–256,\n1992.\n[29] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex\nGraves, Timothy Lillicrap, Tim Harley, David Silver, and Koray\nKavukcuoglu. Asynchronous methods for deep reinforcement learning.\nIn International conference on machine learning, pages 1928–1937.\nPmLR, 2016.\n[30] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.\nSoft actor-critic: Off-policy maximum entropy deep reinforcement\nlearning with a stochastic actor. In International conference on machine\nlearning, pages 1861–1870. Pmlr, 2018.\n[31] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis\nof the multiarmed bandit problem.\nMachine learning, 47:235–256,\n2002.\n[32] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He,\nShuicheng Yan, and Bo An.\nQ*: Improving multi-step reasoning\nfor llms with deliberative planning. arXiv preprint arXiv:2406.14283,\n2024.\n[33] Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao,\nZiran Yang, and Yi Wu. Offline reinforcement learning for llm multi-\nstep reasoning. arXiv preprint arXiv:2412.16145, 2024.\n[34] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang,\nRunxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, and Xiao Bi.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforce-\nment learning. arXiv preprint arXiv:2501.12948, 2025.\n[35] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey\nLevine.\nOffline rl for natural language generation with implicit\nlanguage q learning. In International Conference on Learning Rep-\nresentations (ICLR), February 2023. Poster presentation.\n[36] Jianing Qi, Hao Tang, and Zhigang Zhu.\nVerifierq: Enhancing llm\ntest time compute with q-learning-based verifiers.\narXiv preprint\narXiv:2410.08048, 2024.\n[37] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai\nWang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, Limao Xiong,\nLu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng\nChang, Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran\nHuang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu,\nand Xuanjing Huang. Secrets of rlhf in large language models part i:\nPpo. arXiv preprint arXiv:2307.04964, 2023.\n[38] J Fernando Hernandez-Garcia and Richard S Sutton. Understanding\nmulti-step deep reinforcement learning: A systematic study of the dqn\ntarget. arXiv preprint arXiv:1901.07510, 2019.\n[39] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.\nConservative q-learning for offline reinforcement learning. Advances\nin neural information processing systems, 33:1179–1191, 2020.\n[40] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song,\nXiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya\nGuo. Deepseekmath: Pushing the limits of mathematical reasoning in\nopen language models. arXiv preprint arXiv:2402.03300, 2024.\n[41] Souradip Chakraborty, Amrit Bedi, Alec Koppel, Huazheng Wang,\nDinesh Manocha, Mengdi Wang, and Furong Huang. Parl: A unified\nframework for policy alignment in reinforcement learning from human\nfeedback.\nIn The Twelfth International Conference on Learning\nRepresentations, 2024.\n[42] Kimin Lee, Laura M Smith, and Pieter Abbeel.\nPebble: Feedback-\nefficient interactive reinforcement learning via relabeling experience\nand unsupervised pre-training. In International Conference on Machine\nLearning, pages 6152–6163, 2021.\n[43] Jongjin Park, Younggyo Seo, Jinwoo Shin, Honglak Lee, Pieter Abbeel,\nand Kimin Lee. SURF: Semi-supervised Reward Learning with Data\nAugmentation for Feedback-efficient Preference-based Reinforcement\nLearning.\nIn International Conference on Learning Representations\n(ICLR), March 2022. Poster.\n[44] Mudit Gaur, Amrit Singh Bedi, Raghu Pasupathu, and Vaneet Ag-\ngarwal.\nOn the sample complexity bounds in bilevel reinforcement\nlearning. arXiv preprint arXiv:2503.17644, 2025.\n[45] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge\nAkkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,\nSam Altman, and Shyamal Anadkat. Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774, 2023.\n[46] Noah Lee, Jiwoo Hong, and James Thorne. Evaluating the consistency\nof LLM evaluators. In Owen Rambow, Leo Wanner, Marianna Apidi-\nanaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert,\neditors, Proceedings of the 31st International Conference on Compu-\ntational Linguistics, pages 10650–10659, Abu Dhabi, UAE, January\n2025. Association for Computational Linguistics.\n[47] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring,\nJohn Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irv-\ning. Red teaming language models with language models. In Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings\nof the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 3419–3448, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics.\n[48] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu\nMei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo\nfor llm alignment? a comprehensive study. In Proceedings of the 41st\nInternational Conference on Machine Learning, ICML’24. JMLR.org,\n2024.\n[49] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah\nSiegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Hig-\ngins. Solving math word problems with process- and outcome-based\nfeedback.\nIn International Conference on Learning Representations\n(ICLR) 2023, February 2023. Poster.\n[50] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards,\nBowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever,\nand Karl Cobbe. Let’s verify step by step. In The Twelfth International\nConference on Learning Representations, 2023.\n[51] Jian Hu, Li Tao, June Yang, and Chandler Zhou. Aligning language\nmodels with offline learning from human feedback.\narXiv preprint\narXiv:2308.12050, 2023.\n[52] Zihan Ye, Oleg Arenz, and Kristian Kersting.\nLearning from less:\nGuiding deep reinforcement learning with differentiable symbolic\nplanning. arXiv preprint arXiv:2505.11661, 2025.\n[53] Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang,\nYining Zheng, Yuxin Wang, Zhangyue Yin, and Xipeng Qiu. R3-rag:\nLearning step-by-step reasoning and retrieval for llms via reinforcement\nlearning. arXiv preprint arXiv:2505.23794, 2025.\n[54] Z.Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang,\nWanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z.F.\nWu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun\nGao, Daya Guo, and Chong Ruan. Deepseek-prover-v2: Advancing\nformal mathematical reasoning via reinforcement learning for subgoal\ndecomposition. arXiv preprint arXiv:2504.21801, 2025.\n[55] Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen.\nEfficient rlhf: Reducing the memory usage of ppo.\narXiv preprint\narXiv:2309.00754, 2023.\n[56] Yizhao Jin, Greg Slabaugh, and Simon Lucas.\nAdapter-rl: Adap-\ntation of any agent using reinforcement learning.\narXiv preprint\narXiv:2311.11537, 2023.\n[57] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya\nRamesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, and\nAlec Radford. Gpt-4o system card. arXiv preprint arXiv:2410.21276,\n2024.\n[58] Yaxi Lu, Haolun Li, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin,\nZhiyuan Liu, Fangming Liu, and Maosong Sun. Learning to generate\nstructured output with schema reinforcement learning. arXiv preprint\narXiv:2502.18878, 2025.\n[59] Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/\nnews/claude-3-5-sonnet, June 2024.\n[60] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li,\nSainbayar Sukhbaatar, Jing Xu, and Jason Weston.\nSelf-rewarding\nlanguage models. In Proceedings of the 41st International Conference\non Machine Learning, ICML’24. JMLR.org, 2024.\n[61] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-\nKishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, and\nAlex Carney. Openai o1 system card. arXiv preprint arXiv:2412.16720,\n2024.\n[62] Google DeepMind.\nIntroducing gemini 2.0: Our new ai model\nfor the agentic era. https://blog.google/technology/google-deepmind/\ngoogle-gemini-ai-update-december-2024/, December 2024.\n[63] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde\nDe Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas\nJoseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke\nChan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe\nPetroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,\nElizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir\nBalaji, Shantanu Jain, William Saunders, Christopher Hesse, An-\ndrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie\nMayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba.\nEvaluating large language\nmodels trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[64] OpenAI.\nHumaneval: A human-centric code evaluation benchmark.\nhttps://github.com/openai/human-eval, July 2021.\n[65] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian\nSchrittwieser, R´emi Leblond, Tom Eccles, James Keeling, Felix\nGimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cy-\nprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov,\nJames Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,\nPushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol\nVinyals. Competition-level code generation with alphacode. Science,\n378(6624):1092–1097, 2022.\n[66] GitHub.\nResponsible use of github copilot code review.\nhttps://\ndocs.github.com/en/copilot/responsible-use-of-github-copilot-features/\nresponsible-use-of-github-copilot-code-review, May 2025.\n[67] RLHFlow. Self-rewarding reasoning llm. https://github.com/RLHFlow/\nSelf-rewarding-reasoning-LLM, March 2025.\n[68] OpenAI. Openai o3 and o4-mini system card, April 2025.\n[69] Max Liu, Chan-Hung Yu, Wei-Hsu Lee, Cheng-Wei Hung, Yen-Chun\nChen, and Shao-Hua Sun. Synthesizing programmatic reinforcement\nlearning policies with large language model guided search. In Interna-\ntional Conference on Learning Representations (ICLR) 2025, January\n2025. Poster.\n[70] Anthropic. Claude 3.5 sonnet model card addendum. https://www-cdn.\nanthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model\nCard Claude 3 Addendum.pdf, June 2024.\n[71] Geon-Hyeong Kim, Youngsoo Jang, Yu Jin Kim, Byoungjip Kim,\nHonglak Lee, Kyunghoon Bae, and Moontae Lee. Safedpo: A simple\napproach to direct preference optimization with enhanced safety. arXiv\npreprint arXiv:2505.20065, 2025.\n[72] Isaac Lim, Shaun Khoo, Watson Chua, Goh Jiayi, and Jessica Foo.\nSafe at the margins: A general approach to safety alignment in low-\nresource english languages–a singlish case study.\narXiv preprint\narXiv:2502.12485, 2025.\n[73] Google.\nGoogle ai updates: Bard and new ai features in search,\nFebruary 2023.\n[74] Ling Yang, Zhaochen Yu, Bin Cui, and Mengdi Wang. Reasonflux:\nHierarchical llm reasoning via scaling thought templates. arXiv preprint\narXiv:2502.06772, 2025.\n[75] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin,\nBaoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong.\nRetool: Reinforcement learning for strategic tool use in llms. arXiv\npreprint arXiv:2504.11536, 2025.\n[76] Qwen Team. Qwq-32b: Embracing the power of reinforcement learn-\ning. https://qwenlm.github.io/blog/qwq-32b/, March 2025.\n[77] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang,\nCheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, and Chonghua\nLiao. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv\npreprint arXiv:2501.12599, 2025.\n[78] Google\nDeepMind.\nGemini\n2.5:\nOur\nmost\nintelligent\nai\nmodel.\nhttps://blog.google/technology/google-deepmind/\ngemini-model-thinking-updates-march-2025/, March 2025.\n[79] The claude 3 model family: Opus, sonnet, haiku.\n[80] HuggingFaceH4. Math-500: A subset of the math benchmark, Decem-\nber 2023. Dataset hosted on Hugging Face.\n[81] Minghui Jia. Aime 2024: American invitational mathematics exami-\nnation 2024 dataset, February 2025.\n[82] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn,\nMohamed Amin, Le Hou, Kevin Clark, Stephen R. Pfohl, Heather\nCole-Lewis, Darlene Neal, Qazi Mamunur Rashid, Mike Schaeker-\nmann, Amy Wang, Dev Dash, Jonathan H. Chen, Nigam H. Shah, Sami\nLachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green,\nEwa Dominowska, Blaise Ag¨uera y Arcas, Nenad Tomaˇsev, Yun Liu,\nRenee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral,\nDale R. Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan\nKarthikesalingam, and Vivek Natarajan. Toward expert-level medical\nquestion answering with large language models.\nNature Medicine,\npages 1–8, 2025.\n[83] Sina Gogani-Khiabani, Varsha Dewangan, Nina Olson, Ashutosh\nTrivedi, and Saeid Tizpaz-Niari.\nTechnical challenges in maintain-\ning tax prep software with large language models.\narXiv preprint\narXiv:2504.18693, 2025.\n[84] Shuhe Wang, Shengyu Zhang, Jie Zhang, Runyi Hu, Xiaoya Li,\nTianwei Zhang, Jiwei Li, Fei Wu, Guoyin Wang, and Eduard Hovy.\nReinforcement learning enhanced llms: A survey.\narXiv preprint\narXiv:2412.10400, 2024.\n[85] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang,\nJingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin\nMeng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian\nRen, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards\nlarge reasoning models: A survey of reinforced reasoning with large\nlanguage models. arXiv preprint arXiv:2501.09686, 2025.\n[86] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and\nDouwe Kiela. Kto: Model alignment as prospect theoretic optimization.\narXiv preprint arXiv:2402.01306, 2024.\n[87] Jiwoo Hong, Noah Lee, and James Thorne.\nORPO: Monolithic\npreference optimization without reference model. In Yaser Al-Onaizan,\nMohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing,\npages 11170–11189, Miami, Florida, USA, November 2024. Associa-\ntion for Computational Linguistics.\n[88] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna\nChen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli,\nTom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds,\nDanny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec,\nLiane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared\nKaplan. Training a helpful and harmless assistant with reinforcement\nlearning from human feedback.\narXiv preprint arXiv:2204.05862,\n2022.\n[89] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi\nMunos, Mark Rowland, Michal Valko, and Daniele Calandriello. A\ngeneral theoretical paradigm to understand learning from human pref-\nerences.\nIn International Conference on Artificial Intelligence and\nStatistics, pages 4447–4455. PMLR, 2024.\n[90] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang,\nand Fei Huang. Rrhf: Rank responses to align language models with\nhuman feedback. Advances in Neural Information Processing Systems,\n36:10935–10950, 2023.\n[91] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin\nLi, and Houfeng Wang. Preference ranking optimization for human\nalignment.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 38, pages 18990–18998, 2024.\n[92] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry\nLepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige\nBailey, Zhifeng Chen, and ....\nPalm 2 technical report.\nTechni-\ncal Report 2023, Google Research, May 2023.\nAlso available as\narXiv:2305.10403.\n[93] Zhichao Wang, Bin Bi, Can Huang, Shiva Kumar Pentyala, Zixu James\nZhu, Sitaram Asur, and Na Claire Cheng. Una: unifying alignments of\nrlhf/ppo, dpo and kto by a generalized implicit reward function. arXiv\npreprint arXiv:2408.15339, 2024.\n[94] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand,\nGianna Lengyel, Guillaume Lample, Lucile Saulnier, L´elio Renard\nLavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed.\nMistral 7b. https://arxiv.org/abs/2310.06825, October 2023.\n[95] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang\nZhao, Chengqi Dengr, Chong Ruan, Damai Dai, and Daya Guo.\nDeepseek-v2: A strong, economical, and efficient mixture-of-experts\nlanguage model. arXiv preprint arXiv:2405.04434, 2024.\n[96] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Am-\nmar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari,\nJianmin Bao, Harkirat Behl, and .... Phi-3 technical report: A highly\ncapable language model locally on your phone.\nTechnical report\nmsr-tr-2024-12, Microsoft Research, April 2024.\nAlso available as\narXiv:2404.14219.\n[97] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur\nMensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot,\nDiego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna\nLengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud,\nLucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subra-\nmanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th´eophile\nGervet, Thibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William\nEl Sayed. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n[98] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nZhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang,\nand Guanting Dong.\nQwen2 technical report.\narXiv preprint\narXiv:2407.10671, 2024.\n[99] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk\nMichalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry,\nQuoc Le, and Charles Sutton. Program synthesis with large language\nmodels. arXiv preprint arXiv:2108.07732, 2021.\n[100] Meta AI. Llama 3.3 model card and release information. https://www.\nllama.com/docs/model-cards-and-prompt-formats/llama3 3/, 2024.\n[101] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang\nHu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizard-\ncoder: Empowering code large language models with evol-instruct. In\nInternational Conference on Learning Representations (ICLR) 2024,\nJanuary 2024. Poster.\n[102] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang,\nRunxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding\nZeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue\nZhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan\nWang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang\nYou, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen,\nYaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong\nRuan, Fuli Luo, and Wenfeng Liang.\nDeepseek-coder-v2: Breaking\nthe barrier of closed-source models in code intelligence. arXiv preprint\narXiv:2406.11931, 2024.\n[103] Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain,\nZachary Mueller, Harm de Vries, Leandro von Werra, Arjun Guha, and\nLingming Zhang. Selfcodealign: Self-alignment for code generation. In\nAdvances in Neural Information Processing Systems (NeurIPS 2024),\nvolume 37, 2024.\n[104] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa,\nCassidy Hardin, Surya Bhupatiraju, L´eonard Hussenot, Thomas Mes-\nnard, Bobak Shahriari, and Alexandre Ram´e.\nGemma 2: Improv-\ning open language models at a practical size.\narXiv preprint\narXiv:2408.00118, 2024.\n[105] Stephanie Lin, Jacob Hilton, and Owain Evans.\nTruthfulQA: Mea-\nsuring how models mimic human falsehoods. In Smaranda Muresan,\nPreslav Nakov, and Aline Villavicencio, editors, Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin, Ireland, May\n2022. Association for Computational Linguistics.\n[106] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang,\nHamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu,\nNouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf,\nJena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord,\nChris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep\nDasigi, and Hannaneh Hajishirzi. T¨ulu 3: Pushing frontiers in open\nlanguage model post-training. arXiv preprint arXiv:2411.15124, 2024.\n[107] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gul-\nrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.\nAlpacaeval: An automatic evaluator of instruction-following models.\nhttps://github.com/tatsu-lab/alpaca eval, May 2023.\n[108] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhang-\nhao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P.\nXing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-\nas-a-judge with mt-bench and chatbot arena.\nAdvances in Neural\nInformation Processing Systems, 36:46595–46623, 2023.\n[109] Bhashithe Abeysinghe and Ruhan Circi. The challenges of evaluating\nllm applications: An analysis of automated, human, and llm-based\napproaches. arXiv preprint arXiv:2406.03339, 2024.\n[110] Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang,\nWenwei Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, and Yingchun\nWang.\nFake alignment: Are LLMs really aligned well?\nIn Kevin\nDuh, Helena Gomez, and Steven Bethard, editors, Proceedings of the\n2024 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Volume\n1: Long Papers), pages 4696–4712, Mexico City, Mexico, June 2024.\nAssociation for Computational Linguistics.\n[111] Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger,\nMonte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax,\nJack Chen, David Duvenaud, Akbir Khan, Julian Michael, S¨oren Min-\ndermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan,\nBuck Shlegeris, Samuel R. Bowman, and Evan Hubinger. Alignment\nfaking in large language models.\narXiv preprint arXiv:2412.14093,\n2024.\n",
    "content": "# **Interpretation and Analysis: Application of Reinforcement Learning in Large Language Models**\n\n## 1. Core Content and Key Contributions\n\nThis paper, *“A Technical Survey of Reinforcement Learning Techniques for Large Language Models”*, presents a comprehensive overview of how reinforcement learning (RL) techniques are applied to enhance and align large language models (LLMs), addressing challenges in instruction following, ethical alignment, and reasoning capabilities.\n\n### **Core Content**\n- **Integration of RL with LLMs**: Introduces the basic RL framework (e.g., Markov Decision Processes, policy gradient methods) and discusses how they can be adapted for LLMs. Due to the vast action space (large vocabulary size) and subjective, sparse reward signals in LLMs, traditional RL approaches must be modified accordingly.\n  \n- **Mainstream RL Algorithms**:\n  - **PPO (Proximal Policy Optimization)**: The core algorithm in the widely used RLHF (Reinforcement Learning from Human Feedback) framework.\n  - **Q-learning and Off-policy Methods**: Such as ILQL (Implicit Language Q-Learning) and VerifierQ, suitable for offline training scenarios.\n  - **Group Relative Policy Optimization (GRPO)**: Improves training efficiency through relative advantage estimation, particularly effective for complex reasoning tasks.\n\n- **Alignment Techniques**:\n  - **RLHF**: A three-stage process based on human feedback (supervised fine-tuning, reward modeling, policy optimization) remains dominant.\n  - **RLAIF (AI Feedback)**: Uses AI evaluators instead of manual annotations to reduce costs but may introduce biases.\n  - **Constitutional AI**: Introduces \"constitutional\" principles for self-critique and correction to enhance safety.\n  - **DPO (Direct Preference Optimization)**: Simplifies the RLHF process by directly optimizing preference data, avoiding explicit reward modeling.\n\n- **Reasoning Enhancement Techniques**:\n  - **Outcome-Based RL**: Rewards based solely on the correctness of final answers.\n  - **Chain-of-Thought Reward Optimization (CoT-RO)**: Provides feedback at each step of reasoning to improve logical consistency.\n  - **Verifier-Guided RL**: Uses verifiers to guide reasoning paths.\n  - **Debate & Self-Play**: Multi-agent interactions reveal errors and enhance robustness.\n  - **Tool-Augmented Reasoning**: Integrates external tools (e.g., calculators, code interpreters) into reasoning processes.\n  - **Program-Synthesis RL**: Generates code that passes tests.\n\n- **Application Scenarios**:\n  - Instruction following (InstructGPT, Claude, Llama series)\n  - Code generation (Codex, AlphaCode, StarCoder)\n  - Ethical alignment (safe, unbiased outputs)\n  - Tool usage (function calls, browser interaction)\n  - Reasoning abilities (mathematical proofs, scientific problem-solving)\n\n- **Challenges and Future Directions**:\n  - Reward hacking\n  - High computational cost\n  - Quality and representativeness of feedback\n  - Multi-objective alignment (helpfulness, harmlessness, honesty)\n  - Offline RL, hybrid RL, verifier-guided training\n\n### **Key Contributions**\n- Provides a systematic technical perspective summarizing current mainstream RL methods and their applications in LLM alignment and reasoning enhancement.\n- Proposes a taxonomy analyzing RL techniques from the perspectives of reward modeling, feedback mechanisms, and optimization strategies.\n- Offers comparative analysis of different RL methods across multiple domains (e.g., alignment, reasoning, code generation).\n- Analyzes limitations of current RL approaches and suggests future research directions (e.g., hybrid RL, verifier guidance, multi-objective optimization).\n\n---\n\n## 2. Innovation and Breakthrough Points\n\n### **Theoretical Innovations**\n- **Unified Reward Function Framework (UNA)**: Unifies RLHF, DPO, KTO, etc., under an implicit reward function framework, transforming them into supervised learning forms to reduce training complexity.\n- **Offline RL Methods** (e.g., CQL, Offline DPO): Reduces reliance on online interaction and improves sample efficiency.\n- **Verifier-Guided RL** (e.g., VerifierQ, RLVR): Leverages automated verifiers to provide finer-grained feedback, enhancing reasoning quality.\n- **Hybrid RL Methods** (e.g., PPO+DPO): Combines advantages of multiple RL techniques for more robust training.\n\n### **Engineering Practice Breakthroughs**\n- **Large-scale Model Training**: Demonstrates successful application of RL in ultra-large LLMs (e.g., DeepSeek, Qwen32B).\n- **Efficient Training Methods** (e.g., LoRA-PPO, Adapter-based Q-learning): Lowers computational resource requirements, making RL fine-tuning feasible on consumer-grade hardware.\n- **Real-world Applications**: Shows practical value of RL-enhanced LLMs in healthcare, education, law, finance, and other fields.\n\n### **Performance Improvements**\n- Significant improvements in accuracy, consistency, and safety on multiple benchmarks (e.g., TruthfulQA, IFEval, GSM8K) using RL methods.\n- Mechanisms like KL-divergence penalties and entropy regularization enhance training stability and reduce catastrophic forgetting.\n\n---\n\n## 3. Startup Project Suggestions\n\nBelow are startup project ideas derived from this paper, combined with current technological and market trends:\n\n---\n\n### 🧠 **Project One: AI-Powered Ethical Alignment and Content Moderation Platform**\n\n#### **Background**\nWith the widespread use of AI-generated content (AIGC), ensuring safe, unbiased, and ethically aligned outputs has become critical. Methods such as RLHF, RLAIF, and Constitutional AI discussed in the paper can be used to build robust ethical alignment systems.\n\n#### **Product Description**\nDevelop an **automated ethical alignment and content moderation platform** for enterprise clients (e.g., social media platforms, news agencies, educational platforms) to perform real-time auditing and optimization of AI-generated content, preventing the spread of harmful, discriminatory, or false information.\n\n#### **Core Technologies**\n- Use RLHF/RLAIF to build reward models that detect harmful content.\n- Apply Constitutional AI principles to define “ethical guidelines” and enable self-correction.\n- Integrate DPO, SafeDPO, and similar technologies to optimize content generation.\n\n#### **Business Model**\n- **SaaS Subscription**: Charge based on API call volume.\n- **Custom Solutions**: Provide compliance-oriented content moderation services for governments, financial institutions, and healthcare providers.\n- **Open Source Plugins**: Offer lightweight SDKs for developer communities.\n\n#### **Market Opportunity**\n- Growing demand from digital media companies, social platforms, and edtech firms for AI content compliance.\n- Aligns with regulatory frameworks like the EU AI Act and China’s *Administrative Measures for Generative AI*.\n\n---\n\n### 🤖 **Project Two: Enterprise-Level Intelligent Assistant Training Platform**\n\n#### **Background**\nMany enterprises aim to train intelligent assistants that understand specific business processes and possess domain-specific knowledge. RL techniques mentioned in the paper can significantly enhance LLM reasoning, tool usage, and personalized adaptation.\n\n#### **Product Description**\nCreate an **enterprise-level intelligent assistant training platform** allowing users to upload industry knowledge bases, historical conversations, and task templates, using RL to train highly customized internal AI assistants.\n\n#### **Core Technologies**\n- Use **Hierarchical RL** to distinguish high-level task planning from low-level execution.\n- Apply **Self-Rewarding RL** for self-assessment of task completion.\n- Combine **tool-augmented reasoning** (HRL-TAR) to support database queries, API calls, etc.\n\n#### **Business Model**\n- **On-demand Training Services**: Enterprises upload data; the platform trains customized models.\n- **Private Deployment Options**: Provide on-premise versions for sensitive industries.\n- **API Integration Services**: Embed into existing customer service, CRM, ERP systems.\n\n#### **Market Opportunity**\n- Strong demand for vertical AI assistants in finance, healthcare, law, manufacturing.\n- Market gap for SMEs lacking resources to train their own models.\n\n---\n\n### 💻 **Project Three: AI Programming Assistant & Code Review System**\n\n#### **Background**\nCode generation is now a major LLM application. As noted in the paper, Program-Synthesis RL, RLEIF, SelfCodeAlign, and similar techniques can significantly improve code generation accuracy and security.\n\n#### **Product Description**\nDevelop an **AI programming assistant and code review tool** that not only helps programmers write code but also detects potential vulnerabilities, optimizes performance, and enhances code readability.\n\n#### **Core Technologies**\n- Train code generation models using **RL**, enabling generated code to pass unit tests.\n- Introduce a **static analysis + RL joint scoring system** to improve code quality.\n- Support **multi-language, multi-IDE plugins**, compatible with Python, Java, C++, etc.\n\n#### **Business Model**\n- **Developer Subscription**: Monthly fee for individual developers.\n- **Enterprise Collaboration Edition**: Supports shared knowledge bases and collaborative model training.\n- **Competitor to GitHub Copilot**: Targets higher security and customization.\n\n#### **Market Opportunity**\n- Massive global developer base with strong demand for programming assistance tools.\n- Increasing emphasis on code quality and security in enterprises.\n\n---\n\n### 📚 **Project Four: Personalized Learning AI Tutor**\n\n#### **Background**\nAs noted in the paper, methods like RLHF, DPO, and Self-Rewarding LLMs can be used for personalized teaching and tutoring. AI can dynamically adjust explanation difficulty based on student progress, even simulating Socratic questioning to guide thinking.\n\n#### **Product Description**\nDevelop a **personalized AI learning tutor platform** covering K12 to higher education, supporting subjects like math, physics, and computer science.\n\n#### **Core Technologies**\n- Use **Step-wise CoT-RO** to strengthen reasoning chain coherence.\n- Implement **Self-Rewarding mechanisms** where student models assess solution reasonableness.\n- **Multi-turn Dialogue Understanding + Knowledge Tracing** to dynamically adjust learning paths.\n\n#### **Business Model**\n- **Subscription Education Platform**: Parents/schools subscribe per semester.\n- **Teacher Management Dashboard**: Assign homework, track student progress.\n- **Corporate Training Partnerships**: Provide skill development courses for employees.\n\n#### **Market Opportunity**\n- Rapid growth in the EdTech sector with huge potential for AI tutoring systems.\n- Strong unmet demand for personalized learning in both K12 and adult education.\n\n---\n\n### 🔬 **Project Five: Scientific Paper Writing & Peer Review Assistance System**\n\n#### **Background**\nThe academic community increasingly needs tools for high-quality paper writing and peer review. Techniques like RL-enhanced reasoning, verifiable rewards, and debate-style training mentioned in the paper can improve logical structure, readability, and rigor in academic writing.\n\n#### **Product Description**\nBuild an **AI paper writing and peer review system** to help researchers outline papers, optimize arguments, detect potential errors, and even simulate reviewer suggestions.\n\n#### **Core Technologies**\n- **Verifier-Guided RL**: Evaluates paper logic and experimental design via verifiers.\n- **Debate-Style Training**: Enables two models to question each other's conclusions, deepening argumentation.\n- **Preference Optimization (DPO, KTO)**: Optimizes writing style based on expert feedback.\n\n#### **Business Model**\n- **Academic Institution Partnerships**: Universities and research institutes purchase for writing assistance.\n- **Open Platform + Premium Subscriptions**: Free basic version + paid premium features (e.g., plagiarism checks, formatting optimization).\n- **Journal/Conference Collaborations**: Provide initial review suggestions to lighten reviewers' workload.\n\n#### **Market Opportunity**\n- Expanding academic publishing market with growing interest in AI-assisted writing.\n- High acceptance and willingness to pay among researchers for efficiency tools.\n\n---\n\n## ✅ Summary\n\n| Project | Technical Foundation | Business Model | Market Potential |\n|--------|----------------------|----------------|------------------|\n| **Ethical Alignment Platform** | RLHF, DPO, Constitutional AI | SaaS, Customization | High (policy-driven) |\n| **Enterprise Intelligent Assistant Platform** | PPO, HRL, Self-Rewarding | API Services, Private Deployment | High (Vertical AI Demand) |\n| **AI Programming Assistant** | RLAIF, RLVR, SelfCodeAlign | Developer Subscription, Enterprise Edition | Very High (DevOps Ecosystem) |\n| **Personalized Learning Tutor** | CoT-RO, RLHF, Preference Optimization | Education Subscription, Teacher Tools | High (Education Equity) |\n| **Paper Writing & Review System** | Verifier-Guided RL, Debate Training | Academic Institutions, Open Platform | Medium-High (Research Efficiency) |\n\nThese projects align with current AI and LLM development trends, offer clear technical foundations, and present viable commercial opportunities—making them promising directions for entrepreneurial exploration.",
    "github": "",
    "hf": ""
}