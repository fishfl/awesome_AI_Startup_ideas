{
    "id": "2510.15842",
    "title": "Paper2Web: Let's Make Your Paper Alive!",
    "summary": "This article introduces Paper2Web, a benchmark dataset and multidimensional evaluation framework for assessing academic web page generation, as well as a self-contained pipeline named PWAgent for converting scientific papers into interactive and multimedia-rich academic homepages.",
    "abstract": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Yuhang Chen,Tianpeng Lv,Siyi Zhang,Yixiang Yin,Yao Wan,Philip S. Yu,Dongping Chen",
    "subjects": [
        "Computation and Language (cs.CL)",
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "comments": "Comments:Under Review. Checkthis https URLfor the unified platform to streamline all academic presentation",
    "keypoint": "Introduces PAPER2WEB, a new task to transform academic papers into interactive websites.\nPresents a dataset of 10,716 papers with human-created project homepages for evaluation.\nProposes PWAGENT, a multi-agent framework using MCP tools for webpage generation.\nPWAGENT iteratively refines content and layout through emphasis, balance, and presentation optimization.\nAchieves higher connectivity and completeness compared to baselines like arXiv HTML and alphaXiv.\nOutperforms end-to-end models in holistic evaluation metrics including interactivity, aesthetics, and informativeness.\nIntroduces PaperQuiz, a QA-based metric to assess knowledge retention from webpages.\nIncludes a verbosity penalty in evaluation to discourage overly text-heavy pages.\nDemonstrates cost efficiency with PWAGENT requiring only $0.025 per website on average.\nSets a Pareto-front in academic webpage generation by balancing quality and cost effectively.",
    "date": "2025-10-21",
    "paper": "Preprint. Under Review.\nPAPER2WEB: LET’S MAKE YOUR PAPER ALIVE!\nYuhang Chen1∗, Tianpeng Lv1∗, Siyi Zhang1, Yixiang Yin1, Yao Wan1†,\nPhilip S. Yu2, Dongping Chen3‡\n1ONE Lab, Huazhong University of Science and Technology,\n2University of Illinois Chicago, 3University of Maryland\n{u202315752, wanyao}@hust.edu.cn, dongping@umd.edu\n∗Equal Contribution. † Corresponding author. ‡Project Lead.\n Project Website: https://francischen3.github.io/P2W_Website\n§ Code Repository: https://github.com/YuhangChen1/Paper2All\nSlide：\nPresentagent\nPPTagent\nConvert the paper into slides\nSocial Media\nDissemination\nAutoPR\nSummarize the paper for social media\nPoster：\nP2P\nPaper2Poster\nPostergen\nVideo：\nPaper2video\nTransform paper into video presentation\nAgent:\nPaper2Agent\nAcademic \nPresentation\nWebsite：\nPaper2web\nTransform the paper into \n   interactive website\nConvert the paper into AI assistant\nConvert paper into conference poster\nFigure 1: Our work, PAPER2WEB, constitutes an important piece of the puzzle for the presentation and dissem-\nination of academic papers. We build a unified platform to streamline all academic presentation at Paper2All.\nABSTRACT\nAcademic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction. How-\never, current approaches such as direct Large Language Model (LLM) generation,\ntemplates, or direct HTML conversion struggle to produce layout-aware, inter-\nactive sites, and a comprehensive evaluation suite for this task has been lack-\ning. In this paper, we introduce PAPER2WEB, a benchmark dataset and multi-\ndimensional evaluation framework for assessing academic webpage generation.\nIt incorporates rule-based metrics like Connectivity, Completeness and human-\nverified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness),\nand PaperQuiz, which measures paper-level knowledge retention.\nWe further\npresent PWAGENT, an autonomous pipeline that converts scientific papers into\ninteractive and multimedia-rich academic homepages. The agent iteratively re-\nfines both content and layout through MCP tools that enhance emphasis, balance,\nand presentation quality. Our experiments show that PWAGENT consistently out-\nperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv\nversions by a large margin while maintaining low cost, achieving the Pareto-front\nin academic webpage generation.\n1\narXiv:2510.15842v1  [cs.CL]  17 Oct 2025\nPreprint. Under Review.\n1\nINTRODUCTION\nResearch papers are predominantly distributed in PDF format, conveying information solely through\nstatic text and images (Tkaczyk et al., 2015; Li et al., 2020; Clark & Divvala, 2016; Lo et al., 2020).\nHowever, PDFs offer limited support for interactivity and multimedia content (W3C Web Accessi-\nbility Initiative, 2018; Government Digital Service and Central Digital and Data Office, 2024; NHS\nDigital, 2025; Kumar & Wang, 2024), resulting in substantial information loss during dissemina-\ntion (Tkaczyk et al., 2015; Li et al., 2020). As a result, transforming academic papers into more\nvisual and accessible formats has emerged as a promising direction for enhancing scholarly commu-\nnication and accelerating knowledge dissemination (Fischhoff, 2013; Thorlacius, 2007).\nRecently, growing efforts have sought richer and more efficient ways to transform scholarly arti-\ncles—such as converting papers into concise posters with Paper2Poster (Pang et al., 2025), pre-\nsentation slides with PresentAgent (Shi et al., 2025), videos with Paper2Video (Zhu et al., 2025),\npublic-facing content with AutoPR (Chen et al., 2025a). However, these approaches either discard\nthe fine-grained details present in the original text or retain only the main ideas while overlooking the\ncommunicative advantages of multimedia content such as videos and animated graphics. This cre-\nates a gap for formats that preserve core textual knowledge while seamlessly integrating multimedia\nto enhance scientific communication across diverse communities.\nCompared with the above methods, an online web page can integrate textual content with multime-\ndia and present in a coordinated and navigable manner. As illustrated in Figure 6, well-designed\nwebpages can bridge the gap between scholarly content and interactive digital presentation, thereby\nenabling broader and more effective dissemination of research outcomes. However, this poses chal-\nlenges in requiring deliberate spatial organization to accommodate rich media and interactive com-\nponents. Recent efforts have explored converting full academic papers into web pages to broaden\naccessibility and dissemination. The arXiv HTML initiative (Frankston et al., 2024) is one represen-\ntative example, yet such approaches often produce disordered layouts and redundant text, reducing\nreadability, precision, and cross-device accessibility. As illustrated in Figure 2, common failure\nmodes include rigid figure grids with inconsistent scaling, detached captions, missing responsive-\nness, and limited interactivity. AlphaXiv leverages LLMs for content condensation and layout op-\ntimization, yet it still limits author control over multimedia placement and visual design, resulting\nin largely static presentations that fail to fully exploit interactive capabilities. As noted by prior\nwork (Frankston et al., 2024), these issues stem from TeX–HTML pipelines that emulate LaTeX\nbehavior without executing a full TeX engine, leading to missing structures and visual inconsisten-\ncies. On the other hand, directly LLM-driven webpage generation also struggles to process long\ncontexts (Liu et al., 2024b; Hsieh et al., 2024) and to effectively integrate multimedia content while\nmaintaining robust interactivity (Xiao et al., 2024a).\nimage overﬂow\n(a) Web page of arXiv HTML version.\nImage–text      \nImbalance\n(b) Web page generated by alphaXiv.\nFigure 2: Problems in current scholar web page generation, including distorted layout and limited interactivity.\n2\nPreprint. Under Review.\nFigure 3: Pareto-front comparison of each website gen-\neration methods. Our PWAGENT achieve the highest\nquality with moderate and affordable cost.\nOur Work: PAPER2WEB. In this paper, we\nintroduce PAPER2WEB, a new task that aims\nto transform full academic papers into interac-\ntive websites that preserve core content while\nintegrating multimedia and improving usabil-\nity.\nWe begin by constructing a dataset of\npaired academic papers and their correspond-\ning webpages. Specifically, we crawl accepted\npapers from selected conferences, parse their\ntexts to extract reliable metadata such as au-\nthorship, and augment each record with cita-\ntion counts from Semantic Scholar. We then\nperform multi-stage filtering: using cues in the\npaper body and related code repositories to lo-\ncate candidate project homepages, employing\nan LLM to assess their relevance, and rely-\ning on human annotators to resolve ambiguous\ncases. This pipeline yields 10,700 papers with\nverified homepages, forming the basis for our analysis of effective research websites.\nTo address these limitations, we propose PWAGENT, a multi-agent framework for transforming\nscholarly documents into structured and interactive web content. PWAGENT first decompose a\npaper into structured assets and organize links and executable artifacts under a unified schema. It\nthen performs Model Context Protocol (MCP) ingestion to construct a semantically aligned resource\nrepository enriched with relational metadata and exposed through standardized tools for downstream\nuse. A content-aware allocation heuristic estimates each asset’s spatial footprint and assigns provi-\nsional layout budgets to guide rendering and navigation. Finally, agent-driven iterative refinement\ndrafts an initial website, inspects rendered views, and issues targeted edits via tool calls to correct vi-\nsual imbalance, enhance information hierarchy, and appropriately anchor multimedia elements. This\nloop alternates between global assessment and localized adjustment, linking segmented screenshots\nto corresponding HTML fragments for precise editing.\nUsing the PAPER2WEB dataset, we also construct a benchmark for PAPER2WEB. We introduce the\nfirst metric to measure the interactivity and dynamic elements of the webpage, as well as Connec-\ntivity and Completeness, human-assisted MLLM-as-a-Judge for comprehensive assessments. Fur-\nthermore, we propose PaperQuiz to evaluate knowledge transfer from webpage screenshots through\nboth verbatim and interpretive questions, incorporating a verbosity penalty to discourage overly text-\nheavy pages. On this benchmark, PWAGENT improves connectivity and completeness by roughly\n12% on average across methods, achieving a 28% gain over the arXiv HTML baseline. It also yields\nan 18% average improvement via MLLM-as-a-Judge and triples the average score of the strongest\nend-to-end baseline and remains competitive with template-assisted variants.\nContributions. The key contributions of this paper are as follows:\n• A New Task, Dataset and Evaluation Suite. We build the PAPER2WEB dataset, a large-scale\ncorpus that links scientific papers to their corresponding project homepages, enabling quantitative\nanalysis of web-based academic dissemination.\n• Comprehensive Benchmark. We establish a benchmark with autonomous metrics aligned well\nwith human preference to comprehensively assess the quality of web page generation, reveal prob-\nlems within current automatic webpage generation methods.\n• A State-of-the-Art Automatic Approach. We propose PWAGENT, a MCP-based agent for the\nend-to-end transformation of academic papers into structured,interactive pages.\n2\nPAPER2WEB: A NEW TASK AND DATASET\nSince no dataset exists for analyzing academic website content and layout, we collect data from\nrecent AI papers. We harvest project links from papers and code repositories, then crawl the corre-\nsponding webpage. Finally, we collect a comprehensive dataset covering multiple conferences and\n3\nPreprint. Under Review.\nAuthor\nCitations\nYear\nCategory\nConference\nPaper metadata collection\nProject website search\nSearch Source\nPaper\nContent\nGithub\nRepo\nSearch for URL\nHuman \nJudge\nLLM \nJudge\nFinal：\nONLY ONE URL\nAnd More…\nFigure 4: To transform static papers into exploratory web pages, we collect the first paper-webpage dataset by\ncrawling across multiple top-tier conferences and filtering by online search and human annotators.\ncategories with 10,716 papers and their human-created project homepages. Figure 4 presents our\ndata collection pipeline.\n2.1\nDATA COLLECTION\nPaper Metadata Collection. We focus on AI papers as they are recent, peer-reviewed, cover diverse\nsubfields with varied modalities, and attract attention that motivates high-quality dissemination. Us-\ning automated tools, we collect papers from major AI conferences (ICML, NeurIPS, WWW, ICLR,\netc., 2020-2025). We extract source links, parse full texts for metadata (title, authors, venue, year),\nand retrieve citation counts from Semantic Scholar. Each paper’s introduction is submitted to an\nLLM that assigns one of thirteen topical categories (Figure 6, right panel), enabling standardized\ncross-paper analysis.\nInteractive\n     9.9%\n         \n    Multimedia\n        38.9%\n         \nMultimedia\nInteractive\n     9.8%\n         \n    Static\n     42.4%\n         \nFigure 5: PAPER2WEB dataset\nstatistics.\nProject Website Search. Our pipeline retrieves external links from\neach paper and its code repository, scanning the paper body and\nREADME files. We parse local context around each link, crawl\nthe target HTML, and use an LLM to analyze the content. Human\nreviewers resolve ambiguous cases to ensure each paper maps to at\nmost one canonical project website. Papers lacking relevant links\nin either source are defined as having no project homepage.\n2.2\nDATA CHARACTERISTICS\nFinally, we curate a comprehensive dataset comprising 10,716 papers with human-created project\nhomepages and 85,843 without. We group papers into 13 categories following ICML/NeurIPS/I-\nCLR conference taxonomies. The right panel of Figure 6 shows computer vision has the strongest\ndemand for project websites, with homepage adoption rising steadily in recent years. To charac-\nterize webpage features, we manually audited 2,000 samples. We define interactive sites as pages\nwith dynamic behaviors and explorable components responding to user intent; multimedia pages\nas those embedding rich media like videos; and static sites as pages delivering primarily text and\nstill images in linear presentation. Figure 5 shows the distribution by feature set. While many\npages remain static, multimedia dissemination through embedded videos and animations is notable.\nInteractive capabilities that enhance user experience remain comparatively rare and unevenly im-\nplemented, representing the first systematic characterization of interactive behavior and multimedia\norchestration in academic webpages.\n3\nEVALUATION METRICS\nTo systematically assess the quality of generated academic web pages, we introduce the PA-\nPER2WEB benchmark.\nThis comprehensive metric suite centers on the dual principles of in-\nformation efficiency and a balanced text–visual composition.\nThe framework evaluates web\npages across three key dimensions: (1) Connectivity & Completeness, (2) Holistic Evaluation via\n4\nPreprint. Under Review.\n0-24\n25-49\n50-74\n75-99\n100-124\n125-149\n175-199\n200-224\n225-249\n250-274\n275-299\n300-349\n350+\n150-174\n0%\n20%\n40%\n60%\n80%\n100%\nPercentage\n1500-1600\n1400-5000\n1000-1100\n0%\n5%\n10%\n15%\n20%\n30%\n25%\n35%\n40%\n1200-1300\n1300-1400\n1600-1700\n1700-1800\n1800-1900\n1900-2000\n2000+\nWith Website\n       Without Website\n33.76%\n18.83%\n12.25%\n9.75%\n7.76%\n5.93%\n1.82%\n2.99%\n1.81%\n1.67%\n1.31%\n0.68%\n \n ICML5.73%\n2021  9.33% \n AAAI\n4.68%\n \n 11.68%\n1.24%\n2025   \n21.88% \n2023   \n18.24% \n2022  \n10.74% \n2021  9.33% \nOther\nYears \n5.76% \n   2024  \n34.04% \n  28.49%\n \n 15.42%\n \n NIPS\n14.9%\n \n ICML\n5.73%\n AAAI\n4.68%\nEMNLP\n3.67%\nOthers \n ACL\n3.67%\nMultimodal\n       Learning\n 3D Vision and \nComputational \n       Graphics\nGeneration\n Model\nSpeech and \nAudio Process\nAI for Science\nML System and\nInfrastructure\nDeep learning \nArchitecture\nProbabilistic\nInference\n Reinforcement \n Learning \n    \nNature Language\nUnderstanding\nML Theory \nand Optimization \n    \nInformation \nRetrieval       \nRecommend System   \n    \nTrustworthy\n    \nPaper\nWith\nWebsite\n \n 12.48%\nCitation Range\nCitation Range\nFigure 6: The right panel shows the categorization of our data. We divided the dataset into 13 categories\nand counted items in each. In addition, we show distributions by conference and by year. The top-left panel\npresents, for each category, the relative proportions of papers without and with a website among papers with low\ncitation counts. The bottom-left panel depicts the distribution of papers without and with a website restricted\nto highly cited papers (those with over 1,000 citations).\nHuman/MLLM-as-a-Judge, and (3) PaperQuiz, which measures how effectively the website trans-\nfers knowledge.\n3.1\nCONNECTIVITY & COMPLETENESS\nThis metric jointly evaluates the hyperlink quality and structural fidelity of generated web pages.\nBoth indicators are assessed through LLM analysis of the HTML source code, supplemented by\nhuman evaluation for reliability. For connectivity, we examine how effectively the webpage links\ninternal and external resources to support coherent navigation and information access. To reduce\nevaluation bias, a dedicated URL parser is employed to count and verify valid hyperlinks, ensur-\ning objective measurement of link quality. For completeness, we measure how well the generated\nwebpage reproduces the core sections of the source paper. To enhance consistency, two quantita-\ntive priors, image–text balance and information efficiency, are applied to further evaluate structural\nintegrity and content compactness.\nImage–Text Balance Prior. Let D denote the weighted deviation between the observed image–text\nratio and the ideal 1:1 balance, and let γ > 0 be a scaling factor (Pang et al., 2025). We define the\npenalty term and score as:\nζ =\n5\n1 + γ · D ,\nSimg-txt = 5 −ζ .\n(1)\nInformation Efficiency Prior. To encourage concise, information-dense presentation, let r = L/W\ndenote the ratio between the generated text length L and the median human-designed length W, with\nβ > 0 a scaling factor (e.g., β=0.6) (Tufte & Graves-Morris, 1983). We define the efficiency as:\np(r) =\n5\n1 + β · max(0, r −1) .\n(2)\n5\nPreprint. Under Review.\nCompleteness\nImage–Text Balance\n Information Efficiency\n{‘Scores’:4/5,\n  ‘Reason’:After analyzing the HTML code\n with its nested CSS and JavaScript, I \nfound that the webpage has many \nSections…}\n{Score: 3.5/5\nReason:The webpage has a navigation \nbar, author, title, academic \ninstitution, and a complete citation \nformat, but it is missing...\"}\nHuman\nEvaluation\nLLM as a Judge\nConnectivity\nRule-Based Parser\nExternal/Internal links\n{‘Scores’:3.6/5,\n  ‘Reason’:After analyzing the HTML code\n with its nested CSS and JavaScript, I \nfound that the webpage has many \ninternal navigation links…}\nLLM as a Judge\n{Score: 3.8/5\nReason:The navigation bar allows users \nto jump directly to different sections. \nThe external links mention the \nauthor's homepage and a code \nrepository.\"}\nPaperQuizQA\nPaper\nQuestion Generation\nYou are a highly precise \nQuestion-Generation\nagent…\nQuiz\nQ2  : Which tuition\n is identified as \naffiliation …\nQ1 : Who is listed \nas the first author \nof the paper?\nVerbatim \nQuestions\nInterpretive \nQuestions\nWebsite Screenshot\nAnswer\nAnswer\nAccuracy:\n 93%\nAccuracy:\n 50%\nExaminee\nBetter！\nPenalty for \nlong text\nMLLM/Human Judge\nBaseline & \nOur Method Website\nInteractive\nAesthetic\nInformative\nMLLM as a Judge\n{‘Criteria’：Aesthetic\n‘Scores’:2.0/5,\n‘Reason’:The overall aes-\nthetics are poor due to \nthe excessive amount of \nempty space}\nHuman\nEvaluation\n{‘Criteria’:Interactive,\n‘Score’: 3/5\n‘Reason’:On the whole, the \nquality is adequate. … offer \na rich collection of links to \nreference materials...\"}\nHuman\nEvaluation\nFigure 7: Our evaluation metrics include multiple modules: (1) Connectivity and Completeness by parsing\nHTML links and structure with image–text balance and information-efficiency priors, (2) an MLLM/Human\nJudge to rate interactivity, aesthetics, and informativeness in a holistic manner, and (3) a QA PaperQuiz on\nwebpage screenshots with a verbosity penalty.\n3.2\nHOLISTIC EVALUATION WITH HUMAN-VERIFIED MLLM-AS-A-JUDGE\nTo evaluate the overall effectiveness of web pages at a holistic level, we employ a MLLM as an au-\ntomated judge, combined with human verification to mitigate bias. The model outputs a quantitative\nscore ranging from 1 to 5 for each webpage. Specifically, it evaluates three key dimensions: Inter-\nactive, which measures element responsiveness, saliency emphasis, and overall usability; Aesthetic,\nwhich assesses element quality, layout balance, and visual appeal; and Informative, which evaluates\nthe clarity and logical coherence of webpage content. See Appendix B for scoring guidelines.\n3.3\nPAPERQUIZ\nInspired by Paper2Poster (Pang et al., 2025), we focus on the academic web page and acknowledge\nits central role in communicating research as a dynamic bridge between authors and a broader audi-\nence. Therefore, we design an evaluation protocol that simulates this knowledge-transfer scenario.\nWe first employ an LLM as an examiner to generate a comprehensive set of 50 questions from the\nsource paper. These questions are divided into two types: 25 Verbatim questions, which are directly\nanswerable from specific text, figures, or tables on the webpage, and 25 Interpretive questions, which\nrequire a higher-level comprehension of the paper’s core contributions, methodology, and results. In\nthe second stage, we present a screenshot of the rendered webpage to a diverse panel of MLLMs\n(including both open and closed source models). These models are tasked with answering the quiz\nbased solely on the provided webpage content. By comparing the quiz scores across different gener-\nated web pages, we can quantitatively assess which one most effectively conveys the original paper’s\nessential information. To prevent high scores resulting from excessive text transfer, we introduce a\npenalty term ζ, defined in Eq. 1, to discount for verbosity.\n4\nPWAGENT: A STRONG BASELINE\nTo address the core challenges of the PAPER2WEB, we introduce PWAGENT, an automated pipeline\nfor converting scientific papers into project homepages. The core of our approach involves parsing\nthe paper’s content into a structured format managed by an MCP server (Hou et al., 2025; Ehtesham\net al., 2025; Krishnan, 2025). This server encapsulates key paper assets, along with predefined\n6\nPreprint. Under Review.\nOutput: Website\nStep1:  Paper Decomposition \nLLM\nPictures\nTables\nLinks\nDocling\nText\nInput: Paper\nConstruct\nStep2: MCP Ingestion \nStep3: Agent-driven Iterative Reﬁnement \nMCP tool use\nResource \nBefore \nOptimization\nAfter \nOptimization\n{“Content”: …\n“Chapter_Name”: …\n“Chapter_Number”: …\n}\n \nText Resources\nMCP Resource Repository\n      {“Picture_id”:3,\n       “Description”: … \n       “Chapter_Number: 3\n       }\nPicture Resources\n\"The \nquality \nof the \nwebpage …\nMLLM as \nOrchestrator \nWebpage to be \nOptimized\n      {“Table_id”:3,\n       “Description”: … \n       “Chapter_Number: 6\n       }\nTable Resources\n      {\n       Link_URL”: …\n       “Description”: … \n       }\n \nLink Resources\nInvoke\n<title>.....</title> \n<script …></script>\n<style>Body{...} .\nhero{..}.fade-in{}...\n<title>.....</title> \n<script …> </script>\n<style></style>\nFigure 8: PWAGENT turns papers into interactive and multimedia-rich project homepages. Papers are decon-\nstructed via Docling/Marker + LLM into multiple assets and stored in an MCP repository. An agent drafts a\npage, then iteratively optimizes until layout and UX are solid.\nprompts for webpage generation and stylistic refinement, organizing them into a centralized resource\nrepository. During this process, the agent leverages the tool-use capabilities of the MCP to access\nthe resource repository, enabling a continuous optimization loop. The overall process includes the\nfollowing key stages: (1) Paper Decomposition, which isolates key contributions from the paper.\n(2) MCP Ingestion, which encapsulates these contributions as a resource repository managed by the\nMCP server. (3) Agent-driven Iterative Refinement, which connects the MCP server to LLM-based\nagents that autonomously perform content matching and optimization through tool calls.\n4.1\nPAPER DECOMPOSITION\nWe first deconstruct an unstructured scientific paper into structured intellectual assets that popu-\nlate the MCP Resource Repository. Starting from the source PDF, the document is converted to\nMarkdown using tools such as MARKER1 or DOCLING2. An LLM then performs semantic de-\ncomposition that extracts metadata, reconstruct tables, and model detailed page layout and reading\norder, yielding a machine-readable representation like JSON or Markdown that captures the paper’s\nkey contributions.\nInstead of summarizing, the LLM analyzes the Markdown text against a predefined schema to iden-\ntify, isolate, and organize the paper’s key assets. These assets fall into three categories: (1) Textual\nAssets: each logical section is represented as a distinct resource object containing its title, LLM-\ngenerated synopsis, full text, and metadata; (2) Visual Assets: figures and tables are extracted as\nimages and linked to their original captions, labels, and textual references to preserve context; and\n(3) Link Assets: external URLs and internal citations are systematically captured and categorized to\nprovide structured access to supplementary materials and related work.\n4.2\nMCP INGESTION\nHere we apply the MCP to the task of transforming scholarly papers into structured, queryable\nresources. We first instantiate a fully instrumented MCP server, which converts static assets into\nqueryable resources with stable IDs and standardized tool access points. The server is responsible for\nresource construction, materializing assets with relational metadata and provisional layout budgets,\nand for tool registration, exposing a minimal, consistent API for downstream retrieval, composition,\nand editing.\n1https://github.com/datalab-to/marker\n2https://github.com/docling-project/docling\n7\nPreprint. Under Review.\nWe enrich the parsed outputs with cross-modal semantics: (1) An LLM is used to align each visual\nelement with its most relevant textual description and adds back-references to the citing paragraphs.\n(2) Link assets are typed by function to support structured cross-references. To achieve a coherent\nvisual presentation, a content-aware spatial allocation heuristic estimates each asset’s footprint and\nassigns a proportional layout budget to balance visual density across the page.\nThese enriched records are then committed to MCP server as MCP Resource Repository, where each\nresource is stored with a unique rid and fields for grounding and navigation. Concretely, the text re-\nsource stores the full paragraph and an LLM-generated synopsis; a Visual resource stores the image\nand its caption; and a Link resource stores the URL, its semantic role, and a short descriptor. To-\ngether, these resources form a structured, cross-referenced repository that serves as the foundation\nfor webpage synthesis. Finally, the server registers a compact tool suite that provides enumera-\ntion of resource IDs, access to grounded content and metadata for rendering, typed references for\nconnectivity-aware placement, and initial layout allocation. This lightweight yet expressive inter-\nface is sufficient to synthesize a well-grounded HTML first draft for subsequent refinement by the\nmulti-agent workflow.\n4.3\nAGENT-DRIVEN ITERATIVE REFINEMENT\nFinally, we propose an agent-driven iterative refinement mechanism to progressively enhance the\nlayout, visual coherence, and semantic alignment of generated webpages. The process begins with\ninitial page generation, where the agent retrieves essential metadata and relevant assets from the\nresource repository using MCP tools. Based on this information, it rapidly constructs a foundational\nwebpage that serves as the baseline for subsequent refinement.\nFollowing initialization, the system enters an iterative refinement loop that continues until no further\ncorrective actions are needed or a predefined iteration limit is reached. At its core is an MLLM act-\ning as the Orchestrator Agent, which conducts holistic visual assessments of the rendered webpage\nand invokes MCP tools to fix detected flaws. To address complex layout and visual consistency is-\nsues, the Orchestrator performs joint global–local reasoning and coordinates targeted optimizations\nthrough tool calls. To reduce hallucinations during long-range reasoning, the agent segments the\nrendered page into independent visual tiles linked to their corresponding HTML fragments, sequen-\ntially analyzing each to detect imbalances and misalignments and propose precise edits. After each\nround of local refinement, adjacent tiles are merged, borrowing the spirit of merge sort. Therefore,\nneighboring regions can be jointly optimized by integrating their HTML and imagery. This aggre-\ngation allows the MLLM to capture inter-section dependencies and prevent visual artifacts such as\noverflow, occlusion, or cross-section drift. Finally, the Orchestrator performs a global pass to as-\nsess overall content completeness and visual harmony, realizing a part-to-whole optimization path\nthat further mitigates hallucinations. The process terminates once optimization is complete or the\nmaximum refinement cycles are reached.\n5\nHOW PWAGENT MAKE PAPER ALIVE?\n5.1\nEXPERIMENT SETUPS\nWe evaluate four distinct baseline methodologies to rigorously assess the performance of our pro-\nposed approach. These serve as crucial benchmarks for gauging information dissemination efficacy\nand human-centered friendliness. (1) Oracle Method, original websites created by authors. They\nserve as the gold standard for optimal presentation and content delivery; (2) End-to-End Gener-\nation, where GPT-4o, Gemini-2.5-Flash (Gemini), DeepSeek-V3.2-Exp (DeepSeek) and Qwen3-\nCoder-480B-A35B (Qwen) generate websites either through text-based rendering from scratch or\nby adapting the widely adopted Nerfies academic website template (Park et al., 2021) (The above\nmodels combined with a template will be referred to respectively as GPT-4o-Template, Gemini-\nTemplate, DeepSeek-Template, and Qwen-Template); (3) Existing HTML Versions, where re-\nsearch papers from arXiv and alphaXiv provide public HTML versions, we scrape their screenshots\nand source code, noting some lack official web formats; (4) PWAGENT (Our), where Qwen3-30B-\nA3B is responsible for paper deconstruction and MCP ingestion, while the Orchestrator Agent is\npowered by the Qwen2.5-VL-32B model.\n8\nPreprint. Under Review.\nTable 1: Detailed comparison between PAPER2WEB and other baselines across Completeness, Connectivity\nand holistic MLLM evaluation.\nMethods\nConnectiveness\nCompleteness\nHolistic Evaluation\nInteractive\nAesthetic\nInformative\nRule ↑LLM ↑Human ↑Avg.↑Rule↑LLM↑Human↑Avg.↑MLLM↑Human↑Avg.↑MLLM↑Human↑Avg.↑MLLM↑Human↑Avg.↑\nOriginal Website\n3.20\n3.47\n2.99\n3.22\n3.17\n3.93\n4.00\n3.70\n1.70\n3.37\n2.54\n3.14\n3.63\n3.39\n4.49\n3.86\n4.18\nModel end-to-end methods\nGPT-4o\n1.81\n2.07\n2.05\n1.98\n2.11\n3.15\n3.43\n2.90\n0.53\n1.85\n1.19\n2.61\n2.13\n2.37\n2.01\n2.56\n2.29\nGemini-2.5-flash\n2.26\n2.11\n2.16\n2.18\n2.72\n3.56\n3.43\n3.24\n1.30\n2.15\n1.73\n2.80\n2.41\n2.61\n3.63\n2.68\n3.16\nDeepSeek-V3.2-Exp\n1.83\n2.09\n2.16\n2.03\n2.09\n3.21\n3.51\n2.94\n0.54\n2.01\n1.28\n2.63\n2.20\n2.42\n2.21\n2.61\n2.41\nQwen3-Coder-480B-A35B\n2.52\n3.05\n2.82\n2.80\n2.79\n3.58\n3.62\n3.33\n1.44\n2.43\n1.94\n2.74\n2.49\n2.62\n3.92\n2.81\n3.37\nModel end-to-end methods + Template\nGPT-4o-Template\n1.83\n2.26\n2.77\n2.29\n2.25\n3.37\n3.54\n3.05\n0.56\n1.47\n1.02\n2.63\n2.35\n2.49\n3.87\n2.58\n3.23\nGemini-Template\n2.47\n2.87\n2.78\n2.71\n2.73\n3.72\n3.78\n3.41\n1.47\n1.58\n1.53\n2.75\n2.46\n2.61\n4.28\n2.67\n3.48\nDeepSeek-Template\n2.38\n2.91\n2.80\n2.70\n2.75\n3.68\n3.84\n3.42\n1.45\n1.60\n1.53\n2.74\n2.46\n2.60\n4.26\n2.67\n3.47\nQwen-Template\n3.01\n3.21\n2.87\n3.03\n2.88\n3.90\n3.80\n3.53\n1.47\n1.58\n1.53\n2.77\n2.93\n2.85\n4.31\n3.22\n3.77\nAutomated generation methods\narXiv (HTML)\n3.70\n2.23\n1.34\n2.42\n2.49\n3.81\n3.75\n3.35\n1.05\n1.51\n1.28\n2.72\n2.65\n2.69\n4.01\n3.06\n3.54\nalphaxXiv\n3.43\n3.01\n2.91\n3.12\n2.88\n3.95\n3.85\n3.56\n1.25\n1.61\n1.43\n2.73\n2.80\n2.77\n4.20\n3.46\n3.83\nPWAGENT (Our)\n3.06\n3.30\n2.94\n3.10\n2.91\n4.02\n3.86\n3.56\n1.39\n3.16\n2.28\n2.82\n3.35\n3.09\n4.31\n3.56\n3.93\nTable 2: PaperQuiz evaluation on the PAPER2WEB, based on open and closed-source MLLMs. The evaluation\nmetrics include Raw Score and Score with Penalty under two settings: “Verbatim” and “Interpretive”.\nMethods\nVerbatim\nInterpretive\nAvg\nScore with Penalty\nopen-source ↑closed-source ↑V-Avg ↑open-source ↑closed-source ↑I-Avg ↑Avg ↑Penalty ↓V avg ↑I avg ↑Avg ↑\nOriginal Website\n2.94\n2.14\n2.54\n3.81\n3.09\n3.45\n3.00\n1.43\n1.11\n2.02\n1.57\nModel end-to-end methods\nGPT-4o\n2.53\n1.46\n1.99\n3.38\n2.32\n2.85\n2.42\n3.03\n-0.93\n-0.18\n-0.56\nGemini-2.5-flash\n2.60\n1.59\n2.10\n3.14\n2.72\n2.93\n2.52\n2.18\n-0.19\n0.71\n0.24\nDeepSeek-V3.2-Exp\n2.55\n1.54\n2.00\n3.21\n2.55\n2.88\n2.44\n2.26\n-0.26\n0.62\n0.18\nQwen3-Coder-480B-A35B\n2.65\n1.64\n2.15\n3.22\n3.02\n3.12\n2.64\n2.12\n0.03\n1.00\n0.52\nModel end-to-end methods + Template\nGPT-4o-Template\n2.58\n1.42\n2.00\n3.48\n2.25\n2.87\n2.43\n2.50\n-0.50\n0.37\n-0.07\nGemini-Template\n3.62\n3.36\n3.49\n4.40\n4.45\n4.42\n3.96\n2.01\n1.48\n2.41\n1.95\nDeepSeek-Template\n3.55\n3.19\n3.37\n4.11\n4.25\n4.18\n3.78\n1.96\n1.41\n2.22\n1.82\nQwen-Template\n3.70\n3.44\n3.57\n4.52\n4.41\n4.47\n4.02\n2.00\n1.57\n2.47\n2.02\nAutomated generation methods\narXiv (HTML)\n3.62\n3.42\n3.52\n4.52\n4.43\n4.47\n4.00\n2.87\n0.65\n1.60\n1.13\nalphaxXiv\n3.57\n3.60\n3.58\n4.58\n4.54\n4.56\n4.07\n1.97\n1.61\n2.59\n2.10\nPWAGENT (Our)\n3.76\n3.42\n3.59\n4.56\n4.40\n4.48\n4.04\n2.00\n1.59\n2.48\n2.03\n5.2\nMAIN RESULTS\nCompleteness & Connectivity. As shown in the left half of Table 1, we evaluate website complete-\nness and connectivity. arXiv-HTML attains high rule-based connectivity but receives 64% lower\nhuman ratings, as it indiscriminately converts every citation into links, inflating metric scores while\ndegrading user experience. alphaXiv shows balanced connectivity by selectively surfacing important\nlinks. For completeness, arXiv-HTML preserves verbose text with few images, scoring well with\nLLM and human judges but poorly on rule-based metrics. In contrast, our PWAGENT achieves 2%\nhigher LLM-judged completeness than ground truth, demonstrating superior content condensation\nand balanced layout of text, images, and links. These findings reveal that code-based metrics miss\nreal user experience, motivating our user-centered evaluation next.\nHolistic Evaluation. As shown in the right half of Table 1, our PWAGENT achieves highest scores\nacross all dimensions. While alphaXiv performs well in completeness and connectivity, it lacks\ninteractive components, scoring 37% lower than our method in interactivity. Template-based meth-\nods effectively guide layout but constrain interactive element generation. Overall, PWAGENT out-\nperforms all generation methods, achieving 91% of ground truth quality in aesthetics and 94% in\ninformativeness, with a 59% improvement in interactivity over alphaXiv.\nPaperQuiz. As shown in Table 2, we observe: (1) Without the conciseness penalty, arXiv-HTML\nscores strongly; once applied, both arXiv-HTML and end-to-end GPT-4o receive large deduc-\ntions, highlighting the value of concise, engineered sites and supporting website generation as ef-\nfective context compression. (2) Gemini and Qwen are strong and generally outperform GPT-4o\nand DeepSeek; templates lift all models—DeepSeek-Template nears Gemini-Template, and Qwen-\nTemplate approaches the ground-truth site. (3) Across methods, open-source reader models con-\n9\nPreprint. Under Review.\n(a) Ground truth\n(b) GPT-4o\n(c) Gemini\n(d) GPT-4o-Template\n(e) Gemini-Template\n(f) arXiv-HTML\n(g) alphaXiv\n(h) PWAgent (ours)\nFigure 9: Illustration of website variants for the paper generated by different methods. GPT-4o fails to cover\nall components of a paper and amounts to only a simple paradigm; even with template, the sections remain\nincomplet. The arXiv-HTML is content-rich but is essentially a direct transfer of the original. The alphaXiv\nmethod is complete and concise in content, but it lacks a layout paradigm and visual aesthetic quality. Our\nPWAGENT show interactive and rich multimedia content to enrich presentation quality.\nsistently beat closed-source ones, indicating some open-source MLLMs (e.g., Qwen) can match or\nexceed closed models on certain visual tasks. (4) PWAGENT achieves best or near-best results across\ntasks and models, with total information coverage rivaling arXiv-HTML; after the penalty, it still at-\ntains the highest overall score. (5) PWAGENT ’s penalty remains nontrivial, and the ground-truth\nsite scores lower than expected, likely because it includes many videos and animations; in practice,\nauthors can start from PWAGENT and add multimedia to reach the most desirable design.\n5.3\nIN-DEPTH ANALYSIS\nEfficiency Analysis. Figure 3 presents the average token cost per website. Our PWAGENT is highly\ntoken-efficient, requiring only $0.025 to produce a high-quality academic page. By contrast, end-\nto-end methods are costlier: GPT-4o is about $0.141 and Gemini about $0.054 per website. This\nyields 82% and 54% cost reductions, respectively, while maintaining strong page quality and us-\nability. Even template-aided open models around $0.069 remain 2.8× more expensive, yet offer no\nclear advantage. Overall, PWAGENT delivers state-of-the-art cost efficiency with high presentation\nquality.\nCase Study. In Figure 9 and 10, we present a qualitative comparison of different website baselines\nfor a paper. GPT-4o evidently struggles to generate a structurally coherent HTML webpage from\nthe source PDF, and its content completeness remains poor even when provided with a template.\nIn contrast, the website generated by Gemini appears content-rich at first glance, and its internal\nstructure is significantly improved with a template. However, it suffers from an unbalanced image-\nto-text ratio with very few visuals, which hinders the reader’s ability to systematically understand\nthe project. The official arXiv-HTML page, while comprehensive, is overly verbose. Although the\nalphaXiv website is well-illustrated with both images and text, its design is monotonous and lacks\naesthetic appeal. In contrast, our PWAGENT not only preserves the structural integrity of the original\npaper but also achieves a well-balanced image-to-text ratio. Furthermore, it offers versatile styling\nand superior aesthetic quality. However, there is still room for improvement when compared to the\nhuman-designed version.\n10\nPreprint. Under Review.\n(a) Ground truth\n(b) GPT-4o\n(c) Gemini\n(d) 4o-Template\n(e) Gemini-Template\n(f) alphaxiv\n(g) PWAgent(ours)\nFigure 10: Illustration of website variants for the paper “MLLM-as-a-Judge: Assessing Multimodal LLM-as-\na-Judge with Vision-Language Benchmark”3generated by different methods.\n6\nRELATED WORK\nHTML Code Generation. The field of automated front-end development has seen significant\nprogress, with a primary focus on generating HTML from diverse inputs like screenshots, design\nprototypes, and natural language descriptions. This research has led to the establishment of several\nkey benchmarks, including Design2Code (Si et al., 2024; Yang et al., 2025), Websight (Laurenc¸on\net al., 2024) and WebCode2M (Gui et al., 2025a). A variety of code generation strategies have\nbeen explored, ranging from direct translation to more structured approaches, such as the divide-\nand-conquer strategy of DCGen (Wan et al., 2024b) and the hierarchical generation process used by\nUICopilot (Gui et al., 2025b). These technologies have been applied to create mobile UIs(Xiao et al.,\n2024a; Zhou et al., 2024), multi-page websites (Wan et al., 2024a), and enhance web design (Xiao\net al., 2024b; Li et al., 2024; Zhang et al., 2024), with performance often improved through model\nfine-tuning (Liang et al., 2024). More recently, multi-agent systems are being increasingly adopted\nfor complex development tasks (Han et al., 2024; Liu et al., 2024a). For example, agentic work-\nflows are now used to convert designs into functional code (Islam et al., 2024; Ding et al., 2025),\nand some systems assign distinct agents to specific sub-tasks, refining their output through iterative\nhuman feedback (Wang et al., 2024b).\nAutomated Processing of Scholarly Articles. Early methods for generating derivative content from\nacademic papers primarily relied on template-based (Xu & Wan, 2021; Qiang et al., 2019; Cheng\net al., 2024) or rule-driven models (Huang et al., 2022; Lin et al., 2023).Recently, with the matura-\ntion of AI agent technology, a substantial body of work has emerged for academic poster generation.\nA series of methods and benchmarks, including P2P (Sun et al., 2025), Paper-to-Poster (Pang et al.,\n2025), PosterGen (Zhang et al., 2025c), CreatiDesign (Zhang et al., 2025a), PosterCraft (Chen et al.,\n2025b), and DreamPoster (Hu et al., 2025), have explored pipelines for automatically converting pa-\npers into posters. These studies demonstrate that through well-designed multi-agent collaboration,\nthe generated posters can achieve high fidelity with human-designed counterparts in terms of layout,\n3https://mllm-judge.github.io/\n11\nPreprint. Under Review.\ncontent summarization, and visual aesthetics. Similarly, notable progress has been made in presen-\ntation slide generation. PresentAgent (Shi et al., 2025), Preacher (Liu et al., 2025), SciGA (Kawada\net al., 2025), and SlideCode (Tang et al., 2025) introduce specialized datasets, benchmarks, and\nmethodologies. The trend in these task-specific applications is gradually evolving towards broader\nautomated visual design, as exemplified by systems like BannerAgency (Wang et al., 2025) for\nbanner creation and VideoAgent (Wang et al., 2024a; Fan et al., 2024; Soni et al., 2024) for video\nproduction. With the advent of the MCP, researchers have begun to utilize MCP to empower agents\nfor more sophisticated tasks. A prominent example is Paper2Agent (Miao et al., 2025), which\nunderscores the potent capabilities of advanced agent systems in handling complex, unstructured\nacademic information.\n7\nCONCLUSION AND DISCUSSION\nWe introduce PAPER2WEB, a novel task and benchmark for generating project homepages from\nacademic papers, and identify key challenges faced by current generative models and automated\nmethods in handling long-context and layout-sensitive tasks. Our framework, PWAGENT narrows\nthe gap between machine- and human-designed webpages and sets a new efficiency standard for\nweb-based scholarly communication, offering a practical and scalable solution.\nWhile our work represents an initial step toward transforming static papers into exploratory web\npages, it primarily aims to define the scope and standards of this emerging area rather than offer\na definitive solution. We also propose simple yet multi-dimensional evaluation criteria that lay the\ngroundwork for richer future assessments. Nonetheless, evaluating how multimedia elements con-\ntribute to effective academic communication remains an open challenge, which we plan to address\nthrough more robust agentic workflows and comprehensive evaluation methods in future work. We\ncall for continued research on integrating multi-agent reasoning and multimodal understanding to\nadvance the transformation of scholarly communication beyond static formats.\nACKNOWLEDGMENT\nWe thank Gui Yi and Jianuo Huang from Huazhong University of Science and Technology for their\nvaluable input and feedback.\nREFERENCES\nQiguang Chen, Zheng Yan, Mingda Yang, Libo Qin, Yixin Yuan, Hanjing Li, Jinhao Liu, Yiyan\nJi, Dengyun Peng, Jiannan Guan, Mengkang Hu, Yantao Du, and Wanxiang Che. Autopr: Let’s\nautomate your academic promotion!, 2025a.\nSiXiang Chen, Jianyu Lai, Jialin Gao, Tian Ye, Haoyu Chen, Hengyu Shi, Shitong Shao, Yunlong\nLin, Song Fei, Zhaohu Xing, et al. Postercraft: Rethinking high-quality aesthetic poster genera-\ntion in a unified framework. arXiv preprint arXiv:2506.10741, 2025b.\nXianfu Cheng, Weixiao Zhou, Xiang Li, Jian Yang, Hang Zhang, Tao Sun, Wei Zhang, Yuying Mai,\nTongliang Li, Xiaoming Chen, et al. Sviptr: Fast and efficient scene text recognition with vision\npermutable extractor. In Proceedings of the 33rd ACM International Conference on Information\nand Knowledge Management, pp. 365–373, 2024.\nChristopher Clark and Santosh Divvala. Pdffigures 2.0: Mining figures from research papers, 2016.\nURL https://pdffigures2.allenai.org/.\nZijian Ding, Qinshi Zhang, Mohan Chi, and Ziyi Wang. Frontend diffusion: Empowering self-\nrepresentation of junior researchers and designers through agentic workflows. arXiv preprint\narXiv:2502.03788, 2025.\nAbul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, and Saket Kumar. A survey of agent interoper-\nability protocols: Model context protocol (mcp), agent communication protocol (acp), agent-to-\nagent protocol (a2a), and agent network protocol (anp). arXiv preprint arXiv:2505.02279, 2025.\n12\nPreprint. Under Review.\nYue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A\nmemory-augmented multimodal agent for video understanding. In European Conference on Com-\nputer Vision, pp. 75–92. Springer, 2024.\nBaruch Fischhoff. The sciences of science communication. Proceedings of the National Academy\nof Sciences, 110(supplement 3):14033–14039, 2013.\nCharles Frankston, Jonathan Godfrey, Shamsi Brinn, Alison Hofer, and Mark Nazzaro. Html papers\non arxiv–why it is important, and how we made it happen. arXiv preprint arXiv:2402.08954,\n2024.\nGovernment\nDigital\nService\nand\nCentral\nDigital\nand\nData\nOffice.\nPublish-\ning\naccessible\ndocuments,\n2024.\nURL\nhttps://www.gov.uk/guidance/\npublishing-accessible-documents. Last updated 2024-08-14.\nYi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Bohua Chen, Yi Su, Dongping Chen, Siyuan\nWu, Xing Zhou, et al. Webcode2m: A real-world dataset for code generation from webpage\ndesigns. In Proceedings of the ACM on Web Conference 2025, pp. 1834–1845, 2025a.\nYi Gui, Yao Wan, Zhen Li, Zhongyi Zhang, Dongping Chen, Hongyu Zhang, Yi Su, Bohua Chen,\nXing Zhou, Wenbin Jiang, et al. Uicopilot: Automating ui synthesis via hierarchical code genera-\ntion from webpage designs. In Proceedings of the ACM on Web Conference 2025, pp. 1846–1855,\n2025b.\nShanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, and Zhaozhuo Xu. Llm multi-agent sys-\ntems: Challenges and open problems. arXiv preprint arXiv:2402.03578, 2024.\nXinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Land-\nscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\nZhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language\nmodels?\narXiv preprint arXiv:2404.06654, 2024.\ndoi: 10.48550/arXiv.2404.06654.\nURL\nhttps://arxiv.org/abs/2404.06654. COLM 2024.\nXiwei Hu, Haokun Chen, Zhongqi Qi, Hui Zhang, Dexiang Hong, Jie Shao, and Xinglong Wu.\nDreamposter: A unified framework for image-conditioned generative poster design.\narXiv\npreprint arXiv:2507.04218, 2025.\nYupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for\ndocument ai with unified text and image masking. In Proceedings of the 30th ACM international\nconference on multimedia, pp. 4083–4091, 2022.\nMd Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. Mapcoder: Multi-agent code\ngeneration for competitive problem solving. arXiv preprint arXiv:2405.11403, 2024.\nTakuro Kawada, Shunsuke Kitada, Sota Nemoto, and Hitoshi Iyatomi. Sciga: A comprehensive\ndataset for designing graphical abstracts in academic papers. arXiv preprint arXiv:2507.02212,\n2025.\nNaveen Krishnan. Advancing multi-agent systems through model context protocol: Architecture,\nimplementation, and applications. arXiv preprint arXiv:2504.21030, 2025.\nAnukriti Kumar and Lucy Lu Wang. Uncovering the new accessibility crisis in scholarly pdfs.\nIn Proceedings of the 26th ACM SIGACCESS Conference on Computers and Accessibility (AS-\nSETS ’24), 2024. doi: 10.1145/3663548.3675634. URL https://arxiv.org/abs/2410.\n03022.\nHugo Laurenc¸on, L´eo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots\ninto html code with the websight dataset. arXiv preprint arXiv:2403.09029, 2024.\nMinghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank:\nA benchmark dataset for document layout analysis. arXiv preprint arXiv:2006.01038, 2020.\n13\nPreprint. Under Review.\nRyan Li, Yanzhe Zhang, and Diyi Yang.\nSketch2code: Evaluating vision-language models for\ninteractive web design prototyping. arXiv preprint arXiv:2410.16232, 2024.\nShanchao Liang, Nan Jiang, Shangshu Qian, and Lin Tan. Waffle: Multi-modal model for automated\nfront-end development. arXiv preprint arXiv:2410.18362, 2024.\nJiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang Yang, Jian-Guang Lou, and Dongmei Zhang. Layout-\nprompter: Awaken the design ability of large language models. Advances in Neural Information\nProcessing Systems, 36:43852–43879, 2023.\nJiaheng Liu, Zehao Ni, Haoran Que, Sun Sun, Noah Wang, Jian Yang, Hongcheng Guo, Zhongyuan\nPeng, Ge Zhang, Jiayi Tian, et al. Roleagent: Building, interacting, and benchmarking high-\nquality role-playing agents from scripts. Advances in Neural Information Processing Systems,\n37:49403–49428, 2024a.\nJingwei Liu, Ling Yang, Hao Luo, Fan Wang Hongyan Li, and Mengdi Wang. Preacher: Paper-to-\nvideo agentic system. arXiv preprint arXiv:2508.09632, 2025.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts. Transactions of the\nAssociation for Computational Linguistics, 12:157–173, 2024b. doi: 10.1162/tacl a 00638. URL\nhttps://aclanthology.org/2024.tacl-1.9/.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld.\nS2orc: The\nsemantic scholar open research corpus.\nIn Proceedings of ACL 2020, 2020.\nURL https:\n//aclanthology.org/2020.acl-main.447/.\nZimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun\nZhou, Mingjie Zhan, and Hongsheng Li. Webgen-bench: Evaluating llms on generating interac-\ntive and functional websites from scratch. arXiv preprint arXiv:2505.03733, 2025.\nReuben A Luera, Ryan Rossi, Franck Dernoncourt, Samyadeep Basu, Sungchul Kim, Subhojyoti\nMukherjee, Puneet Mathur, Ruiyi Zhang, Jihyung Kil, Nedim Lipka, et al. Mllm as a ui judge:\nBenchmarking multimodal llms for predicting human perception of user interfaces. arXiv preprint\narXiv:2510.08783, 2025.\nJiacheng Miao, Joe R Davis, Jonathan K Pritchard, and James Zou. Paper2agent: Reimagining\nresearch papers as interactive and reliable ai agents. arXiv preprint arXiv:2509.06917, 2025.\nNHS\nDigital.\nPdfs\nand\nother\nnon-html\ndocuments\n—\nnhs\ndigital\nservice\nmanual,\n2025.\nURL\nhttps://service-manual.nhs.uk/content/\npdfs-and-other-non-html-documents. Updated 2025-02.\nWei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, and Philip Torr. Paper2poster: Towards\nmultimodal poster automation from scientific papers. arXiv preprint arXiv:2505.21497, 2025.\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M\nSeitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 5865–5874, 2021.\nYu-Ting Qiang, Yan-Wei Fu, Xiao Yu, Yan-Wen Guo, Zhi-Hua Zhou, and Leonid Sigal. Learning\nto generate posters of scientific papers by probabilistic graphical models. Journal of Computer\nScience and Technology, 34(1):155–169, 2019.\nJingwei Shi, Zeyu Zhang, Biao Wu, Yanjie Liang, Meng Fang, Ling Chen, and Yang Zhao. Presen-\ntagent: Multimodal agent for presentation video generation. arXiv preprint arXiv:2507.04036,\n2025.\nChenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code:\nBenchmarking multimodal code generation for automated front-end engineering. arXiv preprint\narXiv:2403.03163, 2024.\n14\nPreprint. Under Review.\nAchint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang,\nBo Dai, and Sherry Yang.\nVideoagent: Self-improving video generation.\narXiv preprint\narXiv:2410.10076, 2024.\nTao Sun, Enhao Pan, Zhengkai Yang, Kaixin Sui, Jiajun Shi, Xianfu Cheng, Tongliang Li, Wenhao\nHuang, Ge Zhang, Jian Yang, et al. P2p: Automated paper-to-poster generation and fine-grained\nbenchmark. arXiv preprint arXiv:2505.17104, 2025.\nWenxin Tang, Jingyu Xiao, Wenxuan Jiang, Xi Xiao, Yuhang Wang, Xuxin Tang, Qing Li, Yuehe\nMa, Junliang Liu, Shisong Tang, et al. Slidecoder: Layout-aware rag-enhanced hierarchical slide\ngeneration from design. arXiv preprint arXiv:2506.07964, 2025.\nLisbeth Thorlacius. ] the role of aesthetics in web design. Nordicom Review, 28(1), 2007.\nDominika Tkaczyk, Paweł Szostek, Mateusz Fedoryszak, Piotr Jan Dendek, and Łukasz Bolikowski.\nCermine: automatic extraction of structured metadata from scientific literature. International\nJournal on Document Analysis and Recognition (IJDAR), 18(4):317–335, 2015.\nEdward R Tufte and Peter R Graves-Morris. The visual display of quantitative information, vol-\nume 2. Graphics press Cheshire, CT, 1983.\nW3C Web Accessibility Initiative.\nUnderstanding success criterion 1.4.10: Reflow (wcag 2.x),\n2018.\nURL https://www.w3.org/WAI/WCAG21/Understanding/reflow.html.\nAccessed 2025-10-06.\nYuxuan Wan, Yi Dong, Jingyu Xiao, Yintong Huo, Wenxuan Wang, and Michael R Lyu. Mrweb:\nAn exploration of generating multi-page resource-aware web code from ui designs. arXiv preprint\narXiv:2412.15310, 2024a.\nYuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang, Shuqing Li, Yintong Huo, and Michael R\nLyu. Automatically generating ui code from screenshot: A divide-and-conquer-based approach.\narXiv preprint arXiv:2406.16386, 2024b.\nHeng Wang, Yotaro Shimose, and Shingo Takamatsu. Banneragency: Advertising banner design\nwith multimodal llm agents. arXiv preprint arXiv:2503.11060, 2025.\nXiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video\nunderstanding with large language model as agent. In European Conference on Computer Vision,\npp. 58–76. Springer, 2024a.\nZheng Wang, Bingzheng Gan, and Wei Shi. Multimodal query suggestion with multi-agent rein-\nforcement learning from human feedback. In Proceedings of the ACM Web Conference 2024, pp.\n1374–1385, 2024b.\nJingyu Xiao, Yuxuan Wan, Yintong Huo, Zhiyao Xu, and Michael R Lyu. Interaction2code: How far\nare we from automatic interactive webpage generation? arXiv e-prints, pp. arXiv–2411, 2024a.\nShuhong Xiao, Yunnong Chen, Jiazhi Li, Liuqing Chen, Lingyun Sun, and Tingting Zhou. Pro-\ntotype2code: End-to-end front-end code generation from ui design prototypes. In International\nDesign Engineering Technical Conferences and Computers and Information in Engineering Con-\nference, volume 88353, pp. V02BT02A038. American Society of Mechanical Engineers, 2024b.\nSheng Xu and Xiaojun Wan. Neural content extraction for poster generation of scientific papers.\narXiv preprint arXiv:2112.08550, 2021.\nJian Yang, Wei Zhang, Jiaxi Yang, Yibo Miao, Shanghaoran Quan, Zhenhe Wu, Qiyao Peng, Liqun\nYang, Tianyu Liu, Zeyu Cui, et al. Multi-agent collaboration for multilingual code instruction\ntuning. arXiv preprint arXiv:2502.07487, 2025.\nHui Zhang, Dexiang Hong, Maoke Yang, Yutao Cheng, Zhao Zhang, Jie Shao, Xinglong Wu, Zux-\nuan Wu, and Yu-Gang Jiang. Creatidesign: A unified multi-conditional diffusion transformer for\ncreative graphic design. arXiv preprint arXiv:2505.19114, 2025a.\n15\nPreprint. Under Review.\nTao Zhang, Yige Wang, ZhuHangyu ZhuHangyu, Li Xin, Chen Xiang, Tian Hua Zhou, and Jin Ma.\nWebquality: A large-scale multi-modal web page quality assessment dataset with multiple scoring\ndimensions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\nPapers), pp. 583–596, 2025b.\nTianhao Zhang, Fu Peiguo, Jie Liu, Yihe Zhang, and Xingmei Chen. Nldesign: A ui design tool for\nnatural language interfaces. In Proceedings of the ACM Turing Award Celebration Conference-\nChina 2024, pp. 153–158, 2024.\nZhilin Zhang, Xiang Zhang, Jiaqi Wei, Yiwei Xu, and Chenyu You. Postergen: Aesthetic-aware\npaper-to-poster generation via multi-agent llms. arXiv preprint arXiv:2508.17188, 2025c.\nTing Zhou, Yanjie Zhao, Xinyi Hou, Xiaoyu Sun, Kai Chen, and Haoyu Wang. Bridging design and\ndevelopment with automated declarative ui code generation. arXiv preprint arXiv:2409.11667,\n2024.\nZeyu Zhu, Kevin Qinghong Lin, and Mike Zheng Shou. Paper2video: Automatic video generation\nfrom scientific papers, 2025. URL https://arxiv.org/abs/2510.05096.\n16\nPreprint. Under Review.\nAppendix\nTable of Contents\nA Detailed of rule-base metric\n18\nA.1\nRule-based Metric for Connectivity\n. . . . . . . . . . . . . . . . . . . . . . . . .\n18\nA.2\nRule-based Metric for Completeness . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB\nHuman Annotation and Verification Details\n18\nB.1\nInteractivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nB.2\nAesthetic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.3\nInformative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.4\nCompleteness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nB.5\nConnectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC PaperQuiz\n21\nC.1\nQA Dataset Curation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.2\nEvaluation Workflow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nC.3\nCase Study for PaperQuiz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nD Prompt Template\n26\nD.1\nBaseline Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nD.2\nParsing Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nD.3\nOrchestrating Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nE\nMore Examples of Case Study\n33\n17\nPreprint. Under Review.\nA\nDETAILED OF RULE-BASE METRIC\nA.1\nRULE-BASED METRIC FOR CONNECTIVITY\nConnectivity in a web-based academic project can be divided into external links and internal naviga-\ntions. To quantify this aspect, we first parse the HTML structure of the generated webpage to iden-\ntify relevant syntactic patterns. Specifically, external links are represented by <a href=\"...\">\nelements pointing to URLs outside the current domain, while internal navigations are defined by\nanchor links of the form href=\"#section-id\", which reference local sections within the same\ndocument.\nWe record the number of detected external and internal links as Sexternal and Sinternal, respectively. For\nexternal links, we further employ a URL parser to verify the validity, relevance, and accessibility of\neach link. Only those URLs that are reachable and contextually relevant to the webpage content are\ncounted toward Sexternal.\nThe overall rule-based connectivity score SCon is defined as:\nS = Sexternal + Sinternal\n2\n(3)\nA.2\nRULE-BASED METRIC FOR COMPLETENESS\nImage–Text Balance Prior. The Image–Text Balance Prior encodes a heuristic rule: an effective\nacademic project webpage should maintain an approximate balance between visual and textual con-\ntent, avoiding extremes such as image-only pages or text-dense “wall-of-text” layouts. Concretely,\nwe compute the image–text ratio of a generated webpage as follows:\n1. When rendering the full page in a standard viewport, we first measure the area of all con-\ntainers on the page and calculate the proportion of each container’s area occupied by image\nelements. The image areas are weighted according to the container size.\n2. Text content is treated as the remaining area within each container (excluding images) and\nis weighted in the same manner by container area proportion.\nFinally, the weighted image–text ratios of all containers are aggregated according to the relative\narea of each container within the entire page, yielding the overall page-level image–text ratio. This\napproach ensures that a few large images (e.g., full-width banners) and many small icons are appro-\npriately distinguished based on their actual proportions, while the text proportion remains consistent\nwith both container and overall page layout.\nInformation Efficiency Prior. The Information Efficiency Prior rewards concise and information-\ndense presentations by comparing the generated text length L with the median human-authored\nlength W for comparable sections. In the main text, we introduced the ratio r = L/W together\nwith a scaling factor β. The median is chosen because human-designed webpages often favor short\ntext supplemented by multimedia, leading to large standard deviations in length; the median better\nreflects typical requirements while mitigating the influence of extreme cases. The hyperparameter\nβ controls the decay rate of the efficiency reward when L > W: smaller β values impose a stricter\npenalty on overly verbose text. The overall rule-based connectivity score SCon is defined as:\nS = Simg-txt + p(r)\n2\n(4)\nB\nHUMAN ANNOTATION AND VERIFICATION DETAILS\nThe annotation is conducted by 6 authors of this paper independently. The diversity of annotators\nplays a crucial role in reducing bias and enhancing the reliability of the benchmark. These anno-\ntators have knowledge in this domain, with different genders, ages, and educational backgrounds.\nTo ensure the annotators can proficiently mark the data, we provide them with detailed tutorials,\nteaching them how to evaluate model responses more objectively. Specifically, they are required to\n18\nPreprint. Under Review.\nFigure 11: Human annotation instruction\ngive judgments without considering answer lengths, and certain names or positions of the response.\nFurthermore, we implement cross-validation between different annotators and conduct continuous\nmonitoring to ensure they are maintaining objectivity and fairness. We rate paper webpages with 1–5\ninteger scores on five indicators: Interactivity, Aesthetic, Informative, Completeness, and Connec-\ntivity. Raters inspect the same rendering variant in the tool and score each indicator independently.\nB.1\nINTERACTIVITY\nThis metric evaluates the quality and responsiveness of interactions. It also considers discoverability\nand learnability, where key actions should be obvious and controls self-explanatory. Furthermore,\nit assesses accessibility and reachability, including keyboard navigation, screen-reader cues, and\nresponsive/mobile usability.\nTo systematically assess this, we evaluate interactivity across four key areas:\n• Basic Interactions: This criterion covers fundamental dynamic elements that enhance readability.\nRaters check for common interactions like over effects, expand/collapse sections for content, and\nclickable tabs.\n• Interactive Visualizations: This assesses dynamic presentations of results. Raters look for in-\nteractive charts, comparison sliders, or other elements that allow users to actively explore model\noutputs.\n19\nPreprint. Under Review.\n• Live Demo: This evaluates the presence of hands-on experiences. Raters check for an embedded\nonline demo or a video that allows users to observe the model’s performance directly.\n• Navigation Aids: This focuses on features that improve browsing on long pages. Raters look for\ntools such as a floating table of contents, quick jump-to-section links, or a back-to-top button.\nB.2\nAESTHETIC\nThis dimension focuses on a clear layout and visual hierarchy that guide the user’s attention. The\nWebQuality (Zhang et al., 2025b) benchmark emphasizes that a well-structured design, which avoids\nhindering a user’s information acquisition, is a cornerstone of quality assessment. This evaluation\nincludes typographic readability, ensured by appropriate font sizes, line height, and stable styling.\nColor and contrast are also evaluated for being harmonious, accessible, and providing sufficient\ndistinction for all text and interface elements.\nWe evaluate the aesthetic quality based on four criteria adapted from Paper2Poster (Pang et al.,\n2025):\n• Element Quality: This criterion assesses the individual visual components on the page. Raters\nevaluate the clarity and resolution of images, the design quality of illustrations or figures, and\nwhether charts and tables are not only easy to understand but also thoughtfully designed.\n• Layout Balance: This criterion focuses on the overall spatial organization and structure. Raters\ncheck for consistent alignment of all elements, reasonable sizing of images, and appropriate spac-\ning between different sections to ensure a clean and flexible layout.\n• Engagement and Style: This criterion evaluates the overall artistic and sensory appeal. Raters\nassess the consistency and harmony of the color scheme, the readability and appropriateness of\nthe typography, and the creativity of the overall style and its effectiveness in engaging the user.\n• Clarity: Inspired by MLLM as a UI judge (Luera et al., 2025), this criterion evaluates how clear\nand uncluttered the interface appears, encompassing both textual legibility and overall visual de-\nsign. A clear interface avoids overwhelming the user with too many UI elements. This extends to\nthe fundamental readability of the text, which should feature a clear, discernible font and sufficient\ncontrast between the text and its background to ensure a comfortable reading experience without\nstrain.\nB.3\nINFORMATIVE\nThis indicator measures the completeness and depth. It also assesses the information architecture\nand findability, which are supported by a logical structure, clear labels, and cross-links or search\nfunctionality. Scannability, achieved through effective use of headings, bullet points, callouts, and\nsummaries, is another key aspect.\nTo formalize this assessment, we evaluate the informative quality based on the following three di-\nmensions:\n• Logical Flow and Coherence: Drawing from the criterion in Paper2Poster (Pang et al., 2025),\nthis dimension evaluates the overall structure and narrative of the webpage. A high-quality page\nshould present information in a logical sequence that mirrors the research process. The content\nshould be coherent, guiding the reader through the project’s story without confusion.\n• Depth of Content: This dimension assesses whether the core sections are explained with suffi-\ncient detail and insight. The webpage should provide a substantive discussion of key concepts,\nmethodologies, and findings, demonstrating a thorough understanding and presentation of the re-\nsearch.\n• Scannability and Readability: Raters assess whether the page structures content with visually\ndistinct section titles, applies text formatting like bolding to emphasize key terms, and organizes\nparallel items or sequential steps into clear lists.\n20\nPreprint. Under Review.\nB.4\nCOMPLETENESS\nThis metric assesses whether the essential elements of a research webpage are present and suffi-\nciently developed. Recent benchmarks like WebGen-Bench (Lu et al., 2025) have highlighted the\nimportance of moving beyond static content to evaluate the generation of truly functional web-\nsites.Accordingly, our definition of completeness encompasses not only the presence of core content\nbut also its operational integrity. Raters consider the coverage of core content, the adequacy and\ncoherence of accompanying text and media, and whether the information feels up-to-date and self-\ncontained.\nRaters evaluate the presence and thoroughness of items within each dimension.\n• Element Completeness: This dimension evaluates whether the webpage effectively summarizes\nthe fundamental components of the research paper. Drawing from principles in the WebQual-\nity (Zhang et al., 2025b) dataset, raters assess the presence of essential elements, such as the\nfoundational metadata, a summary of core contributions, descriptions of the experimental setup,\nand a presentation of key results.\n• Rich Media and Artifacts: This dimension assesses the inclusion of supplementary materials\nthat go beyond static text to enhance understanding and demonstrate practical applications. This\nincludes elements like an embedded video presentation or demo and any interactive visualizations\nthat allow users to explore the data or results.\n• Scholarly Utility: This dimension evaluates features that provide direct practical value to other\nresearchers and facilitate the work’s dissemination. This primarily involves tools like an easy-to-\ncopy citation format and clearly labeled links to official resources such as the paper’s PDF, source\ncode, or datasets.\nB.5\nCONNECTIVITY\nThis metric evaluates the richness, relevance, and reliability of outward links. High-quality pages\nfeature working links to code and reproducible artifacts, official paper pages, author or lab websites,\ndatasets, and pertinent related work. Links should be contextually introduced with clear anchor text,\nfree of dead or circular references, and should help readers navigate to deeper resources without\nfriction.\nBuilding upon the work on MRWeb (Wan et al., 2024a), we evaluate connectivity across three di-\nmensions:\n• Resource Connectivity: This dimension assesses the linkage to core research assets that enable\nreproducibility and deeper engagement. Raters check for direct, functional links to the source\ncode repository, the official paper, and any associated project or data resources.\n• Scholarly Context Connectivity: This dimension measures how well the webpage connects the\nresearch to the wider academic landscape. This is primarily evaluated by the presence and quality\nof links to related work.\n• Internal Navigation and Linking: This dimension evaluates how effectively the webpage facili-\ntates smooth and intuitive movement between its internal sections. Raters assess the presence and\nclarity of navigational elements—such as anchored headings, menus, or in-page links—that allow\nusers to easily access key content areas without losing contextual flow.\nC\nPAPERQUIZ\nC.1\nQA DATASET CURATION.\nEach paper PDF is converted to markdown via our PDF parser. We then prompt o3 to generate 50\nmultiple-choice questions per paper, where we have 25 verbatim and 25 interpretive questions as\nfollows:\n• Verbatim questions (25): directly answerable from the paper text, covering 13 orthogonal content\naspects (e.g., objectives, methodology, key results).\n21\nPreprint. Under Review.\n• Interpretive questions (25): requires high-level comprehension beyond verbatim text, spanning\n10 conceptual dimensions (e.g., motivation, contribution synthesis, implication analysis).\nThe following is a prompt example of 25 Verbatim questions and 25 Interpretive questions, generated\nby GPT-o3.\nPrompt: Generated Verbatim Questions\nSystem prompt:\nYou are a highly precise Question-Generation agent for academic project websites. Your task\nis to read the supplied Markdown text and produce a structured set of exactly 25\nmultiplechoice QA items. Your primary goal is to strictly adhere to a mandatory Question\nDistribution Plan and a set of critical formatting rules. Failure to follow these rules precisely\nwill result in an invalid output. The answers to your questions must be located verbatim or\nalmost verbatim in the provided text. The questions must be suitable for website visitors:\navoid deep theoretical proofs, reference lists, or citation minutiae.\nInstructions:\nYou MUST generate questions according to the following to ensure all aspects are covered\nand the total is exactly 25 questions.\n• Locate Fact: Find a specific, clear factual statement, number, or detail in the\n‘document markdown.\n• Classify Aspect: Critically determine which single aspect (from A-M) this fact\n*most accurately* represents. Be extremely strict in your classification.\n• Formulate Question: Based ONLY on the located fact, create a clear,\nanswerable-from-text question.\n• Create Options: Write the correct answer and three high-quality distractors as\ndefined in the rules below.\nAspect Definitions & Special Instructions:\nYou will generate questions for the following aspects:\n• A. Research domain & background context.\n• B. Central problem / motivation / research gap\n• C. Primary goal, hypothesis, or research question\n• D. Key contributions or novelty statements\n• E. Overall methodology or workflow (summarized)\n• F. Qualitative insights or illustrative examples\n.....\nMANDATORY Formatting Rules:\nEach question object MUST have exactly four options, labelled \"A.\", \"B.\", \"C.\", and\n\"D.\". Do not generate more or fewer than four. The ”aspect” key is required and must\ncontain a single letter from the list above.\nFinal Pre-Output Check: Before providing the final JSON, mentally perform this check:\n• Is the total number of questions EXACTLY 25?\n• Is the Question Distribution Plan followed perfectly?\n• Does EVERY single question have EXACTLY four options?\n• Is every question accurately classified with its aspect and do they follow all\nspecial instructions?\n• If any check fails, you must restart and correct the errors.\ndocument markdown:\n{{ document markdown }}\n22\nPreprint. Under Review.\nPrompt: Generated Interpretive Questions\nSystem prompt:\nYou are a highly precise QuestionGeneration agent for academic project websites. Your task\nis to read the supplied Markdown text and produce a structured set of exactly 25\nmultiple-choice QA items. Your primary goal is to strictly adhere to a mandatory Question\nDistribution Plan and a set of critical formatting rules. The answers to your questions must be\nlocated verbatim or almost verbatim in the provided text. The questions must be suitable for\nwebsite visitors.\nInstructions:\nFor each question you generate, you MUST follow these mental steps:\n• Locate Fact: Find a specific, clear factual statement, number, or detail in the\ndocument markdown.\n• Classify Aspect: Critically determine which single aspect (from A–M) this fact\nmost accurately represents. Be extremely strict in your classification.\n• Formulate Question: Based ONLY on the located fact, create a clear,\nanswerable-from-text question.\n• Create Options: Write the correct answer and three high-quality distractors as\ndefined in the rules below.\nAspect Definitions & Special Instructions:\nYou will generate questions for the following aspects:\n• A. Title & authorship (title, author names, affiliations, keywords): For questions\nabout author names, the incorrect options (distractors) MUST be fabricated but\nplausible-sounding names. Do not use real names from other contexts.\n• B. Motivation / problem statement / research gap\n• C. Objectives or hypotheses\n• D. Dataset(s) or experimental materials\n• E. Methodology (algorithms, model architecture, workflow steps)\n• F. Key parameters or hyper-parameters (values, settings)\n.....\nMANDATORY Formatting Rules:\nEach question object MUST have exactly four options, labelled \"A.\", \"B.\", \"C.\", and\n\"D.\". Do not generate more or fewer than four. The ”aspect” key is required and must\ncontain a single letter from the list above.\nFinal Pre-Output Check: Before providing the final JSON, mentally perform this check:\n• Is the total number of questions EXACTLY 25?\n• Is the Question Distribution Plan followed perfectly?\n• Does EVERY single question have EXACTLY four options?\n• Is every question accurately classified with its aspect and do they follow all\nspecial instructions?\n• If any check fails, you must restart and correct the errors.\nAdhere to Mandatory JSON Format:\ndocument markdown:\n{{ document markdown }}\nC.2\nEVALUATION WORKFLOW.\nFor each website snapshot, we query six MLLMs reader models to answer curated questions. These\nmodels include three open-source models (LLaVA-OneVision-Qwen2-7B-ov-hf, DeepSeek-V3.2-\n23\nPreprint. Under Review.\nExp, and Qwen3-Coder-480B-A35B) and three closed-source models (o1, Gemini 2.5 Flash, and\nGrok Code Fast 1). Their outputs are evaluated according to two enforced rules:\n• No external knowledge. Models must base answers solely on information present in the website\nsnapshot.\n• Visual citation. Each answer must include a reference to the website region supporting it (e.g.,\n“See the ‘Results’ section”); if no region contains the answer, the model responds “NA.”\nPrompt: Answer Qusetions\nSystem prompt:\nYou are an answering agent. You will be provided with:\n• An image of a project website snapshot.\n• A JSON object called “questions” which contains multiple questions. Each question\nhas four possible answers: A, B, C, or D.\nYour goal is to analyze the website snapshot thoroughly and answer each question based on\nthe information it provides. You should NOT use any external knowledge or context beyond\nthe website snapshot image. You must rely solely on the content of the website snapshot to\nanswer the questions.\nFor each question:\n• If you find enough evidence in the website snapshot to decide on a specific option\n(A, B, C, or D), then choose that option. Also include a brief reference to the part of\nthe webpage that supports your answer (e.g., “Top-left text”, “Header section”, etc.).\n• If the website snapshot does not offer sufficient information to confidently choose\nany of the options, respond with “NA” for both the answer and the reference.\nYour final output must be returned as a JSON object. For each question, the structure should\nbe:\n\"Question N\": {\n\"answer\": \"A\" | \"B\" | \"C\" | \"D\" | \"NA\",\n\"reference\": \"<short description or 'NA'>\"\n}\nTemplate:\nFollow these steps to create your response:\n1. Study the website snapshot image along with the “questions” provided.\n2. For each question:\n• Decide if the website snapshot clearly supports one of the four options (A, B,\nC, or D). If so, pick that answer.\n• Otherwise, if the website snapshot does not have adequate information, use\n“NA” for the answer.\n3. Provide a brief reference indicating where on the webpage you found the answer. If\nno reference is available (i.e., your answer is “NA”), use “NA” for the reference too.\n4. Format your output strictly as a JSON object with this pattern:\n{\n\"Question 1\": {\n\"answer\": \"X\",\n\"reference\": \"some reference or 'NA'\"\n},\n\"Question 2\": {\n\"answer\": \"X\",\n\"reference\": \"some reference or 'NA'\"\n},\n...\n}\n24\nPreprint. Under Review.\n5. Do not include any explanations or extra keys beyond the specified structure.\n6. You must provide an answer entry for all questions in the “questions” object.\nExample Output:\nQuestions:\n{{questions}}\nC.3\nCASE STUDY FOR PAPERQUIZ\nHere we provide a simple Q&A example of PaperQuiz.\nPaperQuiz Example\n{\n\"questions\": {\n\"Question 1\": {\n\"question\": \"What is the full title of the paper discussed in\nthe document?\",\n\"options\": [\n\"A. Universal Audio-Video Diffusion Networks for Multimodal\nSynthesis\",\n\"B. Multisensory Diffusion: A Joint Model for Sound and\nVision\",\n\"C. Cross-Modal Transformer: Unified Audio and Video\nGeneration\",\n\"D. A Versatile Diffusion Transformer with Mixture of Noise\nLevels for Audiovisual Generation\"\n]\n},\n...\n\"Question 25\": {\n\"question\": \"In the context of this paper, what does the term\n\\\"time-segment\\\" specifically refer to?\",\n\"options\": [\n\"A. An entire training epoch\",\n\"B. One complete diffusion timestep in noise addition\",\n\"C. A full audio clip of any length\",\n\"D. A single unit in the temporal dimension such as a video\nframe\"\n]\n}\n},\n\"answers\": {\n\"Question 1\": \"D. A Versatile Diffusion Transformer with Mixture\nof Noise Levels for Audiovisual Generation\",\n...\n\"Question 25\": \"D. A single unit in the temporal dimension such\nas a video frame\"\n},\n\"aspects\": {\n\"Question 1\": \"A\",\n...\n\"Question 25\": \"M\"\n},\n\"understanding\": {\n\"questions\": {\n\"Question 1\": {\n25\nPreprint. Under Review.\n\"question\": \"What multimodal generation challenge is\nidentified as still open in the paper's introduction?\",\n\"options\": [\n\"A. Inferring audio labels from isolated spectrogram\nsnapshots\",\n\"B. Classifying large multimodal datasets into predefined\ncategories\",\n\"C. Producing single high-resolution images from textual\ncaptions\",\n\"D. Generating sequences across multiple modalities such as\nvideo and audio\"\n]\n},\n...\n\"Question 25\": {\n\"question\": \"Which unified approach is claimed by the authors\nto enable a single model to generate and manipulate sequences\nacross modalities and time?\",\n\"options\": [\n\"A. The mixture of noise levels strategy introduced in this\npaper\",\n\"B. An unsupervised text summarization algorithm\",\n\"C. A rule-based system for audio classification\",\n\"D. A curriculum learning schedule for GANs\"\n]\n}\n},\n\"answers\": {\n\"Question 1\": \"D. Generating sequences across multiple\nmodalities such as video and audio\",\n...\n\"Question 25\": \"A. The mixture of noise levels strategy\nintroduced in this paper\"\n},\n\"aspects\": {\n\"Question 1\": \"A\",\n...\n\"Question 25\": \"J\"\n}\n}\n}\nD\nPROMPT TEMPLATE\nD.1\nBASELINE TEMPLATE\nWe exhibit the prompt templates used to generate end-to-end model generation baselines. When\nincorporating the template from the popular Nerfies academic website (Park et al., 2021), you only\nneed to include this template as part of the prompt.\nPrompt: Baseline LLM Generation\nSystem prompt:\nYou are a document-to-website generation agent and n expert full-stack web developer and\nUI/UX designer specializing in creating beautiful, modern, and interactive academic project\nwebsites. Your task is to generate a complete, production-ready website based on research\npaper content and visual asset allocations. Your task is to read the supplied Markdown text\n26\nPreprint. Under Review.\nand design a professional, visually appealing academic conference website by generating an\nHTML file. Follow the guidelines below precisely.\n• Is visually stunning and modern with a professional, clean, and academic design.\n• Has rich interactivity and smooth animations.\n• Effectively presents research content in an engaging way.\n• Integrates external links and resources strategically.\n• Uses advanced CSS and JavaScript for enhanced user experience.\nInstructions:\nYou are creating a complete, beautiful, and interactive website for an academic research\nproject. This is NOT a simple static page - it should be a sophisticated, modern web\napplication with rich interactivity. Your task is to read the supplied Markdown text and design\na professional, visually appealing academic conference website by generating an HTML file.\n• Design Requirements\n– Visual Design\n* Modern, professional, academic aesthetic.\n* Sophisticated color scheme (dark/light themes with multiple color\nvariations).\n* Professional typography with hierarchy and multiple font weights.\n* Smooth animations and transitions with multiple animation types.\n* Interactive elements and hover effects with complex state changes.\n* Professional spacing and layout with multiple breakpoints.\n* Advanced visual effects (shadows, gradients, transforms).\n* Background Style: Avoid background images (especially in hero section);\nprefer solid colors such as #2d3748 (dark gray) or #ffffff (white) or subtle\ngradients. Do not fetch images from external sources like Unsplash.\n– Layout Structure\n* Hero section with project title, authors, and key highlights.\n* Multi-level navigation with smooth scrolling and active state indicators.\n* Content sections with dynamic layouts based on importance.\n* Interactive visualizations and image galleries with lightbox and carousel.\n* External resources section with categorized link placement.\n* Footer with information, social links, and contact details.\n* Sidebar navigation with quick links and progress indicators.\n* Multiple columns and grid layouts.\n* Card-based content presentation.\n– Interactivity Features\n* Smooth scrolling navigation with progress bars and scroll indicators.\n* Interactive image galleries with lightbox, zoom, and slideshow.\n* Animated counters and number transitions.\n* Hover effects and micro-interactions.\n* Responsive navigation menu with hamburger and dropdowns.\n* Loading animations and skeleton screens.\n* Interactive charts and visualizations with tooltips.\n* Modal dialogs and popup windows.\n* Form validation and interactive feedback.\n* Search with autocomplete.\n* Dark/light theme toggle with transitions.\n– External Links Integration\n* Place important links strategically within content.\n* Dedicated “Resources & Tools” section with categories.\n27\nPreprint. Under Review.\n* Integrate links naturally in context.\n* Attractive buttons for external links with hover effects.\n* Provide descriptive context for each resource.\n• Technical Requirements\n– CSS Requirements\n* Advanced animations and transitions with varied timing.\n* Responsive design for mobile, tablet, and desktop.\n* CSS Grid and Flexbox layouts.\n* CSS variables for theming.\n* Advanced selectors and pseudo-elements.\n* Center single and multiple images responsively (max 3 per row).\n* Smooth scrolling and scroll animations.\n* Hover effects and micro-interactions.\n* Professional color schemes with multiple variations.\n* Advanced typography with clear hierarchy.\n– JavaScript Requirements\n* Modern ES6+ syntax with error handling.\n* Interactive image galleries with lightbox.\n* Smooth scrolling navigation and progress indicators.\n* Mobile menu with animations.\n* Intersection Observer for scroll animations.\n* Local storage for user preferences.\n* Form validation and interactive feedback.\n* Performance optimization and error handling.\n* Advanced image handling and gallery functionality.\n– Critical JavaScript Best Practices (MUST FOLLOW)\n* DOM Element Access Timing: All DOM element access must occur\nwithin a DOMContentLoaded listener.\n* Intersection Observer Setup:\n· Set up observer before adding classes.\n· Observe elements immediately after adding fade-in class.\n· Never query .fade-in elements before setup.\n· Example: element.classList.add(’fade-in’);\nobserver.observe(element);\n* Event Listener Safety: Always verify element existence before adding\nlisteners.\n* Animation Class Management: Ensure fade-in classes start invisible\n(opacity:\n0) and become visible (opacity:\n1) when animated.\n* Function Organization: Wrap DOM-dependent code in initialization\nfunctions triggered by DOMContentLoaded.\n• Final Checklist\n– Header includes title, authors, and affiliations.\n– Images sized using responsive CSS (width:\n100%).\n– Dedicated “Resources & External Links” section with clickable URLs.\n– Each URL accompanied by description.\n– Preserve all original text content.\n– Images fit properly within containers.\n– Lists rendered as responsive grids.\n• Critical Checks\n– Consistent, professional typography using fonts like Inter or Manrope.\n– Prominent author display below title with affiliations.\n28\nPreprint. Under Review.\n– “How to Cite” section with BibTeX and “Copy” button.\n– No fixed image sizes in HTML; control via CSS (w-full).\n– Implement Scroll-Spy in navigation.\n– Encourage interactive demos over static images.\n– Add elegant hover and scaling effects to all buttons.\ndocument markdown:\n{{ document markdown }}\njinja args:\n- document markdown\nD.2\nPARSING TEMPLATE\nWe present the prompt templates used for paper deconstruction: (1) the prompt for paper summa-\nrizeing, and (2) the prompt for image and table filtering.\nPrompt: Paper Summarizeing\n• You are the author of the paper, and you will create a comprehensive content\nsummary for a project website. Your task is to extract and expand the key\ninformation from the research paper to create detailed, informative content for each\nsection.\n• IMPORTANT REQUIREMENTS:\n– Dual Constraint Adherence: Each section must strictly meet BOTH of the\nfollowing constraints.\n– Content Richness: On the premise of ensuring the character and sentence\ncounts are not exceeded, each section must be rich with substantial detail.\n– Information Completeness: Include comprehensive coverage of all paper\ncontent, not just summaries.\n– Website Depth: Provide enough detail for website visitors to fully understand\nthe research without reading the paper.\n– Technical Thoroughness: Explain technical concepts, methods, and results in\ndetail.\n• CONTENT STRUCTURE FOR EACH SECTION:\nThe constraints below apply to every section (Introduction, Related Work, etc.).\n– Introduction: Write sentences covering the research background, core\nmotivation, challenges, main contributions, and a general overview.\n– Related Work: Write sentences covering existing approaches, their detailed\nlimitations, and the specific gaps in current research.\n– Dataset Overview: Write sentences covering the dataset’s composition, key\nfeatures, core statistics, comparisons with other datasets, and its detailed\ncharacteristics.\n– Methodology/Approach: Writh sentences covering core technical details, key\nalgorithms, the implementation process, and the specific methods used.\n– Results/Evaluation: Write sentences covering the experimental setup, detailed\ncore results, analysis of the results, and comprehensive performance\ncomparisons.\n– Applications: Write sentences covering specific use cases, benefits, practical\napplication scenarios, and representative examples.\n– Conclusion: Write sentences covering a summary of the research, reiterating\nthe contributions, pointing out limitations, and providing a detailed outlook on\nfuture work.\n29\nPreprint. Under Review.\n• OUTPUT FORMAT:\nGenerate a JSON object with the following structure:\n• CONTENT GUIDELINES:\nOn the premise of ensuring the character and sentence counts are not exceeded,\nplease adhere to the following as much as possible:\n– Expand information: Provide comprehensive coverage of paper content.\n– Include specific numbers: Use actual statistics, dimensions, and\nmeasurements from the paper.\n– Be thorough and detailed: Explain concepts, methods, and results in depth.\n– Explain significance: Why is this important? What problems does it solve?\n– Compare and contrast: How does this compare to existing approaches?\n– Future implications: What are the broader impacts and applications?\n– Provide examples: Include concrete examples and use cases.\n– Maintain technical depth: Do not oversimplify technical concepts.\n• Paper content to analyze:\n{{ markdown_document }}\nPrompt: Image/Table Filtering\n• You are an assistant that reviews a research paper’s content (json content),\nalong with corresponding image information and table information.\nYour task is to filter out any image or table entries that are irrelevant to the content\ndescribed in json content, specifically for creating a project website.\n• Specifically:\n– Read through the full research paper data described in json content.\n– Examine each entry within image information and\ntable information.\n– Decide if each entry is relevant for a project website based on its caption, path,\nor any other information provided.\n* For example, if an image has a caption that obviously does not fit into any\nsection or does not relate to the paper’s content outline, deem it\n”unimportant.”\n* Consider which images/tables would be most valuable for a project website.\n– Keep all images/tables that are relevant to the project website (i.e., related to\nthe topics, sections, or discussions mentioned in json content).\n– Do not impose any artificial quantity limits—include every visual element that\nenhances understanding of the research.\n– Produce an output containing just two keys: ”image information” for the\nfiltered images, and table information” for the filtered tables. Each of these\nkeys should map to an array of filtered objects.\n– The user will provide JSON:\n* \"json content\": The content of the research paper (sections, text, etc.)\n* \"image information\": A dict of images (each with caption, path, size\nconstraints)\n* \"table information\": A dict of tables (each with caption, path, size\nconstraints)\n– Your task:\n* Read the research paper outline (json content).\n* Filter image information and table information so that only\nentries relevant to the project website content remain.\n30\nPreprint. Under Review.\n* Relevance is determined by matching or relating captions to the paper’s\nsections or content.\n* Consider which visual elements would be most valuable for a project\nwebsite (e.g., methodology diagrams, result charts, data summaries).\n* If an image or table does not clearly match or support any content in\njson content, remove it.\n* Keep all relevant visual elements—do not limit the quantity artificially.\n• You must output valid JSON containing only:\n{\n\"image_information\": {...},\n\"table_information\": {...}\n}\n• Template Instructions:\n• Please provide only the JSON object as your final output.\njson_content:\n{{ json_content }}\nimage_information:\n{{ image_information }}\ntable_information:\n{{ table_information }}\n• Jinja arguments:\n- image_information\n- table_information\n- json_content\nD.3\nORCHESTRATING TEMPLATE\nWe introduce the prompt templates that guide the Agent-Driven Iterative Refinement procedure.\nPrompt for MLLM as Orchestrator\nSystem message:\nYou are an expert web developer and UI/UX designer with extensive experience in analyzing\nwebsite layouts, visual design, and user experience. Your task is to analyze website\nscreenshots and provide targeted recommendations for improvement.\nYour mission is to first classify the type of web component shown in a screenshot, and then\nanalyze and optimize it based on a deep understanding of modern design systems and\nprinciples, using a protocol tailored to that specific component type. You must provide\nsurgically precise feedback to guide code fixes.\nCore Mission\n• Protocol for ”Navigator”\nFocus: Ensure clarity, usability, and responsiveness in navigation elements.\n1. Component Flow & Alignment\nDiagnosis: Are navigation links properly aligned?\nAction: Suggest adjusting flexbox/grid properties (justify-content, gap) or applying\nuniform margins.\n2. Typography & Readability\n31\nPreprint. Under Review.\nDiagnosis: Are the link labels easy to read?\nAction: Recommend increasing font size, adjusting font weight, or modifying colors\nfor contrast.\n...\n• Protocol for ”Header/Hero”\nFocus: Maximize visual impact, establish a clear hierarchy, and communicate the\nprimary purpose.\n1. Visual Hierarchy & Flow\nDiagnosis: Is the main heading prominent?\nAction: Adjust font sizes or positioning to create a clear focal point.\n2. Image Dominance & Sizing\nDiagnosis: Does the background image enhance or overwhelm content?\nAction: Suggest constraining height or applying a semi-transparent overlay.\n...\n• Protocol for ”Content Block” & ”Component/Card”\nFocus: Ensure logical structure, effortless readability, and visual consistency.\n1. Component Flow & Layout\nDiagnosis: Are grouped elements laid out logically?\nAction: Suggest using CSS Flexbox or Grid for adaptive alignment.\n2. Typography & Readability\nDiagnosis: Is the text comfortable to read?\nAction: Recommend adjusting line-height and ensuring adequate contrast.\n...\nResponse format:\n{\n\"is_needed_to_fix\": true/false,\n\"category\": \"The identified category of the component: Navigator |\nHeader/Hero | Content Block | Component/Card\",\n\"fix_suggest\": \"Detailed analysis and suggestions\"\n}\n32\nPreprint. Under Review.\nE\nMORE EXAMPLES OF CASE STUDY\n(a) Ground truth\n(b) GPT-4o\n(c) Gemini\n(e) Gemini-Template\n(g) alphaxiv\n(d) 4o-Template\n(h) PWAgent(ours)\n(f) arXiv-HTML\nFigure 12: Illustration of website variants for the paper “SMIRK: 3D Facial Expressions through Analysis-by-\nNeural-Synthesis”4generated by different methods.\n4https://georgeretsi.github.io/smirk/\n33\nPreprint. Under Review.\n(a) Ground truth\n(b) GPT-4o\n(c) Gemini\n(e) Gemini-Template\n(g) alphaxiv\n(d) 4o-Template\n(h) PWAgent(ours)\n(f) arXiv-HTML\nFigure 13: Illustration of website variants for the paper “Interactive3D: Create What You Want by Interactive\n3D Generation”5generated by different methods.\n(a) Ground truth\n(b) GPT-4o\n(c) Gemini\n(e) Gemini-Template\n(g) alphaxiv\n(d) 4o-Template\n(h) PWAgent(ours)\n(f) arXiv-HTML\nFigure 14: Illustration of website variants for the paper “Masked Audio Generation using a Single Non-\nAutoregressive Transformer”6generated by different methods.\n5https://interactive-3d.github.io/\n6https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT/\n34\nPreprint. Under Review.\n(a) Ground truth\n(b) GPT-4o\n(c) Gemini\n(e) Gemini-Template\n(g) alphaxiv\n(d) 4o-Template\n(h) PWAgent(ours)\n(f) arXiv-HTML\nFigure 15: Illustration of website variants for the paper “MVDream: Multi-view Diffusion for 3D Genera-\ntion”7generated by different methods.\n7https://mv-dream.github.io/\n35\n",
    "content": "# Paper Interpretation: PAPER2WEB: LET’S MAKE YOUR PAPER ALIVE!\n\n## 1. Core Content and Key Contributions\n\nThis paper introduces **PAPER2WEB**, a novel task, dataset, and evaluation framework designed to automatically transform traditional static academic papers into **interactive, multimedia-rich project websites**, thereby enhancing the dissemination and impact of research outcomes.\n\nThe central goal is to address the widespread issue of \"information loss\" in current scientific communication—PDFs lack interactivity and multimedia support, limiting effective presentation and public understanding of research. To this end, the authors present a systematic solution encompassing data collection, automated generation, and quality assessment.\n\n### Key Contributions:\n\n- **Proposing the New Task: PAPER2WEB**  \n  The paper formally defines for the first time the research task of transforming full academic papers into structured, interactive web pages, emphasizing its value in advancing science communication.\n\n- **Building the First Large-Scale Paired Dataset (PAPER2WEB Dataset)**  \n  By scraping top-tier AI conferences (e.g., ICML, NeurIPS), the authors collected papers along with their manually created official project websites, resulting in a dataset containing **10,716 papers with website links** and over 80,000 without. This provides a valuable benchmark for future research.\n\n- **Designing a Multidimensional Evaluation Framework (Benchmark):**\n  - **Connectivity & Completeness**: Rule-based metrics assessing internal/external link quality and content completeness;\n  - **Human/MLLM-as-a-Judge**: Leveraging multimodal large language models (MLLMs) as automated evaluators, combined with human validation, to assess **interactivity, aesthetic design, and information clarity**;\n  - **PaperQuiz**: An innovative knowledge retention test using question-answer pairs to evaluate whether readers can accurately extract key information from the generated webpage, including a \"verbosity penalty\" to discourage excessive text copying.\n\n- **Introducing an Advanced Automated System: PWAGENT**  \n  A smart agent system built on the Model Context Protocol (MCP) that iteratively optimizes content structure and visual layout, significantly outperforming existing approaches.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nThe paper achieves notable advances across several dimensions:\n\n### (1) First Definition of the \"Paper-to-Webpage\" Task and Standardized Dataset  \nWhile prior work has focused on converting papers into posters, slides, or videos (e.g., Paper2Poster, PresentAgent), no prior study systematically addressed the transformation into **fully functional, well-structured, interactive project websites**. This paper fills that gap by establishing a standardized task and high-quality real-world dataset, paving the way for rigorous research in this direction.\n\n### (2) Comprehensive Evaluation Framework Aligned with User Experience  \nTraditional webpage generation evaluations often rely solely on code correctness or static metrics. In contrast, this work introduces three complementary evaluation categories:\n- **Rule-driven + automatic parsing** (Connectivity/Completeness)\n- **Human preference simulation** (MLLM-as-a-Judge)\n- **Knowledge transfer effectiveness testing** (PaperQuiz)\n\nNotably, **PaperQuiz** is highly innovative—it doesn’t just check “whether content exists,” but evaluates “whether users can learn from it.” This cognition-efficiency-centered paradigm represents a significant upgrade in evaluating AI-generated content quality.\n\n### (3) MCP-Driven Multi-Agent Collaborative Architecture (PWAGENT)  \nInstead of end-to-end HTML generation, PWAGENT innovatively uses the **Model Context Protocol (MCP)** to decompose the paper into a structured asset library (text, figures, links, etc.) and employs an \"orchestrator agent\" to perform **iterative optimization**.\n\nKey advantages include:\n- Step-by-step processing of long-context inputs, avoiding information loss when LLMs handle entire papers;\n- Fine-grained control over layout and interactive elements;\n- Global-local joint reasoning for improved visual consistency;\n- Low cost (~$0.025 per page) and high efficiency.\n\nExperiments show that PWAGENT outperforms arXiv's HTML version by up to 28% across multiple metrics and approaches human-level design quality, achieving Pareto superiority in performance-to-cost ratio.\n\n### (4) Revealing Fundamental Flaws in Current Automated Web Generation Systems  \nThrough analysis of arXiv HTML and alphaXiv outputs, the paper identifies critical issues:\n- Direct conversion leads to **chaotic layouts, poor responsiveness, and unbalanced text-image ratios**;\n- While LLM-based summarization simplifies content, it often sacrifices **interactivity and visual appeal**;\n- Lack of proactive integration strategies for multimedia components.\n\nThese insights provide clear directions for future improvements.\n\n---\n\n## 3. Viable Startup Project Ideas\n\nBased on the core concepts of PAPER2WEB, several market-ready startup opportunities emerge. Below are promising entrepreneurial ventures worth exploring:\n\n### 🚀 Startup Idea 1: **ResearchHub — One-Click Research Publication Platform**\n\n> **Positioning**: A one-stop digital platform for researchers to publish and promote their scholarly outputs  \n> **Slogan**: “Give every paper its own official website”\n\n#### Core Features:\n- Upload a paper PDF → Automatically generate a professional-grade project website (with dynamic charts, embedded demo, BibTeX citation buttons);\n- Customizable templates (academic, minimalist, futuristic styles);\n- Auto-generate social media copy (integrated AutoPR), posters (Paper2Poster), and presentation scripts (PresentAgent);\n- Provide visitor analytics, reader feedback collection, and collaboration request portals;\n- Integrate with GitHub, Hugging Face, OpenReview, etc., for resource aggregation.\n\n#### Business Model:\n- Free basic features (to attract early adopters);\n- Subscription fees for premium templates, branded domains, team collaboration spaces, SEO enhancement;\n- Partner with journals/publishers to offer as part of “enhanced publishing” packages.\n\n#### Competitive Edge:\n- Powered by the PAPER2WEB dataset and PWAGENT technology, delivering superior output quality;\n- Builds a “digital academic identity” ecosystem, creating network effects.\n\n---\n\n### 🎓 Startup Idea 2: **SciCompass — Science Communication Engine**\n\n> **Positioning**: An AI assistant helping scientists communicate research to the public  \n> **Slogan**: “Turn your paper into a story everyone can understand”\n\n#### Core Features:\n- Input a paper → Output an interactive, public-friendly科普 webpage;\n- Auto-generate explanatory animations, short video scripts, Twitter threads, podcast outlines;\n- Rewrites technical details in plain language using analogies, metaphors, and visualizations;\n- Supports multilingual localization;\n- Built-in compliance module to prevent exaggeration and ensure ethical standards.\n\n#### Use Cases:\n- Purchased by university communications offices;\n- Tool for fulfilling “public engagement” requirements in grant applications;\n- Personal branding for individual scientists.\n\n#### Business Model:\n- B2B licensing to universities, research institutes, foundations;\n- B2C pay-per-use or monthly subscription for individual researchers.\n\n---\n\n### 💼 Startup Idea 3: **PubliFlow — Intelligent Enhanced Publishing Solution for Publishers**\n\n> **Positioning**: Next-generation \"Enhanced Articles\" production system for academic publishers  \n> **Slogan**: “More than PDF — Redefining the academic publishing experience”\n\n#### Core Features:\n- Integrates with publisher manuscript systems to auto-generate interactive web versions for every accepted paper;\n- Supports rich media embedding: 3D models, code sandboxes, reproducible experiment environments, interactive data visualizations;\n- Provides DOI binding, citation tracking, and reader behavior dashboards;\n- Complies with WCAG accessibility standards;\n- Seamlessly integrates into platforms like SpringerLink, IEEE Xplore.\n\n#### Differentiation:\n- Solves the “looks okay but feels empty” problem of conventional HTML versions through truly high-quality automation;\n- Dramatically reduces editorial labor costs;\n- Increases journal impact and reader engagement.\n\n#### Business Model:\n- Annual SaaS subscription;\n- Tiered pricing based on article volume;\n- Premium analytics add-ons.\n\n---\n\n### 🔍 Additional Suggestion: Open Ecosystem & Developer Tools\n\nConsider launching a **PAPER2WEB SDK** or open API platform allowing third-party developers to build plugins, such as:\n- “One-click Gradio Demo embed code generator”\n- “Auto-extract method flowcharts and convert to SVG animations”\n- “Generate audio-guided tour scripts for paper explanation”\n\nBy combining an open-source community with commercial APIs, you can create a thriving ecosystem around **intelligent presentation of academic content**.\n\n---\n\n## Summary\n\n| Dimension | Content |\n|--------|--------|\n| **Core Idea** | Transform static papers into vivid, interactive project websites to enhance scientific communication |\n| **Key Innovation** | First integrated framework combining task definition, dataset, and evaluation; introduces MCP-based agent system |\n| **Technical Highlights** | PWAGENT enables high-quality, low-cost, iterative webpage generation |\n| **Entrepreneurial Potential** | Applicable to research services, science communication, and publishing innovation—with broad market prospects |\n\n> ✅ **One-sentence takeaway**: This isn’t just another paper about AI-generated webpages—it’s the key to unlocking the era of *living papers*. The future of research shouldn’t just be read—it should be experienced.",
    "github": "",
    "hf": ""
}