{
    "id": "2510.20393",
    "title": "Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval",
    "summary": "This article proposes a novel causal approach that predicts cooking elements that may be overlooked in images and explicitly injects these elements into cross-modal representation learning to mitigate biases and improve recipe retrieval performance.",
    "abstract": "Existing approaches for image-to-recipe retrieval have the implicit assumption that a food image can fully capture the details textually documented in its recipe. However, a food image only reflects the visual outcome of a cooked dish and not the underlying cooking process. Consequently, learning cross-modal representations to bridge the modality gap between images and recipes tends to ignore subtle, recipe-specific details that are not visually apparent but are crucial for recipe retrieval. Specifically, the representations are biased to capture the dominant visual elements, resulting in difficulty in ranking similar recipes with subtle differences in use of ingredients and cooking methods. The bias in representation learning is expected to be more severe when the training data is mixed of images and recipes sourced from different cuisines. This paper proposes a novel causal approach that predicts the culinary elements potentially overlooked in images, while explicitly injecting these elements into cross-modal representation learning to mitigate biases. Experiments are conducted on the standard monolingual Recipe1M dataset and a newly curated multilingual multicultural cuisine dataset. The results indicate that the proposed causal representation learning is capable of uncovering subtle ingredients and cooking actions and achieves impressive retrieval performance on both monolingual and multilingual multicultural datasets.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Multi-Agent",
    "authors": "Qing Wang,Chong-Wah Ngo,Yu Cao,Ee-Peng Lim",
    "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Multimedia (cs.MM)"
    ],
    "comments": "Comments:ACM Multimedia 2025",
    "keypoint": "The proposed causal approach mitigates cross-modal representation bias by predicting overlooked culinary elements and injecting them into representation learning.\nThe method uses a backdoor adjustment to refine cosine similarity, reducing biases caused by spurious correlations from ingredients and cooking actions.\nNeural networks with culinary-specific classifiers and dictionaries are introduced to approximate the debiasing terms and can be plugged into existing models.\nThe approach improves retrieval performance on both monolingual (Recipe1M) and newly curated multilingual multicultural datasets.\nDebiasing both ingredients and actions yields better performance than using either one alone, with consistent improvements in Recall@1 and medR.\nIn monolingual retrieval, the best configuration (+Both) improves medR by 1.0 and Recall@1 by up to 5.6% on the 10K test set compared to baselines.\nThe method outperforms recent SOTA models like DAR and FMI in image-to-recipe retrieval on the 10K test set.\nScalability tests show consistent gains across larger test sets (20Kâ€“50K), with ingredient-and-action debiasing achieving the highest Recall@1 improvement.\nZero-shot retrieval experiments demonstrate robustness, especially when action predictions help distinguish visually similar dishes like pizza and cheesecake.\nFor multicultural retrieval, culture-specific ingredient and action dictionaries are used to handle cuisine-specific biases.\nThe multicultural dataset includes five cuisines: Indonesia, Malaysia, Thailand, Vietnam, and India, with significant variation in ingredient and action overlap.\nDebiasing improves retrieval most for low-resource cultures like Vietnam and India, reducing medR by over 100 ranks in some cases.\nUsing multilingual CLIP variants as backbones, the method achieves substantial gains, with OpenCLIP +Both reducing medR from 18.9 to 15.4 under oracle conditions.\nThe culture prediction classifier enables near-oracle performance, showing that accurate cultural context enhances debiasing effectiveness.\nThe proposed modules introduce minimal inference overhead, with only 12ms added per query when using OpenCLIP +Both.",
    "date": "2025-10-25",
    "paper": "Mitigating Cross-modal Representation Bias for Multicultural\nImage-to-Recipe Retrieval\nQing Wang\nqwang@smu.edu.sg\nSingapore Management University\nSingapore, Singapore\nChong-Wah Ngo\ncwngo@smu.edu.sg\nSingapore Management University\nSingapore, Singapore\nYu Cao\nyu.cao.2022@msc.smu.edu.sg\nSingapore Management University\nSingapore, Singapore\nEe-Peng Lim\neplim@smu.edu.sg\nSingapore Management University\nSingapore, Singapore\nAbstract\nExisting approaches for image-to-recipe retrieval have the implicit\nassumption that a food image can fully capture the details textu-\nally documented in its recipe. However, a food image only reflects\nthe visual outcome of a cooked dish and not the underlying cook-\ning process. Consequently, learning cross-modal representations\nto bridge the modality gap between images and recipes tends to\nignore subtle, recipe-specific details that are not visually apparent\nbut are crucial for recipe retrieval. Specifically, the representations\nare biased to capture the dominant visual elements, resulting in\ndifficulty in ranking similar recipes with subtle differences in use\nof ingredients and cooking methods. The bias in representation\nlearning is expected to be more severe when the training data is\nmixed of images and recipes sourced from different cuisines. This\npaper proposes a novel causal approach that predicts the culinary el-\nements potentially overlooked in images, while explicitly injecting\nthese elements into cross-modal representation learning to mitigate\nbiases. Experiments are conducted on the standard monolingual\nRecipe1M dataset and a newly curated multilingual multicultural\ncuisine dataset. The results indicate that the proposed causal repre-\nsentation learning is capable of uncovering subtle ingredients and\ncooking actions and achieves impressive retrieval performance on\nboth monolingual and multilingual multicultural datasets.\nCCS Concepts\nâ€¢ Information systems â†’Information retrieval.\nKeywords\nCross-modal retrieval, recipe retrieval, food computing\nACM Reference Format:\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim. 2025. Mitigat-\ning Cross-modal Representation Bias for Multicultural Image-to-Recipe\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMM â€™25, Dublin, Ireland.\nÂ© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-2035-2/2025/10\nhttps://doi.org/10.1145/3746027.3755583\nRetrieval. In Proceedings of the 33rd ACM International Conference on Multi-\nmedia (MM â€™25), October 27â€“31, 2025, Dublin, Ireland. ACM, New York, NY,\nUSA, 18 pages. https://doi.org/10.1145/3746027.3755583\n1\nIntroduction\nCross-modal recipe retrieval offers a scalable alternative to tradi-\ntional classification in food analysis [4, 22, 29]. Previous research [5,\n28, 29, 48, 50] has framed recipe retrieval as a cross-modal repre-\nsentation learning problem, where recipes and images are encoded\nwith separate encoders and projected into a shared embedding\nspace to maximize pairwise similarity. An implicit assumption is\nthat an image can capture and reflect its recipe content. Therefore,\nthe learning aims to embed the ingredients and cooking proce-\ndure jointly observed in images and recipes into the shared space.\nNevertheless, ingredients and cooking actions cannot be treated\nequally due to their visual impressions in food images. For example,\nseasoning ingredients to enhance flavor are not as visible as the ma-\njor ingredients of a dish. Similarly, transformative cooking actions\n(e.g., cutting, baking), which change the structure and texture of\ningredients, are more visible than preservative actions (e.g., salting,\nsmoking), which only introduce subtle changes to appearance. As\nthe visibility of ingredients and cooking actions is unequal, repre-\nsentation learning by maximizing the correlation between images\nand recipes can inevitably result in representation biases.\nTo address biased representation learning, we focus on removing\nspurious correlations that negatively impact the accuracy of cross-\nmodal similarity measurement. By treating both ingredients and\ncooking actions as confounders in food preparation, we propose a\nnovel backdoor adjustment to refine cosine similarity commonly\nused for this problem and thus improve retrieval performance.\nSpecifically, we inject two additional terms corresponding to the\nrepresentations of ingredients and cooking actions, respectively,\nto reduce the biases in similarity measurement. We also propose\nneural networks comprising culinary-specific classifiers and dic-\ntionaries to approximate these two terms, which are lightweight\nand can be plugged-and-played into the existing SOTA models,\nincluding H-T [28], TFood [31], VLPCook [32] and multilingual\nCLIP variants [1, 12, 35], for image-to-recipe retrieval.\nThe main contributions of this paper are twofold. First, we pro-\npose a causal view of cross-modal image-to-recipe retrieval, which\nleads to an elegant formulation for alleviating the representation\nbias. A novel backdoor adjustment is thus proposed to mitigate\narXiv:2510.20393v1  [cs.CV]  23 Oct 2025\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nthe representation biases introduced by ingredients and cooking\nmethods. Second, we consider multilingual multicultural recipe\nretrieval, where the bias in representation can become even more\nsevere with partially overlapping ingredients and cooking actions\namong cuisines. By the proposed backdoor adjustment, we pro-\npose plug-and-play neural modules to reduce the cuisine-specific\nbiases in representation learning. To our best knowledge, there is\nno prior research addressing the issue of representation bias for\nmulticultural image-to-recipe retrieval.\n2\nRelated Work\nCross-modal recipe retrieval is to retrieve a recipe corresponding\nto a dish image or vice versa. Most approaches in this area use sep-\narate encoders to map images and recipes into a shared embedding\nspace and maximize pairwise similarity. Recipe encoders are based\non LSTMs [30], hierarchical Transformer [28], and multilingual\nBERT [9, 49]. Image encoders include ResNet-50 pre-trained on Ima-\ngeNet [2, 8, 28, 29, 44], and CLIP-ViT with CLIP weights [11, 31, 32].\nCross-modal multilingual alignment has seldom been explored\nfor recipe retrieval, except for recipe augmentation [9, 49]. In X-\nMRS [9], multilingual BERT is exploited to augment recipes by back\ntranslation (e.g., translate an English recipe to German, and then\nthe German recipe back to the English version) for representation\nlearning. In Recipe Mixup [49], multilingual BERT is also employed\nfor recipe augmentation to address cross-lingual domain adaptation.\nNone of these works address the issue of representation bias.\nRecent works [11, 32, 33, 47] enhance representation learning\nusing multimodal contexts extracted from foundation models. VLP-\nCook [32] utilizes CLIP to identify ingredients and titles that best\nmatch a query image as context. Similarly, FMI [47] uses title and\ningredient features extracted from recipes to enhance image rep-\nresentation. Recently, DAR [33], employs SAM [14] to segment\ningredients in images. The segmented regions are used to align\nwith Llama2-generated visual descriptions extracted from its recipe\nfor representation learning. Rather than enriching representation\nwith contexts as in [11, 32, 33, 47], we leverage causal inference\nto identify the causes and then propose backdoor adjustment to\nalleviate bias in representation learning.\nCausal inference has been widely applied to representation learn-\ning across various tasks [18, 20, 41, 42], focusing on single-modal\nimage representation learning. In the context of multimodal learn-\ning, [25] identifies that the visual dialogue task is confounded by\nan unobserved variable (i.e., user preference), introducing spurious\ncorrelations between questions and answers. Similarly, in video\nmoment retrieval [45], an unobserved confounder (i.e., moment\ntemporal location) induces spurious correlations between model\ninput and prediction. In E-commerce cross-modal retrieval [21],\ncommon-sense biases learned in pretrained models are identified as\nconfounders. Unlike these approaches, we aim to mitigate dataset\nbias by identifying observable confounders within the dataset.\nIt is worth noting that there are also efforts aimed at learning\nrobust representations by reconstructing cooking programs [23]\nand recipes [6, 27, 37] from images. For instance, in [23], both food\nimages and recipes are represented as cooking programs. To achieve\nthis, cooking programs are first crowdsourced for the Recipe1M\ndataset, and a program decoder is employed to generate cooking\nFigure 1: Left: Causal graph with ingredients and actions as\nconfounders. Right: Backdoor adjustment mitigates spurious\ncorrelations by removing incoming edges to the image node.\nprograms based on food images or cooking recipes. By encouraging\nthe generated programs to closely match the crowdsourced ones,\nimproved cross-modal retrieval and food recognition performance\nare attained. Although these approaches are not explicitly grounded\nin causal theory, the multi-modal representations learned in this\nmanner may also capture the causal effects inherent in cooking.\n3\nCausality-based Representation Learning\nA Causal View. We denote the image, recipe, ingredient, and cook-\ning action as ğ¼, ğ‘…, ğ¼ğ‘›ğ‘”, and ğ´ğ‘ğ‘¡, respectively. Their relationships are\nillustrated in the directed graph in Figure 1, with directed edges\npresenting the causal relationships between nodes. For example,\nğ¼ğ‘›ğ‘”â†’ğ‘…â†ğ´ğ‘ğ‘¡indicates that a recipe (ğ‘…) is composed of (or\ncaused by) ingredients (ğ¼ğ‘›ğ‘”) and actions (ğ´ğ‘ğ‘¡). ğ¼ğ‘›ğ‘”â†’ğ¼â†ğ´ğ‘ğ‘¡\nsymbolizes an image (I) as the cause of applying a sequence of\nactions on ingredients as prescribed in a recipe, i.e., ğ‘…â†’ğ¼. The\nconfounders, ğ¼ğ‘›ğ‘”and ğ´ğ‘ğ‘¡, skew the information flow through the\npathways ğ‘…â†ğ¼ğ‘›ğ‘”â†’ğ¼and ğ‘…â†ğ´ğ‘ğ‘¡â†’ğ¼, respectively, creating\nspurious correlation and biased similarity measure (ğ‘†). The dis-\ntorted flow is further amplified by the fact that major ingredients\nand the effects of transformative cooking actions, which are more\nvisible in food images, tend to exhibit greater influence on represen-\ntation learning. The biases impede accurate cross-modal similarity\nmeasures. By Bayes rule, we model the image-recipe similarity as:\nğ‘ƒ(ğ‘†|ğ¼, ğ‘…)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘†,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ¼, ğ‘…)\n(1a)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘†|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ¼, ğ‘…)\n(1b)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘†|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼, ğ‘…)ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”).\n(1c)\nIn Eq. (1c), ingredients observed in both image and recipe exhibit\na higher value of ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼, ğ‘…). Conversely, ingredients, which appear\nin ğ‘…but are hardly observed in ğ¼, will have less impact on the sim-\nilarity score, ğ‘ƒ(ğ‘†|ğ¼, ğ‘…). Similarly, ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”) will bias towards\ncooking methods such as chopping, which will alter the visual ap-\npearance of ingredients in terms of shape and structure, more than\nactions such as simmering and marinating, which are harder to\nobserve in the image. Note that the subtle variations introduced by\ningredients and cooking methods, which are poorly or partially cap-\ntured in images, play a crucial role in ğ‘ƒ(ğ‘†|ğ¼, ğ‘…) for disambiguating\nsimilar recipes. This includes recipes that use the same ingredients\nbut differ in cooking methods, as well as those that share both\ncooking technique and ingredients but vary in seasoning.\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nBackdoor Adjustment. To remove spurious correlations caused\nby the confounders, we apply backdoor adjustment to intervene\nthe image variable (i.e., ğ‘‘ğ‘œ(ğ¼)) by removing all the incoming edges\nto image ğ¼(Figure 1 (right)), resulting in the similarity measure as:\nğ‘ƒ(ğ‘†|ğ‘‘ğ‘œ(ğ¼), ğ‘…)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘†|ğ‘‘ğ‘œ(ğ¼), ğ‘…,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ‘‘ğ‘œ(ğ¼), ğ‘…)\n(2a)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘†|ğ‘‘ğ‘œ(ğ¼), ğ‘…,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ‘…)\n(2b)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘†|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ‘…)\n(2c)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘†|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…)ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”),\n(2d)\nwhere by the rule-3 of ğ‘‘ğ‘œ-calculus (Theorem 3.4.1 [24]), the ğ‘‘ğ‘œ(ğ¼)\nin ğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ‘‘ğ‘œ(ğ¼), ğ‘…) can be omitted. This is due to ğ‘†is a col-\nlider of ğ‘…and ğ¼and blocks the information flow from ğ¼ğ‘›ğ‘”to ğ¼, i.e.,\nğ¼ğ‘›ğ‘”â†’ğ‘…â†’ğ‘†â†ğ¼. Hence, ğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ‘‘ğ‘œ(ğ¼), ğ‘…) = ğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ‘…) and\nwe have Eq. (2b). By rule-2 of do-calculus, we obtain Eq. (2c) be-\ncause ğ‘†is independent of ğ¼after removing the outgoing edges from\nğ¼. Using the chain rule of conditional probability, we decompose\nğ‘ƒ(ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡|ğ‘…) in Eq. (2c) and arrive at Eq. (2d).\nNeural Approximation. Eq. (2d) mitigates the bias by weight-\ning the similarity with the true distributions of ingredients and\nactions in a recipe, denoted as ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…) and ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”), rather\nthan the confounded distributions ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼, ğ‘…) and ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”).\nIn Eq. (2d), we set ğ‘ƒ(ğ‘†|ğ¼, ğ‘…,ğ‘–ğ‘›ğ‘”,ğ‘ğ‘ğ‘¡) = ğ‘“ğ‘ (ğ‘’ğ¼,ğ‘’ğ‘…,ğ‘’ğ‘–ğ‘›ğ‘”,ğ‘’ğ‘ğ‘ğ‘¡), where\nğ‘“ğ‘ () is a similarity function, and ğ‘’ğ¼,ğ‘’ğ‘…,ğ‘’ğ‘–ğ‘›ğ‘”,ğ‘’ğ‘ğ‘ğ‘¡are the embedding\nof image ğ¼, recipe ğ‘…, ingredient ğ¼ğ‘›ğ‘”and action ğ´ğ‘ğ‘¡, respectively:\nğ‘ƒ(ğ‘†|ğ‘‘ğ‘œ(ğ¼), ğ‘…)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘“ğ‘ (ğ‘’ğ¼,ğ‘’ğ‘…,ğ‘’ğ‘–ğ‘›ğ‘”,ğ‘’ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”)ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…)\n(3a)\nâ‰ˆğ‘’ğ‘…Â·\n \nğ‘’ğ¼+\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼) Â· ğ‘’ğ‘–ğ‘›ğ‘”\n+ ğ‘ƒ(ğ‘–ğ‘›ğ‘”1|ğ¼) Â·\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”1) Â· ğ‘’ğ‘ğ‘ğ‘¡\n+ . . . + ğ‘ƒ(ğ‘–ğ‘›ğ‘”ğ¾|ğ¼) Â·\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”ğ¾) Â· ğ‘’ğ‘ğ‘ğ‘¡\n!\n,\n(3b)\nwhere Eq. (3b) approximates the backdoor adjustment formula.\nPlease refer to Section E of the supplementary document for the\nfull derivation. In Eq. (3b), besides estimating ğ‘ƒ(ğ¼ğ‘›ğ‘”ğ‘–|ğ¼), the actions\nassociated with an ingredient Ã\nğ‘ğ‘ğ‘¡ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼, ğ¼ğ‘›ğ‘”ğ‘–) are also estimated.\nDiscussion. To facilitate the comparison between Eq. (3b) and\nthe conventional similarity measure in [28], we simplify Eq. (3b) to:\nğ‘ƒ(ğ‘†|ğ‘‘ğ‘œ(ğ¼), ğ‘…)\nâ‰ˆğ‘’ğ‘…Â·\n \nğ‘’ğ¼+\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼) Â· ğ‘’ğ‘–ğ‘›ğ‘”\n|                {z                }\ningredient debiasing\n+\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼, Ë†\nğ¼ğ‘›ğ‘”) Â· ğ‘’ğ‘ğ‘ğ‘¡\n|                      {z                      }\naction debiasing\n!\n(4a)\n= ğ‘’ğ‘…Â· (ğ‘’ğ¼+ ğ‘’ğ¼ğ‘›ğ‘”+ ğ‘’ğ´ğ‘ğ‘¡).\n(4b)\nThe term Ã\nğ‘˜ğ‘(ğ‘–ğ‘›ğ‘”ğ‘˜|ğ¼) Ã\nğ‘ğ‘ğ‘¡ğ‘(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”ğ‘˜) Â· ğ‘’ğ‘ğ‘ğ‘¡in Eq. (3b) is ab-\nbreviated as ğ‘’ğ´ğ‘ğ‘¡= Ã\nğ‘ğ‘ğ‘¡ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼, Ë†\nğ¼ğ‘›ğ‘”) Â· ğ‘’ğ‘ğ‘ğ‘¡, where Ë†\nğ¼ğ‘›ğ‘”represents\ningredient composition which is introduced to simplify the visu-\nalization of the equation. Eq. (4b) extends the conventional dot\nproduct term [28] (i.e., ğ‘’ğ‘…Â· ğ‘’ğ¼) with two debiasing terms. The first\nterm adjusts image representation ğ‘’ğ¼by adding a linear sum of\ningredient embeddings weighted by their probabilities. The second\nterm performs adjustment by supplementing ğ‘’ğ¼with cooking action\nembeddings conditioned on the ingredient composition.\n4\nMulti-lingual Multi-cultural Recipe retrieval\nThe two debiasing terms in Eq. (3b) can be implemented using one\nneural network to predict the presence of ingredients conditioned\non an image, and another network to predict the presence of cook-\ning actions conditioned on the predicted ingredients. These two\nnetworks can be \"added\" or plugged into the existing cross-modal\nrepresentation models [28, 31, 32] to alleviate the potential bias in\nrepresentation learning. The existing models, nevertheless, consider\nmostly retrieving recipes from a dataset composed of monolingual\nWestern-dominated cuisines (e.g., Recipe1M [29]). In this paper, we\nfurther explore the proposed work for multilingual multicultural\nrecipe retrieval. Specifically, in a multi-cuisine dataset, the recipes\nare written in different native languages. Two cuisines can differ in\nterms of ingredient and cooking action distributions, and only share\na partial set of ingredients and cooking techniques. Learning to\nremove representation bias in such a scenario is highly challenging.\nFigure 2 depicts the overall framework, where the cross-modal\nretrieval module is plugged with culture-specific ingredients and\naction debiasing modules based on Eq. (3b). The ingredient debi-\nasing module (Figure 3) predicts ingredient distribution using a\nmulti-label classifier, then retrieves relevant ingredients from an\ningredient dictionary. Meanwhile, the action debiasing (Figure 4)\nmodule generates a sequence of cooking actions with a generation\nmodel, followed by retrieving corresponding actions from an action\ndictionary. The dual modules are specifically tailored and trained\nfor each culture. In other words, each culture maintains its own\nlocal predictors and dictionaries to debias the image representations\nglobally learned in the cross-modal retrieval module.\nCross-modal retrieval. The image encoder can be implemented\nwith ResNet-50 [10] or Vision Transformer (ViT) [7]. In a similar\nway, the recipe encoder can be implemented with a hierarchical\ntransformer [28] to embed the three sections (i.e., title, ingredients,\nand cooking instructions) of a recipe. We employ multilingual CLIP\nvariants [1, 12, 35] for embedding both images and recipes written in\ndifferent native languages to derive image embedding ğ‘’ğ¼and recipe\nembedding ğ‘’ğ‘…. We finetune the CLIP models using the recipes of\nall cultures in a dataset.\nCulture-specific dictionary. Training a universal dictionary\ncomprising culinary elements of different cultures is not practical.\nIn general, the usage and popularity of ingredients and culinary\ntechniques vary across cultures. For example, in Vietnam, ingredi-\nents such as â€œrice paperâ€ are unique and almost never used in other\nregions such as Indonesia, Malaysia, or India. Similarly, cooking\nactions such as â€œtemperingâ€ are popular in India but rarely used\nin Indonesia, Malaysia, or Vietnam. Hence, we propose culture-\nspecific dictionaries for debiasing. For each culture, we compile the\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nFigure 2: The proposed framework for multicultural recipe retrieval. Given a query image of Culture X, the ingredient and action\ndebiasing modules in the culture will derive the embeddings ğ‘’ğ´ğ‘ğ‘¡and ğ‘’ğ¼ğ‘›ğ‘”, respectively. The two embeddings are then added to\nthe image embedding, ğ‘’ğ¼, learnt globally in the cross-modal retrieval module for alleviating representation biases. Please refer\nto Figure 3 and Figure 4 for the architectures of ingredient debiaisng module and action debiasing module, respectively.\nmost frequent ingredients and actions from the training recipes, and\nstore their embeddings in the respective dictionaries. The embed-\ndings are encoded by the recipe encoder of the cross-modal retrieval\nmodule. During training, the embeddings stored in dictionaries are\nfrozen while the recipe decoder is finetuned.\nFigure 3: The ingredient debiasing module takes image em-\nbeddings ğ‘’ğ¼act as keys and values, and ingredient label em-\nbeddings as queries. The decoder output is passed to a sig-\nmoid to produce ingredient probabilities ğ‘ƒğ‘–ğ‘›ğ‘”, which weight\nthe dictionary ğ·ğ‘–ğ‘›ğ‘”to yield the debiasing embedding ğ‘’ğ¼ğ‘›ğ‘”.\nIngredient debiasing module, which aims to implement ğ‘’ğ¼ğ‘›ğ‘”=\nÃ\nğ‘–ğ‘›ğ‘”ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼) Â· ğ‘’ğ‘–ğ‘›ğ‘”in Eq (4a), uses a multi-label classifier [19] to\npredict the ingredient probability distribution, as shown in Figure 3.\nSpecifically, we employ a Transformer decoder as the classifier,\nfeeding image embeddings ğ‘’ğ¼as both key and value, while using\nlearnable label embeddings as queries. Using a sigmoid function,\nonly ingredients with a probability above 0.5 are used for debiasing.\nThe probabilities of selected ingredients are normalized to sum to 1.\nThe ingredient embeddings are then retrieved from the dictionary\nand linearly combined, weighted by their probabilities, as shown in\nFigure 3. Note that the selected ingredients ğ‘’ğ¼ğ‘›ğ‘”ğ‘˜will be channeled\nto the action debiasing module for further processing.\nFigure 4: The action debiasing takes the predicted ingredients\nas input. For each ingredient ğ‘’ğ¼ğ‘›ğ‘”ğ‘˜, we generate the sequence\nof cooking actions, and then retrieve the corresponding ac-\ntion embeddings from the dictionary ğ·ğ‘ğ‘ğ‘¡. We normalize the\naction prediction probabilities to weight each action embed-\nding, and then compute a weighted sum of the embeddings to\nobtain the action embeddingğ‘’ğ‘˜\nğ´ğ‘ğ‘¡. The final action embedding,\nğ‘’ğ´ğ‘ğ‘¡, used to enhance the image representation, is obtained by\nfirst normalizing the ingredient probabilities and then using\nprobabilities to compute a weighted sum of the action em-\nbeddings, ğ‘’ğ‘˜\nğ´ğ‘ğ‘¡, corresponding to each ingredient. Decoders\nare shared by all ingredients for generation.\nAction debiasing module, implements the second term ğ‘’ğ´ğ‘ğ‘¡in\nEq (4a), and employs the action decoder [27], as shown in Figure 4.\nConditioned on the image embedding ğ‘’ğ¼and a predicted ingredient\nembedding ğ‘’ğ¼ğ‘›ğ‘”ğ‘˜, the decoder generates cooking action sequences.\nAn action embedding of an ingredient is computed by retrieving\nthe corresponding action embeddings from the action dictionary,\nweighted by the normalized action probabilities. After computing\nthe action embeddings for each ingredient (ğ‘’1\nğ´ğ‘ğ‘¡,ğ‘’2\nğ´ğ‘ğ‘¡, ...,), we ob-\ntain the final action embedding ğ‘’ğ´ğ‘ğ‘¡by normalizing the ingredient\nprobabilities and calculating the weighted sum of these embeddings\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nassociated with each ingredient using the normalized probabilities.\nThe final action embedding ğ‘’ğ´ğ‘ğ‘¡is then used to adjust the image\nrepresentation, aligning it with the recipe embeddings.\nTraining Objective. The overall training loss combines a bi-\ndirectional triplet loss, Lğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’[38], to bring image-recipe pairs\ncloser in the joint embedding space; a classification loss, Lğ‘ğ‘™ğ‘ [26],\nfor training the ingredient classifier; and a generation loss, Lğ‘”ğ‘’ğ‘›,\nfor the action generator.\nFor the multi-label ingredient classifier, we adopt the asymmetric\nloss [26] to address the challenges of long-tailed distribution of\ningredients. Given the ingredient probabilities ğ‘= [ğ‘ğ‘–ğ‘›ğ‘”1, . . . , ğ‘ğ‘–ğ‘›ğ‘”ğ‘˜]\nfor an image ğ¼, the loss function for ğ¼is defined as:\nLğ¼= 1\nğ¾\nğ¾\nâˆ‘ï¸\nğ‘˜=1\n(\u00001 âˆ’ğ‘ğ‘–ğ‘›ğ‘”ğ‘˜\n\u0001ğ›¾+ log \u0000ğ‘ğ‘–ğ‘›ğ‘”ğ‘˜\n\u0001 ,\nğ‘¦ğ‘–ğ‘›ğ‘”ğ‘˜= 1,\n\u0000ğ‘ğ‘–ğ‘›ğ‘”ğ‘˜\n\u0001ğ›¾âˆ’log \u00001 âˆ’ğ‘ğ‘–ğ‘›ğ‘”ğ‘˜\n\u0001 ,\nğ‘¦ğ‘–ğ‘›ğ‘”ğ‘˜= 0,\n(5)\nwhere ğ‘¦ğ‘–ğ‘›ğ‘”ğ‘˜indicates the presence of the ingredient. Parameters\nğ›¾+ and ğ›¾âˆ’adjust the weighting for positive and negative samples,\nset empirically to ğ›¾+ = 1 and ğ›¾âˆ’= 1.\nFor the action debiasing module, Lğ‘”ğ‘’ğ‘›is implemented using\ncross-entropy loss:\nLgen = âˆ’1\nğ¿\nğ¿\nâˆ‘ï¸\nğ‘™=1\nğ‘‡âˆ‘ï¸\nğ‘¡=1\nlogğ‘ğœƒ(ğ‘¦ğ‘™\nğ‘¡| ğ‘¦ğ‘™\n1:ğ‘¡âˆ’1),\n(6)\nwhere ğ‘¦ğ‘™\nğ‘¡represents the probability of the ğ‘¡ğ‘¡â„action for ğ‘™ğ‘¡â„gener-\nated ingredient and ğ¿is the number of generated ingredients. The\noverall objective function is defined as:\nL = Lğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’+ ğœ†ğ‘ğ‘™ğ‘ Lğ‘ğ‘™ğ‘ + ğœ†ğ‘”ğ‘’ğ‘›Lğ‘”ğ‘’ğ‘›,\n(7)\nwhere ğœ†ğ‘ğ‘™ğ‘ = 0.001 and ğœ†ğ‘”ğ‘’ğ‘›= 0.001 are hyperparameters to balance\nthe triple loss and two debiasing losses.\nTraining Procedure. For monolingual recipe retrieval, we train\nthe framework in Figure 2 in three steps. First, the cross-modal\nretrieval models, specifically the image and recipe encoders, are fine-\ntuned from the pretrained weights. Second, leveraging the encoders,\nthe ingredient and action dictionaries are constructed. Finally, the\nthree modules (retrieval model, ingredient classifier, and action\ngenerator) are trained end-to-end, while the ingredient and action\nembeddings in the dictionaries are frozen without further updating.\nFor multilingual recipe retrieval, we leverage multilingual CLIP\nvariants that have been pretrained on billions of image-text pairs.\nThese pretrained models are directly used to extract ingredient and\naction embeddings for dictionary construction. Subsequently, the\ntwo debiasing modules are plugged into the cross-modal retrieval\nmodule, with multilingual CLIP variants as the image and recipe\nencoders, for end-to-end training.\n5\nExperiment I: Monolingual Recipe Retrieval\nTo validate the proposed backdoor adjustment, we conduct experi-\nments on a monolingual recipe dataset, Recipe1M [29]. The dataset\ncontains 238,999, 51,119, and 51,303 image-recipe pairs for train-\ning, validation, and testing, respectively. We sample search sets\nin multiples of 10K, and unless otherwise specified, we follow the\nevaluation protocol [28] to report performance on 1K and 10K test\nsets. For each test set, we conduct 10 random samplings and report\nthe average performance. The evaluation metrics are median rank\n(medR) and Recall@K, where K=1,5,10. For retrieval models, lower\nmedR and higher Recall@K indicate better retrieval performance.\nImplementation details We follow the settings of baseline\nmethods, i.e., H-T [28], TFood [31], VLPCook [32]. For image en-\ncoders, we adopt ResNet-50, ViT-B/16, and CLIP-ViT-B/16 for H-T,\nTFood, and VLPCook, respectively, where CLIP-ViT-B/16 is initial-\nized with CLIP weights while the rest two with ImageNet weights.\nFor recipe encoders, we use transformer encoders with 2 layers\nand 4 heads for all three models. For the multi-label ingredient\nrecognition, similar to [19], 1 Transformer encoder layer and 2\nTransformer decoder layers are utilized and both have 4 heads. For\nthe action decoder, following [27], we employ a transformer with 4\nblocks and 2 multi-head attention. The batch size is 64 and Adam\noptimizer is used with a base learning rate 10âˆ’4 for H-T and 10âˆ’5\nfor the rest. The ingredient and action debiasing models contain\napproximately 75M and 65M parameters, respectively.\nModel zoo As discussed in Sec. 3, Eq. (4b) offers three distinct\napproaches for debiasing retrieval models:\nâ€¢ +Ingredient: ingredient-only debiasing (i.e., ğ‘’ğ‘…Â·ğ‘’ğ¼+ğ‘’ğ‘…Â·ğ‘’ğ¼ğ‘›ğ‘”),\nwhere a multi-label ingredient classifier predicts the ingredi-\nent distribution, and corresponding ingredient embeddings\nare retrieved to augment the image embeddings.\nâ€¢ +Action: action-only debiasing (i.e., ğ‘’ğ‘…Â· ğ‘’ğ¼+ ğ‘’ğ‘…Â· ğ‘’ğ´ğ‘ğ‘¡). An\naction generator predicts the action distribution, which is\nused to enhance the image embeddings.\nâ€¢ +Both: debiasing both ingredients and actions (i.e., ğ‘’ğ‘…Â· ğ‘’ğ¼+\nğ‘’ğ‘…Â· ğ‘’ğ¼ğ‘›ğ‘”+ ğ‘’ğ‘…Â· ğ‘’ğ´ğ‘ğ‘¡), where a multi-label ingredient classifier\nand a conditional action generator are employed to refine\nthe image embeddings, as shown in Figure 2.\n5.1\nPerformance Comparison\nTable 1 shows the results of image-to-recipe retrieval. Debiasing\nthe model with ingredients or actions leads to the same medR value\nacross the different sizes of the test set. Meanwhile, debiasing in-\ngredients introduces more degree of improvement over cooking\nactions in terms of Recall@1 with around 1% Recall@1 difference.\nDebiasing both ingredients and actions yields the best retrieval per-\nformance, improving the medR of baselines (H-T, VLPCook) by 1.0\non 10K test set. A consistent improvement is also observed, ranging\nfrom 1.7% to 5.6% of difference in Recall@1 across different test\nsizes. Compared to the most recently published results in DAR [33]\nand FMI [47], our results achieves better performance in terms of\nR@1, R@5, and R@10 on 10K test set.\nWe attribute the improvement over the ingredient-only or action-\nonly debiasing to the ability to distinguish the recipes sharing simi-\nlar sets of ingredients or actions. Figure 5 shows an example where\nH-T+ingredient cannot distinguish â€œCarrot pineapple cupcakes\"\nand â€œNutneg cookies logs\", which share a similar set of ingredients\n(i.e., eggs, butter, white sugar, and flour). However, by predicting\nthe actions (i.e., grease and insert) that are unique to the query\nimage, and augmenting both the predicted ingredients and actions\nto the image embedding, the ground-truth recipe is alleviated from\n55ğ‘¡â„(by H-T ingredient) to the top-1 position. Note that using H-\nT+action alone cannot distinguish these two recipes due to some\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nTable 1: Comparison on 1k and 10k test sets for image-to-\nrecipe retrieval. medR (â†“), Recall@k (â†‘) are reported. The\nproposed debiasing boosts the performance of existing cross-\nmodal retrieval methods (H-T, TFood, VLPCook), especially\non the 10k set.\n1k\n10k\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nX-MRS [9]\n1.0\n64.0\n88.3\n92.6\n3.0\n32.9\n60.6\n71.2\nFARM [36]\n1.0\n73.7\n90.7\n93.4\n2.0\n44.9\n71.8\n80.0\nCREAMY [51]\n1.0\n73.3\n92.5\n95.6\n2.0\n44.6\n71.6\n80.4\nCIP [11]\n1.0\n77.1\n94.2\n97.2\n2.0\n44.9\n72.8\n82.0\nDAR [33]\n1.0\n77.3\n95.3\n97.7\n2.0\n47.8\n75.9\n84.3\nFMI [47]\n1.0\n77.4\n95.8\n97.6\n1.0\n48.4\n76.3\n81.9\nH-T [28]\n1.0\n61.8\n88.0\n93.2\n4.0\n29.9\n58.3\n69.6\n+Ingredient\n1.0\n65.7\n89.8\n94.1\n3.0\n34.4\n62.9\n73.6\n+Action\n1.0\n63.6\n88.1\n92.6\n3.0\n32.1\n60.3\n71.1\n+Both\n1.0\n65.7\n88.8\n93.6\n3.0\n35.5\n63.8\n74.1\nTFood [31]\n1.0\n72.4\n92.5\n95.4\n2.0\n43.9\n71.7\n80.8\n+Ingredient\n1.0\n74.5\n93.2\n96.1\n2.0\n45.6\n73.0\n81.6\n+Action\n1.0\n73.8\n93.1\n95.8\n2.0\n45.1\n72.6\n81.3\n+Both\n1.0\n75.8\n93.6\n96.3\n2.0\n46.9\n74.4\n82.8\nVLPCook [32]\n1.0\n77.4\n94.8\n97.1\n2.0\n48.8\n76.2\n84.5\n+Ingredient\n1.0\n78.3\n95.1\n97.4\n1.4\n50.2\n77.3\n85.2\n+Action\n1.0\n77.9\n95.0\n97.4\n1.5\n50.0\n77.4\n85.4\n+Both\n1.0\n79.1\n94.6\n97.0\n1.0\n51.7\n78.2\n85.9\nTable 2: Scalability test on 20k, 30k, 40k and 50k test set.\n20k\n30k\n40k\n50k\nmedR\nR@1\nmedR\nR@1\nmedR\nR@1\nmedR\nR@1\nH-T [28]\n6.3\n22.2\n9.0\n18.4\n12.0\n16.0\n15.0\n14.3\n+Ingredient\n5.0\n26.2\n7.0\n22.0\n9.0\n19.3\n11.0\n17.4\n+Action\n5.8\n24.3\n8.0\n20.3\n10.0\n17.8\n12.6\n15.9\n+Both\n4.7\n27.3\n6.0\n23.0\n8.0\n20.3\n10.0\n18.2\nTFood [31]\n3.0\n35.5\n4.0\n30.9\n5.0\n27.8\n6.0\n25.7\n+Ingredient\n3.0\n37.6\n3.0\n32.9\n4.0\n29.9\n5.0\n26.9\n+Action\n3.0\n36.3\n4.0\n31.6\n4.0\n28.5\n5.0\n26.2\n+Both\n2.0\n38.6\n3.0\n33.6\n4.0\n30.4\n5.0\n28.1\nVLPCook [32]\n2.0\n40.2\n3.0\n35.2\n4.0\n32.0\n4.0\n29.7\n+Ingredient\n2.0\n41.7\n3.0\n36.9\n3.0\n33.7\n4.0\n31.1\n+Action\n2.0\n41.0\n3.0\n36.0\n3.0\n32.7\n4.0\n30.2\n+Both\n2.0\n42.7\n3.0\n37.7\n3.0\n34.5\n4.0\n32.0\nshared cooking actions (e.g., preheat, mix). More results and ex-\namples, including recipe-to-image retrieval, can be found in the\nsupplementary document.\n5.2\nRobustness Test\nScalability. In this section, we present the retrieval performance on\nlarger test set sizes ranging from 20K to 50K for image-to-recipe, as\nshown in Table 2. We can observe debiasing with either ingredients\nor actions yields consistent improvements as test set sizes increase,\nthough the ingredient-only module achieves slightly better results.\nHowever, the best results are achieved by debiasing both ingredients\nand actions, leading to additional gains in Recall@1 ranging from\n0.8% to 3.8% across all test sizes.\nZero-Shot Retrieval. We evaluate the modelâ€™s robustness in\nretrieving unseen food categories, i.e., zero-shot retrieval. To do\nFigure 5: Example showing how the ingredient and action de-\nbiasing disambiguates similar recipes. The first row displays\nthe query image, predicted ingredients, and predicted actions.\nThe second row is the retrieved recipe by H-T+ingredient\nand H-T+action, while the last row is the recipe retrieved by\nH-T+both. The correctly predicted ingredients and cooking\nactions are bolded. The predicted ingredients and cooking ac-\ntions are marked in red and blue, respectively, in the recipes.\nThe ground-truth is boxed in blue.\nso, we exclude all recipes from specific categories in both the train-\ning and validation sets. For instance, recipes from any category\ncontaining the word burger (e.g., turkey burgers, chicken burgers)\nare removed. In total, 78 dish categories, involving 14,415 recipes,\nare excluded. These categories are further grouped based on the\nremoved keywords, and the search results are presented in Ta-\nble 3. For example, the first row labeled â€œpizza\" shows the median\nrank (medR) results for all queries categorized as â€œpizzaâ€-related\nin the test set. Debiasing H-T with ingredients yields substantial\nimprovements in medR across all categories. With the addition of\nthe action debiasing module, some categories, such as â€œpizzaâ€ and\nâ€œcheesecakeâ€, show improved medR performance. For instance, in\nthe case of â€œpizzaâ€, the H-T model with ingredient debiasing often\nconfuses it with visually similar dishes like â€œfrittatasâ€ and â€œpiesâ€,\nwhich are seen during training. However, the action debiasing mod-\nule correctly generates actions such as preheat, bake, and spread,\nwhich are relatively unique to the pizza-making process, allowing\nthe model to rank pizzas higher. However, if the primary cooking\nstyles are misidentified, the action debiasing module can degrade\nretrieval results. For instance, while the major ingredients for a\nâ€œburgerâ€ dish, such as beef or chicken, can be identified, the module\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nTable 3: Median rank comparison for unseen dish categories\non the 50k test set.\nFood type\nOracle\nH-T\nH-T+Ingredient\nH-T+Both\npizza\n1.0\n23.0\n20.0\n16.0\nsteak\n1.0\n27.0\n19.0\n18.0\npancakes\n1.0\n32.0\n19.0\n19.0\ncheesecake\n1.0\n29.0\n18.0\n16.0\ncupcake\n1.0\n22.0\n19.0\n12.0\nlasagna\n1.0\n18.0\n12.0\n15.0\nrice\n1.0\n15.0\n11.0\n12.0\ntacos\n1.0\n17.0\n11.0\n12.0\nburger\n1.0\n23.0\n11.0\n17.0\nwaffles\n1.0\n19.0\n12.0\n12.5\nTable 4: Multi-cultural cuisine recipe dataset.\nTrain\nVal\nTest\nIndonesia\n18,001\n3,177\n3,588\nMalaysia\n13,099\n2,312\n3,437\nThailand\n16,833\n2,971\n3,977\nVietnam\n15,045\n2,656\n3,145\nIndia\n10,618\n1,874\n4,109\nTotal\n73,596\n12,990\n18,256\nmight generate actions like spreading and topping instead of the\nkey cooking methods for burgers, such as grilling or baking. This\ncreates confusion with sandwich dishes, deteriorating the rank of\nthe sandwich dish and resulting in a higher medR value for burgers.\n6\nExperiment II: Multicultural Recipe Retrieval\n6.1\nDataset Curation\nNext, we conduct the experiments on a newly curated dataset com-\nposed of five different cultures: Indonesia, Malaysia, Thailand, Viet-\nnam, and India. The image-recipe pairs are crawled from Cookpad1,\nusing the dish titles compiled from Wikipedia2,3,4,5,6. Given a title,\na rank list of 1 to 6,469 image-recipe pairs are retrieved. For exam-\nple, the recipes â€œnasi lemak ipin upinâ€, â€œnasi lemak hijau pandanâ€\nand â€œsambal nasi lemakâ€ are retrieved by using â€œnasi lemak\" as\nthe search keywords. In total, a dataset composed of 104,842 pairs\nwas curated using 776 dish titles from five different cultures. Please\nrefer to Section F for statistics on the crawled image-recipe pairs,\nas well as ingredient and action overlaps across different cultures.\nTo ensure data quality, we perform deduplication by removing\nsamples with duplicate recipe titles from the test set. Specifically,\nwe randomly retain one sample for each group of duplicate recipes\nand discard the rest. To account for the randomness in this process,\nwe conduct 10 independent samplings and report the average per-\nformance. The statistics of training, validation, and testing sets are\nlisted in Table 4. The dataset is shared publicly7.\n1https://cookpad.com/\n2https://en.wikipedia.org/wiki/List_of_Indonesian_dishes\n3https://en.wikipedia.org/wiki/List_of_Malaysian_dishes\n4https://en.wikipedia.org/wiki/List_of_Thai_dishes\n5https://en.wikipedia.org/wiki/List_of_Vietnamese_dishes\n6https://en.wikipedia.org/wiki/List_of_Indian_dishes\n7https://github.com/GZWQ/multilingual-image-recipe-retrieval\nTable 5: Performance of multicultural recipe retrieval. â€œOra-\ncle\" assumes the culture of a search query is known. â€œClassi-\nfier\" predicts the culture of a query for retrieval.\nOracle\nClassifier\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nNLLB-SigLIP [35]\n176.9\n5.2\n12.9\n17.9\n176.9\n5.2\n12.9\n17.9\n+Ingredient\n165.2\n5.4\n13.3\n18.5\n168.3\n5.1\n13.1\n18.2\n+Action\n175.3\n5.5\n13.1\n18.0\n178.1\n5.0\n12.8\n17.5\n+Both\n151.3\n6.0\n14.1\n19.4\n153.5\n5.6\n13.6\n18.7\nM-CLIP [1]\n72.7\n9.4\n20.0\n26.2\n72.7\n9.4\n20.0\n26.2\n+Ingredient\n58.9\n9.7\n21.3\n28.3\n59.0\n9.4\n20.8\n27.5\n+Action\n57.0\n9.8\n21.1\n28.2\n57.0\n9.5\n20.5\n27.4\n+Both\n55.8\n9.8\n21.4\n28.6\n56.0\n9.6\n21.0\n28.5\nOpenCLIP [12]\n18.9\n16.9\n33.3\n42.0\n18.9\n16.9\n33.3\n42.0\n+Ingredient\n16.0\n18.0\n35.5\n44.2\n16.0\n17.5\n35.0\n43.8\n+Action\n16.0\n18.2\n35.5\n44.2\n16.3\n17.6\n35.0\n43.6\n+Both\n15.4\n18.4\n35.8\n44.5\n16.0\n18.0\n35.1\n44.1\nTable 6: MedR performance for five cultures (ID: Indonesia,\nMY: Malaysia, TH: Thailand, VN: Vietnam, IN: India).\nID\nMY\nTH\nVN\nIN\nNLLB-SigLIP [35]\n119.9\n95.3\n106.7\n420.5\n312.8\n+Ingredient\n115.3\n86.9\n83.6\n426.9\n301.9\n+Action\n133.5\n91.4\n104.1\n411.4\n301.1\n+Both\n113.8\n79.9\n87.3\n329.3\n300.5\nM-CLIP [1]\n35.5\n29.6\n30.9\n123.7\n373.5\n+Ingredient\n29.2\n23.3\n27.7\n108.3\n300.7\n+Action\n28.6\n21.9\n26.4\n102.2\n302.6\n+Both\n28.0\n23.2\n24.5\n99.6\n276.1\nOpenCLIP [12]\n14.9\n11.0\n5.9\n25.0\n74.8\n+Ingredient\n14.4\n9.0\n5.0\n20.5\n64.5\n+Action\n14.4\n9.7\n5.0\n20.1\n60.9\n+Both\n13.8\n9.9\n5.0\n18.9\n63.9\n6.2\nZero-shot Retrieval\nWe conduct a zero-shot retrieval experiment to assess the robust-\nness of the proposed method. To reduce the influence of visual-\nlanguage models (e.g., multilingual CLIP variants) on the results,\nwe use less popular dishes as query images. We follow two protocols\nto ensure that the recipes in the testing set are both less popular\nand unseen in the training and validation sets. First, the testing set\nis composed of recipes that are retrieved with the search keywords\ndifferent from the other two sets. Second, these keywords corre-\nspond to the dishes that are less popularly consumed. We verify\nthe dish popularity in two steps: (1) prompting GPT-4o to rank the\nsearch keywords based on dish popularity in a particular culture,\n(2) sorting the keywords based on the number of returned recipes\nfrom Cookpad. Finally, the image-recipe pairs that are retrieved\nby the search keywords corresponding to the less popular dishes\nidentified by both steps are included in the test set. All the images in\nthe test set are used as search queries in the zero-shot experiment.\nDuring retrieval, the culture of a query image needs to be known\nas a priori to activate the appropriate culture-specific module for\nrepresentation debiasing. To this end, we train a classifier using the\ntraining data to predict the cultures of search queries. Table 5 lists\nthe average retrieval performance for 18,256 search queries. Note\nthat we experiment with three different backbones that support the\nnative languages of five cultures for cross-modal retrieval: NLLB-\nSigLIP [35], multilingual CLIP (mClip) [1], and OpenCLIP [12].\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nTable 5 lists the retrieval performances of the backbones with dif-\nferent plugged-in debiasing models. For reference, we also list the\noracle result, assuming the culture of a search query is known.\nAs shown in Table 5, either the ingredient or action debias-\ning module contributes to performance improvement consistently\nacross three multilingual CLIP variants. Combining both modules\nleads to the largest margin of improvement, for example, elevating\nmedR by about 15 and 3 ranks on the M-CLIP and OpenCLIP back-\nbones, respectively. Although action debiasing on NLLB-SigLP with\na classifier underperforms compared to the baseline, our proposed\ndebiasing methods consistently improve both R@1 and R@10 on M-\nCLIP and OpenCLIP. Compared to the result for monolingual recipe\nretrieval on 10K and 20K test sets, the margin of improvement is\nlarger. This basically indicates the benefit of mitigating representa-\ntion bias for a dataset composed of multiple cuisines. Note that our\nresult (â€œClassifier\") is close to the oracle performance, even though\nthe cultural predictions of the query images are suboptimal. For de-\ntails on the performance of the culture prediction classifier, please\nrefer to Section H in the supplementary. It is also worth mentioning\nthat both debiasing modules introduce minimal overhead to the\nbackbone model. For example, when using OpenClip with both\ndebiasing modules, the retrieval speed for a single query is only 12\nmilliseconds. Please refer to Section I in the supplementary for a\ndetailed training and inference times comparison across different\nmodels.\nTable 6 further details the retrieval performances on different cul-\ntures. The baseline results (without debiasing) for Vietnamese and\nIndian cultures are relatively poor compared to other cultures. The\neffect of debiasing representation for these cultures is particularly\neffective, for example, by elevating the medR for about 100 ranks\non the Indian culture when using mCLIP as the backbone. By debi-\nasing the biases in ingredients and actions, Vietnamese and Indian\ncultures yield a relatively large margin of improvement. The MedR\nperformances are somewhat correlated with the training data size.\nThe training sets for Indian and Vietnamese cultures are smaller\nthan Indonesia and Thailand, which result in higher values of medR.\nAlthough the training size for Malaysian culture is not larger than\nVietnam, it benefits from the Indonesian training data for shar-\ning similar dishes. Our results generally indicate that debiasing\nrepresentation using our approach benefits low-resource cultures\n(e.g., Vietnam, India) more than mid or high-resource cultures (e.g.,\nThailand, Indonesia). Please see Section G in the supplementary for\nthe full set of results, including recipe-to-image retrieval.\nFigure 6 shows an example illustrating the benefit of debiasing\nrepresentation related to both ingredients and actions. Given a\nquery image of Indonesian culture, debiasing by either ingredient or\naction modules will result in an Indian culture recipe being returned\nas the top-1 result. By enhancing the query image representation\nwith the ingredients (e.g., banana leaf) and cooking actions (e.g.,\nboil and steam), the groundtruth recipe is retrieved. More examples\ncan be found in Section J of the supplementary, including failure\ncases where dishes are covered by soup or obscured by toppings.\n7\nConclusion\nInspired by causal inference, we have presented a backdoor adjust-\nment approach to alleviate the representation biases in cross-modal\nFigure 6: Example showing how the ingredient and action\ndebiasing disambiguates similar recipes across cultures. The\nfirst row displays the query image, ingredients and actions\npredicted by debiasing modules. The following rows show\nthe top-1 retrieved recipes (and the ground-truth images)\nby different debiasing modules. The ingredients and actions\ncorrectly predicted are marked in red and blue, respectively.\nimage-to-recipe training. Experimental results on both monolingual\nand multicultural datasets show noticeable retrieval improvement\nintroduced by our proposed apprach. Particularly, debiasing biases\ndue to both ingredients and actions lead to the largest margin of\nimprovement. Furthermore, the results indicate that debiasing rep-\nresentation benefits retrieval more on the multicultural dataset\nthan the monolingual dataset. The medR improvement is more\npronounced for low-resource cultures (e.g., Vietnam, India) than\nfor high-resource ones in our dataset.\nAcknowledgment\nThis research / project is supported by the Ministry of Education,\nSingapore, under its Academic Research Fund Tier 2 (Proposal ID:\nT2EP20222-0046). Any opinions, findings and conclusions or recom-\nmendations expressed in this material are those of the author(s) and\ndo not reflect the views of the Ministry of Education, Singapore.\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nAppendix\nIn this supplementary document, we first present additional re-\nsults and analyses for the monolingual recipe retrieval task. This\nincludes an extensive set of performance comparisons for both\nimage-to-recipe and recipe-to-image retrieval tasks (Section A). We\nalso provide detailed results from scalability tests (Section B) and\nablation studies that explore the selection of ingredients and cook-\ning actions for dictionary construction (Section C). Furthermore,\nwe include additional examples for qualitative and error analyses\n(Section D). Finally, we provide the derivation of the debiasing\nequations (Section E).\nFor the multicultural recipe retrieval task, we begin by presenting\nadditional statistics on the curated multicultural dataset (Section F),\nincluding data distribution and the overlap of ingredients and ac-\ntions across cultures. This is followed by a comprehensive set of\nretrieval results for both image-to-recipe and recipe-to-image tasks\n(Section G). We also include the confusion matrix for the culture\nprediction classifier (Section H), along with a comparison of model\ntraining and inference times (Section I). Lastly, we present further\nexamples for qualitative and error analyses (Section J).\nA\nRecipe-to-Image Retrieval Results\nTable 7 presents the complete results for image-to-recipe and recipe-\nto-image retrieval after debiasing retrieval models using various\nconfounders. For image-to-recipe retrieval, consistent improve-\nments are observed across all models with the proposed debiasing\nmethods. Ingredient-only debiasing achieves slightly greater gains\nthan action-only debiasing, while debiasing both ingredients and\nactions yields the most significant improvements. In the recipe-\nto-image retrieval task, models like TFood and VLPCook show\ncompetitive performance with action-only debiasing compared to\ningredient-only approaches. However, combining both ingredients\nand actions for bias removal during representation learning leads\nto further performance gains, with R1 improvements ranging from\n0.9% to 1.8% compared to single-factor debiasing across the three\nbaseline methods.\nB\nScalability Test\nWe present the full results for both image-to-recipe and recipe-\nto-image retrieval tasks on larger test sets, ranging from 20K to\n50K samples. Results for image-to-recipe are shown in Table 8,\nand for recipe-to-image in Table 9. For both retrieval tasks, the\nproposed debiasing module consistently enhances the performance\nof H-T [28], TFood [31], and VLPCook [32].\nC\nIngredient and Cooking Action Dictionaries\nTheoretically, all ingredients and cooking actions should be in-\ncluded during representation learning to eliminate bias. However,\nincreasing the dictionary size complicates the training of ingredient\nand action generators, negatively affecting generation performance.\nThis highlights a trade-off between retrieval and generation. To\naddress this, we can optimize dictionary size by selecting a subset\nof ingredients and cooking actions that maximize retrieval perfor-\nmance while preserving generation accuracy.\nWe first investigate the impact of ingredient dictionary size on\nretrieval performance by debiasing H-T [28] using ingredients only.\nThe dictionary consists of popular ingredients from Recipe1M, in-\ncluding those that can become \"invisible\" during cooking (e.g., salt,\nbutter). Intuitively, invisible ingredients are unlikely to be predicted\nfrom a food image and may be redundant in the dictionary. How-\never, Table 10 provides empirical insights into this intuition. First,\nwe remove 250 invisible ingredients from the default dictionary\nof 500 ingredients, resulting in a slight impact on retrieval perfor-\nmance. Adding 250 more visible ingredients (based on frequency)\nto this reduced dictionary slightly improves retrieval performance\nbut does not surpass the default dictionary containing both visible\nand invisible ingredients. This suggests that invisible ingredients\nstill provide supplementary value in debiasing image representa-\ntions. We attribute this to the ingredient classifierâ€™s ability to infer\nhard-to-see or invisible ingredients based on co-occurrence rela-\ntionships in cooking [3]. As shown in Table 10, smaller dictionaries\ngenerally reduce retrieval performance despite improving classifi-\ncation accuracy. Conversely, increasing the size to include the 1,000\nmost popular ingredients negatively impacts both classification and\nretrieval performance. A dictionary size of 500 ingredients strikes\nan effective balance in our experiments.\nWe fix the ingredient dictionary size at 500 and investigate the im-\npact of action dictionary size on retrieval performance by debiasing\nH-T with both ingredients and cooking actions. Table 11 illustrates\nthe impact of action dictionary size on the performance of both\naction generation and recipe retrieval. In this experiment, actions\nare sorted by their frequency in the Recipe1M training dataset, and\nonly the most frequent actions are retained in the dictionary. As\nshown in Table 11, a dictionary of 100 actions yields high classifica-\ntion accuracy but relatively low retrieval performance. In contrast,\nexpanding the dictionary to 1,000 actions improves debiasing effects\nwith a 1% increase in recall@1 but results in a 2.3% drop of F1 score\nin action generation. A smaller dictionary of 500 actions strikes a\nbetter balance, slightly outperforming the 1,000-action dictionary\nwhile requiring less memory, making it the optimal trade-off in our\nexperiment.\nD\nQualitative Analysis Monolingual Recipe\nRetrieval\nDebiasing both ingredients and cooking actions outperforms action-\nonly debiasing in recipe retrieval, as it better distinguishes recipes\nthat share similar sets of actions. Figure 7 illustrates an example\nwhere H-T+Action fails to differentiate recipes with similar cooking\nactions (e.g., preheat, spread, sprinkle, and bake). By incorporating\npredicted ingredients (e.g., red onions, parmesan cheese, tomato\npaste, and garlic cloves) alongside cooking actions, the ground-truth\nrecipe is ranked in the Top-1 position.\nWhen the transformative actions are misidentified, the action\ndebiasing module may inadvertently harm retrieval performance.\nFigure 8 illustrates examples where action debiasing leads to in-\ncorrect retrieval results. In the first example, although a sequence\nof preservative actions is correctly recognized, the transformative\naction of the query image is mistakenly identified as frying. This\nincorrect transformative action information, when incorporated\ninto the image representation, causes the rank of the ground-truth\nrecipe to drop from 3ğ‘Ÿğ‘‘(using H-T+ingredient) to 5ğ‘¡â„(using H-\nT+ingredient+action). Similarly, in the second example, the actual\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nFigure 7: An example showing how the ingredient debiasing module disambiguates recipes with a similar set of actions. The\nfirst row displays the query image, predicted ingredients, and predicted actions. The following rows are the retrieved recipes by\nH-T+Ingredient, H-T+Action, and H-T+both, respectively. The correctly predicted ingredients and cooking actions are bolded.\nThe predicted ingredients and cooking actions are marked in red and blue, respectively, in the recipes. The ground-truth recipe\nis boxed in blue.\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nTable 7: Comparison on 1k and 10k test sets. medR (â†“), Recall@k (â†‘) are reported. The proposed debiasing successfully boosts\nthe performance of existing cross-modal retrieval methods (H-T, TFood, VLPCook), especially on the 10k set.\n1k\n10k\nimage-to-recipe\nrecipe-to-image\nimage-to-recipe\nrecipe-to-image\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nRIVAE [13]\n2.0\n39.0\n70.0\n79.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nR2GAN [50]\n2.0\n39.1\n71.0\n81.7\n2.0\n40.6\n72.6\n83.3\n13.9\n13.5\n33.5\n44.9\n12.6\n14.2\n35.0\n46.8\nMCEN [8]\n2.0\n48.2\n75.8\n83.6\n1.9\n48.4\n76.1\n83.7\n7.2\n20.3\n43.3\n54.4\n6.6\n21.4\n44.3\n55.2\nACME [38]\n1.0\n51.8\n80.2\n87.5\n1.0\n52.8\n80.2\n87.6\n6.7\n22.9\n46.8\n57.9\n6.0\n24.4\n47.9\n59.0\nSN [46]\n1.0\n52.7\n81.7\n88.9\n1.0\n54.1\n81.8\n88.9\n7.0\n22.1\n45.9\n56.9\n7.0\n23.4\n47.3\n57.9\nIMHF [15]\n1.0\n59.4\n81.0\n87.4\n1.0\n61.2\n81.0\n87.2\n3.5\n36.0\n56.1\n64.4\n3.0\n38.2\n57.7\n65.8\nSCAN [39]\n1.0\n54.0\n81.7\n88.8\n1.0\n54.9\n81.9\n89.0\n5.9\n23.7\n49.3\n60.6\n5.1\n25.3\n50.6\n61.6\nHF-ICMA [16]\n1.0\n55.1\n86.7\n92.4\n1.0\n56.8\n87.5\n93.0\n5.0\n24.0\n51.6\n65.4\n4.2\n25.6\n54.8\n67.3\nMSJE [43]\n1.0\n56.5\n84.7\n90.9\n1.0\n56.2\n84.9\n91.1\n5.0\n25.6\n52.1\n63.8\n5.0\n26.2\n52.5\n64.1\nSEJE [44]\n1.0\n58.1\n85.8\n92.2\n1.0\n58.5\n86.2\n92.3\n4.2\n26.9\n54.0\n65.6\n4.0\n27.2\n54.4\n66.1\nM-SIA [17]\n1.0\n59.3\n86.3\n92.6\n1.0\n59.8\n86.7\n92.8\n4.0\n29.2\n55.0\n66.2\n4.0\n30.3\n55.6\n66.5\nRDE-GAN [34]\n1.0\n55.1\n86.7\n92.4\n1.0\n56.8\n87.5\n93.0\n5.0\n24.0\n51.6\n65.4\n4.2\n25.6\n54.8\n67.3\nX-MRS [9]\n1.0\n64.0\n88.3\n92.6\n1.0\n63.9\n87.6\n92.6\n3.0\n32.9\n60.6\n71.2\n3.0\n33.0\n60.4\n70.7\nCooking Program [23]\n1.0\n66.8\n89.8\n94.6\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nFARM [36]\n1.0\n73.7\n90.7\n93.4\n1.0\n73.6\n90.8\n93.5\n2.0\n44.9\n71.8\n80.0\n2.0\n44.3\n71.5\n80.0\nCREAMY [51]\n1.0\n73.3\n92.5\n95.6\n1.0\n73.2\n92.5\n95.8\n2.0\n44.6\n71.6\n80.4\n2.0\n45.0\n71.4\n80.0\nCIP [11]\n1.0\n77.1\n94.2\n97.2\n1.0\n77.3\n94.4\n97.0\n2.0\n44.9\n72.8\n82.0\n2.0\n45.2\n73.0\n81.8\nDAR [33]\n1.0\n77.3\n95.3\n97.7\n1.0\n77.1\n95.4\n97.9\n2.0\n47.8\n75.9\n84.3\n2.0\n47.4\n75.5\n84.1\nFMI [47]\n1.0\n77.4\n95.8\n97.6\n1.0\n77.1\n95.4\n97.7\n1.0\n48.4\n76.3\n81.9\n1.0\n49.5\n79.2\n83.1\nH-T [28]\n1.0\n61.8\n88.0\n93.2\n1.0\n62.1\n88.3\n93.5\n3.95\n29.9\n58.3\n69.6\n3.6\n30.4\n58.6\n69.7\n+Ingredient\n1.0\n65.7\n89.8\n94.1\n1.0\n66.0\n89.9\n94.2\n3.0\n34.4\n62.9\n73.6\n3.0\n34.7\n63.2\n73.7\n+Action\n1.0\n63.6\n88.1\n92.6\n1.0\n63.3\n88.5\n92.9\n3.0\n32.1\n60.3\n71.1\n3.0\n32.4\n60.1\n70.9\n+Both\n1.0\n65.7\n88.8\n93.6\n1.0\n65.9\n89.3\n94.0\n3.0\n35.5\n63.8\n74.1\n3.0\n36.5\n64.2\n74.3\nTFood [31]\n1.0\n72.4\n92.5\n95.4\n1.0\n72.5\n92.1\n95.3\n2.0\n43.9\n71.7\n80.8\n2.0\n43.7\n71.6\n80.6\n+Ingredient\n1.0\n74.5\n93.2\n96.1\n1.0\n73.7\n93.1\n96.0\n2.0\n45.6\n73.0\n81.6\n2.0\n44.9\n72.7\n81.5\n+Action\n1.0\n73.8\n93.1\n95.8\n1.0\n73.6\n93.1\n96.0\n2.0\n45.1\n72.6\n81.3\n2.0\n45.6\n72.8\n81.3\n+Both\n1.0\n75.8\n93.6\n96.3\n1.0\n76.3\n94.0\n96.6\n2.0\n46.9\n74.4\n82.8\n2.0\n47.4\n74.8\n83.2\nVLPCook [32]\n1.0\n77.4\n94.8\n97.1\n1.0\n78.0\n94.9\n97.1\n2.0\n48.8\n76.2\n84.5\n1.6\n49.9\n76.9\n85.0\n+Ingredient\n1.0\n78.3\n95.1\n97.4\n1.0\n78.6\n95.2\n97.4\n1.4\n50.2\n77.3\n85.2\n1.0\n51.0\n77.9\n85.6\n+Action\n1.0\n77.9\n95.0\n97.4\n1.0\n79.0\n95.4\n97.8\n1.5\n50.0\n77.4\n85.4\n1.0\n51.3\n78.1\n85.8\n+Both\n1.0\n79.1\n94.6\n97.0\n1.0\n78.3\n95.0\n97.2\n1.0\n51.7\n78.2\n85.9\n1.0\n52.2\n78.4\n86.0\ntransformative action is baking, but the prediction includes both\nbaking and frying, which pulls baked-then-fried chicken dishes\ncloser and pushes the ground-truth recipeâ€™s rank from 4ğ‘¡â„to 11ğ‘¡â„.\nE\nDebiasing Equation Derivation\nIn Section 3 of the main paper, we derive the similarity computation\nfor recipe retrieval by approximating the backdoor adjustment.\nHere, we provide the complete details of the equation derivations\nfor the approximation.\nğ‘ƒ(ğ‘†|ğ‘‘ğ‘œ(ğ¼), ğ‘…)\n=\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘“ğ‘ (ğ‘’ğ¼,ğ‘’ğ‘…,ğ‘’ğ‘–ğ‘›ğ‘”,ğ‘’ğ‘ğ‘ğ‘¡)ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”)ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…)\n(8a)\n= E[ğ‘–ğ‘›ğ‘”|ğ‘…]\n\u0002\nE[ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”]\n\u0002\nğ‘“ğ‘ (ğ‘’ğ¼,ğ‘’ğ‘…,ğ‘’ğ‘–ğ‘›ğ‘”,ğ‘’ğ‘ğ‘ğ‘¡)\n\u0003\u0003\n(8b)\n= E[ğ‘–ğ‘›ğ‘”|ğ‘…]\n\u0002\nE[ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”]\n\u0002\nğ‘’ğ‘…Â· (ğ‘’ğ¼+ ğ‘’ğ‘–ğ‘›ğ‘”+ ğ‘’ğ‘ğ‘ğ‘¡)\n\u0003\u0003\n(8c)\n= E[ğ‘–ğ‘›ğ‘”|ğ‘…]\n\u0002\nğ‘’ğ‘…Â· (ğ‘’ğ¼+ ğ‘’ğ‘–ğ‘›ğ‘”+ E[ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”] [ğ‘’ğ‘ğ‘ğ‘¡])\n\u0003\n(8d)\n= ğ‘’ğ‘…Â· \u0000ğ‘’ğ¼+ E[ğ‘–ğ‘›ğ‘”|ğ‘…] [ğ‘’ğ‘–ğ‘›ğ‘”] + E[ğ‘–ğ‘›ğ‘”|ğ‘…] [E[ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”] [ğ‘’ğ‘ğ‘ğ‘¡]]\u0001\n(8e)\n= ğ‘’ğ‘…Â·\n \nğ‘’ğ¼+\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…) Â· ğ‘’ğ‘–ğ‘›ğ‘”\n+\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…)\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ‘…,ğ‘–ğ‘›ğ‘”) Â· ğ‘’ğ‘ğ‘ğ‘¡\n!\n(8f)\nâ‰ˆğ‘’ğ‘…Â·\n \nğ‘’ğ¼+\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼) Â· ğ‘’ğ‘–ğ‘›ğ‘”\n+\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼)\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”) Â· ğ‘’ğ‘ğ‘ğ‘¡\n!\n(8g)\n= ğ‘’ğ‘…Â·\n \nğ‘’ğ¼+\nâˆ‘ï¸\nğ‘–ğ‘›ğ‘”\nğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼) Â· ğ‘’ğ‘–ğ‘›ğ‘”\n+ ğ‘ƒ(ğ‘–ğ‘›ğ‘”1|ğ¼) Â·\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”1) Â· ğ‘’ğ‘ğ‘ğ‘¡\n+ ğ‘ƒ(ğ‘–ğ‘›ğ‘”2|ğ¼) Â·\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”2) Â· ğ‘’ğ‘ğ‘ğ‘¡\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nTable 8: Scalability test on 20k, 30k, 40k and 50k test set for the image-to-recipe retrieval task.\n20k\n30k\n40k\n50k\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nH-T [28]\n6.3\n22.2\n47.0\n58.8\n9.0\n18.4\n41.1\n52.5\n12.0\n16.0\n36.9\n47.9\n15.0\n14.3\n33.8\n44.4\n+Ingredient\n5.0\n26.2\n52.4\n63.7\n7.0\n22.0\n46.2\n57.7\n9.0\n19.3\n41.9\n53.2\n11.0\n17.4\n38.7\n49.6\n+Action\n5.8\n24.3\n49.7\n60.9\n8.0\n20.3\n43.7\n54.8\n10.0\n17.8\n39.5\n50.4\n12.6\n15.9\n36.3\n47.1\n+Both\n4.7\n27.3\n53.3\n64.3\n6.0\n23.0\n47.4\n58.4\n8.0\n20.3\n43.4\n54.2\n10.0\n18.2\n40.2\n50.7\nTFood [31]\n3.0\n35.5\n62.0\n72.5\n4.0\n30.9\n56.0\n66.7\n5.0\n27.8\n52.2\n62.8\n6.0\n25.7\n49.1\n59.7\n+Ingredient\n3.0\n37.6\n64.3\n73.9\n3.0\n32.9\n58.6\n69.0\n4.0\n29.9\n54.5\n65.1\n5.0\n26.9\n51.2\n61.5\n+Action\n3.0\n36.3\n63.5\n73.4\n4.0\n31.6\n57.8\n68.1\n4.0\n28.5\n53.7\n64.3\n5.0\n26.2\n50.5\n61.2\n+Both\n2.0\n38.6\n65.5\n75.4\n3.0\n33.6\n59.8\n70.0\n4.0\n30.4\n55.6\n66.2\n5.0\n28.1\n52.5\n63.2\nVLPCook [32]\n2.0\n40.2\n67.4\n77.2\n3.0\n35.2\n61.6\n72.2\n4.0\n32.0\n57.5\n68.4\n4.0\n29.7\n54.5\n65.3\n+Ingredient\n2.0\n41.7\n69.1\n78.5\n3.0\n36.9\n63.5\n73.5\n3.0\n33.7\n59.7\n69.9\n4.0\n31.1\n56.4\n66.7\n+Action\n2.0\n41.0\n68.4\n78.1\n3.0\n36.0\n62.7\n73.0\n3.0\n32.7\n58.6\n69.1\n4.0\n30.2\n55.5\n66.1\n+Both\n2.0\n42.7\n69.7\n78.8\n3.0\n37.7\n64.4\n74.2\n3.0\n34.5\n60.4\n70.6\n4.0\n32.0\n57.4\n67.7\nTable 9: Scalability test on 20k, 30k, 40k and 50k test set for the recipe-to-image retrieval task.\n20k\n30k\n40k\n50k\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nH-T [28]\n6.0\n22.9\n47.8\n59.3\n9.0\n19.1\n41.8\n53.0\n11.2\n16.6\n37.5\n48.5\n14.0\n14.8\n34.3\n45.1\n+Ingredient\n5.0\n26.7\n52.6\n63.9\n7.0\n22.5\n46.6\n58.0\n8.8\n19.8\n42.3\n53.5\n10.0\n17.9\n39.1\n50.1\n+Action\n5.9\n24.6\n49.6\n60.9\n8.0\n20.7\n43.6\n54.9\n10.0\n18.1\n39.5\n50.4\n12.9\n16.3\n36.4\n47.0\n+Both\n4.0\n28.4\n53.9\n64.7\n6.0\n24.2\n48.0\n58.7\n8.0\n21.6\n44.1\n54.7\n10.0\n19.6\n40.9\n51.4\nTFood [31]\n3.0\n35.6\n62.2\n72.5\n4.0\n31.0\n56.6\n67.2\n5.0\n28.0\n52.3\n63.0\n6.0\n25.8\n49.1\n59.8\n+Ingredient\n3.0\n37.0\n63.9\n73.8\n3.0\n32.4\n58.3\n68.7\n4.0\n29.3\n54.4\n64.9\n5.0\n27.0\n51.2\n61.7\n+Action\n3.0\n37.2\n64.8\n73.4\n3.1\n32.4\n58.2\n68.4\n4.0\n29.2\n54.1\n64.4\n5.0\n26.9\n51.0\n61.5\n+Both\n2.1\n38.8\n65.6\n75.4\n3.0\n34.1\n60.2\n70.4\n4.0\n30.8\n56.1\n66.5\n5.0\n28.5\n53.0\n63.5\nVLPCook [32]\n2.0\n41.4\n68.6\n78.2\n3.0\n36.3\n62.8\n73.0\n3.0\n33.1\n58.8\n69.3\n4.0\n30.6\n55.6\n66.2\n+Ingredient\n2.0\n42.5\n69.3\n78.6\n3.0\n37.6\n64.1\n73.9\n3.0\n34.3\n60.0\n70.2\n4.0\n31.9\n57.0\n67.3\n+Action\n2.0\n42.4\n69.6\n79.0\n3.0\n37.5\n64.1\n74.0\n3.0\n34.2\n59.9\n70.3\n4.0\n31.8\n56.7\n67.4\n+Both\n2.0\n43.3\n70.3\n79.4\n2.4\n38.4\n64.8\n74.5\n3.0\n35.2\n60.8\n70.9\n4.0\n32.8\n57.7\n68.0\nTable 10: Impact of dictionary size and visibility of ingre-\ndients (on size of 500 ingredients). The table shows the in-\ngredient classification and retrieval performances for H-\nT+Ingredient on 10k test size. Note that the columns marked\nwith (visible only) show the results of using a dictionary that\nincludes only ingredients that will likely be visible in a final\ncooked dish.\nSize\nClassification\nRecall@1\nPrecision\nRecall\nF1\n100\n35.6\n49.0\n41.2\n32.2\n250 (Visible only)\n30.8\n37.5\n33.8\n34.0\n500\n30.7\n38.1\n34.0\n34.4\n500 (Visible only)\n29.1\n33.9\n31.3\n34.3\n1000\n29.7\n35.2\n32.2\n34.0\n+ . . . + ğ‘ƒ(ğ‘–ğ‘›ğ‘”ğ¾|ğ¼) Â·\nâˆ‘ï¸\nğ‘ğ‘ğ‘¡\nğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”ğ¾) Â· ğ‘’ğ‘ğ‘ğ‘¡\n!\n(8h)\nTable 11: Impacts of dictionary size on action classification\nand recipe retrieval for H-T+Both on 10k test size.\nSize\nClassification\nRecall@1\nPrecision\nRecall\nF1\n100\n0.482\n0.325\n0.388\n34.3\n500\n0.471\n0.303\n0.368\n35.5\n1000\n0.464\n0.300\n0.365\n35.3\nwhere Eq. (8b) is derived according to the definition of expectation.\nAs the similarity function ğ‘“ğ‘ is often implemented as a dot product\noperation, we set ğ‘“ğ‘ (ğ‘’ğ¼,ğ‘’ğ‘…,ğ‘’ğ‘–ğ‘›ğ‘”,ğ‘’ğ‘ğ‘ğ‘¡) = ğ‘’ğ‘…Â·(ğ‘’ğ¼+ğ‘’ğ‘–ğ‘›ğ‘”+ğ‘’ğ‘ğ‘ğ‘¡) in Eq. (8c),\nwhere a similar implementation is also used by [25, 40]. Eq. (8d)\nand Eq. (8e) are obtained by moving the expectations inside the\nparentheses. Eq. (8f) is obtained based on the definition of expecta-\ntion. Since ğ‘…is our search target during retrieval, we approximate\nğ‘…with ğ¼Eq. (8f), i.e., approximating ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…) and ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ‘…,ğ‘–ğ‘›ğ‘”) in\nEq. (8f) with ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼) and ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼,ğ‘–ğ‘›ğ‘”), respectively, and Eq. (8g)\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nFigure 8: Failure examples of using action debiasing: query image (a), the corresponding ground-truth recipe (b), predicted\ningredients and actions (c), retrieved recipe and its associated image by debiasing H-T with both ingredient and action (d) and\n(e).\nis derived. After expanding Ã\nğ‘–ğ‘›ğ‘”ğ‘ƒ(ğ‘–ğ‘›ğ‘”|ğ¼) Ã\nğ‘ğ‘ğ‘¡ğ‘ƒ(ğ‘ğ‘ğ‘¡|ğ¼,ğ‘–ğ‘›ğ‘”) Â· ğ‘’ğ‘ğ‘ğ‘¡in\nEq. (8g), we have Eq. (8h).\nF\nMulticultural Recipe Cookpad Dataset\nTable 12 shows the distribution of the dataset across different cul-\ntures, which is notably imbalanced. Indonesia has the highest num-\nber of query keywords and consequently the most crawled image-\nrecipe pairs, while India has the fewest keywords and the least\nnumber of pairs. This disparity is influenced by the number of dish\ntitles provided by Wikipedia and the corresponding number of\nsuccessfully crawled image-recipe pairs.\nTable 12: Statistics of the crawled Cookpad dataset.\nCulture\nQuery keywords\nCrawled pairs\nPercentage (%)\nIndonesia\n310\n24,766\n24\nMalaysia\n112\n18,848\n18\nThailand\n182\n23,781\n22\nVietnam\n102\n20,846\n20\nIndia\n70\n16,601\n16\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nTable 13: Pairwise ingredient overlap percentages (%) between\ncultures.\nIndonesia\nMalaysia\nThailand\nVietnam\nIndia\nIndonesia\n100\n39\n31\n36\n21\nMalaysia\n39\n100\n26\n29\n23\nThailand\n31\n26\n100\n31\n13\nVietnam\n36\n29\n31\n100\n18\nIndia\n21\n23\n13\n18\n100\nTable 14: Pairwise action overlap percentages (%) between\ncultures.\nIndonesia\nMalaysia\nThailand\nVietnam\nIndia\nIndonesia\n100\n47\n35\n34\n31\nMalaysia\n47\n100\n43\n31\n31\nThailand\n35\n43\n100\n35\n32\nVietnam\n34\n31\n35\n100\n29\nIndia\n31\n32\n32\n29\n100\nWe also present the overlap percentages (%) in ingredients and\nactions are shown in Table 13 and Table 14, respectively. As shown,\nIndonesia and Malaysia share a high degree of overlap in both in-\ngredients and cooking actions, while India exhibits the least overlap\nwith other cultures in both categories.\nG\nMultilingual Recipe-to-Image Retrieval\nResults\nTable 15 presents the results for image-to-recipe (I2R) and recipe-\nto-image (R2I) retrieval. The performance trends are similar, where\nsimilar degrees of improvements are introduced by different debi-\nasing modules for both I2R and R2I. Table 16 further details the\nperformance of I2R for five different cultures.\nH\nCulture Prediction Classifier Confusion\nMatrix\nIn Table 5 of the main paper, we list both results: Oracle (with prior\nknowledge of culture being assumed) and Classifier (a classifier\nfor predicting culture). To better explain our results in Table 5, we\nshow the confusion matrix of the classifier in Table 17. As seen,\nwhile classification is suboptimal for some cultures, the retrieval\nresult of Classifier is still close to that of Oracle.\nTable 17: Normalized confusion matrix of the culture-\npredicting classifier.\nIndonesia\nMalaysia\nThailand\nVietnam\nIndia\nIndonesia\n0.39\n0.29\n0.11\n0.17\n0.04\nMalaysia\n0.23\n0.58\n0.06\n0.09\n0.04\nThailand\n0.06\n0.06\n0.66\n0.20\n0.02\nVietnam\n0.05\n0.02\n0.20\n0.70\n0.03\nIndia\n0.01\n0.01\n0.01\n0.03\n0.94\nI\nTraining and Testing Time Comparison\nWe show the training time (minutes per epoch) for both monolin-\ngual and multilingual models in Table 18. None means no debiasing;\nSingle means debiasing with either ingredients or cooking actions;\nBoth means debiasing with both ingredients and actions. For the\nmulticultural dataset, the number of training epochs is 30 for the\nmodels with and without debiasing. For the monolingual dataset,\nthe number of epochs is 100, as the backbones used are weaker\nthan OpenCLIP.\nTable 18: Training time comparison per epoch.\nNone\nSingle\nBoth\nH-T [28]\n14min\n26min\n30min\nTFood [31]\n21min\n94min\n123min\nVLPCook [32]\n77min\n118min\n134min\nOpenCLIP [12]\n17min\n22min\n25min\nThe average inference time (including retrieval time) is presented\nin Table 19 (milliseconds per image query). The overhead is con-\nsidered acceptable. Even for large models such as VLPCook and\nOpenCLIP, our model (Both) can process 65 and 77 queries per\nsecond, respectively.\nTable 19: Inference speed comparison per query.\nNone\nSingle\nBoth\nH-T [28]\n5.9ms\n6.7ms\n7.1ms\nTFood [31]\n6.1ms\n8.8ms\n10.6ms\nVLPCook [32]\n6.6ms\n11.2ms\n15.3ms\nOpenCLIP [12]\n7ms\n11ms\n13ms\nJ\nQualitative Analysis Multicultural Recipe\nRetrieval\nWe present an example in Figure 9 to illustrate how our debiasing\nmodules help the model attend to different image regions, thereby\nimproving retrieval performance. As shown, although â€œsoy sauceâ€\nis barely visible in the dish image, its presence affects the color of\nthe pork due to pickling, and the model correctly associates the pre-\ndiction of â€œsoy sauceâ€ with the pork, as highlighted in the activation\nmap. Similarly, the action of â€œchoppingâ€ is localized to the shallots,\nwhich have undergone this preparation. Without debiasing, the\nactivation maps often fail to capture such fine-grained ingredient\nand action-level cues.\nWe present two examples of distinguishing visually similar recipes\nusing our proposed debiasing method in Figure 10. In the first ex-\nample, the two recipes share many common ingredients, such as\nchicken wings, garlic, and salt. Despite their visual similarity and\noverlapping ingredients, the query image corresponds to a dish\nof chicken sticky rice, which uses sticky rice as a key ingredient,\nwhereas the visually similar dish is chicken rice, made with reg-\nular rice. Our debiasing model correctly predicts the presence of\nsticky rice, enabling it to distinguish between these two visually\nsimilar recipes. Additionally, we provide the class activation map\nfor â€œsticky riceâ€ in Figure 10(c), which highlights the rice regions\nin the image, indicating the modelâ€™s attention to the relevant area.\nIn the second example, the debiasing module identifies ingredi-\nents unique to the query image such as lime juice and mint leaves,\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nTable 15: Comparison on the full test sets (size = 18,256) for both image-to-recipe and recipe-to-image retrieval tasks. medR (â†“),\nRecall@k (â†‘) are reported. The â€œOracleâ€ setting assumes known cultural origin per image, enabling culture-specific debiasing.\nThe â€œClassifierâ€ setting predicts the culture origin first and then performs the corresponding culture debiasing.\nImage-to-Recipe\nRecipe-to-Image\nOracle\nClassifier\nOracle\nClassifier\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nNLLB-SigLIP [35]\n176.9\n5.2\n12.9\n17.9\n176.9\n5.2\n12.9\n17.9\n156.5\n5.7\n13.6\n19.1\n156.5\n5.7\n13.6\n19.1\n+Ingredient\n165.2\n5.4\n13.3\n18.5\n168.3\n5.1\n13.1\n18.2\n148.2\n5.9\n14.5\n19.9\n153.1\n5.7\n14.2\n19.6\n+Action\n175.3\n5.5\n13.1\n18.0\n178.1\n5.0\n12.8\n17.5\n163.0\n6.0\n14.3\n19.3\n168.3\n5.6\n14.0\n19.0\n+Both\n151.3\n6.0\n14.1\n19.4\n153.5\n5.6\n13.6\n18.7\n135.6\n6.6\n15.3\n20.7\n139.0\n6.2\n14.5\n19.9\nM-CLIP [1]\n72.7\n9.4\n20.0\n26.2\n72.7\n9.4\n20.0\n26.2\n73.7\n8.3\n19.0\n25.2\n73.7\n8.3\n19.0\n25.2\n+Ingredient\n58.9\n9.7\n21.3\n28.3\n59.0\n9.4\n20.8\n27.5\n60.9\n8.9\n20.1\n27.0\n61.3\n8.6\n19.5\n26.5\n+Action\n57.0\n9.8\n21.1\n28.2\n57.0\n9.5\n20.5\n27.4\n60.7\n8.9\n20.1\n27.1\n61.1\n8.5\n19.4\n26.5\n+Both\n55.8\n9.8\n21.4\n28.6\n56.0\n9.6\n21.0\n28.5\n55.4\n10.0\n22.1\n28.6\n56.1\n9.6\n21.8\n28.3\nOpenCLIP [12]\n18.9\n16.9\n33.3\n42.0\n18.9\n16.9\n33.3\n42.0\n19.0\n16.0\n32.5\n41.5\n19.0\n16.0\n32.5\n41.5\n+Ingredient\n16.0\n18.0\n35.5\n44.2\n16.0\n17.5\n35.0\n43.8\n17.0\n17.4\n34.8\n43.7\n17.0\n16.9\n34.3\n43.3\n+Action\n16.0\n18.2\n35.5\n44.2\n16.3\n17.6\n35.0\n43.6\n17.0\n17.2\n34.4\n43.2\n17.0\n16.7\n34.7\n42.9\n+Both\n15.4\n18.4\n35.8\n44.5\n16.0\n18.0\n35.1\n44.1\n17.0\n17.8\n34.8\n44.0\n17.0\n17.1\n34.3\n43.4\nTable 16: MedR and Recall@{1,5,10} results for five cultures (ID: Indonesia, MY: Malaysia, TH: Thailand, VN: Vietnam, IN: India).\nID\nMY\nTH\nVN\nIN\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nmedR\nR@1\nR@5\nR@10\nNLLB-SigLIP [35]\n119.9\n5.6\n14.5\n20.1\n95.3\n8.6\n18.5\n24.1\n106.7\n6.8\n16.6\n23.2\n420.5\n2.3\n7.2\n10.9\n312.8\n2.4\n7.5\n11.0\n+Ingredient\n115.3\n5.5\n14.5\n20.2\n86.9\n8.6\n17.9\n24.6\n83.6\n7.5\n17.9\n24.2\n426.9\n2.5\n7.7\n10.9\n301.9\n2.7\n8.1\n12.0\n+Action\n133.5\n5.7\n14.6\n20.1\n91.4\n9.0\n18.6\n24.6\n104.1\n7.7\n17.7\n23.4\n411.4\n2.5\n7.2\n10.4\n301.1\n2.3\n7.2\n11.1\n+Both\n113.8\n6.8\n15.7\n21.8\n79.9\n9.5\n19.9\n25.8\n87.3\n7.9\n17.7\n25.1\n329.3\n2.8\n8.1\n12.0\n300.5\n2.6\n7.8\n11.8\nM-CLIP [1]\n35.5\n10.1\n23.1\n31.7\n29.6\n17.6\n30.7\n37.7\n30.9\n13.1\n27.4\n35.1\n123.7\n4.2\n13.2\n18.3\n373.5\n2.1\n6.3\n9.3\n+Ingredient\n29.2\n11.2\n25.9\n34.6\n23.3\n17.6\n30.7\n37.7\n27.7\n13.2\n28.5\n37.0\n108.3\n4.5\n13.6\n19.8\n300.7\n2.2\n6.9\n10.6\n+Action\n28.6\n11.5\n25.2\n34.1\n21.9\n17.8\n31.4\n40.0\n26.4\n12.8\n28.4\n36.8\n102.2\n4.9\n13.8\n19.8\n302.6\n2.3\n7.4\n11.2\n+Both\n28.0\n11.1\n25.2\n34.1\n23.2\n18.3\n32.5\n40.7\n24.5\n13.2\n29.1\n38.1\n99.6\n4.8\n14.2\n20.5\n276.1\n2.0\n6.8\n10.4\nOpenCLIP [12]\n14.9\n15.7\n33.7\n44.0\n11.0\n23.9\n40.6\n49.0\n5.9\n27.6\n49.7\n59.4\n25.0\n12.0\n27.8\n36.8\n74.8\n5.7\n15.3\n21.4\n+Ingredient\n14.4\n16.1\n35.1\n45.1\n9.0\n25.3\n43.2\n51.8\n5.0\n28.8\n51.5\n60.7\n20.5\n13.0\n30.1\n38.8\n64.5\n6.6\n17.8\n24.9\n+Action\n14.4\n16.2\n34.6\n44.9\n9.7\n25.6\n43.2\n51.0\n5.0\n28.5\n51.4\n61.0\n20.1\n13.5\n31.0\n39.9\n60.9\n6.9\n17.7\n24.6\n+Both\n13.8\n16.5\n35.4\n45.8\n9.9\n25.8\n43.1\n50.8\n5.0\n29.0\n52.0\n61.5\n18.9\n13.4\n30.9\n40.8\n63.9\n7.1\n17.9\n24.2\nFigure 9: Class activation map (CAM) of ingredient and action prediction: (a) the query image, (b) the corresponding recipe, (c)\nclass activation map of ingredient â€œsoy sauceâ€, and (d) class activation map of ingredient â€œchoppingâ€.\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nFigure 10: Distinguishing visually similar recipes: (a) the query image and its ingredient composition, (b) a visually similar\nimage and its ingredients, and (c) ingredient prediction with class activation map visualization of â€œsticky riceâ€ (top) and â€œlime\njuiceâ€ (bottom).\nwhich helps distinguish it from a visually similar recipe that shares\ningredients like steak, garlic, and peppers. Although lime juice is\nnot directly visible in the image, the class activation map shows\nthat its prediction is based on attention to the entire dish, which\naligns with the fact that the juice is mixed throughout the food.\nFigure 11 presents examples where the debiasing module fails\nto distinguish between recipes with similar visual appearances.\nIn the first case, â€œVegetable Cream Soupâ€ and â€œKFC-style Cream\nSoupâ€ appear visually similar and share most ingredients. However,\nâ€œVegetable Cream Soupâ€ uses â€œcornstarchâ€, while â€œKFC-style Cream\nSoupâ€ contains â€œflourâ€. The model, after debiasing, incorrectly pre-\ndicts â€œflourâ€ for the image of â€œVegetable Cream Soupâ€, reducing\nthe correct recipeâ€™s rank from 4 to 10 and mistakenly selecting\nâ€œKFC-style Cream Soupâ€ as the top-1 result. In the second example,\nalthough most ingredients are correctly identified, the model in-\ncorrectly predicts the cooking action as baking for â€œFried Chicken\nwith Coconut Serundengâ€. This leads it to favor â€œGrilled Chickenâ€,\nwhich involves baking, thereby pushing the correct recipe from\nrank 3 to 10.\nReferences\n[1] Fredrik Carlsson, Philipp Eisen, Faton Rekathati, and Magnus Sahlgren. 2022.\nCross-lingual and multilingual clip. In Proceedings of the thirteenth language\nresources and evaluation conference. 6848â€“6854.\n[2] Micael Carvalho, RÃ©mi CadÃ¨ne, David Picard, Laure Soulier, Nicolas Thome, and\nMatthieu Cord. 2018. Cross-modal retrieval in the cooking context: Learning\nsemantic text-image embeddings. In The 41st International ACM SIGIR Conference\non Research & Development in Information Retrieval. 35â€“44.\n[3] Jingjing Chen, Liangming Pan, Zhipeng Wei, Xiang Wang, Chong-Wah Ngo,\nand Tat-Seng Chua. 2020. Zero-shot ingredient recognition by multi-relational\ngraph convolutional network. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 10542â€“10550.\n[4] Jingjing Chen, Lei Pang, and Chong-Wah Ngo. 2017. Cross-modal recipe retrieval:\nHow to cook this dish?. In MultiMedia Modeling: 23rd International Conference,\nMMM 2017, Reykjavik, Iceland, January 4-6, 2017, Proceedings, Part I 23. Springer,\n588â€“600.\n[5] Jing-Jing Chen, Chong-Wah Ngo, Fu-Li Feng, and Tat-Seng Chua. 2018. Deep un-\nderstanding of cooking procedure for cross-modal recipe retrieval. In Proceedings\nof the 26th ACM international conference on Multimedia. 1020â€“1028.\n[6] Prateek Chhikara, Dhiraj Chaurasia, Yifan Jiang, Omkar Masur, and Filip Ilievski.\n2024. Fire: Food image to recipe generation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision. 8184â€“8194.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).\n[8] Han Fu, Rui Wu, Chenghao Liu, and Jianling Sun. 2020. Mcen: Bridging cross-\nmodal gap between cooking recipes and dish images with latent variable model. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n14570â€“14580.\n[9] Ricardo Guerrero, Hai X Pham, and Vladimir Pavlovic. 2021. Cross-modal retrieval\nand synthesis (x-mrs): Closing the modality gap in shared subspace learning. In\nProceedings of the 29th ACM International Conference on Multimedia. 3192â€“3201.\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition. 770â€“778.\n[11] Xu Huang, Jin Liu, Zhizhong Zhang, and Yuan Xie. 2023. Improving Cross-\nModal Recipe Retrieval with Component-Aware Prompted CLIP Embedding. In\nProceedings of the 31st ACM International Conference on Multimedia. 529â€“537.\n[12] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas\nCarlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John\nMiller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. 2021. OpenCLIP.\ndoi:10.5281/zenodo.5143773\nMitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nFigure 11: Failure examples in multicultural retrieval: query image (a), the corresponding ground-truth recipe (b), predicted\ningredients and actions (c), retrieved recipe and its associated image by debiasing OpenCLIP with both ingredient and action (d)\nand (e). Common failure cases occur with dishes that are either covered by soup (top) or obscured by toppings (bottom).\n[13] Minyoung Kim, Ricardo Guerrero, and Vladimir Pavlovic. 2021. Learning disen-\ntangled factors from paired data in cross-modal retrieval: An implicit identifiable\nVAE approach. In Proceedings of the 29th ACM International Conference on Multi-\nmedia. 2862â€“2870.\n[14] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\n2023. Segment anything. In Proceedings of the IEEE/CVF International Conference\non Computer Vision. 4015â€“4026.\n[15] Jiao Li, Jialiang Sun, Xing Xu, Wei Yu, and Fumin Shen. 2021. Cross-modal\nimage-recipe retrieval via intra-and inter-modality hybrid fusion. In Proceedings\nof the 2021 International Conference on Multimedia Retrieval. 173â€“182.\n[16] Jiao Li, Xing Xu, Wei Yu, Fumin Shen, Zuo Cao, Kai Zuo, and Heng Tao Shen. 2021.\nHybrid fusion with intra-and cross-modality attention for image-recipe retrieval.\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. 244â€“254.\n[17] Lin Li, Ming Li, Zichen Zan, Qing Xie, and Jianquan Liu. 2021. Multi-subspace\nimplicit alignment for cross-modal retrieval on cooking recipes and food im-\nages. In Proceedings of the 30th ACM International Conference on Information &\nKnowledge Management. 3211â€“3215.\n[18] Bing Liu, Dong Wang, Xu Yang, Yong Zhou, Rui Yao, Zhiwen Shao, and Jiaqi Zhao.\n2022. Show, deconfound and tell: Image captioning with causal inference. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n18041â€“18050.\n[19] Shilong Liu, Lei Zhang, Xiao Yang, Hang Su, and Jun Zhu. 2021. Query2Label: A\nSimple Transformer Way to Multi-Label Classification. arXiv:2107.10834 [cs.CV]\n[20] Fangrui Lv, Jian Liang, Shuang Li, Bin Zang, Chi Harold Liu, Ziteng Wang, and Di\nLiu. 2022. Causality inspired representation learning for domain generalization. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n8046â€“8056.\n[21] Haoyu Ma, Handong Zhao, Zhe Lin, Ajinkya Kale, Zhangyang Wang, Tong Yu,\nJiuxiang Gu, Sunav Choudhary, and Xiaohui Xie. 2022. Ei-clip: Entity-aware\ninterventional contrastive learning for e-commerce cross-modal retrieval. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n18051â€“18061.\n[22] Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and Ramesh Jain. 2019. A\nsurvey on food computing. ACM Computing Surveys (CSUR) 52, 5 (2019), 1â€“36.\n[23] Dim P Papadopoulos, Enrique Mora, Nadiia Chepurko, Kuan Wei Huang, Ferda\nOfli, and Antonio Torralba. 2022. Learning program representations for food im-\nages and cooking recipes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 16559â€“16569.\n[24] Judea Pearl. 2009. Causality. Cambridge university press.\n[25] Jiaxin Qi, Yulei Niu, Jianqiang Huang, and Hanwang Zhang. 2020. Two causal\nprinciples for improving visual dialog. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition. 10860â€“10869.\n[26] Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman,\nMatan Protter, and Lihi Zelnik-Manor. 2021. Asymmetric Loss for Multi-Label\nClassification. In Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV). 82â€“91.\n[27] Amaia Salvador, Michal Drozdzal, Xavier GirÃ³-i Nieto, and Adriana Romero.\n2019. Inverse cooking: Recipe generation from food images. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 10453â€“10462.\n[28] Amaia Salvador, Erhan Gundogdu, Loris Bazzani, and Michael Donoser. 2021.\nRevamping cross-modal recipe retrieval with hierarchical transformers and self-\nsupervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. 15475â€“15484.\n[29] Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar\nWeber, and Antonio Torralba. 2017. Learning cross-modal embeddings for cook-\ning recipes and food images. In Proceedings of the IEEE conference on computer\nvision and pattern recognition. 3020â€“3028.\n[30] JÃ¼rgen Schmidhuber et al. 1997. Long short-term memory. Neural Comput 9, 8\n(1997), 1735â€“1780.\n[31] Mustafa Shukor, Guillaume Couairon, Asya Grechka, and Matthieu Cord. 2022.\nTransformer decoders with multimodal regularization for cross-modal food re-\ntrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 4567â€“4578.\n[32] Mustafa Shukor, Nicolas Thome, and Matthieu Cord. 2024. Vision and structured-\nlanguage pretraining for cross-modal food retrieval. Computer Vision and Image\nUnderstanding 247 (2024), 104071.\n[33] Fangzhou Song, Bin Zhu, Yanbin Hao, and Shuo Wang. 2024. Enhancing Recipe\nRetrieval with Foundation Models: A Data Augmentation Perspective. In European\nConference on Computer Vision.\n[34] Yu Sugiyama and Keiji Yanai. 2021. Cross-modal recipe embeddings by disentan-\ngling recipe contents and dish styles. In Proceedings of the 29th ACM International\nConference on Multimedia. 2501â€“2509.\n[35] Alexander Visheratin. 2023. NLLB-CLIPâ€“train performant multilingual image\nretrieval model on a budget. arXiv preprint arXiv:2309.01859 (2023).\n[36] Muntasir Wahed, Xiaona Zhou, Tianjiao Yu, and Ismini Lourentzou. 2024. Fine-\nGrained Alignment for Cross-Modal Recipe Retrieval. In Proceedings of the\nMM â€™25, October 27â€“31, 2025, Dublin, Ireland.\nQing Wang, Chong-Wah Ngo, Yu Cao, and Ee-Peng Lim\nIEEE/CVF Winter Conference on Applications of Computer Vision. 5584â€“5593.\n[37] Hao Wang, Guosheng Lin, Steven CH Hoi, and Chunyan Miao. 2022. Learn-\ning structural representations for recipe generation and food retrieval. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 45, 3 (2022), 3363â€“3377.\n[38] Hao Wang, Doyen Sahoo, Chenghao Liu, Ee-peng Lim, and Steven CH Hoi. 2019.\nLearning cross-modal embeddings with adversarial networks for cooking recipes\nand food images. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition. 11572â€“11581.\n[39] Hao Wang, Doyen Sahoo, Chenghao Liu, Ke Shu, Palakorn Achananuparp, Ee-\npeng Lim, and Steven CH Hoi. 2021. Cross-modal food retrieval: learning a joint\nembedding of food images and recipes with semantic consistency and attention\nmechanism. IEEE Transactions on Multimedia 24 (2021), 2515â€“2525.\n[40] Tan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. 2020. Visual\ncommonsense r-cnn. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition. 10760â€“10770.\n[41] Wenjie Wang, Fuli Feng, Xiangnan He, Xiang Wang, and Tat-Seng Chua. 2021.\nDeconfounded recommendation for alleviating bias amplification. In Proceedings\nof the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining.\n1717â€“1725.\n[42] Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Min Lin, and Tat-Seng Chua.\n2022. Causal representation learning for out-of-distribution recommendation. In\nProceedings of the ACM Web Conference 2022. 3562â€“3571.\n[43] Zhongwei Xie, Ling Liu, Yanzhao Wu, Lin Li, and Luo Zhong. 2021. Learning\ntfidf enhanced joint embedding for recipe-image cross-modal retrieval service.\nIEEE Transactions on Services Computing (2021).\n[44] Zhongwei Xie, Ling Liu, Yanzhao Wu, Luo Zhong, and Lin Li. 2021. Learning\ntext-image joint embedding for efficient cross-modal retrieval with deep feature\nengineering. ACM Transactions on Information Systems (TOIS) 40, 4 (2021), 1â€“27.\n[45] Xun Yang, Fuli Feng, Wei Ji, Meng Wang, and Tat-Seng Chua. 2021. Deconfounded\nvideo moment retrieval with causal intervention. In Proceedings of the 44th In-\nternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 1â€“10.\n[46] Zichen Zan, Lin Li, Jianquan Liu, and Dong Zhou. 2020. Sentence-based and noise-\nrobust cross-modal retrieval on cooking recipes and food images. In Proceedings\nof the 2020 International Conference on Multimedia Retrieval. 117â€“125.\n[47] Fan Zhao, Yuqing Lu, Zhuo Yao, and Fangying Qu. 2025. Cross modal recipe\nretrieval with fine grained modal interaction. Scientific Reports 15, 1 (2025), 4842.\n[48] Bin Zhu and Chong-Wah Ngo. 2020. CookGAN: Causality based text-to-image\nsynthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 5519â€“5527.\n[49] Bin Zhu, Chong-Wah Ngo, Jingjing Chen, and Wing-Kwong Chan. 2022. Cross-\nlingual adaptation for recipe retrieval with mixup. In Proceedings of the 2022\nInternational Conference on Multimedia Retrieval. 258â€“267.\n[50] Bin Zhu, Chong-Wah Ngo, Jingjing Chen, and Yanbin Hao. 2019. R2gan: Cross-\nmodal recipe retrieval with generative adversarial network. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 11477â€“11486.\n[51] Zhuoyang Zou, Xinghui Zhu, Qinying Zhu, Yi Liu, and Lei Zhu. 2024. CREAMY:\nCross-Modal Recipe Retrieval By Avoiding Matching Imperfectly. IEEE Access\n(2024).\n",
    "content": "# Interpretation of the Paper \"Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval\"\n\n## 1. Core Content and Main Contributions\n\nThis paper focuses on addressing **representation bias in cross-modal image-to-recipe retrieval**, particularly under multilingual and multicultural contexts. Traditional approaches assume that a food image fully captures all information about its corresponding recipe. However, images only depict the visual appearance of the final dish and cannot reveal ingredient details (e.g., salt, spices) or non-visual cooking procedures (e.g., marinating, fermenting). This \"visibility bias\" causes models to overemphasize dominant visual elements, leading to poor performance when distinguishing dishes with similar main ingredients but different seasonings or preparation methods.\n\nThe main contributions of the paper are:\n\n- **Proposing a causal inference framework for cross-modal retrieval**: The study is the first to apply causal reasoning to the image-to-recipe retrieval domain. It identifies *ingredients* and *cooking actions* as confounding factors and employs **backdoor adjustment** to correct spurious correlations caused by these variables, thereby mitigating representation bias.\n- **Designing plug-and-play debiasing modules**: Two lightweight neural network modulesâ€”**ingredient debiasing module** and **action debiasing module**â€”are introduced. These can be seamlessly integrated into existing state-of-the-art models (e.g., H-T, TFood, VLPCook, multilingual CLIP), enhancing their retrieval performance in both monolingual and multilingual settings.\n- **Constructing and releasing a multicultural, multilingual dataset**: To validate the methodâ€™s effectiveness across diverse cultures, the authors curated a novel multilingual and multicultural recipe dataset from Cookpad, covering Indonesian, Malaysian, Thai, Vietnamese, and Indian cuisines. This publicly shared dataset serves as a valuable resource for future research.\n- **Highlighting benefits for low-resource cultures**: Experiments show that the proposed method achieves particularly significant performance gains on low-resource cultures (e.g., Vietnamese, Indian), advancing fairness and inclusivity in cross-cultural culinary computing.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nThe paper achieves several key advances in both technical methodology and practical application:\n\n### (1) Theoretical Innovation: Causal Inference for Cross-modal Bias Mitigation\n\nUnlike prior work that improves retrieval through enhanced context (e.g., titles, ingredient matching) or better feature fusion, this paper innovatively uses a **causal graphical model** to analyze relationships among image $I$, recipe $R$, ingredients $Ing$, and actions $Act$:\n- Ingredients and actions simultaneously influence both image and recipe, creating two paths: $R \\leftarrow Ing \\rightarrow I$ and $R \\leftarrow Act \\rightarrow I$, resulting in **confounding bias**.\n- By applying **do-calculus**, the input edges to the image node are severed via backdoor adjustment. This ensures similarity computation depends not on visually observable components, but on the underlying distributions $P(ing|R)$ and $P(act|R, ing)$.\n\nThis theoretical framework provides a new paradigm for understanding and systematically eliminating cross-modal representation bias.\n\n### (2) Architectural Innovation: Dual-channel Debiasing + Culture-Specific Dictionary\n\n- **Dual-channel debiasing mechanism**: The model predicts ingredients likely present in an image (ingredient debiasing), then further generates associated cooking action sequences based on those predicted ingredients (action debiasing). The resulting embeddings are added back to the original image embedding, forming a more comprehensive representation.\n- **Culture-specific dictionary**: To address large variations and low co-occurrence of ingredients and techniques across cultures, each culture maintains its own localized dictionary of ingredient and action embeddings, along with dedicated predictors. This avoids noise from one-size-fits-all universal dictionaries and enhances generalization for underrepresented cuisines.\n\n### (3) Empirical Innovation: Validation in Real-World Complex Scenarios\n\n- On the standard monolingual dataset Recipe1M, combining both ingredient and action debiasing improves Recall@1 by **5.6%** over baseline models.\n- On the newly constructed multicultural dataset, MedR metrics improve by over **100 positions** for low-resource cultures like Indian and Vietnamese, demonstrating strong potential in alleviating cultural imbalance.\n- Even with a simple classifier predicting the cultural origin of query images, overall performance remains close to the ideal case where culture is known, indicating robustness.\n\n---\n\n## 3. Entrepreneurial Ideas Inspired by the Paper\n\nLeveraging the paper's technical strengths and societal needs, here are several highly promising startup concepts:\n\n### ğŸ½ï¸ Startup Idea 1: **\"Taste Memory\" â€” Global Ethnic Cuisine Recognition & Recreation Platform**\n\n#### Core Concept:\nDevelop a mobile app for everyday users: take a photo of a foreign dish, and the system accurately identifies its cultural origin, recommends the closest authentic traditional recipe, and even offers multilingual voice-guided cooking instructions.\n\n#### Technical Foundation:\n- Use the paperâ€™s **multicultural debiased retrieval model** as the core engine, enabling precise differentiation between visually similar but flavor-distinct dishes (e.g., Thai Tom Yum vs. Vietnamese sour soup).\n- Integrate a culture classifier with dual debiasing modules to boost recognition accuracy for niche cuisines (e.g., Indonesian Padang, Indian Kerala).\n- Build a high-quality image-text database covering major global cuisines, continuously updated with regional specialties.\n\n#### Business Model:\n- **B2C subscription**: Free basic features; premium features (offline mode, chef-level recipes, personalized recommendations) offered via monthly subscription.\n- **B2B partnerships**: Collaborate with airlines and international hotel chains to provide in-flight/room dining cultural commentary services.\n- **E-commerce referral**: Recommend spices and kitchen tools, earning commissions via affiliate links.\n\n#### Social Impact:\nPromotes cross-cultural exchange, helps diaspora communities reconnect with home flavors, and supports intangible cultural heritage preservation.\n\n---\n\n### ğŸ§ª Startup Idea 2: **\"Kitchen Lab\" â€” AI-Powered Home Dietary Health Optimizer**\n\n#### Core Concept:\nUse AI to analyze daily meal photos from households, automatically extract actual ingredients and cooking methods (especially hidden oil, salt, sugar usage), link to nutritional databases, and offer personalized advice for weight loss, sugar control, or low-sodium diets.\n\n#### Technical Foundation:\n- Leverage the paperâ€™s **action debiasing module** to detect not just \"stir-fry\" or \"fry,\" but also health-impacting invisible operations such as \"overnight marination\" or \"thickening with starch.\"\n- Combine ingredient frequency analysis to identify habitual use of high-sodium condiments (e.g., soy sauce, fish sauce), and suggest alternatives (e.g., lemon juice for umami).\n- Support multilingual input/output, suitable for cosmopolitan families.\n\n#### Business Model:\n- Partner with insurance companies and health management providers as a digital tool for chronic disease prevention.\n- Develop enterprise versions for monitoring dietary balance in corporate cafeterias.\n- Launch companion smart devices (e.g., smart scales, smart spice jars) to create integrated hardware-software solutions.\n\n#### Differentiation:\nGoes beyond simple â€œphoto-to-dishâ€ recognition by deeply understanding **health risks embedded in cooking behaviors**, offering actionable improvement pathways.\n\n---\n\n### ğŸŒ Startup Idea 3: **\"Recipes Without Borders\" â€” Multilingual, Cross-Cultural Recipe Co-Creation Community**\n\n#### Core Concept:\nBuild an open recipe platform with automatic multilingual translation and cultural adaptation. Users worldwide can upload local dishes; AI automatically generates translated versions and adapts them to target-region tastes (e.g., reducing spiciness, substituting rare ingredients).\n\n#### Technical Foundation:\n- Apply the paperâ€™s model for **reverse retrieval**: generate structured recipe text from images (inverse cooking), then align across languages using multilingual CLIP.\n- Introduce a **cultural sensitivity filter** to prevent misinterpretations from literal translations (e.g., labeling pork dishes as halal-certified).\n- Use debiasing mechanisms to ensure translated recipes retain key flavor profiles of the original cuisine.\n\n#### Business Model:\n- User-generated content (UGC) ecosystem with ad/sponsorship revenue.\n- Offer API access to overseas Chinese takeaway platforms for automatic menu description optimization.\n- Publish a series of e-books: *AI Translation Guide to World Home Cooking*.\n\n#### Vision:\nBreak down language barriers so every home-cooked meal can be understood and preserved globally.\n\n---\n\n> **Summary**: This paper pioneers a new path in academia by combining **causal inference with cross-modal retrieval**, laying a solid foundation for intelligent culinary applications that truly understand \"cultural semantics.\" The entrepreneurial ideas outlined above all center on the core technical breakthroughâ€”**accurately capturing non-visual cooking knowledge**â€”and are grounded in clear market demand and technical feasibility.",
    "github": "",
    "hf": ""
}