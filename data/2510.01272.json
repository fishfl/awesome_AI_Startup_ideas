{
    "id": "2510.01272",
    "title": "Modeling Others' Minds as Code",
    "summary": "This article proposes a new algorithm called ROTE, which efficiently synthesizes and reasons about behavior programs by combining large language models and probabilistic reasoning, thereby accurately predicting human and AI behavior.",
    "abstract": "Accurate prediction of human behavior is essential for robust and safe human-AI collaboration. However, existing approaches for modeling people are often data-hungry and brittle because they either make unrealistic assumptions about rationality or are too computationally demanding to adapt rapidly. Our key insight is that many everyday social interactions may follow predictable patterns; efficient \"scripts\" that minimize cognitive load for actors and observers, e.g., \"wait for the green light, then go.\" We propose modeling these routines as behavioral programs instantiated in computer code rather than policies conditioned on beliefs and desires. We introduce ROTE, a novel algorithm that leverages both large language models (LLMs) for synthesizing a hypothesis space of behavioral programs, and probabilistic inference for reasoning about uncertainty over that space. We test ROTE in a suite of gridworld tasks and a large-scale embodied household simulator. ROTE predicts human and AI behaviors from sparse observations, outperforming competitive baselines -- including behavior cloning and LLM-based methods -- by as much as 50% in terms of in-sample accuracy and out-of-sample generalization. By treating action understanding as a program synthesis problem, ROTE opens a path for AI systems to efficiently and effectively predict human behavior in the real-world.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Multi-Agent",
    "authors": "Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner",
    "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
    ],
    "comments": "",
    "keypoint": "ROTE models human and AI behaviors as executable code programs rather than policies based on beliefs and desires.\nROTE uses large language models (LLMs) to generate a hypothesis space of behavioral programs from observed trajectories.\nROTE applies probabilistic inference to refine the likelihood of candidate programs over time.\nROTE outperforms behavior cloning, LLM-based baselines, and inverse planning methods by up to 50% in prediction accuracy.\nROTE achieves superior generalization on both in-sample and out-of-sample tasks across gridworld and household simulation environments.\nROTE predicts human-level behavior accurately even from sparse and noisy observations.\nROTE enables zero-shot transfer of inferred behavioral programs to novel environments without retraining.\nROTE’s code-based representations are reusable, interpretable, and allow for efficient multi-step prediction.\nROTE scales more efficiently than baseline methods during long-horizon predictions due to rapid program execution after initial inference.\nROTE performs Bayesian inference over program hypotheses using Sequential Monte Carlo with rejuvenation.\nROTE generates shorter, more concise programs that correlate with higher prediction accuracy, aligning with Occam’s razor.\nROTE does not rely on assumptions about agent rationality or access to reward signals.\nROTE treats action understanding as a program synthesis problem, enabling structured reasoning about social routines.\nROTE infers deterministic finite-state-like programs even when agents exhibit stochastic behavior.\nROTE’s performance is robust across different LLM backbones, with DeepSeek-Coder-V2-Lite-Instruct yielding best results.\nROTE achieves human-level predictive accuracy in single-agent gridworld tasks involving scripted and human-generated behaviors.\nROTE excels at predicting repetitive, routine behaviors but shows limitations in complex goal-directed tasks compared to humans.\nROTE maintains high accuracy in partially observable, complex environments like Partnr, outperforming all baselines significantly.\nROTE is particularly effective at modeling tasks involving object interaction, cleaning, and state-dependent actions in household settings.\nROTE’s two-stage observation parsing improves performance in gridworlds but harms it in natural language-rich environments like Partnr.\nROTE’s ablation studies show that moderate structural constraints on code generation yield the best balance between flexibility and accuracy.\nROTE demonstrates that script-like behavioral modeling can reduce cognitive load for both machines and observers.\nROTE opens a path for scalable, adaptable, and interpretable social AI systems capable of real-world human-AI collaboration.",
    "date": "2025-10-04",
    "paper": "Modeling Others’ Minds as Code\nKunal Jha1, Aydan Yuenan Huang2, Eric Ye1,\nNatasha Jaques1∗& Max Kleiman-Weiner1∗\nAbstract\nAccurate prediction of human behavior is essential for robust and safe\nhuman-AI collaboration. However, existing approaches for modeling peo-\nple are often data-hungry and brittle because they either make unrealistic\nassumptions about rationality or are too computationally demanding to\nadapt rapidly. Our key insight is that many everyday social interactions\nmay follow predictable patterns; efficient “scripts” that minimize cogni-\ntive load for actors and observers, e.g., “wait for the green light, then\ngo.” We propose modeling these routines as behavioral programs instan-\ntiated in computer code rather than policies conditioned on beliefs and\ndesires. We introduce ROTE, a novel algorithm that leverages both large\nlanguage models (LLMs) for synthesizing a hypothesis space of behavioral\nprograms, and probabilistic inference for reasoning about uncertainty over\nthat space. We test ROTE in a suite of gridworld tasks and a large-scale\nembodied household simulator. ROTE predicts human and AI behaviors\nfrom sparse observations, outperforming competitive baselines—including\nbehavior cloning and LLM-based methods—by as much as 50% in terms\nof in-sample accuracy and out-of-sample generalization. By treating action\nunderstanding as a program synthesis problem, ROTE opens a path for\nAI systems to efficiently and effectively predict human behavior in the\nreal-world. Code for environments, algorithms, evalution scripts and more\ncan be found at https://github.com/KJha02/mindsAsCode.\n1\nIntroduction\nPredicting the behavior of others (Theory of Mind) is a core challenge for building intelligent\nsocial agents. Whether anticipating a pedestrian’s movements, coordinating with teammates,\nor interacting safely in public spaces, machines must infer what others are likely to do next.\nExisting approaches such as behavior cloning (BC) and inverse reinforcement learning (IRL)\nrely on learning models to predict low-level actions or infer latent reward functions Abbeel\n& Ng (2004); Ng et al. (2000); Torabi et al. (2018); Wulfmeier et al. (2016). However, these\nmethods are often data-hungry and brittle because they try to learn what an agent might do\nin every possible state, frequently overfitting to specific environments or overcomplicating\nbehaviors that are surprisingly routine for humans (Skalse & Abate, 2024; Yildirim et al.,\n2024).\nAlternatively, probabilistic methods for goal inference (Fuchs et al., 2023; Zhi-\nXuan et al., 2020; 2024) are more sample efficient but demand computationally intensive\nonline reasoning about potential intentions and beliefs, alongside human-specified priors\nand hypothesis spaces. Thus, conventional methods for modeling others present a trade-\noff illustrated in Figure 1: data-intensive and brittle, or compute-intensive and manually\nconstructed for each new domain.\nRecent work in cognitive science shows that when humans interact with one another, we do\nnot always imbue others with deeply held mental states such as goals or beliefs. Instead we\noften perceive others as following a script or mindlessly applying a set of rules (Ullman &\nBass, 2024; Bass et al., 2024). For example, when someone steps into a crosswalk, we do\nnot need to infer their ultimate destination, their complex mental states, or their opinion on\n∗Equal contribution. 1 Department of Computer Science, University of Washington, Seattle, WA\n2 Department of Computer Science, Johns Hopkins University, Baltimore, MD. Correspondence to\nKunal Jha <kjha@uw.edu>\n1\narXiv:2510.01272v1  [cs.AI]  29 Sep 2025\ndef act(obs): à int\nif valid(obs):\n...\nBehavior Cloning\nROTE (Ours)\nInverse Planning\nBeliefs\nGoals\nPlanner\nAction\nFigure 1: Comparison of action prediction methods: Behavior cloning requires large datasets\nand has limited generalization, while inverse planning is computationally expensive at\ntest time. Our approach, ROTE, uses LLMs to generate efficient and interpretable code\nrepresentations of observed behavior, providing a superior balance of efficiency and accuracy.\npineapple on pizza. It is enough to apply a commonly understood “crosswalk script” shaped\nby social convention. While there are perspectives on how people adopt roles in societies or\nprescribe agency to others (Dennett, 1972; Field, 1978; Dennett & Gorey, 1981; Dennett,\n1987; 2017; Jara-Ettinger & Dunham, 2024), to the best of our knowledge, there are currently\nno computational models that adequately describe how machines can represent and reason\nabout other agents acting in a script-like manner.\nThe notion of representing an intelligent agent through logical rules and predetermined\ndecision-making processes is a foundational idea in computer science (Newell & Simon, 1956;\nSchank & Abelson, 2013; Newell & Simon, 1976), influencing fields from planning (Campbell\net al., 2002; Zhu et al., 2025) to game theory (Axelrod, 1980). Finite State Machines (FSMs),\nfor instance, are still used in video games to efficiently simulate large numbers of agents.\nBy defining a sequence of states and transitions (e.g., patrol border →find agents →chase\nagents), code can flexibly model the causal behaviors underpinning social norms and routines.\nHere we develop ROTE — Representing Others’ Trajectories as Executables — a novel\nalgorithm that leverages LLMs as code synthesis tools to predict others’ actions. We prompt\nLLMs to generate computer programs explaining observed behavioral traces, then perform\nBayesian inference to reason about which programs are most likely. This gives us a dynamic\nrepresentation that can be analyzed, modified, and composed across agents and environments.\nROTE significantly improves generalization and efficiency in predicting complex agent\nbehavior, showing up to a 50% increase in accuracy across multiple challenging embodied\ndomains. Our results in gridworlds and the scaled-up Partnr household robotics simulator\ndemonstrate that code is a highly effective representation for modeling and predicting\nbehavior. To validate its applicability to real-world complexity, we collected human gameplay\ndata and found that our method achieves human-level accuracy in predicting human actions,\noutperforming all baselines. This offers a promising new path for creating scalable, adaptable,\nand interpretable socially intelligent AI systems. Concretely, our contributions are:\n1. Modeling Agentic Behavior via Program Synthesis: We develop ROTE, a novel\nalgorithm that combines LLMs with Sequential Monte Carlo to model other agents’\nbehavior as programs from sparse observations.\n2. Superior, Scalable Action Prediction: Across two embodied domains, we show\nthat ROTE offers superior generalization for predicting others’ behaviors, outperforming\nalternative methods by as much as 50%. Our method generates executable code that is\nreusable across environments, bypassing costly reasoning over goals and beliefs. These\ncode-based representations scale more efficiently than behavior cloning or inverse planning\nalternatives, even when the ground truth behavior does not come from a known program.\n2\n3. Human Studies Validation: We recruit real human participants to generate behavior\nand predict others’ actions. We find that ROTE outperforms baselines and achieves human-\nlevel performance in predicting human behaviors, even for noisy and sparse trajectories.\n2\nRelated Work\nAction Prediction. Prior work developing AI for action prediction follows two dominant\ncategories: symbolic methods and neural networks. Symbolic methods, such as Bayesian\nInverse Planning (BIP), infer an agent’s goals and beliefs by calculating their probabilities\nbased on observed actions (Ullman et al., 2009; Baker et al., 2017; Shum et al., 2019;\nNetanyahu et al., 2021; Kleiman-Weiner et al., 2016; Wang et al., 2020; Kleiman-Weiner et al.,\n2020; Serrino et al., 2019; Kleiman-Weiner et al., 2025). While robust, these methods are not\nscalable due to the exponential complexity of a multi-agent environment (Rathnasabapathy\net al., 2006; Doshi & Gmytrasiewicz, 2009; Seaman et al., 2018).\nIn contrast, neural\napproaches like behavioral cloning (BC) and inverse reinforcement learning (IRL) train\nmodels to directly mimic actions (Torabi et al., 2018; Ng et al., 2000; Abbeel & Ng, 2004;\nWulfmeier et al., 2016; Wang et al., 2021; Christiano et al., 2023), but are often data-intensive,\nfragile, and prone to overfitting. Recent work has tried modeling reward functions as finite-\nstate automatons, a concept known as “reward machines” (Icarte et al., 2018; Toro Icarte\net al., 2022; Li et al., 2025). This method, which does not use LLMs, allows for structured\nrepresentation of reward and can provide non-Markovian feedback to agents. While primarily\nused for training agents to solve compositional tasks, there has been work on inferring\nreward machines from expert demonstrations (Zhou & Li, 2022) or learning safety constraints\n(Malik et al., 2021; Lindner et al., 2024; Liu et al., 2025). Despite these advances, neural\nmodels still struggle with generalization, particularly in social reasoning, as they often fail to\ncapture the causal structure of behavior (de Haan et al., 2019; Codevilla et al., 2019; Bain &\nSammut, 1995). This brittleness persists even with advanced techniques that learn contextual\nrepresentations (Rabinowitz et al., 2018; Chuang et al., 2020; Jha et al., 2024) and does not\ndisappear at scale under an assumption of imperfect rationality (Poddar et al., 2024). In\ncontrast, our approach, which uses an LLM to generate open-ended code describing observed\nbehavior, makes fewer assumptions about the nature of the agents being modeled. This\nallows it to capture everyday decision-making processes that may not be reward-maximizing.\nLarge Language Models (LLMs) for Behavior Modeling. LLMs may be a more\neffective bridge between the neural and symbolic paradigms. They enable enumerative\ninference for social reasoning (Wilf et al., 2023; Jung et al., 2024; Huang et al., 2024; Jin\net al., 2024; Kim et al., 2025; Zhang et al., 2025), while neuro-symbolic frameworks (e.g.,\nBIP + LLMs) improve robustness in embodied cooperation (Ying et al., 2024; Ding et al.,\n2024; Ying et al., 2025; Wan et al., 2025).\nHowever, existing implementations remain\ncomputationally intensive, often generating thousands of tokens for each prediction. In\nrealistic settings, we need methods capable of rapid inference that still capture the structure\nof culturally shaped conventions and behaviors performed without deep cognitive processing\n(Bargh, 1994; Wood, 2024). By learning a code-based agent representation, ROTE avoids\nthe high computational cost that BIP must incur to enumerate every possible goal.\nProgram Induction. Program synthesis has proven effective for world modeling (Guan\net al., 2023; Wong et al., 2023b;a; Zhu & Simmons, 2024), action selection (Verma et al.,\n2021; Wang et al., 2023; Yao et al., 2023), and has even achieved near-expert performance on\nmathematical reasoning tasks such as International Math Olympiad problems (Trinh et al.,\n2024). Neurosymbolic approaches, which combine LLMs or domain-specific neural networks\nwith probabilistic program inference, have enabled agents to learn environment dynamics\n(Das et al., 2023) and master complex games like Sokoban and Frostbite with impressive\nsample efficiency (Tang et al., 2024; Tsividis et al., 2021; Tomov et al., 2023). Code-like\nrepresentations have been used to infer reward functions from state-action transitions (Yu\net al., 2023; Davidson et al., 2025), and LLMs have been harnessed to synthesize policies\nor planning strategies in domain-specific contexts (Liang et al., 2023; Sun et al., 2023;\nTrivedi et al., 2022). However, these prior approaches typically rely on well-defined rewards,\ndomain-specific constraints, or focus on partial aspects of agent behavior, such as reward\ninference or demonstration summarization. In contrast, ROTE aims to infer an agent’s\n3\nWhat will the \nAgent do next?\nRepresenting Others’ Trajectories as Executables (ROTE)\n2) Refine Program Likelihood \nvia Bayesian Inference\nclass ToyChair:\ndef act(self, obs):\nif found_toy(obs):\nmove_to_chair(obs)\nelse: find_toy(obs)\nclass ToyBedroom:\ndef act(self, obs):\nif found_toy(obs):\nmove_to_room(obs)\nelse: find_toy(obs)\nclass CleanKitchen:\ndef act(self, obs):\nif in_kitchen(obs):\nscrub_dishes(obs)\nelse: go_kitchen(obs)\nclass WanderHouse:\ndef act(self, obs):\nt = obs.timestep\nrID = t % len(rooms)\nvisitRoom(rID, obs)\n1) Generate Candidate Agent Programs\nwith LLMs given History\nGround Truth \nState\nt=1\nt=3\nt=5\na. Toys to the Chairs\nb. Toys to the Bedroom\nc. Clean dishes\nd. Wander Between Rooms\nFigure 2: Overview of ROTE. ROTE predicts an agent’s next action by generating and\nweighting Python programs that explain its observed behavior. From t = 0 to t = 7, ROTE\nobserves a blue robot’s trajectory. Initially, at t = 1, programs related to moving to the\ndining room are up-weighted. However, at t = 3, the robot picks up a toy, and ROTE\nremains uncertain if the goal is to clean up toys in the bedroom or place them on chairs\nin the living room. After the robot places the toy on a chair at t = 5, ROTE confidently\nupdates its program weights to reflect the “bringing toys to chairs” script. By t = 7, ROTE\ncan use this inferred script to rapidly and accurately predict future actions.\ncausal decision-making process directly from observed behavior and assumes no access to\nreward signals or domain-specific structure.\n3\nRepresenting Others’ Trajectories as Executables\nDrawing upon recent conceptualizations of “agents” in reinforcement learning and theoretical\ncomputer science Abel et al. (2023a); Dong et al. (2021); Lu et al. (2023); Leike (2016);\nLattimore et al. (2013); Majeed & Hutter (2018); Majeed (2021); Cohen et al. (2019), we\nrepresent computationally bounded agents as programs with internal states, which can be\nconceptualized as Finite State Machines. This is formally represented using the notation\nλ = (S, s0, π, u) from Abel et al. (2023b), where finite internal states st ∈S used for decision-\nmaking in the policy π : S →∆A evolve via a transition function u(st−1, at−1, ot) →st,\nwhich maps the observations from the external world to the agent’s next internal decision-\nmaking state. In the following section, we will demonstrate how we can search for the minimal\nprogram in the space of agents λ ∈Λ that best explains observed history of (observation\no ∈O, action a ∈A) pairs, h ∈H. For the rest of this section, we use the notation h0:t to\nindicate the history of pairs from time 0 to t.\n3.1\nAgent Program Synthesis with Large Language Models\nGiven a finite length history h0:t−1 ∈H, from time 0 to t −1, our objective is to find an\nagent ˆλ ∈Λ that both (1) takes the same action at as the ground truth agent λ∗when\npresented with observation ot, and (2) minimizes its program size |ˆλ|. Encouraging concise\nprogram synthesis is not just a matter of engineering preference but is theoretically grounded\nin the foundations of algorithmic probability and inductive inference. Solomonoff’s theory of\ninductive inference formalizes Occam’s razor, demonstrating that the best scientific model\nfor a given set of observations is the shortest algorithm (in terms of description length) that\ngenerates the data in question Solomonoff (1964; 1978; 1996). Under this framework, shorter\nprograms are assigned higher prior probability, providing a universal solution to the problem\nof induction with strong convergence guarantees: the expected cumulative prediction error\nis bounded by the Kolmogorov complexity of the true data-generating process Solomonoff\n(1978; 1996). Thus, searching for minimal agent representations is not only computationally\ndesirable but also theoretically optimal for generalization, a bias also observed in human\nprogrammatic reasoning (Bigelow & Ullman, 2025).\nWe operationalize our search through the space of agents Λ through a two-stage approach:\nFirst, we optionally prompt an LLM to transform raw perceptual inputs into a natural\n4\nlanguage description of an agent’s path. These percepts can be low-level observations like\nobject coordinates in gridworlds, or even natural language scene-graphs from datasets like\nPartnr (Chang et al., 2025). Next, we have the LLM generate many possible Python programs\nto obtain a distribution over possible code-based agent models which explain the observed\nbehavior, ∆(Λ). Python is chosen for its readability, widespread use in AI research, and its\npower as a Turing-complete language, enabling the representation of arbitrarily complex\ndecision-making logic in the worst case where |S| = |O| for the ground-truth agent λ∗. Our\nprompting strategy makes two key assumptions: (1) the observed agent follows deterministic\ntransitions between finite internal states S contingent on environmental/historical cues\nrather than executing complex adaptive policies, and (2) generated code should produce\ndeterministic actions a ∈A. Importantly, we ask the LLM to assume these properties of the\nobserved trajectories even if the ground truth agent generating the behavior is probabilistic\nand following sophisticated, goal-directed plans. While this assumes a deterministic agent, we\naccount for potential stochasticity in behavior with a noise model, allowing our approach\nto best approximate the underlying deterministic policy. We instruct the LLM to generate\ncode that is efficient (low runtime complexity) and concise (minimize |λ|).\n3.2\nRefining Generations through Bayesian Inference\nTo form a more robust estimate of the true underlying agent program λ∗, we refine the\ndistribution over candidate programs ∆(Λ) obtained from the language model using Sequential\nMonte Carlo. Specifically, we estimate the posterior probability of a candidate agent program\nλ given the observed history h0:t−1 using the relationship:\np(λ|h0:t−1) ∝p(h0:t−1|λ)p(λ).\n(1)\nThis approach is related to inverse planning-based methods that infer latent goals given\nobserved behavior (Ullman et al., 2009; Baker et al., 2017; Shum et al., 2019; Netanyahu\net al., 2021). However, instead of assuming a fixed, often complex, planner (like MCTS or\nbrute-force search) and performing inference over a space of goals, our method condenses\nall behavioral conventions and scripts an agent might follow into a single programmatic\nrepresentation λ. Since λ is a deterministic program, we give the action ˆat it predicts the\nground-truth agent will take at observation ot a probability of (1 −ϵ) and all other actions\na−∈A −{ ˆat} a probability of\nϵ\n|A|−1. This effectively allows λ to predict a distribution\nover actions ∆(A) it might take at each step. Then, we can perform inference directly over\nthe space of likely decision-making processes encoded as Python programs by calculating\np(λ|h0:t−1) ∝Πoi,ai∈h0:t−1p(ai|oi, λ) · pprior(λ). With this refined posterior distribution, we\nselect the k most likely agent programs, and execute the corresponding Python code for each\nfrom the current observation ot. Then, ROTE performs a weighted combination of agent\nprograms to form our approximation λ∗≈ˆλ = P\nλ∈∆(Λ) p(λ|h0:t−1) · λ(·|ot).\nThe combination of LLM-based program synthesis with Bayesian Inference results in our\nmethod for inferring others’ behaviors, ROTE. Pseudocode for our approach can be found\nin Algorithm 1, and in Figure 2, we provide an overview of ROTE on an intuitive example\nin the Partnr environment, an embodied robotics simulator where an agent tries to help a\nhuman complete a variety of household chores (Chang et al., 2025). We additionally include\nexamples of agent code inferred by ROTE in Construction and Partnr in Appendix A.10.2.\n4\nExperiments\nEnvironments.\nWe evaluate ROTE across two distinct environments.\nFirst, we use\nConstruction (shown in Figure 7), a fully-observable 2D grid-world where agents actively\nnavigate obstacles like walls and other agents, and can transport colored blocks to different\nlocations on the map (Jha et al., 2024). Then, we explore the efficacy of our method on\nPartnr (shown in Figure 5), a large-scale embodied robotics simulator where an AI-assistant\nperceives a realistic home or office space as a natural language scene-graph (Chang et al.,\n2025). Built on the Habitat benchmark, this environment requires the agent to utilize tools\nto help a human complete tasks in a partially observable world (Puig et al., 2023).\n5\nBaselines. We compare ROTE against three baselines: Behavior Cloning (BC). In\nthe Construction environment, the BC model is a neural network with an LSTM trained\non pixel-based observations of agent trajectories (Rabinowitz et al., 2018); for Partnr, we\nfine-tuned Llama-3.1-8b to imitate a ground-truth LLM agent’s behaviors using a training\nset of (scene-graph, action) pairs (Chang et al., 2025). Automated Theory of Mind\n(AutoToM) (Zhang et al., 2025). AutoToM is a neuro-symbolic approach which uses LLMs\nto generate open-ended hypotheses about an agent’s beliefs, goals, and desires, then applies\nBayesian Inverse Planning to find the most likely action. Naive LLM (NLLM). NLLM\nsimply prompts an LLM with observed states and environment dynamics to predict the next\naction directly. Our evaluation for all methods except for BC uses a suite of LLMs: Llama-3.1-\n8b Instruct, DeepSeek-V2-Lite (16b), DeepSeek-Coder-V2-Lite-Instruct (16b), and we report\nthe highest accuracy achieved for each baseline to ensure the most competitive comparison.\nAll results for ROTE were obtained using DeepSeek-Coder-V2-Lite-Instruct, while other\nbaselines show the highest-performing model for each environment. Appendix A.7 provides\na detailed breakdown of per-task and per-LLM accuracy for all methods, demonstrating our\napproach’s consistent success across different LLM model types.\nDataset Generation.\nFor the fully observable Construction environment, we hand-\ndesigned 10 distinct Finite State Machines to generate 50, 000 state-action pairs across 100\ntrajectories/agent ×10 agents = 1000 trajectories. Behaviors ranged from simple tasks,\nsuch as patrolling, where agents rely on planning heuristics, to complex goal-directed tasks\nusing A* search, like finding all green blocks. We have included some for illustration in\nFigure 7 and the full list of behaviors in Appendix A.1. For the partially observable Partnr\nenvironment, we used the LLM agents defined in (Chang et al., 2025) to generate state-action\npairs for a robot assistant completing diverse tasks (i.e. “clean all toys in the bedroom”)\nfrom their “train” and “validation” datasets. In these datasets, states are represented as\nnatural language scene graphs, and actions are high-level tools.\nEvaluation Protocol. We evaluate using two protocols: (1) single-step prediction, where\ngiven observations from timesteps 0 to t, the task is to predict the action at; and (2) multi-step\nprediction, where we iteratively predict actions ˆat, . . . , ˆat+10 conditioned on the ground-truth\nobserved states o0, . . . , ot. For the BC model in Construction, we hold out 100 trajectories\nfor evaluation, training on the remaining data. All baselines are evaluated on these 100\nheld-out trajectories. For Partnr, we evaluate single-step prediction with t = |H| −2, since\nvarying trajectory lengths make multi-step evaluation inconsistent, and the final timestep\nis always the terminal action. We evaluate all models on the entire “validation” dataset,\nusing the “train” dataset to finetune the BC model. We only predict high-level tools used by\nagents in Partnr, since AutoToM requires static-sized action spaces (Zhang et al., 2025).\nHuman Studies. We conducted human studies in the single-agent Construction environment\nto evaluate ROTE’s ability to predict human behavior and to benchmark its performance\nBC\nAutoToM NLLM\nROTE\nHuman\n0.00\n0.22\n0.44\n0.66\nAccuracy\nSingle-Step\nMulti-Step\n(a) Single-step vs. multi-step prediction accuracy\nfor Construction with scripted agents\nBC\nAutoToM NLLM\nROTE\nHuman\n0.00\n0.22\n0.44\n0.66\nAccuracy\nSingle-Step\nMulti-Step\n(b) Single-step vs. multi-step prediction accuracy\nfor Construction with human agents\nFigure 3: ROTE outperforms all baselines in both single-step and multi-step action prediction\nfor scripted (a) and human agents (b). ROTE’s code-based representations, which treat\nhuman actions as efficient scripts, enable it to generalize effectively from limited observations.\nFor single-step predictions, ROTE was significantly more accurate than all baselines for\nboth scripted (p < 0.05 for NLLM, p < 0.001 for BC and AutoToM) and human agents\n(p < 0.05 for BC, p < 0.01 for NLLM, p < 0.001 for AutoToM). This superior performance\nwas maintained in multi-step predictions for both agent types (scripted: p < 0.001 for BC,\nAutoToM, and NLLM; human: p < 0.01 for BC, p < 0.001 for NLLM and AutoToM). ROTE\nachieved human-level predictive accuracy of human behavior.\n6\nBC\nAutoToM\nNLLM\nROTE\n0.00\n0.22\n0.44\n0.66\nAccuracy\nGeneralization Accuracy\nFigure 4: ROTE demonstrates superior zero-\nshot generalization to novel environments in\nConstruction. Without any additional condi-\ntioning on an agent’s behavior, the programs\nROTE infers from one environment transfer to\nnovel settings more effectively than all other\nbaselines (p < 0.001 in a two-sided t-test).\nagainst human predictions. For the first study, 10 participants were recruited to perform\ntheir interpretation of each of the 10 handcrafted FSMs without observing the ground-truth\ncode, generating 30 state-action pairs/person/script. In a separate study, we recruited 25\nhumans to act as predictors. They were shown a human’s trajectory from t = 0 to t = 19\nand the state at t = 20, and asked to predict its next five actions, from t = 20 to t = 24.\nWe use the same setup for a third study to explore how well people predict the behavior of\nthe ground-truth FSM’s next actions instead. We compared peoples’ prediction accuracy\nto ROTE and the other baselines to benchmark different behavior modeling algorithms.\nAll studies were approved by our university’s Institutional Review Board (IRB) and were\ndesigned using NiceWebRL (Carvalho et al., 2025). We used Prolific for crowdsourcing data.\nWe plan to open-source the code for our baselines, datasets, and human evaluations.\n5\nResults\nHow well does ROTE model and predict scripted agent behavior? To evaluate\nthe effectiveness of ROTE, we first examined its predictive accuracy in a controlled setting\nwhere agents in the Construction environment followed one of 10 handcrafted programs.\nThese programs were not provided to ROTE at any point during evaluation. Our results in\nFigure 3a demonstrate that ROTE consistently surpasses all baselines in both single-step and\nmulti-step prediction accuracy in this evaluation setting and does not statistically significantly\nunderperform human performance (p=0.3087 for single-step and p=0.1679 for multi-step in\na two-sided t-test). While these initial results were promising, a potential concern was that\nROTE might simply be exploiting repetitive patterns, rather than learning the underlying\npolicy. We investigated this by measuring how often an agent revisited a state or repeated\nan action. We found an extremely low correlation between ROTE’s accuracy and either of\nthese metrics (0.303 for matching states, 0.064 for matching actions), confirms that ROTE is\nnot exploiting simple data regularities. This finding, paired with ROTE’s strong multi-step\nperformance, suggests that code-based representations can be effective for learning the\nunderlying policies that enable robust, long-term predictions.\nHow well does ROTE model and predict human behavior? Having established that\nROTE’s code-based representations are effective in controlled, scripted environments, we next\nwanted to test its ability to model more complex, nuanced behaviors. We began by evaluating\nBC\nAutoToM NLLM\nROTE\n0.00\n0.22\n0.44\n0.66\nPartnr Accuracy\n(a) Action prediction accuracy in Partnr\nRobot Observation \nin Partnr\nclass FSMAgent:\ndef __init__(self): self.state = “IDLE”\ndef act(self, obs):\ntransitions = {“IDLE”: (“Explore”, “SEARCHING”),\n“SEARCHING”: (“Navigate”, “MOVING”), “MOVING”: \n(“Place”, “IDLE”)}\ncheck_inventory(obs)\nif self.has_firetruck:\naction, self.state = transitions[self.state]\nelse: action = “Navigate”\nreturn action\nInferred Agent Code \nfrom ROTE\nGround-Truth Task: “Put the fire truck on the chest”\n(b) Example Partnr task with ROTE’s inferred program\nFigure 5: (a) Prediction accuracy in the large-scale, partially observable Partnr environment.\nROTE demonstrated a superior ability to anticipate the behavior of goal-directed, LLM-based\nagents, with a two-sided t-test showing ROTE significantly outperformed all other models\n(p < 0.001). (b) The pseudocode example illustrates how ROTE’s inferred programs capture\ncomplex task logic using conditionals and state-tracking.\n7\n0\n5\n10\n15\n20\n25\n30\n# of Predictions\n100\n101\n102\n103\nTotal Prediction \nTime (s)\nBC\nAutoToM\nNLLM\nROTE\nFigure 6: Total multi-step prediction time in\nConstruction. Despite being slower than BC\nand Naive LLM prompting in the single-step\nprediction case, ROTE’s programmatic rep-\nresentations enable its multi-step compute\ncost to scale orders of magnitude more ef-\nficiently than other approaches, making it\nbetter suited for long-horizon settings than\nother approaches for predicting individual\nbehavior.\nROTE against human agents performing 10 tasks in the Construction environment. As\nillustrated in Figure 3b, ROTE outperforms all baseline algorithms and achieves human-\nlevel predictive accuracy of next-step human actions. A deeper per-task accuracy\nanalysis, shown in Figure 9, reveals that ROTE has greater accuracy than humans on some\ntasks with repetitive patterns, such as “move up if possible, otherwise down” or “move in\nan L-shape.” However, humans are still much better at anticipating scripts for tasks such\nas “patrol the grid clockwise” and goal-directed tasks such as “move all pink blocks to the\ncorner of the grid.” This gap highlights that while the code produced by ROTE is expressive\nenough to capture many behaviors, more powerful LLMs with enhanced reasoning may be\nneeded to achieve human-level prediction in all settings.\nGeneralizing to Novel Environments: A key advantage of modeling behavior with\nscripts is the potential for rapid generalization to new, but similar, environments. We wanted\nto know if ROTE’s inferred programs could transfer without needing to be relearned. To\ntest this, we first observed a scripted agent following a pattern like “patrol counterclockwise”\nfor 20 timesteps, and then showed the same agent in a distinct environment. We then asked\nROTE and the baselines to predict the next 10 actions of the same agent. For ROTE, this\nwas done by using the same set of programs inferred in the first environment for prediction\nwithout updating their likelihoods. Figure 4 shows that ROTE can still predict the agent’s\nbehavior accurately in the new setting, outperforming all baselines without needing to\nre-incur the cost of text generation, a necessary step for NLLM and AutoToM. Although\nROTE’s performance decreases compared to predicting behavior in the original environment\nin Figure 3a, its ability to generalize makes it a more accurate and efficient alternative to\ntraditional Inverse Planning or purely neural methods.\nCan ROTE’s code-based approach scale to model behavior in complex, realistic\nenvironments? To further push the boundaries of ROTE’s capabilities, we tested it on the\nembodied robotics benchmark Partnr, where the task is to predict the next tool utilized by\nLLM-agents simulating a human or robot completing chores. This environment is particularly\nchallenging due to partial observability and long-horizon, compositional tasks such as “find a\nplate and clean it in the kitchen” or “look for toys and organize them neatly in the bedroom.”\nDespite this complexity, Figure 5 shows our approach significantly outperformed all baselines,\nincluding inverse planning and behavior cloning methods, and those incorporating LLMs. To\nbetter understand the types of problems ROTE excels at, we used Llama-3.1-8B-Instruct to\ncluster the ground-truth tasks from our test set into three categories, as shown in Figure 10.\nWhile baselines like AutoToM and Behavior Cloning showed success with tasks involving\nsimple navigation, ROTE demonstrated a superior ability to handle more intricate problems,\nsuch as turning items on/off and cleaning objects. This demonstrates its generalizability in\ncreating code for agents that face uncertainty and possess beliefs about their environment.\nHow does the computational efficiency of ROTE compare to other approaches?\nForming long-horizon plans in the presence of other agents requires predicting their behaviors\nover time quickly and not just accurately. To understand whether ROTE scales effectively,\nwe plot the time in seconds required for different baselines to make predictions about agents’\nbehaviors multiple times into the future in Construction. As shown in Figure 6, while ROTE\nis initially slower for single-step predictions compared to BC and NLLM baselines due to the\nneed to generate and prune candidate programs, its test-time compute costs scale orders of\nmagnitude more efficiently with the number of predictions. This is because once ROTE’s\n8\ncode-based representations are inferred, it can execute these programs rapidly for all future\nsteps. In contrast, other LLM-based methods must re-generate a response for every new\ntime step. We analyze additional factors contributing to this efficiency in Appendix A.5 and\nFigure 14. Taken together with the results from Figures 3, 4, and 5, this illustrates that\ncode-based representations can balance predictive power with prediction efficiency.\nWhat is the relationship between ROTE’s core components and its predictive\nperformance? To understand how ROTE achieves its superior generalization and human-\nlevel accuracy, we conducted a series of ablation studies on its core components. We found\nthat ROTE’s two-stage observation parsing, which converts observations into a natural\nlanguage description before generating code, had a minimal effect on accuracy for the FSM\nand human gameplay datasets in Construction (Figure 11). However, this process significantly\nhurt performance in Partnr. This is likely because Partnr’s observations are already rich\nscene graphs (Chang et al., 2025), and the abstraction step removes crucial details needed\nfor effective program generation. Additionally, we investigated the use of Sequential Monte\nCarlo (SMC) with rejuvenation versus standard Importance Sampling. SMC, which replaces\nlow-likelihood programs with new ones, improved early-stage accuracy when the number of\nsampled hypotheses was small (Figure 12). This benefit, however, diminished as the initial\nset of candidate hypotheses increased, suggesting that the initial diversity provided by the\nLLM is often sufficient.\nLastly, we analyzed the impact of imposing different degrees of structural constraints on\nROTE’s program generation, inspired by methods for inferring reward functions (Yu et al.,\n2023). We evaluated three variants: “Light” (assuming agents are FSMs without providing\nexamples), “Moderate” (defining FSM states explicitly but allowing open-ended code), and\n“Severe” (a two-stage process converting natural language predictions of FSMs into code).\nOur previous results were based on the “Light” condition. The optimal level of structure,\nhowever, varied by environment, as shown in Figure 13. In the Construction environment,\nwhere agents followed predictable FSMs, the Severe approach performed as well as others.\nThis suggests that for predictable, rote behaviors, an explicitly structured representation can\nbe just as effective while also being computationally efficient (Callaway et al., 2018; Lieder\n& Griffiths, 2020; Callaway et al., 2022; Icard, 2023). Conversely, modeling human behavior\nproved less suited for strict FSMs. The Moderate condition was superior for human gameplay,\nhighlighting the need for representational flexibility when agents are following a general script\nbut exhibiting inherent variability. In the partially observable Partnr environment, forcing\nagents into a strict FSM representation performed significantly worse than open-ended code\ngeneration, suggesting these scenarios might be better suited for traditional Inverse Planning\nmethods that can handle a wider range of states and tasks. These findings reveal a gradient of\nagentic representations, from automatic to goal-directed, which allows for flexible prediction\nacross different scenarios. Future work could use meta-reinforcement learning to dynamically\nselect the appropriate level of representational structure based on the task.\n6\nDiscussion\nIn this work, we framed behavior inference as a program synthesis problem, showing that\nour approach, ROTE, can accurately and efficiently predict the actions of machines and real\npeople in complex environments. ROTE offers a scalable alternative to traditional methods\nthat require extensive datasets or significant computational resources. This has immediate\nimplications for domains where real-time adaptability and interpretability are crucial, such\nas with caregiver robots that could use ROTE’s representations to anticipate daily routines.\nLimitations: While our results highlight the effectiveness of program synthesis for text-\nbased observations, we note the limitations of the applicability of our findings in Partnr since\nwe only predicted high-level tools used by agents, which was done to accommodate baselines\nwhich required static action spaces. While our evaluation in Partnr still involved more\ntools than our other experiments (19 actions in Partnr compared to 6 in the Construction),\nfuture research should explore ROTE in high-dimensional, continuous control settings. In\nthose cases, ROTE might need to be integrated with vision-language models (VLMs) to\nparse pixel-based inputs (e.g., raw video feeds for assistive robots) and neural control\nmechanisms to execute plans, effectively operating at the level of option prediction (Sutton\net al., 1999). Another interesting direction would be to explore how the size of LLMs used\n9\nfor behavioral program inference impacts prediction quality in more sophisticated scenarios,\nsuch as modeling team coordination in workplaces or norm enforcement on social platforms.\nLastly, unlike traditional Theory of Mind approaches that predict beliefs and goals, our\nwork focuses solely on action prediction. If we view beliefs as dispositions to act (Ramsey &\nMoore, 1927; Ryle, 1949), predicting a distribution over an agent’s internal decision-making\nstates and logic for transitioning between them is functionally equivalent to belief inference.\nThat said, further research is needed to empirically validate whether the structure of ROTE’s\ninferred programs aligns with how humans intuitively reason about others’ mental states.\n7\nAcknowledgements\nWe would like to thank the Cooperative AI Foundation, the Foresight Institute, the UW-\nAmazon Science Gift Hub, the Sony Research Award Program, UW-Tsukuba Amazon\nNVIDIA Cross Pacific AI Initiative (XPAI), the Microsoft Accelerate Foundation Models\nResearch Program, Character.AI, DoorDash, and the Schmidt AI2050 Fellows program for\ntheir generous support of our research. This material is based upon work supported by the\nDefense Advanced Research Projects Agency and the Air Force Research Laboratory, contract\nnumber(s): FA8650-23-C-7316. Any opinions, findings and conclusions, or recommendations\nexpressed in this material are those of the author(s) and do not necessarily reflect the views\nof AFRL or DARPA. Lastly, we would like to express our gratitude to our colleagues at the\nSocial Reinforcement Lab and the Computational Minds and Machines Lab, as well as Guy\nDavidson, Tan Zhi-Xuan, and Tomer Ullman for inspiring conversations and insights during\nthe course of this project.\nReferences\nPieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning.\nIn Proceedings of the twenty-first international conference on Machine learning, pp. 1,\n2004.\nDavid Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, and\nSatinder Singh. A definition of continual reinforcement learning, 2023a. URL https:\n//arxiv.org/abs/2307.11046.\nDavid Abel, André Barreto, Hado van Hasselt, Benjamin Van Roy, Doina Precup, and\nSatinder Singh. On the convergence of bounded agents, 2023b. URL https://arxiv.\norg/abs/2307.11044.\nRobert Axelrod. Effective Choice in the Prisoner’s Dilemma. Journal of Conflict Resolution,\n24(1):3–25, 1980. doi: 10.1177/002200278002400101. URL https://doi.org/10.1177/\n002200278002400101. _eprint: https://doi.org/10.1177/002200278002400101.\nMichael Bain and Claude Sammut. A framework for behavioural cloning. In Machine\nintelligence 15, pp. 103–129, 1995.\nChris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum. Rational\nquantitative attribution of beliefs, desires and percepts in human mentalizing. Nature\nHuman Behaviour, 1(4):0064, 2017.\nJohn A. Bargh. The four horsemen of automaticity: Awareness, intention, efficiency, and\ncontrol in social cognition. In Handbook of social cognition: Basic processes; Applications,\nVols. 1-2, 2nd ed., pp. 1–40. Lawrence Erlbaum Associates, Inc, Hillsdale, NJ, US, 1994.\nISBN 0-8058-1056-0 (Hardcover); 0-8058-1057-9 (Hardcover).\nIlona Bass, Cristian Espinoza, Elizabeth Bonawitz, and Tomer D Ullman. Teaching without\nthinking: Negative evaluations of rote pedagogy. Cognitive science, 48(6):e13470, 2024.\nEric Bigelow and Tomer Ullman. People evaluate agents based on the algorithms that drive\ntheir behavior. Open Mind, 9:1411–1430, 08 2025. ISSN 2470-2986. doi: 10.1162/opmi.a.26.\nURL https://doi.org/10.1162/opmi.a.26.\n10\nFrederick Callaway, Falk Lieder, Priyam Das, Sayan Gul, and Paul M Krueger. A resource-\nrational analysis of human planning. In Proceedings of the Annual Meeting of the Cognitive\nScience Society, volume 40, 2018.\nFrederick Callaway, Bas van Opheusden, Sayan Gul, Priyam Das, Paul M Krueger, Thomas L\nGriffiths, and Falk Lieder. Rational use of cognitive resources in human planning. Nature\nHuman Behaviour, 6(8):1112–1125, 2022.\nMurray Campbell, A.Joseph Hoane, and Feng-hsiung Hsu. Deep Blue. Artificial Intelligence,\n134(1-2):57–83, January 2002. ISSN 00043702. doi: 10.1016/S0004-3702(01)00129-1. URL\nhttps://linkinghub.elsevier.com/retrieve/pii/S0004370201001291.\nWilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, and Kunal Jha. Nicewebrl: a\npython library for human subject experiments with reinforcement learning environments,\n2025. URL https://arxiv.org/abs/2508.15693.\nMatthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai,\nMichal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar,\nSiddharth Patki, Ishita Prasad, Xavier Puig, Akshara Rai, Ram Ramrakhya, Daniel Tran,\nJoanne Truong, John M. Turner, Eric Undersander, and Tsung-Yen Yang. Partnr: A\nbenchmark for planning and reasoning in embodied multi-agent tasks. In International\nConference on Learning Representations (ICLR), 2025. alphabetical author order.\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences, 2023. URL https://arxiv.org/\nabs/1706.03741.\nYun-Shiuan Chuang, Hsin-Yi Hung, Edwinn Gamborino, Joshua Oon Soo Goh, Tsung-Ren\nHuang, Yu-Ling Chang, Su-Ling Yeh, and Li-Chen Fu. Using machine theory of mind to\nlearn agent social network structures from observed interactive behaviors with targets. In\n2020 29th IEEE International Conference on Robot and Human Interactive Communication\n(RO-MAN), pp. 1013–1019. IEEE, 2020.\nFelipe Codevilla, Eder Santana, Antonio M. López, and Adrien Gaidon. Exploring the\nlimitations of behavior cloning for autonomous driving, 2019. URL https://arxiv.org/\nabs/1904.08980.\nMichael K. Cohen, Elliot Catt, and Marcus Hutter. A strongly asymptotically optimal agent\nin general environments, 2019. URL https://arxiv.org/abs/1903.01021.\nRia Das, Joshua B. Tenenbaum, Armando Solar-Lezama, and Zenna Tavares. Combining\nfunctional and automata synthesis to discover causal reactive programs. Proc. ACM\nProgram. Lang., 7(POPL), January 2023. doi: 10.1145/3571249. URL https://doi.org/\n10.1145/3571249.\nGuy Davidson, Graham Todd, Julian Togelius, Todd M. Gureckis, and Brenden M. Lake.\nGoals as reward-producing programs. Nature Machine Intelligence, 7(2):205–220, February\n2025. ISSN 2522-5839. doi: 10.1038/s42256-025-00981-4. URL https://doi.org/10.\n1038/s42256-025-00981-4.\nPim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning,\n2019. URL https://arxiv.org/abs/1905.11979.\nD. C. Dennett. Content and consciousness. International library of philosophy and scientific\nmethod. Routledge & Kegan Paul [u.a.], London, reprinted edition, 1972. ISBN 978-0-\n7100-6512-4.\nD. C. Dennett. From bacteria to Bach and back: the evolution of minds. W.W. Norton &\nCompany, New York, first edition edition, 2017. ISBN 978-0-393-24207-2.\nD. C. Dennett and Edward Gorey. Brainstorms: philosophical essays on mind and psychology.\nThe MIT Press, Cambridge, Massachusetts ; London, England, 1981. ISBN 978-0-262-\n54037-7.\n11\nDaniel C. Dennett. The Intentional Stance. The MIT Press, Cambridge, MA, 1987.\nWei Ding, Fanhong Li, Ziteng Ji, Zhengrong Xue, and Jia Liu. Atom-bot: Embodied\nfulfillment of unspoken human needs with affective theory of mind.\narXiv preprint\narXiv:2406.08455, 2024.\nShi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Simple agent, complex environment:\nEfficient reinforcement learning with agent states, 2021. URL https://arxiv.org/abs/\n2102.05261.\nPrashant Doshi and Piotr J Gmytrasiewicz. Monte Carlo sampling methods for approximating\ninteractive POMDPs. Journal of Artificial Intelligence Research, 34:297–337, 2009.\nHartry H. Field. Mental representation. Erkenntnis, 13(1):9–61, January 1978. ISSN\n0165-0106, 1572-8420. doi: 10.1007/BF00160888. URL http://link.springer.com/10.\n1007/BF00160888.\nAndrew Fuchs, Andrea Passarella, and Marco Conti. Modeling, replicating, and predicting\nhuman behavior: A survey. ACM Transactions on Autonomous and Adaptive Systems, 18\n(2):1–47, 2023.\nLin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging\npre-trained large language models to construct and utilize world models for model-based\ntask planning, 2023. URL https://arxiv.org/abs/2305.14909.\nX. Angelo Huang, Emanuele La Malfa, Samuele Marro, Andrea Asperti, Anthony Cohn, and\nMichael Wooldridge. A notion of complexity for theory of mind via discrete world models,\n2024. URL https://arxiv.org/abs/2406.11911.\nThomas Icard. Resource rationality. Book manuscript, 434, 2023.\nRodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward\nmachines for high-level task specification and decomposition in reinforcement learning. In\nJennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on\nMachine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2107–2116.\nPMLR, 10–15 Jul 2018. URL https://proceedings.mlr.press/v80/icarte18a.html.\nJulian Jara-Ettinger and Yarrow Dunham.\nThe institutional stance, Apr 2024.\nURL\nosf.io/preprints/psyarxiv/pefsx_v1.\nKunal Jha, Tuan Anh Le, Chuanyang Jin, Yen-Ling Kuo, Joshua B Tenenbaum, and Tianmin\nShu. Neural amortized inference for nested multi-agent reasoning. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 38, pp. 530–537, 2024.\nChuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ull-\nman, Antonio Torralba, Joshua B. Tenenbaum, and Tianmin Shu. Mmtom-qa: Multimodal\ntheory of mind question answering, 2024. URL https://arxiv.org/abs/2401.08743.\nChani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon Seonwoo, Yejin Choi, Alice Oh,\nand Hyunwoo Kim. Perceptions to beliefs: Exploring precursory inferences for theory of\nmind in large language models, 2024. URL https://arxiv.org/abs/2407.06004.\nHyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B.\nTenenbaum, and Yejin Choi. Hypothesis-driven theory-of-mind reasoning for large language\nmodels, 2025. URL https://arxiv.org/abs/2502.11881.\nMax Kleiman-Weiner, Mark K Ho, Joseph L Austerweil, Michael L Littman, and Joshua B\nTenenbaum. Coordinate to cooperate or compete: abstract goals and joint intentions in\nsocial interaction. In Proceedings of the annual meeting of the cognitive science society,\nvolume 38, 2016.\nMax Kleiman-Weiner, Felix Sosa, Bill Thompson, Bas van Opheusden, Thomas L Griffiths,\nSamuel Gershman, and Fiery Cushman. Downloading culture. zip: Social learning by\nprogram induction. In Proceedings of the Annual Meeting of the Cognitive Science Society,\nvolume 42, 2020.\n12\nMax Kleiman-Weiner, Alejandro Vientós, David G. Rand, and Joshua B. Tenenbaum.\nEvolving general cooperation with a bayesian theory of mind. Proceedings of the National\nAcademy of Sciences, 122(25), June 2025. ISSN 1091-6490. doi: 10.1073/pnas.2400993122.\nURL http://dx.doi.org/10.1073/pnas.2400993122.\nTor Lattimore, Marcus Hutter, and Peter Sunehag. The sample-complexity of general\nreinforcement learning, 2013. URL https://arxiv.org/abs/1308.4828.\nJan Leike. Nonparametric general reinforcement learning, 2016. URL https://arxiv.org/\nabs/1611.08944.\nAndrew C. Li, Zizhao Chen, Toryn Q. Klassen, Pashootan Vaezipoor, Rodrigo Toro Icarte,\nand Sheila A. McIlraith. Reward machines for deep rl in noisy and uncertain environments,\n2025. URL https://arxiv.org/abs/2406.00120.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence,\nand Andy Zeng. Code as policies: Language model programs for embodied control, 2023.\nURL https://arxiv.org/abs/2209.07753.\nFalk Lieder and Thomas L Griffiths. Resource-rational analysis: Understanding human\ncognition as the optimal use of limited computational resources. Behavioral and brain\nsciences, 43:e1, 2020.\nDavid Lindner, Xin Chen, Sebastian Tschiatschek, Katja Hofmann, and Andreas Krause.\nLearning safety constraints from demonstrations with unknown rewards, 2024. URL\nhttps://arxiv.org/abs/2305.16147.\nGuiliang Liu, Sheng Xu, Shicheng Liu, Ashish Gaurav, Sriram Ganapathi Subramanian, and\nPascal Poupart. A comprehensive survey on inverse constrained reinforcement learning:\nDefinitions, progress and challenges, 2025. URL https://arxiv.org/abs/2409.07569.\nXiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and\nZheng Wen. Reinforcement learning, bit by bit, 2023. URL https://arxiv.org/abs/\n2103.04047.\nSultan J. Majeed.\nAbstractions of general reinforcement learning, 2021.\nURL https:\n//arxiv.org/abs/2112.13404.\nSultan Javed Majeed and Marcus Hutter.\nOn q-learning convergence for non-markov\ndecision processes. In Proceedings of the Twenty-Seventh International Joint Conference\non Artificial Intelligence, IJCAI-18, pp. 2546–2552. International Joint Conferences on\nArtificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/353. URL https:\n//doi.org/10.24963/ijcai.2018/353.\nShehryar Malik, Usman Anwar, Alireza Aghasi, and Ali Ahmed.\nInverse constrained\nreinforcement learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th\nInternational Conference on Machine Learning, volume 139 of Proceedings of Machine\nLearning Research, pp. 7390–7399. PMLR, 18–24 Jul 2021. URL https://proceedings.\nmlr.press/v139/malik21a.html.\nAviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, and Joshua B Tenenbaum. Phase:\nPhysically-grounded abstract social events for machine social perception. In Proceedings\nof the aaai conference on artificial intelligence, volume 35, pp. 845–853, 2021.\nAllen Newell and Herbert Simon. The logic theory machine–a complex information processing\nsystem. IRE Transactions on information theory, 2(3):61–79, 1956.\nAllen Newell and Herbert A. Simon. Computer science as empirical inquiry: symbols and\nsearch. Commun. ACM, 19(3):113–126, March 1976. ISSN 0001-0782. doi: 10.1145/360018.\n360022. URL https://doi.org/10.1145/360018.360022.\nAndrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml,\nvolume 1, pp. 2, 2000.\n13\nSriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques.\nPersonalizing reinforcement learning from human feedback with variational preference\nlearning, 2024. URL https://arxiv.org/abs/2408.10075.\nXavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Ruslan Partsey, Jimmy\nYang, Ruta Desai, Alexander William Clegg, Michal Hlavac, Tiffany Min, Theo Gervet,\nVladimír Vondruš, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira,\nMrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra,\nAkshara Rai, and Roozbeh Mottaghi. Habitat 3.0: A co-habitat for humans, avatars and\nrobots, 2023.\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew\nBotvinick. Machine theory of mind. In International conference on machine learning, pp.\n4218–4227. PMLR, 2018.\nF. P. Ramsey and G. E. Moore. Vi.–symposium: ?facts and propositions.? Aristotelian\nSociety Supplementary Volume, 7(1):153–206, 1927. doi: 10.1093/aristoteliansupp/7.1.153.\nBharanee Rathnasabapathy, Prashant Doshi, and Piotr Gmytrasiewicz. Exact solutions of\ninteractive pomdps using behavioral equivalence. In Proceedings of the fifth international\njoint conference on Autonomous agents and multiagent systems, pp. 1025–1032, 2006.\nGilbert Ryle. The concept of mind. British Journal for the Philosophy of Science, 1(4):\n328–332, 1949.\nRoger C. Schank and Robert P. Abelson. Scripts, Plans, Goals, and Understanding. Psy-\nchology Press, 0 edition, May 2013. ISBN 978-1-134-91966-6. doi: 10.4324/9780203781036.\nURL https://www.taylorfrancis.com/books/9781134919666.\nIris Rubi Seaman, Jan-Willem van de Meent, and David Wingate. Nested reasoning about\nautonomous agents using probabilistic programs. arXiv preprint arXiv:1812.01569, 2018.\nJack Serrino, Max Kleiman-Weiner, David C Parkes, and Josh Tenenbaum. Finding friend\nand foe in multi-agent games. Advances in Neural Information Processing Systems, 32,\n2019.\nMichael Shum, Max Kleiman-Weiner, Michael L Littman, and Joshua B Tenenbaum. Theory\nof minds: Understanding behavior in groups through inverse planning. In Proceedings of\nthe AAAI conference on artificial intelligence, volume 33, pp. 6163–6170, 2019.\nJoar Skalse and Alessandro Abate. Quantifying the sensitivity of inverse reinforcement\nlearning to misspecification. arXiv preprint arXiv:2403.06854, 2024.\nRay Solomonoff. Complexity-based induction systems: comparisons and convergence theo-\nrems. IEEE transactions on Information Theory, 24(4):422–432, 1978.\nRay Solomonoff. Does algorithmic probability solve the problem of induction. Oxbridge\nResearch, POB, 391887, 1996.\nRay J Solomonoff. A formal theory of inductive inference. part i. Information and control, 7\n(1):1–22, 1964.\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive\nplanning from feedback with language models, 2023. URL https://arxiv.org/abs/2305.\n16653.\nRichard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A\nframework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):\n181–211, August 1999.\nHao Tang, Darren Key, and Kevin Ellis. Worldcoder, a model-based llm agent: Building\nworld models by writing code and interacting with the environment. Advances in Neural\nInformation Processing Systems, 37:70148–70212, 2024.\n14\nMomchil S. Tomov, Pedro A. Tsividis, Thomas Pouncy, Joshua B. Tenenbaum, and Samuel J.\nGershman. The neural architecture of theory-based reinforcement learning. Neuron, 111\n(8):1331–1344.e8, April 2023. ISSN 0896-6273. doi: 10.1016/j.neuron.2023.01.023. URL\nhttps://doi.org/10.1016/j.neuron.2023.01.023. Publisher: Elsevier.\nFaraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation, 2018.\nURL https://arxiv.org/abs/1805.01954.\nRodrigo Toro Icarte, Toryn Q. Klassen, Richard Valenzano, and Sheila A. McIlraith. Reward\nmachines: Exploiting reward function structure in reinforcement learning. Journal of\nArtificial Intelligence Research, 73:173–208, January 2022. ISSN 1076-9757. doi: 10.1613/\njair.1.12440. URL http://dx.doi.org/10.1613/jair.1.12440.\nTrieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad\ngeometry without human demonstrations. Nature, 625(7995):476–482, January 2024.\nISSN 1476-4687. doi: 10.1038/s41586-023-06747-5. URL https://doi.org/10.1038/\ns41586-023-06747-5.\nDweep Trivedi, Jesse Zhang, Shao-Hua Sun, and Joseph J. Lim. Learning to synthesize\nprograms as interpretable and generalizable policies, 2022. URL https://arxiv.org/\nabs/2108.13643.\nPedro A. Tsividis, Joao Loula, Jake Burga, Nathan Foss, Andres Campero, Thomas Pouncy,\nSamuel J. Gershman, and Joshua B. Tenenbaum. Human-level reinforcement learning\nthrough theory-based modeling, exploration, and planning, 2021. URL https://arxiv.\norg/abs/2107.12544.\nTomer Ullman, Chris Baker, Owen Macindoe, Owain Evans, Noah Goodman, and Joshua\nTenenbaum. Help or hinder: Bayesian models of social goal inference. Advances in neural\ninformation processing systems, 22, 2009.\nTomer D Ullman and Ilona Bass. The detection of automatic behavior in other people, May\n2024. URL osf.io/preprints/psyarxiv/8r4yf_v1.\nAbhinav Verma, Hoang M. Le, Yisong Yue, and Swarat Chaudhuri. Imitation-projected\nprogrammatic reinforcement learning, 2021. URL https://arxiv.org/abs/1907.05431.\nYanming Wan, Yue Wu, Yiping Wang, Jiayuan Mao, and Natasha Jaques. Infer human’s\nintentions before following natural language instructions. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 39, pp. 25309–25317, 2025.\nHuaxiaoyue Wang, Gonzalo Gonzalez-Pumariega, Yash Sharma, and Sanjiban Choudhury.\nDemo2code: From summarizing demonstrations to synthesizing code via extended chain-\nof-thought, 2023. URL https://arxiv.org/abs/2305.16744.\nPin Wang, Hanhan Li, and Ching-Yao Chan. Meta-adversarial inverse reinforcement learning\nfor decision-making tasks, 2021. URL https://arxiv.org/abs/2103.12694.\nRose E. Wang, Sarah A. Wu, James A. Evans, Joshua B. Tenenbaum, David C. Parkes, and\nMax Kleiman-Weiner. Too many cooks: Bayesian inference for coordinating multi-agent\ncollaboration, 2020. URL https://arxiv.org/abs/2003.11778.\nAlex Wilf, Sihyun Shawn Lee, Paul Pu Liang, and Louis-Philippe Morency. Think twice:\nPerspective-taking improves large language models’ theory-of-mind capabilities, 2023. URL\nhttps://arxiv.org/abs/2311.10227.\nLionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka,\nJacob Andreas, and Joshua B. Tenenbaum. From word models to world models: Translating\nfrom natural language to the probabilistic language of thought, 2023a. URL https:\n//arxiv.org/abs/2306.12672.\nLionel Wong, Jiayuan Mao, Pratyusha Sharma, Zachary S. Siegel, Jiahai Feng, Noa Korneev,\nJoshua B. Tenenbaum, and Jacob Andreas. Learning adaptive planning representations\nwith natural language guidance, 2023b. URL https://arxiv.org/abs/2312.08566.\n15\nWendy Wood.\nHabits, goals, and effective behavior change.\nCurrent Directions in\nPsychological Science, 33(4):226–232, 2024.\ndoi: 10.1177/09637214241246480.\nURL\nhttps://doi.org/10.1177/09637214241246480.\nMarkus Wulfmeier, Peter Ondruska, and Ingmar Posner. Maximum entropy deep inverse\nreinforcement learning, 2016. URL https://arxiv.org/abs/1507.04888.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and\nYuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL\nhttps://arxiv.org/abs/2210.03629.\nMustafa Yildirim, Barkin Dagda, Vinal Asodia, and Saber Fallah. Behavioral cloning models\nreality check for autonomous driving, 2024. URL https://arxiv.org/abs/2409.07218.\nLance Ying, Jason Xinyu Liu, Shivam Aarya, Yizirui Fang, Stefanie Tellex, Joshua B\nTenenbaum, and Tianmin Shu. Siftom: Robust spoken instruction following through\ntheory of mind. arXiv preprint arXiv:2409.10849, 2024.\nLance Ying, Kunal Jha, Shivam Aarya, Joshua B. Tenenbaum, Antonio Torralba, and\nTianmin Shu. Goma: Proactive embodied cooperative communication via goal-oriented\nmental alignment, 2025. URL https://arxiv.org/abs/2403.11075.\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez\nArenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian\nIchter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie\nTan, Yuval Tassa, and Fei Xia. Language to rewards for robotic skill synthesis, 2023. URL\nhttps://arxiv.org/abs/2306.08647.\nZhining Zhang, Chuanyang Jin, Mung Yao Jia, and Tianmin Shu. Autotom: Automated\nbayesian inverse planning and model discovery for open-ended theory of mind, 2025. URL\nhttps://arxiv.org/abs/2502.15676.\nTan Zhi-Xuan, Jordyn L. Mann, Tom Silver, Joshua B. Tenenbaum, and Vikash K. Mans-\ninghka. Online bayesian goal inference for boundedly-rational planning agents, 2020. URL\nhttps://arxiv.org/abs/2006.07532.\nTan Zhi-Xuan, Lance Ying, Vikash Mansinghka, and Joshua B Tenenbaum. Pragmatic\ninstruction following and goal assistance via cooperative language-guided inverse planning.\narXiv preprint arXiv:2402.17930, 2024.\nWeichao Zhou and Wenchao Li. A hierarchical bayesian approach to inverse reinforcement\nlearning with symbolic reward machines, 2022.\nURL https://arxiv.org/abs/2204.\n09772.\nChuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek\nGupta. Unified world models: Coupling video and action diffusion for pretraining on large\nrobotic datasets, 2025. URL https://arxiv.org/abs/2504.02792.\nFeiyu Zhu and Reid Simmons. Bootstrapping cognitive agents with a large language model,\n2024. URL https://arxiv.org/abs/2403.00810.\nA\nAppendix\nA.1\nGround Truth Agent Behaviors for Construction\nFor research question 1 in Section 5, we hand designed 10 agents, represented as Finite State\nMachines, to engage in diverse behaviors. The agents varied in complexity, with some using\nsophisticated A-star search to achieve a goal, and others using faster, less resource-intensive\nplanning heuristics, namely the Manhattan distance as an approximation of how valuable an\naction is for an agent looking to move to a target location. We summarize the behaviors and\ninternal decision making states for all agents below:\n16\n1. Block Cycle: Using the manhattan distance as the planning heuristic, move from\nthe green block to the blue block to the purple block to the green block and so on.\nIf the agent ever has a block in its inventory, it will immediately drop it and resume\nits cycling behavior.\n2. Clockwise Patrol: If an agent is not along the outermost wall of the grid, it will\nrepeatedly alternate between moving left and moving up until it hits a wall. Then,\nit will follow the wall clockwise: if there is a wall above it, the agent will move right\nrepeatedly until it hits a wall, then repeats this process for going down, left, up and\nright again. If the agent ever has a block in its inventory, it will immediately drop it\nand resume its cycling behavior.\n3. Counter-clockwise Patrol: This agent is the same as Clockwise Patrol, except it\nit will patrol the border wall in a counter-clockwise manner, moving left repeatedly\nuntil it hits a wall, then doing the same for moving down, right, up and left again.\n4. Left-Right Patrol: The agent will move left until it hits a wall, then will move\nright until it hits a wall, and repeat this process. If the agent ever has a block in its\ninventory, it will immediately drop it and resume its patrolling behavior.\n5. Pair Blue Blocks: This agent uses A-star search for planning. If it does not have\na blue block in its inventory, it finds and executes the shortest path to a blue block.\nThen, it uses the “interact” action to add the block to its inventory, and uses A-star\nto find the shortest path to a different blue block.\n6. Patrol with A-Star: Here, the agent’s goal is to repeatedly cycle between the top\nleft, top right, bottom right, and bottom left corners of the grid. While Clockwise\nPatrol has a behavior which on the surface may seem similar, for Patrol with A-star\nwe introduced addition complexity by having the agent believe it incurs a penalty for\ntouching any of the colored blocks. As such, it uses A-star with negative edge values\ngiven to any action which leads an agent to landing on a colored block, thus resulting\nin behavior which tries to patrol but frequently leaves and returns to the border to\navoid colored blocks. Again, if it ever picks up a colored block, it immediately drops\nit.\n7. L-shaped Patrol: This agent, initially at a coordinate (x, y), will move down until\nit collides with a wall, then will move right until it collides with a wall. Then, it\nwill return to its original location, first moving left until its x-coordinate is x, and\nup until its final coordinate is (x, y). It repeatedly does this process. The agent\nimmediately drops any blocks in its inventory.\n8. Transport Green: Here, the agent uses A-star search to move towards a green\nblock and pick it up. Then, it uses A-star search to move the green block as close to\nan empty corner grid cell.\n9. Snake Patrol: This agent has four internal decision-making states: 1) Moving\ndown/right, where the agent moves right until it cannot any more, then moves down\none step; 2) Moving down/left, where the agent moves left until it cannot any more,\nthen moves down one step; 3) Moving up/right, where the agent moves right until\nit cannot any more, then moves up one step; 4) Moving up/left, where the agent\nmoves left until it cannot any more, then moves up one step. The resulting pattern\nappears like a snake moving throughout the grid.\n10. Up/Down Patrol: The agent will move up until it hits a wall, then will move\ndown until it hits a wall, and repeat this process. If the agent ever has a block in its\ninventory, it will immediately drop it and resume its patrolling behavior.\nA.2\nHuman Results Breakdown\nIn Figure 8, we show the accuracy for ROTE compared to humans when predicting FSM\nbehavior in Construction. An example of some of the task are shown in Figure 7. In Figure 9,\nwe show the accuracy for ROTE compared to humans when predicting Human behavior in\nConstruction. We find that humans excel at predicting goal-directed tasks while our method\nperforms better with repetitive tasks, although all of the variance in predictive accuracy\ncannot be captured by this distinction. In subsequent followups, we plan to do a greater\n17\nCycle Between \nPurple à Green à Blue\nMove in an \n‘L’-shape\nSnake through the \nenvironment\nFigure 7: Example scripts from Construction.\nWe designed a suite of goal-directed (planner-\nbased) and automatic (heuristic-based) agentic\nbehaviors, from patrolling to transporting spe-\ncific blocks to a location.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nblock_cycle\nclockwise_patrol\ncounter_patrol\nleft_right\npair_blue\npatrol_with_a_star\npattern_l\npickup_green_a_star\nsnake\nup_down\nFSM ID\nAccuracy by FSM ID and Model\nHuman\nROTE\nNLLM\nAutoToM\nBC\nFigure 8: Per-task accuracy comparison between different methods predicting ground truth\nFSM gameplay.\n18\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nAccuracy\nRepeat green-->blue-->purple\nPatrol Clockwise\nPatrol Counter-clockwise\nLeft to right\nPair blue blocks\nL-shape\nGreen to corner\nPink to corner\nSnake up and down\nUp and down\nFSM ID\nAccuracy by FSM ID and Model\nHuman\nROTE\nNLLM\nAutoToM\nBC\nFigure 9: Per-task accuracy comparison between different methods predicting human game-\nplay. While ROTE succeeds at more routine tasks, humans excel in predicting more goal\ndirected behaviors.\nexploration of the different error modes of humans and other models, as well as scale ROTE\nto larger language models, to see whether ROTE is an accurate computational model of\nhuman behavior.\nA.3\nClustered Task Breakdown in Partnr\nTo understand the types of tasks ROTE excels at compared to baselines in the Partnr\nsimulator, we used Llama-3.1-8B-Instruct to cluster the ground-truth tasks from our test\nset into three categories. As shown in Figure 10, we report the mean prediction accuracy\nand standard error for each algorithm on a per-cluster basis. While AutoToM and Behavior\nCloning show some success on tasks involving simple actions like moving and rearranging\nobjects, they struggle significantly with more complex interactions, such as turning items\non/off or cleaning.\nROTE, in contrast, maintains a degree of accuracy in these more\nchallenging settings.\n19\nOrganizing Items\nMoving Objects Across Rooms Interacting With Objects\nCluster\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAccuracy\nModel\nBC\nAutoToM\nNLLM\nROTE\nFigure 10: Task-specific generalization in Partnr. We used Llama-3.1-8B-Instruct to cluster\nour prediction tasks into three distinct categories. We report the mean accuracy and standard\nerror (SE bars) for each algorithm. While baselines like AutoToM and Behavior Cloning\nperform adequately on tasks involving object manipulation, they struggle with more complex\ninteractions. ROTE, however, maintains performance on these more intricate problems.\nA.4\nModel Component Analysis\nWe show the effect of different model components. While the choice of observation parsing\ndid not have too much of an impact on the Construction evaluations, Figure 11 indicates\nit has a significant effect on predictive performance in Partnr. This is likely because the\nobservations, which are already in natural language, contain critical information on the data\nstructures they are represented as that abstraction removes.\nFigure 12 demonstrates the benefits of different inference algorithms in ROTE. While ROTE\nis not very sensitive to the choice of probabilistic inference method used as it has more\ncandidate agent programs, if agents are constrained by the number of hypotheses they can\nmaintain, performing SMC with rejuvenation proves to be a more effective strategy, since\nthis effectively augments the number of programs considered.\nFigure 13 reveals an interesting gradient along which different degrees of structure influence\nROTE’s ability to predict behaviors. In controlled settings where agents are Finite State\nMachines following deterministic transitions between behaviors, increasing the amount of\nstructure used to predict what they will do next does not significantly harm performance.\nThis can be a useful inductive bias that reduces cognitive load for agents interacting with\nsystems that require prediction in order to effectively interact with, such as a thermostat,\nbut are nevertheless simple enough to represent as a series of rules. In the human-behavior\nsetting, this does not hold as well. We find a moderate amount of structure, where providing\nmore detailed examples about what the internal mechanisms of the observed agent look like\nwithout forcing ROTE to generate code following that structure, performs the best. These\nsettings are closest to realistic encounters with other people: when walking down the street\nor ordering coffee, we may try to follow scripts or conventions for how to interact, but there\nis inherent variability in our behaviors that more open-ended programs must account for.\nLastly, when predicting the behavior of agents that are goal-directed in a partially observable\nworld, imposing FSM structure greatly diminishes performance. These are scenarios where\nprediction might best be performed by more complex reasoning processes about an agent’s\nintentions and beliefs. Here, constraining code to be structured as an FSM might fail to\naccount for how agents react to the presence of unknown unknowns they encounter.\nA.5\nRelationship between Program Size (|λ|) and Accuracy\nAs shown in Figure 14, higher prediction accuracy in Construction and Partnr corresponds\nto shorter programs (in characters). This occurs even though program length is not explicitly\nfactored into the likelihood computation, suggesting that the approach naturally favors a\nsimple, efficient representation of the agent’s behavior. This aligns with our hypothesis,\n20\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.4\nAccuracy\nConstruction Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.3\nAccuracy\nHuman Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.25\n0.50\nAccuracy\nPartnr Dataset\nROTE w/ 2-stage\nROTE w/o 2-stage\nFigure 11: Ablating Observation Parsing\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.4\nAccuracy\nConstruction Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.3\nAccuracy\nHuman Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.1\n0.2\n0.3\nAccuracy\nPartnr Dataset\nROTE w/ Rejuvenation\nROTE w/ Importance Sampling\nFigure 12: Ablating Inference Algorithm\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.4\nAccuracy\nConstruction Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.3\nAccuracy\nHuman Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.1\n0.2\n0.3\nAccuracy\nPartnr Dataset\nROTE w/ Light\nROTE w/ Moderate\nROTE w/ Severe\nFigure 13: Ablating Structure Enforced in Generated Code\n21\n0\n20\n# of Hypotheses\n0.25\n0.30\n0.35\nAccuracy\nConstruction Dataset\n0\n20\n# of Hypotheses\n0.175\n0.200\n0.225\nAccuracy\nHuman Dataset\n0\n20\n# of Hypotheses\n0.10\n0.15\n0.20\nAccuracy\nPartnr Dataset\n4200\n4300\n4400\nProgram Length\n4200\n4400\nProgram Length\n3800\n4000\nProgram Length\nAccuracy\nProgram Length\nFigure 14: Average program length (in characters) versus prediction accuracy as a function\nof the number of generated hypotheses for Construction and Partnr. Shorter programs yield\nhigher accuracy for scripted, human, and LLM agents.\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.4\nAccuracy\nConstruction Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.3\nAccuracy\nHuman Dataset\n0\n10\n20\n30\n# of Hypotheses\n0.2\n0.3\nAccuracy\nPartnr Dataset\nROTE (K=1)\nROTE (K=10)\nROTE (K=30)\nFigure 15: Top-k parameter analysis in Construction. No appreciable difference in accuracy\nas a result of different parameters, suggesting the choice between uncertainty preservation\n(maintaining a larger set of hypotheses from a larger k) and prediction speed (by executing\nless programs with a smaller k) is up to the agent and agent designer.\ninspired by Solomonoff (Solomonoff, 1964; 1978; 1996), that shorter programs will generalize\nmore effectively due to Occam’s razor.\nA.6\nTop-k Effect\nIn Figure 15 we explore the impact of different k values for the top-k hypothesis pruning phase\nafter generation. We tried k = 1, 10, and 30. We did not find any meaningful variation in\nperformance as a function of k. This suggest the choice of which hyperparameter to use may\nbe left to the agent designer. Whereas smaller k values enable faster inference, larger values\nenable better uncertainty estimation. Moreover, because of the largely deterministic nature of\nthe generated programs, there can be an implicit top-k effect at higher hypothesis numbers,\nwherein unlikely programs are assigned very low probabilities throughout a trajectory,\neffectively leading to their pruning during policy selection for action prediction.\nA.7\nPer-llm Results\nIn Tables 1, 2 and 3, we report the raw accuracy of different LLM models using different\nalgorithms, as well as the standard error, on the Scripted, Human, and LLM-agent behavior\ndatasets in Construction and Partnr. For the results reported in the paper, we had to tune\nthe number of hypothesis and other hyperparameters, such as whether to use two-stage\nobservation parsing, on a dataset-by-dataset basis. We did this by running a sweep of\nhyperparameters and comparing their performance on 20% of the data, then utilizing the\nbest performing hyperparameter from that subset, as the selected model configuration for\nthe remaining 80% of the data. The hyperparameters used for each environment can be\nfound in Section A.10.\nA.8\nHuman Experiment Details\nAs described in Section 4, we conducted three separate human experiments: the first was\ncollecting human gameplay data, the second was having humans predict human behavioral\n22\nAlgorithm\nDeepSeek-Coder-V2-\nLite-Instruct (16B)\nDeepSeek-V2-Lite\n(16B)\nLlama-3.1-8B-\nInstruct\nAutoToM\n0.000 ± 0.000\n0.000 ± 0.000\n0.202 ± 0.023\nNLLM\n0.310 ± 0.032\n0.266 ± 0.018\n0.340 ± 0.033\nROTE (light)\n0.479 ± 0.033\n0.312 ± 0.032\n0.477 ± 0.044\nROTE (moderate)\n0.436 ± 0.042\n0.256 ± 0.032\n0.446 ± 0.051\nROTE (severe)\n0.457 ± 0.037\n0.298 ± 0.033\n0.390 ± 0.049\nROTE (two-stage) 0.522 ± 0.046\n0.271 ± 0.028\n0.468 ± 0.052\nTable 1: Multi-step LLM results (with standard error) for Ground-truth Scripted Gameplay\nData Prediction in Construction.\nAlgorithm\nDeepSeek-Coder-V2-\nLite-Instruct (16B)\nDeepSeek-V2-Lite\n(16B)\nLlama-3.1-8B-\nInstruct\nAutoToM\n0.000 ± 0.000\n0.000 ± 0.000\n0.156 ± 0.011\nNLLM\n0.151 ± 0.012\n0.176 ± 0.013\n0.171 ± 0.016\nROTE (light)\n0.296 ± 0.019\n0.199 ± 0.015\n0.305 ± 0.022\nROTE (moderate)\n0.310 ± 0.018\n0.204 ± 0.021\n0.266 ± 0.024\nROTE (severe)\n0.304 ± 0.022\n0.230 ± 0.018\n0.245 ± 0.026\nROTE (two-stage) 0.329 ± 0.031\n0.209 ± 0.014\n0.327 ± 0.026\nTable 2: Multi-step LLM results (with standard error) for Human Gameplay Data Prediction\nin Construction.\ndata, and the third was having humans predict scripted FSM agent behavior. We will\nopen-source all of the code and stimuli used for conducting all three human experiments.\nFor the gameplay collection, we gave participants a tutorial stage to learn the controls,\nand randomized the order of the tasks they played to control for ordering effects. For the\nbehavior prediction experiments, the setup was virtually identical to that of the AI, albeit\nwith two small modifications. The first is that we only had humans predict five timesteps\ninto the future. This was done to make the experiment flow smoother and take less time\nso that participants did not fatigue for later scripts, resulting in lower prediction quality.\nThe second change we made was we showed people 3 distinct trajectories generated by the\nobserved agent before giving them h20 = {(o1, a1), (o2, a2) . . . , (o19, a19), o20} and having\nthem predict an agent’s behavior. This additional context was used to help participants\nfamiliarize themselves with the dynamics of the gridworld and the space of potential agent\nbehaviors. In contrast, all of our baselines only saw the current trajectory h20. While this\nwas done due to the limited context window of the models we used, we feel that this is\nstill a fair comparison between humans and our baselines, since the training corpora for\nLLMs is rich with gridworld implementations and agent programs, and the BC model had an\nextended training period with the agent behavior it is predicting. In future work, we plan on\nrelaxing this constraint by exploring dynamically growing libraries of agent programs which\npersist across multiple context windows, similar to an approach used in (Tang et al., 2024).\nA.9\nBehavior Cloning Model Implementation Details\nWe use an architecture and training methadology similar to the one in (Rabinowitz et al.,\n2018) for training a BC model with recurrence. The model uses a 2-layer ResNet to extract\nfeatures from the input observations. Each observation is an image of size 70×70 pixels. The\nResNet consists of two ResNet blocks, each containing two convolutional layers with batch\nnormalization and a ReLU activation function. The first block uses a feature size of 64 while\nthe second uses a feature size of 32. All blocks use stride length of 1 for all convolutional\nlayersand a kernel size of 3.\n23\nAlgorithm\nDeepSeek-Coder-V2-\nLite-Instruct (16B)\nDeepSeek-V2-Lite\n(16B)\nLlama-3.1-8B-\nInstruct\nAutoToM\n0.000 ± 0.000\n0.000 ± 0.000\n0.050 ± 0.015\nNLLM\n0.113 ± 0.018\n0.333 ± 0.027\n0.170 ± 0.022\nROTE (light)\n0.537 ± 0.029\n–\n0.439 ± 0.066\nROTE (moderate)\n0.472 ± 0.029\n0.026 ± 0.026\n0.426 ± 0.051\nROTE (severe)\n0.440 ± 0.029\n–\n0.510 ± 0.072\nROTE (two-stage) 0.160 ± 0.021\n0.114 ± 0.055\n0.112 ± 0.034\nTable 3: Single-step LLM results (with standard error) for LLM Agent Gameplay Data\nPrediction in Partnr.\nThe features extracted by the ResNet are then passed through a recurrent neural network.\nThe model uses an LSTM with a hidden size of 128. The output of the LSTM is processed by\nseveral fully connected layers with ReLU activations. The final output is passed through a\nsoftmax layer to produce a probability distribution over the possible actions. This probability\ndistribution represents the model’s prediction of the next action an agent will take. The\naction space has a size of 6, corresponding to a set of discrete actions. The entire network is\ndesigned to be fully differentiable, allowing for end-to-end training using cross-entropy as\nthe loss-function. We use the following hyperparameters for training:\nHyperparameter\nPurpose\nValue\n# Agents to Sample\nThe number of agent scripts to\nsample per epoch.\n1\n# Datapoints per Agent\nThe number of trajectories per\nagent to sample from the dataset\nper epoch.\n3\n# Agents\nThe total number of agents in the\ndataset.\n10\n# Steps\nThe number of steps per trajec-\ntory in the dataset.\n50\nEnvironment Size\nThe size of the environment.\n10×10\nImage Size\nThe size of a single observation\nin a trajectory.\n70×70 pixels\nNum Epochs\nThe number of training epochs.\n5000\nA.10\nROTE Implementation Details\nWe will fully open-source our code, including the prompts we used for generating programs\nwith ROTE across the various levels of structure. In Algorithm 1, we show the full algorithm\nfor ROTE and subsequently discuss the implementation details.\nA.10.1\nROTE Hyperparameters for Construction and Partnr\nAcross the prediction tasks for ground-truth scripted agents and humans in Construction, and\nLLM agents in Partnr, we used the same set of hyperparameters, indicating the generality of\nour method with minimal environment-specific finetuning. The only hyperparameter which\nvaried across environments was the use of two-stage observation parsing. We used two-stage\nobservation parsing for predicting scripted agent behavior in Construction and LLM-agent\nbehavior in Partnr. We did not use it for predicting human behavior. As mentioned in\nSection A.7, all hyperparameters were fit by comparing their performance on 20% of the\ndata, then utilizing the best performing hyperparameter from that subset, as the selected\nmodel configuration for the remaining 80% of the data.\n24\nAlgorithm 1 ROTE (Representing Others’ Trajectories as Executables)\nRequire: Observed history h0:t−1 = {(o0, a0), . . . , (ot−1, at−1)}, current observation ot, Environment\nE, Initial set of candidate programs Λcandidates (can be empty), Initial set of program priors Ppriors.\nEnsure: Predicted action ˆat, Predicted programs Λcandidates, Predicted program posterior Pposteriors\n1: procedure PredictAction(h0:t−1, ot, E, k, Λcandidates, Ppriors)\n2:\nfor N −|Λcandidates| generations do\n▷Number of programs to sample\n3:\nPrompt LLM with h0:t−1, ot, E, and synthesize an FSM-like Python program λ\n4:\nΛcandidates ←Λcandidates ∪{λ}\n5:\npprior(λ) ←Π|λ|\nn=1pLLM(tokenn|h0:t−1, ot, E, tokenn−1, · · · , token1)\n6:\nPpriors ←Ppriors ∪{pprior(λ)}\n7:\nend for\n8:\nPpriors ←normalize(Ppriors)\n▷Renormalize priors to account for new hypotheses\n9:\nPposteriors = ∅\n10:\nfor λ ∈Λcandidates do\n11:\np(λ) ∝Πoi,ai∈h0:t−1p(ai|oi, λ) · pprior(λ)\n▷Calculate likelihood p(H[0,t−1]|λ)\n12:\nPposteriors ←Pposteriors ∪{p(λ)}\n13:\nend for\n14:\nPposteriors ←normalize(top-k(Pposteriors, k))\n▷Subsample and Renormalize\n15:\nPredicted action ˆat ←argmaxa∈A\nP\nλ∈Λcandidates pposteriors(λ) · λ(a|ot)\nreturn ˆat, Λcandidates, Pposteriors\n16: end procedure\nHyperparameter\nPurpose\nValue\nStructure Enforcement\nHow strictly we constrain gener-\nated programs to adhere to FSM\nstructure\nLight\nRejuvenation\nWhether to use rejuvenation for\nthe FSM model.\nTrue\nMax rejuvenation attempts\nMaximum number of times to re-\nsample a program during rejuve-\nnation.\n2\nRejuvenation threshold\nThe minimum number of cor-\nrect action predictions a program\nmust make over 20 timesteps to\navoid resampling.\n1\nMax number of retries\nThe number of times a hypoth-\nesis can be revised if it fails to\ncompile.\n2\nNumber of hypotheses\nThe number of hypotheses to gen-\nerate for the thought trace.\n30\nTop K\nThe number of most likely hy-\npotheses to average over.\n30\nMinimum hypothesis probability\nThe minimum probability a hy-\npothesis can have.\n1e-6\nMaximum number of tokens\nThe maximum number of tokens\nthe large language model can gen-\nerate.\n2000\nMinimum action probability\nThe minimum probability an ac-\ntion can have.\n1e-8\nFor our execution speed comparisons in Figure 6, all models ran on a single Nvidia GPU-L40.\nHandling Errors in Program Generation. Given that we are generating programs from\nsmaller LLMs trying to adhere to a consistent Agent API, and that the observation space can\nbe challenging to operate on, there are several cases where the LLMs generate semantically\nmeaningful programs to describe observed behaviors that fail to compile or predict actions\ngiven an observation. As such, we explored two different methods for dealing with erroneous\nprograms. The first was revision, where we prompted an LLM to fix the code it generated\ngiven the full error trace for a program’s prediction. We also gave it the original prompt and\nobservations. The second method was completely resampling a program given the original\n25\nprompt, discarding the erroneous program completely. From preliminary tests, we found\ncompletely resampling was the more effective strategy given the LLMs we were using. Since\nwe paired this error correction process with methods like rejuvenation, we limited the number\nof times we could resample or revise a program to be min(Max rejuvenation attempts, Max\nnumber of retries), shared across the rejuvenation and error corrections steps. This increased\nthe likelihood of a good program which is executable being generated, without significantly\nslowing our single-step inference speed.\nA.10.2\nExamples of Programs Generated By ROTE\nIn Listings 1 and 2, we show sample agent programs inferred by ROTE for the Construction\nand Partnr tasks, respectively. Using the same prompts and hyperparameters for both\nsettings, our approach can flexibly model agents as Finite State Machines when the underlying\nagents are following scripts (Construction, Listing 1) or more open-ended decision makers\ntrying to accomplish goals such as move an item from one room to another (Partnr, Listing 2).\n1\n2 import\nnumpy as np\n3\n4 class\nFSMAgent:\n5\ndef\n__init__(self , num_agents: int , num_blocks: int , num_actions:\nint =6):\n6\nself.num_agents = num_agents\n7\nself.num_blocks = num_blocks\n8\nself.num_actions = num_actions\n9\nself.actions = [0, 1, 2, 3, 4, 5]\n# stay , right , left , down ,\nup , interact\n10\nself. action_to_name = [\"stay\", \"right\", \"left\", \"down\", \"up\",\n\"interact\"]\n11\nself.state = \"IDLE\"\n# Initial\nstate\n12\n13\ndef act(self , observation) -> int:\n14\nagent_id = observation[’agent_id ’]\n15\nagent_location = observation[’agent_locations ’][ agent_id]\n16\ninventory = observation[’agent_inventory ’][ agent_id]\n17\n18\nif self.state == \"IDLE\":\n19\n# Check if there is a block at the agent ’s location\nand we\ncan\ninteract\nwith it\n20\nfor\nblock_location in observation[’block_locations ’]:\n21\nif np.array_equal(block_location , agent_location ):\n22\nif inventory ==\n-1:\n23\nself.state = \"INTERACT\"\n24\nbreak\n25\nelse:\n26\n# No block at the agent ’s location , check for\npossible\nmovements\n27\npossible_actions = []\n28\nfor action in self.actions [: -1]:\n# Exclude\ninteract\n29\nnew_location = self. apply_action(agent_location ,\naction)\n30\nif not self.is_wall(new_location , observation[’\nwall_locations ’]) and not self. is_other_agent (new_location ,\nobservation[’agent_locations ’], agent_id):\n31\npossible_actions .append(action)\n32\nif possible_actions :\n33\nself.state = \"MOVE\"\n34\nself.target_action = np.random.choice(\npossible_actions )\n35\n36\nif self.state == \"MOVE\":\n37\nself.state = \"IDLE\"\n# Transition\nback to IDLE\nafter\nmoving\n38\nreturn\nself.target_action\n26\n39\n40\nif self.state == \"INTERACT\":\n41\nself.state = \"IDLE\"\n# Transition\nback to IDLE\nafter\ninteracting\n42\nreturn 5\n# Interact\naction\n43\n44\ndef\napply_action(self , location , action):\n45\nif action == 1:\n# right\n46\nreturn [location [0], location [1] + 1]\n47\nelif\naction == 2:\n# left\n48\nreturn [location [0], location [1] - 1]\n49\nelif\naction == 3:\n# down\n50\nreturn [location [0] + 1, location [1]]\n51\nelif\naction == 4:\n# up\n52\nreturn [location [0] - 1, location [1]]\n53\nelse:\n54\nreturn\nlocation\n# stay\n55\n56\ndef\nis_wall(self , location , wall_locations ):\n57\nfor wall in wall_locations :\n58\nif np.array_equal(wall , location):\n59\nreturn\nTrue\n60\nreturn\nFalse\n61\n62\ndef\nis_other_agent(self , location , agent_locations , agent_id):\n63\nfor i, agent_loc in enumerate( agent_locations ):\n64\nif i != agent_id\nand np.array_equal(agent_loc , location):\n65\nreturn\nTrue\n66\nreturn\nFalse\nListing 1: Sample Agent Codes Inferred by ROTE for Construction prediction task\n1\n2 import\nnumpy as np\n3\n4 class\nFSMAgent:\n5\ndef\n__init__(self , num_agents: int=1, num_blocks: int =1):\n6\nself.num_agents = num_agents\n7\nself.num_blocks = num_blocks\n# irrelevant , can ignore\n8\n9\ndef\nparse_scene_graph (self , observation):\n10\nfor keys in observation[’scene_graph ’]:\n11\nif keys == ’furniture ’:\n12\nfor room_name , furniture_list in observation[’\nscene_graph ’][ keys ]. items ():\n13\nfor\nfurniture_piece in furniture_list :\n14\npass\n# each\nfurniture_piece is a string\n15\nif keys == ’objects ’:\n16\nif type(observation[’scene_graph ’][ keys ]) == list and\nlen(observation[’scene_graph ’][ keys ]) == 0:\n17\npass\n# no objects\nseen\n18\nelse:\n19\nfor object , object_holder_list\nin observation[’\nscene_graph ’][ keys ]. items ():\n20\nfor\nobject_holder in object_holder_list :\n21\npass # each\nobject is either on or in an\nobject\nholder\n22\nreturn # do whatever is most\nhelpful\nhere\n23\n24\ndef act(self , observation) -> int:\n25\n’’’\n26\nobservation is a dictionary\nwith the\nfollowing\nkeys:\n27\n- tool_list: List of tools\navailable to the agent\n28\n- tool_descriptions : Description of how each tool is used\n27\n29\n- scene_graph: Scene\ngraph of the environment , dictionary\nwith\nkeys\n30\n- \"furniture\" which\nmaps to a dictionary\nwith the keys\n31\n- room\ndescription\nstring (i.e. keys\ncould be \"\nliving_room_1\", \"bathroom_1\", etc.) that maps to list of\n32\n- object_id\nstring (i.e. table_21 , chair_32 , etc.)\n33\n- \"objects\" which\nmaps to a dictionary of\n34\n- object_id\nstring (i.e. keys\ncould be \"\nplate_container_2 \", \"vase_1\" etc.) to list of\n35\n- object_base\nstring (i.e \"table_14\", \"table_21 \")\n36\nif type(observation[’scene_graph ’][’objects ’]) == list\n, then you do not\nobserve\nany\nobjects\n37\n- agent_state: Dictionary\nmapping to\n38\n- string of agent id (i.e. \"0\") maps to string\ndescribing\nwhat\nagent is doing\n39\n’’’\n40\nagent_id = list(observation[’agent_state ’]. keys ())[0]\n41\nagent_state = observation[’agent_state ’][ agent_id]\n42\ntool_list = observation[’tool_list ’]\n43\n44\nif ’Explore ’ in tool_list:\n45\ntool = ’Explore ’\n46\ntarget = list(observation[’scene_graph ’][’furniture ’]. keys\n())[0]\n47\nelif ’Pick ’ in tool_list\nand ’Standing ’ in agent_state:\n48\ntool = ’Pick ’\n49\ntargets = []\n50\nfor key in observation[’scene_graph ’][’objects ’]:\n51\nif ’agent_0 ’ in observation[’scene_graph ’][’objects ’][\nkey]:\n52\ntargets.append(key)\n53\nif targets:\n54\ntarget = targets [0]\n55\nelse:\n56\ntarget = None\n57\nelif ’Place ’ in tool_list\nand ’Standing ’ in agent_state:\n58\ntool = ’Place ’\n59\ntarget = None\n60\nfor key in observation[’scene_graph ’][’objects ’]:\n61\nif agent_id in observation[’scene_graph ’][’objects ’][\nkey]:\n62\ntarget = key\n63\nbreak\n64\nif not target:\n65\nfor key in observation[’scene_graph ’][’furniture ’]:\n66\nfor\nfurniture_piece in observation[’scene_graph ’][\n’furniture ’][key]:\n67\nif agent_id in observation[’scene_graph ’][’\nfurniture ’][key]:\n68\ntarget = key\n69\nbreak\n70\nif not target:\n71\ntarget = list(observation[’scene_graph ’][’objects ’].\nkeys ())[0]\n72\nelse:\n73\ntool = ’Wait ’\n74\ntarget = None\n75\n76\n## DON’T CHANGE\nANYTHING\nBELOW\nHERE\n77\nreturn (tool , target , None)\nListing 2: Sample Agent Codes Inferred by ROTE for Partnr prediction task\n28\nA.10.3\nExamples of High Level Trajectory Summaries Generated by ROTE\nIn Listings 3 and 4, we show sample high-level trajectory summarizations from the optional\ntwo-stage observation parsing step. While in 3 the model attributes the movements of the\nground truth patrolling agent as “exploring randomly,” it still is able to capture some aspects\nof its movement, such as not interacting with blocks. In 4, ROTE can better summarize\nthe behavior of agents in Partnr, but without a clear guess as to which objects the agent is\ntrying to rearrange, it can be difficult to make a program which concisely narrows down the\nhypothesis space.\n1\n2 1. The agent ’s overall\ngoal or strategy: The agent\nappears to be\nexploring\nits environment , possibly\nlooking\nfor a specific\nblock\nor blocks.\n3\nIt is not\nactively\nengaging\nwith the\nenvironment in a goal -directed\nway , as it does not seem to be collecting , storing , or moving\nblocks in a strategic\nmanner.\n4\n5 2. How the agent\nresponds to different\nenvironmental\nfeatures (blocks ,\nwalls): The agent\nmoves\naround the environment , avoiding\nwalls\nand\nseemingly\nindifferent to blocks.\n6\nIt repeatedly\nmoves\nleft and right and up and down , indicating a\nlack of strategy or goal -directed\nbehavior.\n7\n8 3. Any\npatterns in movement or interaction: The agent\nmoves in a\npattern\nthat\nsuggests\nexploration\nbut does not show any\nindication\nof avoiding\nwalls or blocks ,\n9\nindicating a lack of awareness of its\nenvironment or purpose in the\ngrid\nworld.\n10\n11 The agent ’s behavior is essentially\nrandom\nexploration , with no\napparent\nstrategy or goal -directed\nbehavior.\nListing 3: Sample Trajectory Summary Generated by ROTE for Construction prediction\ntask\n1\n2 1. The agent ’s overall\ngoal or strategy: The agent ’s main goal\nseems\nto be to rearrange\nobjects in the environment , specifically\nplacing\nthem on different\nsurfaces\naccording to its\nactions. The\nactions\nare\norganized\ninto\nsequences , each\nsequence\naiming to move\nan object\nfrom its\ninitial\nposition to a target\nposition.\n3 2. The agent\nresponds to different\nenvironmental\nfeatures: The agent\ninteracts\nwith\nspecific\nfurniture\nand\nobjects\nwithin the\nenvironment , responding to their\navailability\nand\npositions. For\nexample , it navigates to the\nkitchen to interact\nwith a chair and\nrearrange an object on it.\n4 3. Any\npatterns in movement or interaction: The agent\ntends to move\nobjects\nfrom one place to another , often\ntowards\nthe garage or\nkitchen , and\ninteracts\nwith\nobjects\nlike\nchairs and\nbenches. It\nalso\narranges\nobjects on benches\nand other\nsurfaces\nwhen not\ncarrying\nthem.\n5\n6 The agent ’s behavior\npattern\nemerges\nfrom a sequence of actions\nthat\ninvolve\npicking up and moving\nobjects to rearrange\nthem\naccording\nto its\nstrategy.\nListing 4: Sample Trajectory Summary Generated by ROTE for Partnr prediction task\nA.10.4\nExamples of Explicit FSMs Generated by ROTE in the “Severe”\nStructure Condition\nIn Listings 6 and 5, we show sample FSMs predicted by ROTE which describe observed\nbehaviors in the Construction and Parntr environments, respectively. These are produced\n29\nby an LLM as an intermediate step during the “Severe” structure condition, where ROTE\ntries to generate agent programs implementing these high-level behavioral abstractions. As\nindicated by Listing 6, sometimes the models will hallucinate internal states and actions\nwhich are not within an agent’s capabilities (i.e. introducing a “charge” state). As a result,\nthe “Severe” condition is not always beneficial for model performance, especially when\npredicting the behaviors of goal-directed agents in partially observable settings (Figure 13).\n1\n2 ‘‘‘python\n3 class\nAgentFSM:\n4\ndef\n__init__(self):\n5\nself.state = ’explore ’\n6\n7\ndef\ntransition(self , state , action , info):\n8\nif state == ’explore ’:\n9\nif action == ’up ’:\n10\nreturn ’move_up ’, info\n11\nelif\naction == ’down ’:\n12\nreturn ’move_down ’, info\n13\nelif\naction == ’left ’:\n14\nreturn ’move_left ’, info\n15\nelif\naction == ’right ’:\n16\nreturn ’move_right ’, info\n17\nelif\naction == ’interact ’:\n18\nreturn ’interact ’, info\n19\nelse:\n20\nreturn ’stay ’, info\n21\nelif\nstate == ’move_up ’:\n22\nreturn ’stay ’, info\n23\nelif\nstate == ’move_down ’:\n24\nreturn ’stay ’, info\n25\nelif\nstate == ’move_left ’:\n26\nreturn ’stay ’, info\n27\nelif\nstate == ’move_right ’:\n28\nreturn ’stay ’, info\n29\nelif\nstate == ’interact ’:\n30\nreturn ’stay ’, info\n31\n32\ndef\nget_action(self , info):\n33\nif self.state == ’explore ’:\n34\n# Randomly\nchoose an action\nfrom the\navailable\noptions\n35\nimport\nrandom\n36\nactions = [’up ’, ’down ’, ’left ’, ’right ’, ’interact ’]\n37\naction = random.choice(actions)\n38\nself.state , _ = self.transition(’explore ’, action , info)\n39\nreturn\naction\n40\nelse:\n41\naction = self.state\n42\nself.state , _ = self.transition(self.state , action , info)\n43\nreturn\naction\n44 ‘‘‘\n45 ‘‘‘\n46\n47 Let ’s write the code for the agent ’s behavior , taking\ninto\naccount\nthe\nexperiences\nprovided:\n48\n49 1. The agent ’s overall\ngoal or strategy: The agent\nappears to be\nexploring\nits environment , possibly\nlooking\nfor a specific\nblock\nor blocks.\n50\nIt is not\nactively\nengaging\nwith the\nenvironment in a goal -directed\nway , as it does not seem to be collecting , storing , or moving\nblocks in a strategic\nmanner.\n51\n30\n52 2. How the agent\nresponds to different\nenvironmental\nfeatures (blocks ,\nwalls): The agent\nmoves\naround the environment , avoiding\nwalls\nand\nseemingly\nindifferent to blocks.\n53\nIt repeatedly\nmoves\nleft and right and up and down , indicating a\nlack of\nListing 5: Sample FSM Transition Logic Generated by ROTE for Construction prediction\ntask (“Severe” Structure Condition)\n1 To model the\nbehavior of the agent in this\nenvironment , we can define\na finite\nstate\nmachine (FSM) with the\nfollowing\nstates and\ntransitions:\n2\n3 ** States :**\n4 1. ** IDLE **: The agent is waiting or resting , possibly\nexploring\nits\nsurroundings to identify\npotential\ntasks or resources.\n5 2. ** SEARCH **: The agent is actively\nsearching\nfor\nspecific\nobjects or\nlocations of interest , such as a target\nobject to collect or a\nspecific\nlocation to navigate to.\n6 3. ** COLLECT **: The agent is moving\ntowards\nand\ncollecting\nthe target\nobject.\n7 4. ** TRANSIT **: The agent is on its way to a designated drop -off or\nstorage\nlocation\nafter\ncollecting an object.\n8 5. ** DROP_OFF **: The agent is depositing\nthe\ncollected\nobject at its\ndestination.\n9 6. ** CHARGE **: If the agent is a robot or uses a battery , it may need\nto recharge. This\nstate is triggered\nwhen the\nbattery\nlevel\nbecomes\ncritical.\n10\n11 ** Transitions :**\n12 - ** IDLE -> SEARCH **: When the agent\nidentifies a task or a resource\nto collect , it transitions\nfrom an idle\nstate to a search\nstate.\n13 - ** SEARCH\n-> COLLECT **: When the agent\nlocates\nthe target object , it\ntransitions\nfrom a search\nstate to a collect\nstate.\n14 - ** COLLECT\n-> TRANSIT **: After\ncollecting\nthe object , the agent\ntransitions to a transit\nstate to move\ntowards\nthe drop -off\nlocation.\n15 - ** TRANSIT\n-> DROP_OFF **: Upon\nreaching\nthe drop -off location , the\nagent\ntransitions to a drop -off state to deposit\nthe object.\n16 - ** DROP_OFF\n-> IDLE **: After\ndepositing\nthe object , the agent\nreturns\nto an idle state , possibly\nsearching\nfor a new task or resource.\n17 - ** COLLECT\n-> CHARGE **: If the agent is battery -operated\nand the\nbattery\nlevel\nbecomes\ntoo low during\ncollection , it transitions to\na charge\nstate to recharge.\n18 - ** TRANSIT\n-> CHARGE **: Similarly , if the agent\nneeds to recharge\nwhile\nmoving to the drop -off location , it transitions to the\ncharge\nstate.\n19 - ** DROP_OFF\n-> CHARGE **: If the agent\nneeds to recharge\nafter\ndepositing an object , it transitions to the charge\nstate.\n20\n21 This FSM design\nallows the agent to efficiently\nmanage its activities ,\ntransitioning\nsmoothly\nbetween\nstates\nbased on its\nobservations\nand needs , such as searching\nfor resources , collecting them ,\nmoving to a drop -off location , and\nrecharging\nwhen\nnecessary.\nListing 6: Sample FSM Transition Logic Generated by ROTE for Partnr prediction task\n(“Severe” Structure Condition)\n31\n",
    "content": "# Interpretation of the Paper \"Modeling Others’ Minds as Code\"\n\n## 1. Core Content and Key Contributions\n\nThis paper introduces a novel algorithm called **ROTE** (Representing Others’ Trajectories as Executables), designed to predict human or agent behavior more efficiently and accurately. Its central idea is to model others' behaviors as executable \"programs\" rather than relying on traditional belief-desire models.\n\n### Main contributions include:\n- **Innovative Methodology**: The first systematic application of the concept \"behavior as code\" to social cognition modeling, using large language models (LLMs) to generate Python programs that explain observed behaviors.\n- **Efficient Inference Mechanism**: Combines LLMs for hypothesis space generation with **Sequential Monte Carlo (SMC)** for probabilistic inference over program spaces, enabling high-accuracy predictions even under sparse observations.\n- **Extensive Empirical Validation**: Tested in two vastly different environments—controlled grid-world tasks (Construction) and complex embodied home simulators (Partnr)—demonstrating strong generalization capability.\n- **Human-AI Comparison Study**: Real human experiments show that ROTE achieves performance close to, and sometimes surpasses, human-level accuracy in behavioral prediction tasks.\n- **Open-Source Support**: Full release of code, environments, and evaluation scripts facilitates reproducibility and further research.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### (1) Paradigm Shift: From “Mental State Inference” to “Behavioral Script Modeling”\nTraditional approaches like inverse reinforcement learning (IRL) or Bayesian inverse planning (BIP) rely on inferring internal mental states such as goals and beliefs—computationally expensive and difficult to adapt to dynamic situations. This paper argues that in everyday social interactions, people rarely engage in deep mentalizing; instead, they treat others as following implicit “social scripts” (e.g., “stop at red lights, go at green”). ROTE leverages this cognitive science insight by reframing behavior understanding as a **program synthesis problem**.\n\n> ✅ Innovation: Replacing \"beliefs + desires\" with \"executable programs\" enables a paradigm shift from complex psychological modeling to efficient pattern recognition of behavioral routines.\n\n---\n\n### (2) Synergistic Architecture: LLM + Probabilistic Reasoning\nROTE employs a two-stage design:\n1. **LLM generates candidate programs**: Given historical trajectories, prompt an LLM to produce multiple plausible Python functions explaining the behavior;\n2. **Bayesian updating of program weights**: Use observational data to perform probabilistic ranking of these programs, retaining the most likely ones and executing them with weighted confidence.\n\nThis approach harnesses the LLM’s powerful generative and expressive capabilities while mitigating randomness and unreliability through a principled probabilistic framework.\n\n> ✅ Innovation: Integrates symbolic AI (interpretable, rule-based programs) with connectionist models (LLMs with strong generalization), creating a hybrid “neuro-symbolic” reasoning pipeline.\n\n---\n\n### (3) Superior Generalization and Efficiency\nExperimental results demonstrate:\n- ROTE outperforms baselines—including Behavior Cloning (BC), AutoToM, and naive LLM methods—by up to **50% higher accuracy** in both single-step and multi-step predictions;\n- Strong zero-shot transfer ability: effectively applies learned programs to unseen environments;\n- Significantly better computational efficiency during multi-step forecasting, since once a program is identified, future steps can be simulated without repeated LLM calls.\n\n> ✅ Innovation: Resolves long-standing trade-offs between “accuracy vs. efficiency” and “generalization vs. interpretability” in existing methods.\n\n---\n\n### (4) Design Philosophy Aligned with Occam’s Razor\nThe authors emphasize finding the **shortest program that explains observed behavior**. Experiments reveal a negative correlation between prediction accuracy and program length (shorter = better), consistent with Solomonoff induction theory—indicating the method naturally favors simple, universal behavioral rules.\n\n> ✅ Innovation: Incorporates the principle of Minimum Description Length from algorithmic information theory into behavioral modeling, enhancing generalization potential.\n\n---\n\n## 3. Startup Ideas Inspired by the Paper\n\n### 🚀 Venture Idea 1: **Smart Care Robotics — “Habit-Aware” Home Care System**\n\n#### Project Name: HabitMind Care\n\n#### Core Concept:\nUse ROTE technology to build AI caregiving assistants that automatically learn users’ daily living habits and proactively assist by anticipating their intentions.\n\n#### Key Features:\n- Automatically learns elderly users’ routines (waking time, hydration, medication, walking);\n- Predicts fall risks (e.g., slow movement after frequent nighttime bathroom visits);\n- Proactively turns on lights, adjusts room temperature, reminds medication intake;\n- Detects abnormal behaviors (e.g., leaving stove unattended, prolonged immobility) and triggers alerts.\n\n#### Business Advantages:\n- Requires minimal labeled training data—learns from limited observation;\n- Behavioral rules are represented as readable code, easing debugging and regulatory compliance;\n- Supports personalization, with separate behavioral models per household member.\n\n#### Target Market:\n- Home care device manufacturers\n- Smart eldercare community service providers\n- Healthcare IoT platforms\n\n---\n\n### 🚀 Venture Idea 2: **Social Behavior Understanding Engine for Autonomous Driving**\n\n#### Project Name: SocialDriver Engine\n\n#### Core Concept:\nEnhance autonomous vehicles’ ability to predict the intentions of pedestrians, cyclists, and drivers—especially critical in complex urban traffic scenarios.\n\n#### Technical Integration:\n- Convert camera/LiDAR inputs into structured scene graphs;\n- Apply ROTE to model individual behavioral scripts (e.g., “pedestrian waits for green light,” “cyclist detours around obstacles”);\n- Output predicted action sequences for path planning and risk assessment.\n\n#### Differentiation:\n- More interpretable than pure neural networks, aiding safety certification;\n- Captures culturally specific traffic behaviors (e.g., jaywalking patterns in certain regions);\n- Robust to sparse/noisy data, suitable for edge deployment.\n\n#### Potential Partners:\n- Tier 1 autonomous driving suppliers (e.g., Mobileye, NVIDIA)\n- EV startups (NIO, XPeng, Li Auto)\n- Intelligent transportation authorities\n\n---\n\n### 🚀 Venture Idea 3: **Enterprise Collaboration AI Assistant — Team Behavior Optimization Platform**\n\n#### Project Name: TeamScript AI\n\n#### Core Concept:\nAnalyze team workflow patterns to predict collaboration bottlenecks and optimize task allocation and communication timing.\n\n#### Application Scenarios:\n- Analyze Slack/DingTalk messages, meeting schedules, Jira workflows;\n- Learn employee work rhythms (e.g., someone always reviews requests Friday afternoons);\n- Predict project delays and recommend optimal communication times;\n- Automatically generate standard operating procedure (SOP) scripts for team coordination.\n\n#### Product Form:\n- SaaS platform + API access\n- Visual dashboard showing “behavioral programs”\n- Real-time collaboration suggestions\n\n#### Market Positioning:\n- HR and operations departments in mid-to-large tech companies\n- Remote team management tool integrators\n- Digital transformation consulting firms\n\n---\n\n### 🚀 Venture Idea 4: **Personalized Learning Companion in Education**\n\n#### Project Name: LearnScript Tutor\n\n#### Core Concept:\nBuild a “learning behavior program” model for each student to identify problem-solving strategies, attention fluctuations, and common error patterns—enabling precise tutoring.\n\n#### Functional Examples:\n- Observe student problem-solving processes (mouse movements, thinking time, revision paths);\n- Infer cognitive patterns (skipping steps? Stuck on specific question types?);\n- Dynamically adjust teaching strategy (hint style, exercise difficulty);\n- Generate personalized “error-correction scripts.”\n\n#### Educational Value:\n- Enables truly adaptive learning experiences;\n- Helps teachers understand students’ “invisible” thought processes;\n- Supports behavioral intervention for special needs learners (e.g., ADHD).\n\n#### Target Customers:\n- K–12 online education platforms\n- Smart learning hardware makers (tutoring devices, e-paper tablets)\n- Special education institutions\n\n---\n\n## Summary\n\nThis paper not only proposes a highly promising technical framework (ROTE), but also reveals a profound cognitive insight: **the essence of human social interaction is often less about deliberate mentalizing and more about默契 cooperation through shared behavioral scripts**. This perspective opens a new pathway for building AI systems that are interpretable, efficient, and closely aligned with real human behavior. Around the core idea of “behavior as code,” numerous high-barrier, socially valuable ventures can emerge across healthcare, transportation, education, and organizational management.",
    "github": "",
    "hf": ""
}