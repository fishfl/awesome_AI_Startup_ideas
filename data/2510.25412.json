{
    "id": "2510.25412",
    "title": "Serve Programs, Not Prompts",
    "summary": "This paper proposes a new architecture for large language model service systems. It addresses the efficiency and adaptability issues faced by existing systems when handling increasingly complex large language model applications by running runtime customizable tag prediction and KV cache management, as well as offloading part of the application logic.",
    "abstract": "Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "In Gim,Lin Zhong",
    "subjects": [
        "Computation and Language (cs.CL)"
    ],
    "comments": "Comments:HotOS 2025. Follow-up implementation work (SOSP 2025) is available athttps://arxiv.org/abs/2510.24051",
    "keypoint": "Current LLM serving systems are inefficient and inflexible for complex applications due to their prompt-centric design.\n\nThe paper proposes a new LLM serving architecture that serves programs instead of prompts, called LLM Inference Programs (LIPs).\n\nLIPs allow users to customize token prediction and KV cache management at runtime.\n\nLIPs enable offloading application logic such as tool execution to the server.\n\nSymphony is introduced as an example system that functions as an operating system for LIPs.\n\nSymphony exposes LLM model computations via system calls.\n\nSymphony virtualizes KV cache using a dedicated file system (KVFS).\n\nSymphony uses a two-level scheduling scheme to ensure GPU efficiency.\n\nSymphony enables application-specific optimizations like custom KV cache replacement policies.\n\nSymphony achieves up to 7 times greater throughput compared to systems like vLLM.\n\nThe proposed architecture improves efficiency by reducing redundant recomputation in multi-round interactions.\n\nSymphony reduces communication overhead by allowing internal execution of function calls within the serving system.\n\nUsers can implement constrained decoding and other advanced generation techniques directly in LIPs.\n\nSymphony supports parallel generation strategies such as Tree-of-Thought through thread spawning.\n\nThe system allows direct manipulation of KV cache files, including extract and merge operations.\n\nSymphony moves beyond API fragmentation by providing composable, fine-grained interfaces.\n\nSecurity challenges arise from executing untrusted user code, requiring sandboxing and access control.\n\nPerformance overhead may result from decentralized control and reduced global optimization.\n\nNew benchmarks are needed to evaluate programmatic LLM serving systems beyond per-prompt metrics.",
    "date": "2025-10-31",
    "paper": "Serve Programs, Not Prompts\nIn Gim\nin.gim@yale.edu\nYale University\nNew Haven, CT, USA\nLin Zhong\nlin.zhong@yale.edu\nYale University\nNew Haven, CT, USA\nAbstract\nCurrent large language model (LLM) serving systems, pri-\nmarily designed for text completion, are neither efficient nor\nadaptable for increasingly complex LLM applications due to\ntheir inflexible design. We propose a new LLM serving sys-\ntem architecture that serves programs instead of prompts to\naddress this problem. These programs, called LLM Inference\nPrograms (LIPs), allow users to customize token prediction\nand KV cache management at runtime and to offload parts of\ntheir application logic, such as tool execution, to the server.\nWe describe an example of this architecture through a system\nnamed Symphony, which functions as an operating system\nfor LIPs. Symphony exposes LLM model computations via\nsystem calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level pro-\ncess scheduling scheme. Symphony has the potential to open\nthe door to a more efficient and extensible ecosystem for\nLLM applications.\nCCS Concepts\n‚Ä¢ Computing methodologies ‚ÜíNatural language pro-\ncessing.\nKeywords\nLarge language models, LLM serving systems, KV cache\nACM Reference Format:\nIn Gim and Lin Zhong. 2025. Serve Programs, Not Prompts. In\nWorkshop on Hot Topics in Operating Systems (HOTOS ‚Äô25), May\n14‚Äì16, 2025, Banff, AB, Canada. ACM, New York, NY, USA, 8 pages.\nhttps://doi.org/10.1145/3713082.3730398\n1\nIntroduction\nCurrent large language model (LLM) serving systems are pri-\nmarily designed for high-throughput text completion. They\naccept prompts as input and stream generated text as out-\nput. This prompt-serving paradigm has become the de facto\nThis work is licensed under a Creative Commons Attribution 4.0 Interna-\ntional License.\nHOTOS ‚Äô25, Banff, AB, Canada\n¬© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-1475-7/2025/05\nhttps://doi.org/10.1145/3713082.3730398\nstandard, commonly available both as cloud APIs and as\nopen-source software [22, 30, 41]. Unfortunately, this para-\ndigm faces growing efficiency and adaptability challenges\nas LLM applications evolve into complex, compound AI sys-\ntems [50]. For instance, the stateless nature of this para-\ndigm makes multi-round, programmatic interactions with\nLLMs [31, 48, 49] inefficient due to redundant recomputa-\ntions. In addition, emerging techniques that deviate from\nstandard token generation process [21, 27, 32] or require\ntool or data-augmented generation workflows [23, 44, 45]\ncan be difficult to implement within these systems, forcing\ndevelopers to modify the core system [1, 15] or create ineffi-\ncient client-side workarounds (See ¬ß2).\nThis paper motivates a new LLM serving architecture that\nshifts the fundamental unit of service from prompts to pro-\ngrams. In this model, users flexibly define and offload their\nown LLM token generation routines to the LLM serving sys-\ntem, utilizing fine-grained APIs provided by the system. That\nis, instead of a prompt, a user sends a program to the serving\nsystem to control the generation process. We term these user-\ndefined routines LLM Inference Programs (LIPs). We identify\nthree core properties for APIs to program LIPs: (1) decou-\npling generation from model computation, (2) application-\nmanaged model states (e.g., KV cache), and (3) co-locating\nexternal interactions. Even fundamental processes like the\nstandard autoregressive generation loop should be explicitly\ndefinable using these APIs when needed (See ¬ß3).\nTo exemplify this idea, we present a new LLM serving\nsystem named Symphony (¬ß4). Symphony directly leverages\nexisting operating system abstractions to design the APIs\nneeded to program LIPs, and serve them efficiently. For exam-\nple, Symphony uses a file system to virtualize the KV cache\nand a system call for model computation. Users can employ\nhost OS APIs (e.g., POSIX) for custom inference strategies,\nsuch as parallel generation or integrating external tools.\nThe proposed program-serving paradigm enables develop-\ners to incorporate application-specific optimizations directly\nat the LLM generation level and implement new LLM tech-\nniques without altering the core LLM serving system. Our\ninitial experiments indicate that Symphony can deliver sub-\nstantial performance improvements, such as achieving up\nto 7 times greater throughput compared to existing systems\nlike vLLM [30], by empowering users to implement custom\nKV cache replacement policies through LIPs (See ¬ß5).\narXiv:2510.25412v1  [cs.CL]  29 Oct 2025\nHOTOS ‚Äô25, May 14‚Äì16, 2025, Banff, AB, Canada\nGim et al.\nWe discuss the main challenges and opportunities of this\nnew approach in ¬ß6, regarding the granularity of the APIs,\nsecurity implications, performance overhead, and the need\nfor new benchmarks.\n2\nMotivation\nPrompt-centric serving systems struggle with efficiency, con-\ntrol, and adaptability when handling complex LLM work-\nflows beyond basic text completion. To address these chal-\nlenges, developers frequently resort to ad-hoc system mod-\nifications or inefficient client-side solutions. Consider the\nexample of developing an LLM-based code editor which pro-\nvides live code autocompletions.\n‚Ä¢ Efficiency: As the user types, each keystroke ideally trig-\ngers an update. A naive prompt-based system recomputes\nthe entire prompt repeatedly. Even with server-side prompt\ncaching [17, 30], the policy is server-defined and not ap-\nplication aware. For example, the serving system might\ncache prompts that will not be used any longer.\n‚Ä¢ Interaction: Suppose the editor implements Retrieval-\nAugmented Generation (RAG) [24] to fetch relevant API\ndocumentation via function calling [37]. This means the\nclient application acts as an intermediary: Prompt ‚ÜíServ-\ning System ‚ÜíFunction Call Spec ‚ÜíClient ‚ÜíExecute\nFunction (e.g., fetching API documentation) ‚ÜíClient ‚Üí\nFunction Result ‚ÜíServing System. Each arrow involves\nboundary crossing overhead.\n‚Ä¢ Control: Enforcing code conventions or ensuring gen-\nerated code fits a specific structure (constrained decod-\ning [7]) requires manipulating the token generation pro-\ncess. Prompt-based APIs offer limited control (e.g., tem-\nperature, basic JSON mode enforcement), insufficient for\ncomplex grammars or custom sampling strategies.\nWe elaborate on these underlying problems below.\n2.1\nResource Inefficiency\nExisting systems suffer from resource inefficiency due to the\nlack of application-driven management over the KV cache.\nThe KV cache is a crucial component in the efficient serving\nof ‚ÄúGPT-style‚Äù LLMs [39, 40]. It enables incremental LLM\nmodel computations by avoiding the need to recompute all\ninput tokens. This is possible because the internal represen-\ntations of a token (K and V states in self-attention) depend\nsolely on preceding tokens in causal Transformers, allowing\nthem to be reused for subsequent LLM model computations\nwhen the preceding token sequences remain unchanged.\nOptimizing the management and reuse of the KV cache is\none of the most critical areas for enhancing LLM serving\nperformance [17, 30, 35, 52].\nHowever, current systems are designed around prompts\nand lack awareness of application-specific reuse patterns,\nwhich limits optimization potential. Typically, KV cache man-\nagement is governed by a system-wide policy (e.g., LRU\neviction) that applies to all requests. For instance, in scenar-\nios involving multi-round prompting, maintaining the KV\ncache from prior interactions can significantly decrease la-\ntency [15]. However, users lack the ability to manage the KV\ncache retention, even when they possess knowledge of reuse\npatterns. Current methods, like automatic prefix caching in\nvLLM, fail to provide this level of flexibility.\nSeveral LLM providers, including Anthropic, offer APIs\nfor prompt caching [2], enabling users to determine what\nshould be cached prior to generation. Systems like SGLang\nand PromptCache [17] further allow users to define the struc-\nture of input prompts [52], enhancing the efficient reuse of\nKV caches in parallel generation strategies such as Tree-of-\nThought [48]. However, while these systems are beneficial in\nvarious contexts, they still handle KV cache management im-\nplicitly and lack the capability to accommodate application-\nspecific reuse patterns beyond their intended design, such\nas using graph [5] or recursive [31] generation strategies.\nOur solution. We propose making KV cache management\nan explicit, application-defined operation‚Äîshifting optimiza-\ntion responsibility from the serving system to user programs.\n2.2\nCommunication Overheads\nCurrent serving systems are only responsible for token gen-\neration, forcing any additional non-token generation logic,\nsuch as function calling, to be implemented on the client side.\nThis introduces communication overheads. LLM function\ncalling [37, 38, 49] is essential for interfacing an LLM with\nexternal APIs and data sources, which is crucial for many\nLLM applications [44]. The typical workflow for LLM func-\ntion calling involves: (1) the user providing a description\nof functions and their parameters as part of the prompt, (2)\nthe LLM generating a function call in response, and (3) the\nuser parsing and executing the function call, followed by a\nrequest to return the execution result to the LLM.\nIn this process, the user effectively acts as a code inter-\npreter, necessitating network round trips between the user\nand the LLM serving system. These round trips can signifi-\ncantly increase the end-to-end latency of an AI application,\nespecially as the number of function calls increases [18, 25].\nHowever, in many cases where function calls do not rely on\nthe user‚Äôs environment for execution (e.g., accessing third-\nparty APIs like weather or stock price APIs, or executing\nsimple code snippets such as NumPy calculations), these\nround trips can be avoided by enabling the LLM serving\nsystem to execute the function directly. Moreover, multi-\nagent LLM applications [46] implement inter-agent com-\nmunication through LLM function calling. However, this\nagent-to-agent communication incurs high costs because\nServe Programs, Not Prompts\nHOTOS ‚Äô25, May 14‚Äì16, 2025, Banff, AB, Canada\nusers must handle the communication logic. We can reduce\nthis overhead by enabling the serving system to manage the\ncommunication autonomously.\nOur solution. We propose that the serving system should\nincorporate a code execution environment to handle LLM\nfunction calls and code executions internally, rather than\nrelying on the user to manage all execution and communica-\ntion.\n2.3\nUncontrollable Generation\nCurrent LLM serving systems integrate the autoregressive\ntoken generation loop into their core, continuously sampling\nthe next token until they encounter an end-of-sequence (EOS)\ntoken. As a result, users have limited control over token gen-\neration, typically restricted to adjusting a few parameters\nin the next-token sampler and the temperature. This limita-\ntion complicates the implementation of emerging techniques\nfor more robust and sophisticated LLM use, such as con-\nstrained generation, which ensures the LLM output adheres\nto a specific format or grammar [7, 16], and policy-based\ngeneration [20, 26, 29] for improved output quality. These\nstateful sampling strategies often necessitate intrusive modi-\nfications to the LLM serving system and expose specialized\nAPIs. While some serving systems support popular methods\nlike constrained decoding through JSON, Regex, and Context-\nFree Grammar [6, 12, 13], these methods do not extend to\narbitrary sampling strategies.\nWe note that exposing the sampling process as an end-\nuser facing API is impractical due to the large size of the\nnext-token distribution. For example, GPT-4‚Äôs vocabulary\nsize exceeds 100K tokens, resulting in a distribution size of\napproximately 200 KB using FP16.\nOur solution. We propose offloading arbitrary programs to\nthe serving system, enabling applications to directly access\nand manipulate the token distribution during generation.\n2.4\nAPI Fragmentation\nWhile ad-hoc solutions can address the limitations of current\nLLM serving system, they often struggle with adaptability\nas LLM workloads grow more varied and complex. For in-\nstance, an application focused on solving complex mathemat-\nical problems might leverage parallel reasoning techniques\nwithout prioritizing latency [43]. Conversely, a robotics ap-\nplication with numerous LLM function calls might prioritize\nreduced latency over absolute accuracy [9].\nThese diverse workloads present challenges in API de-\nsign, as each specialized solution demands unique API re-\nquirements. This has led to a fragmentation of LLM APIs\namong major providers. For example, Google Cloud [11],\nOpenAI [36], and Anthropic [2] each offer distinct API de-\nsigns and semantics for prompt caching, LLM function call-\ning, and constrained decoding.\nOur solution. We emphasize the need for composable, fine-\ngrained APIs that can accommodate the programming of\nvaried LLM workloads.\n2.5\nRelated Work\nRecent research aims to make LLM serving systems more\napplication-aware, but it often addresses challenges in iso-\nlation without a unified approach. Systems like Prompt-\nCache [17], SGLang [52], and Parrot [33] offer APIs for pro-\ngrammatically defining input prompt structures. These sys-\ntems leverage this structure for efficient KV cache reuse.\nHowever, the KV cache management is implicit and can-\nnot express arbitrary reuse patterns beyond the system‚Äôs\npredefined abstractions.\nFor controlled generation, tools such as XGrammar [12],\nOutlines [13], and Guidance [19] allow users to enforce out-\nput constraints using custom rules or domain-specific lan-\nguages (DSLs). These systems embed a fixed set of decoding\nstrategies directly into the serving stack, limiting extensi-\nbility. Symphony generalizes this model by exposing the\nlow-level token sampling loop as a programmable interface,\nenabling arbitrary control strategies that go beyond what\nbuilt-in grammars or templates can support.\nFor LLM function calling, InferCept [1] optimizes the KV\ncache during such function calls. Some other works, such as\nLLMCompiler [25] or AsyncLM [18], make this interaction\nefficient by allowing multiple function calls to be run con-\ncurrently. Symphony makes such workflows native, treating\nfunction execution, caching, and token generation as com-\nposable building blocks within a single user-defined LIPs.\n3\nProgram as the Unit of Service\nIn light of the challenges outlined above, we propose that\nLLM serving system should transition from processing mere\nprompts to handling programs, which we term LLM Infer-\nence Programs (LIPs). We specifically champion three core\nattributes for LIPs, aimed at empowering users to program\ntheir own optimization and generation strategies by utiliz-\ning fine-grained APIs for model computation and KV cache\nmanagement.\n‚Ä¢ Separation of Generation and Model Computation: The logic\nfor generating tokens, such as an autoregressive loop,\nshould be decoupled from the model computation, like\nTransformer model operations on GPUs. The generation\nprocess should be defined within LIPs, while the LLM serv-\ning system only needs to focus on efficiently managing\nthe model computations requests.\nHOTOS ‚Äô25, May 14‚Äì16, 2025, Banff, AB, Canada\nGim et al.\nUser A\nUser B\nprompt\nresponse\nfixed generation logic\nLLM\nUser A\nUser B\ncustom\nprogram\nLLM\nShared KV cache (file system)\ncustom\nprogram\nRPC\nexternal\nAPIs\nexternal\nAPIs\nsyscall\nfile API\nProposed system (Symphony)\nCurrent LLM serving systems\nFigure 1: Comparison with existing serving systems\n(top) and Symphony (bottom). Symphony serves as an\noperating system for user-defined inference programs.\n‚Ä¢ Application-Controlled Model States: LLM states, including\nthe KV cache, should be able to persist beyond a single LIP\nand be explicitly managed by the program. It is the user‚Äôs\nresponsibility, not the LLM serving system‚Äôs, to efficiently\nutilize the KV cache for their tasks. The LLM serving\nsystem should offer an API that allows users to manage\nthe KV cache, such as creating, updating, or deleting it.\n‚Ä¢ Integrated External Interactions: Each LIP should indepen-\ndently manage its external interactions, such as function\ncalls and I/O, without depending on the LLM serving sys-\ntem or external client-side logic for coordination.\nWe provide an example code for LIP in Figure 2. We elaborate\non the API design in the following section.\n4\nLLM Serving System as OS\nWe introduce a LLM serving system called Symphony, de-\nsigned to serve LIPs. Symphony treats the execution of a LIP\nlike a process in an operating system (OS), leveraging exist-\ning OS APIs, abstractions, and implementations. By doing\nso, we aim to extend the core functions of the LLM serving\nsystem beyond next token prediction, to include resource vir-\ntualization (e.g., KV cache), concurrent program execution,\nand I/O management‚Äîkey responsibilities of an OS.\n4.1\nModel Computation via System Calls\nSymphony provides a specialized system call for LLM model\ncomputation, named pred, which stands for next token pre-\ndiction. Its signature is as follows:\n1\n// load precomputed kv cache\n2\nprefix_kv = kv_open (\" sys_msg.kv\");\n3\nsuffixes = {\n4\ntokenize (/** query 1 */), ...\n5\ntokenize (/** query n */)\n6\n};\n7\nfor (suffix : suffixes) {\n8\n// fork prefix kv and thread\n9\nkv = kv_fork(prefix_kv);\n10\npthread_create ({\n11\npos = prefix_kv.len(), step = 0;\n12\nt = suffix;\n13\n// generate until eos token\n14\nwhile (t != EOS) {\n15\nd = pred(kv, t, pos + step);\n16\nt = sample(d);\n17\nstep ++;\n18\nprint(detokenize(t));\n19\n}\n20\nkv_remove(kv);\n21\n});\n22\n}\n23\njoin_all_threads ();\n24\nkv_close(prefix_kv);\nFigure 2: Example program demonstrating parallel to-\nken generation with shared prefix KV cache.\npred(kv: kv_file, tokens, positions) -> list[dist]\nThe pred function accepts two parameters: kv, a pointer\nto the KV cache file (further details in ¬ß4.3), and tokens, a\nlist of tuples where each tuple contains a token ID and its\nabsolute position within the context. Upon completion of\nthe system call, the KV cache file is updated with new ten-\nsors corresponding to the provided tokens, and the function\nreturns a list of next token distributions for each input token.\nThe dist contains a list of floating-point numbers. With\nfull access to the token distribution, LIPs implement vari-\nous decoding algorithms, such as constrained decoding and\nspeculative decoding. For example, to achieve constrained\ndecoding, LIPs integrate a state machine into the generation\nloop to restrict the distribution variables to those that align\nwith the state machine. For speculative decoding, LIPs pass\nmultiple input tokens (draft tokens) to the pred system call\nand verify them by inspecting the distributions of the tokens.\n4.2\nKV Cache Management via File System\nSymphony treats the KV cache as files, enabling it to persist\nbeyond a single process‚Äôs lifecycle, share across multiple\nprocesses, and allow LIPs to dynamically manipulate it. Sym-\nphony introduces a specialized file system called KVFS. KVFS,\nsimilar to traditional file systems, lets LIPs manage files (KV\ncache) using file APIs similar to POSIX, supplemented by spe-\ncialized APIs for operations like extract and merge. KVFS\nServe Programs, Not Prompts\nHOTOS ‚Äô25, May 14‚Äì16, 2025, Banff, AB, Canada\nFigure 3: Estimated performance benefits of prompt caching implemented via LIPs in Symphony. The figure shows\nnormalized mean end-to-end latency per generated token and throughput using the Llama [14] 13B model on\nNVIDIA A100 GPU. Symphony enables application-specific LLM optimizations, such as caching frequently reused\nKV cache, without requiring modifications to the serving system design.\nvirtualizes GPU memory areas that store token-level KV\ntensors in pages, using PagedAttention [30].\nIn a simple text completion scenario, LIPs start by creating\nan empty file, retrieve its handle with kv_open, and use\nthis handle with the prompt in pred (See ¬ß4.1), which then\nfills the file with the KV cache. They use the same file in\nthe subsequent autoregressive generation loop. For parallel\ngeneration with shared prefixes, LIPs clone the prefix file\nfor each thread (See ¬ß4.3), allowing different tokens to fill in\nwithout duplicating the actual tensors behind the KV cache.\nLIPs can directly manipulate files, enabling them to cre-\nate new files from existing ones by extracting specific to-\nken indices with extract or merging existing files into one.\nThis capability benefits inference speedup techniques like\nruntime context prunning [42, 47], by removing invalid or\nunimportant tokens from files. When writing files, LIPs can\napply a file lock to ensure exclusive access. KVFS enforces\naccess control, allowing only authorized parties to access\nfiles. For example, a file containing ‚Äúsystem prompts‚Äù might\nbe readable by all LIPs but writable only by the admin.\n4.3\nGenerations as Threads\nTo support advanced reasoning strategies that leverage par-\nallel generation, LIPs spawn multiple threads using POSIX\nthread APIs. For example, a single LIP implements the entire\nTree-of-Thought [48] reasoning, with each thread generating\none branch of hypotheses, forking recursively if needed, and\njoining if a generated hypothesis proves unlikely.\nLIPs manage I/O independently, eliminating the commu-\nnication overhead of server-user roundtrips. For instance,\nLIPs initiate LLM function calls by themselves or incorporate\narbitrary computation with the generation process. When\nI/O with external APIs blocks thread execution, Symphony\ninterrupts to put the thread in a waiting state. Symphony ex-\nploits this for resource efficiency: when threads wait for I/O,\nSymphony offloads their KV caches from the GPU to the CPU\nand restores them upon I/O completion. Additionally, LIPs\ncommunicate directly with each other using inter-process\ncommunication, which is useful for implementing coopera-\ntive multi-agent systems.\n4.4\nTwo-level Scheduling\nSymphony implements a two-level scheduling scheme with\ntwo distinct schedulers: the thread scheduler and the batch in-\nference scheduler. The thread scheduler handles CPU sched-\nuling tasks typical of operating systems and executes the\nthread. When a thread triggers the pred system call for LLM\ninference, Symphony transfers the thread to the ‚Äúinference\npool‚Äù and marks it as blocked.\nConversely, the inference scheduler aggregates multiple\npred system calls into a single batch and schedules this batch\non the GPU(s). Efficient handling of the pred system call by\nthe GPU(s) necessitates batch processing to optimize GPU uti-\nlization. Consequently, Symphony must strategically batch\nthese calls. The primary challenge lies in timing the batch ex-\necution to achieve peak GPU efficiency. Executing the batch\nprematurely can result in underutilized GPU resources, di-\nminishing throughput, whereas delaying it excessively can\nincrease wait times for the threads. Symphony dynamically\nadjusts batch size according to the average frequency of\nsystem calls, leveraging models like Poisson process.\n5\nPreliminary Results\nWe conduct simulated experiments to demonstrate how Sym-\nphony enables scalable LLM workloads by allowing users to\ndefine custom optimization strategies through LIPs. We com-\npare Symphony with two popular prompt-serving systems,\nvLLM [30] and TGI [22], in a retrieval-augmented generation\n(RAG) application scenario. The application inputs a topic,\nfetches the relevant document, and generates an answer.\nThere are 100 documents, each containing 3,000 tokens.\nIn this setup, a LIP implements prompt caching [17] by\nretaining the KV cache for the top 20 most popular topics\nand discarding it for others. We evaluate throughput and la-\ntency under varying request loads and Pareto indices, which\nmodel the skewness of the popularity distribution. The re-\nsults, shown in Figure 3, indicate that Symphony outper-\nforms vLLM and TGI when the Pareto index is small (i.e.,\nwhen a few topics are queried frequently). This improvement\narises from Symphony‚Äôs ability to leverage increased cache\nHOTOS ‚Äô25, May 14‚Äì16, 2025, Banff, AB, Canada\nGim et al.\nhits through user-controlled KV cache management. Fur-\nthermore, developers can refine LIP logic‚Äîe.g., caching only\nafter consecutive requests for the same topic‚Äîto optimize\nperformance for specific workload patterns.\nWe note that these experiments do not yet fully capture\nthe overheads of Symphony. Specifically, the current imple-\nmentation has a simplified design where all LIPs are the\nsame and start and finish at the same time; therefore, it does\nnot reflect the scheduling overhead and opportunity costs of\nusing different sampling strategies. We will evaluate these\naspects in future work.\n6\nDiscussion\nWe believe that bringing programmability to LLM serving\nsystems is a key step towards creating more efficient and\ncapable AI applications. Symphony is a step towards achiev-\ning this goal. The transition proposed by Symphony ‚Äîfrom\nserving prompts to executing LLM Inference Programs (LIPs)\nopens up interesting questions about the design and oper-\nation of future LLM serving infrastructure. We discuss key\nchallenges and research directions below.\nFinding the right interface granularity. Symphony cur-\nrently provides one system call, pred, for model computation.\nThis treats a single LLM model forward pass as an atomic\noperation. This simplifies system design and batching but\nlimits the application‚Äôs ability to control the model computa-\ntion process. Enabling finer-grained access (e.g., to attention\nmechanisms [3, 51] or layer outputs [10]) could unlock pow-\nerful application-specific optimizations. However, such inter-\nfaces increase complexity, risk violating model abstraction\nboundaries, and complicate efficient batch scheduling. Deter-\nmining the optimal granularity‚Äîbalancing application em-\npowerment against system manageability and performance\npredictability‚Äîremains an open question.\nSecurity implications. Executing user-provided LIPs fun-\ndamentally shifts the trust boundary within the serving sys-\ntem. Unlike prompt-based systems where user input is data,\nSymphony takes code, i.e., LIPs, as input. This introduces\nsecurity vulnerabilities, such as resource exhaustion, model\nconfidentiality, LLM jailbreaking, and parameter extraction\nvia distribution analysis. Practical deployments necessitate\nrobust sandboxing (e.g., WASM, seccomp filters, lightweight\nVMs), resource accounting, and fine-grained access control\nto protect the system and other tenants.\nPerformance overhead. Traditional serving systems achieve\nhigh performance through centralized control over batching,\nscheduling, and GPU pipelining. Symphony decentralizes\ncontrol, empowering LIPs to manage their generation logic.\nThis potentially sacrifices global optimization opportunities.\nSymphony‚Äôs batch scheduler operates with less complete\ninformation, potentially leading to suboptimal GPU utiliza-\ntion, especially with heterogeneous LIPs. Furthermore, de-\ncoupling the sampling logic into LIPs hinders server-side\noptimizations such as pipelining sampling with model ex-\necution. Designing intelligent scheduling algorithms and\npotentially new co-design approaches between LIPs and the\nsystem to mitigate these performance ‚Äúopportunity costs‚Äù is\na key research challenge.\nBeyond the OS analogy. Framing Symphony in terms of\nOS concepts helps convey its broader responsibilities, i.e.,\nresource management, concurrency and I/O, using a famil-\niar mental model. However, this analogy serves more as a\nscaffold than as an implementation prescription. Alterna-\ntive abstractions may be equally, if not more, suitable for\nrealizing the core ideas. For example, language runtimes\nthat support coroutines, actors, or async tasks could offer a\nnatural fit for enabling concurrent, stateful generation work-\nflows. Lightweight execution environments such as WASM\nor secure containers could provide a sandboxed context for\nrunning user-defined logic. Moreover, we can draw inspira-\ntion from other domains where systems evolved into pro-\ngrammable platforms by accepting user-supplied logic, such\nas OS kernels [4, 34], networking [8], and databases [28].\nThese systems show that we can expose carefully scoped\nprogrammability at key abstraction boundaries without re-\narchitecting everything from scratch. This raises a question:\ncan existing LLM serving systems be salvaged? In our view,\nthe tight coupling of sampling, caching, and scheduling in\ntoday‚Äôs prompt-centric architectures makes this difficult. We\nargue that application-level control, spanning model state\nmanagement, fine-grained control over generation, and tool\nuse, requires a clean break from the existing architecture. In\nthis sense, a redesign like Symphony is not just a matter of\nelegance but one of necessity.\nEvaluation space. Standard benchmarks for LLM serving\nfocus on per-prompt throughput and latency and are inade-\nquate for evaluating systems like Symphony. The benefits of\nprogrammability manifest in the context of complex, multi-\nstep application workflows involving state management (KV\ncache reuse), external interactions (function calls), and cus-\ntom control flow (parallel reasoning). Meaningful evaluation\nrequires new benchmarks that capture end-to-end perfor-\nmance, resource consumption (GPU, CPU, memory), and\nresponsiveness for realistic application scenarios, moving\nbeyond isolated token generation metrics.\nAcknowledgments\nThis work is supported in part by National Science Founda-\ntion (NSF) Athena AI Institute (Award #2112562) and Yale\nUniversity. The authors thank the reviewers for their con-\nstructive comments.\nServe Programs, Not Prompts\nHOTOS ‚Äô25, May 14‚Äì16, 2025, Banff, AB, Canada\nReferences\n[1] R. Abhyankar, Z. He, V. Srivatsa, H. Zhang, and Y. Zhang. InferCept:\nEfficient intercept support for augmented large language model infer-\nence. In Proc. ICML, 2024.\n[2] Anthropic. Prompt caching with claude. https://www.anthropic.com/\nnews/prompt-caching. Accessed: 2025-01-16.\n[3] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document\ntransformer. CoRR, abs/2004.05150, 2020.\n[4] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. E. Fiuczynski,\nD. Becker, C. Chambers, and S. J. Eggers. Extensibility, safety and\nperformance in the SPIN operating system. In Proc. ACM SOSP, 1995.\n[5] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gi-\naninazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk, and\nT. Hoefler. Graph of thoughts: Solving elaborate problems with large\nlanguage models. 2024.\n[6] L. Beurer-Kellner, M. Fischer, and M. Vechev. Prompting is program-\nming: A query language for large language models. Proceedings of the\nACM on Programming Languages, 7(PLDI):1946‚Äì1969, 2023.\n[7] L. Beurer-Kellner, M. Fischer, and M. T. Vechev. Guiding llms the right\nway: Fast, non-invasive constrained generation. In Proc. ICML, 2024.\n[8] P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown, J. Rexford,\nC. Schlesinger, D. Talayco, A. Vahdat, G. Varghese, et al. P4: Pro-\ngramming protocol-independent packet processors. ACM SIGCOMM\nComputer Communication Review, 44(3):87‚Äì95, 2014.\n[9] G. Chen, X. Yu, and L. Zhong. Typefly: Flying drones with large\nlanguage model. CoRR, abs/2312.14950, 2023.\n[10] Y. Chen, X. Pan, Y. Li, B. Ding, and J. Zhou. EE-LLM: large-scale\ntraining and inference of early-exit large language models with 3d\nparallelism. In Proc. ICML, 2024.\n[11] G. Cloud. Context caching overview. https://cloud.google.com/vertex-\nai/generative-ai/docs/context-cache/context-cache-overview.\nAccessed: 2025-01-16.\n[12] Y. Dong, C. F. Ruan, Y. Cai, R. Lai, Z. Xu, Y. Zhao, and T. Chen. Xgram-\nmar: Flexible and efficient structured generation engine for large lan-\nguage models. CoRR, abs/2411.15100, 2024.\n[13] Dottxt-AI. Outlines: Structured text generation. https://github.com/d\nottxt-ai/outlines, 2025. Accessed: 2025-04-16.\n[14] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nA. Mathur, A. Schelten, A. Yang, A. Fan, et al. The Llama 3 herd of\nmodels. CoRR, abs/2407.21783, 2024.\n[15] S. Gao, Y. Chen, and J. Shu. Fast state restoration in LLM serving with\nHCache. In Proc. EuroSys, 2025.\n[16] S. Geng, M. Josifoski, M. Peyrard, and R. West. Grammar-constrained\ndecoding for structured NLP tasks without finetuning. In Proc. EMNLP,\n2023.\n[17] I. Gim, G. Chen, S.-S. Lee, N. Sarda, A. Khandelwal, and L. Zhong.\nPrompt cache: Modular attention reuse for low-latency inference. In\nProc. MLSys, 2024.\n[18] I. Gim, S.-S. Lee, and L. Zhong. Asynchronous LLM function calling.\nabs/2412.07017, 2024.\n[19] Guidance-AI. Guidance: A guidance language for controlling large\nlanguage models. https://github.com/guidance-ai/guidance, 2025.\nAccessed: 2025-04-16.\n[20] N. Gupta, H. Narasimhan, W. Jitkrittum, A. S. Rawat, A. K. Menon,\nand S. Kumar. Language model cascades: Token-level uncertainty and\nbeyond. In Proc. ICLR, 2024.\n[21] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case\nof neural text degeneration. In Proc. ICLR, 2020.\n[22] Hugging-Face. Text generation inference. https://github.com/huggi\nngface/text-generation-inference, 2023. Large Language Model Text\nGeneration Inference Server in Rust and Python.\n[23] W. Jiang, S. Subramanian, C. Graves, G. Alonso, A. Yazdanbakhsh, and\nV. Dadu. RAGO: Systematic performance optimization for retrieval-\naugmented generation serving. abs/2503.14649, 2025.\n[24] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts, and\nM. Zaharia. Demonstrate-search-predict: Composing retrieval and\nlanguage models for knowledge-intensive NLP. CoRR, abs/2212.14024,\n2022.\n[25] S. Kim, S. Moon, R. Tabrizi, N. Lee, M. W. Mahoney, K. Keutzer,\nand A. Gholami.\nAn LLM compiler for parallel function calling.\nabs/2312.04511, 2023.\n[26] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein.\nA watermark for large language models. In Proc. ICML, 2023.\n[27] M. Kuchnik, V. Smith, and G. Amvrosiadis. Validating large language\nmodels with ReLM. In Proc. MLSys, 2023.\n[28] C. Kulkarni, S. Moore, M. Naqvi, T. Zhang, R. Ricci, and R. Stutsman.\nSplinter:bare-metal extensions for multi-tenant low-latency storage.\nIn Proc. USENIX OSDI, 2018.\n[29] A. Kumar, C. Agarwal, S. Srinivas, S. Feizi, and H. Lakkaraju. Certifying\nLLM safety against adversarial prompting. abs/2309.02705, 2023.\n[30] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica. Efficient memory management for large\nlanguage model serving with PagedAttention. In Proc. ACM SOSP,\n2023.\n[31] S. Lee and G. Kim. Recursion of thought: A divide-and-conquer ap-\nproach to multi-context reasoning with language models. In Proc. ACL,\n2023.\n[32] Y. Leviathan, M. Kalman, and Y. Matias. Fast inference from transform-\ners via speculative decoding. In Proc. ICML, 2023.\n[33] C. Lin, Z. Han, C. Zhang, Y. Yang, F. Yang, C. Chen, and L. Qiu. Parrot:\nEfficient serving of LLM-based applications with semantic variable. In\nProc. USENIX OSDI, 2024.\n[34] Linux Foundation. ebpf - extended berkeley packet filter. https:\n//ebpf.io/, 2024. Accessed: 2025-04-17.\n[35] Y. Liu, H. Li, Y. Cheng, S. Ray, Y. Huang, Q. Zhang, K. Du, J. Yao,\nS. Lu, G. Ananthanarayanan, M. Maire, H. Hoffmann, A. Holtzman,\nand J. Jiang. CacheGen: KV cache compression and streaming for fast\nlarge language model serving. In Proc. ACM SIGCOMM, 2024.\n[36] OpenAI. Prompt caching guide. https://platform.openai.com/docs/gui\ndes/prompt-caching. Accessed: 2025-01-16.\n[37] OpenAI. Parallel function calling, 2023. Accessed: 2024-10-26.\n[38] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez. Gorilla: Large\nlanguage model connected with massive apis. In Proc. NeurIPS, 2024.\n[39] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek,\nK. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer infer-\nence. In Proc. MLSys, 2023.\n[40] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving\nlanguage understanding by generative pre-training. 2018.\n[41] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang,\nC. R√©, I. Stoica, and C. Zhang. Flexgen: High-throughput generative\ninference of large language models with a single GPU. In Proc. ICML,\n2023.\n[42] Z. Tang, X. Shen, C. Li, J. Ge, L. Huang, Z. Zhu, and B. Luo. Ast-trans:\nCode summarization with efficient tree-structured attention. In Proc.\nACM/IEEE ICSE, 2022.\n[43] T. H. Trinh, Y. Wu, Q. V. Le, H. He, and T. Luong. Solving olympiad\ngeometry without human demonstrations. 2024.\n[44] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,\nX. Chen, Y. Lin, et al. A survey on large language model based au-\ntonomous agents. Frontiers of Computer Science, 2024.\n[45] X. Wang, Y. Chen, L. Yuan, Y. Zhang, Y. Li, H. Peng, and H. Ji. Executable\ncode actions elicit better LLM agents. In Proc. ICML, 2024.\nHOTOS ‚Äô25, May 14‚Äì16, 2025, Banff, AB, Canada\nGim et al.\n[46] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,\nX. Zhang, and C. Wang. AutoGen: Enabling next-gen LLM applications\nvia multi-agent conversation framework. abs/2308.08155, 2023.\n[47] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming\nlanguage models with attention sinks. In Proc. ICLR, 2024.\n[48] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan.\nTree of thoughts: Deliberate problem solving with large language\nmodels. In Proc. NeurIPS, 2023.\n[49] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao.\nReAct: Synergizing reasoning and acting in language models. In Proc.\nICLR, 2023.\n[50] M. Zaharia, O. Khattab, L. Chen, J. Q. Davis, H. Miller, C. Potts, J. Zou,\nM. Carbin, J. Frankle, N. Rao, and A. Ghodsi. The shift from models to\ncompound AI systems. https://bair.berkeley.edu/blog/2024/02/18/co\nmpound-ai-systems/, 2024.\n[51] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian,\nC. R√©, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative\ninference of large language models. In Proc. NeurIPS, 2023.\n[52] L. Zheng, L. Yin, Z. Xie, C. Sun, J. Huang, C. H. Yu, S. Cao, C. Kozyrakis,\nI. Stoica, J. E. Gonzalez, C. W. Barrett, and Y. Sheng. SGLang: Efficient\nexecution of structured language model programs. In Proc. NeurIPS,\n2024.\n",
    "content": "# Interpreting the Paper \"Serve Programs, Not Prompts\"\n\n## 1. Core Content and Key Contributions\n\nThis paper proposes a novel **paradigm for large language model (LLM) service architecture**: shifting from \"serving prompts\" to \"serving programs.\" The authors argue that current mainstream LLM serving systems (e.g., vLLM, TGI) were originally designed for efficient text completion tasks, centered around a static \"prompt ‚Üí response\" pattern, which is increasingly inadequate for complex AI applications.\n\nTo address this limitation, the paper introduces the concept of **LLM Inference Programs (LIPs)** and implements a prototype system called **Symphony** based on this new architecture. LIPs are lightweight, user-defined programs capable of directly controlling, on the server side, the generation logic of LLMs, KV cache management, and external tool calls.\n\n### Key contributions include:\n\n- **Proposing a new \"Program-as-a-Service\" (PaaS) paradigm**: Upgrading the fundamental unit of LLM services from *prompts* to *programs*, granting developers fine-grained control over the generation process.\n- **Defining three core properties of LIPs**:\n  1. Decoupling generation logic from model computation;\n  2. Application-level management of model states (e.g., KV cache);\n  3. Integration with external interactions (e.g., function calling).\n- **Designing and implementing the Symphony system**: Drawing inspiration from operating system abstractions (such as system calls, file systems, and process/thread scheduling), it constructs an LLM service framework supporting LIP execution.\n- **Demonstrating performance advantages through experiments**: In RAG scenarios, using LIPs to implement intelligent KV caching strategies achieves up to a 7x throughput improvement compared to vLLM and TGI.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nThe core breakthrough of this paper lies in **fundamentally redefining the responsibility boundaries and programming model of LLM serving systems**, featuring several significant innovations:\n\n### (1) **Delegating KV Cache Management to the Application Layer**\n\nTraditional systems use unified policies like LRU to manage KV caches but lack understanding of application semantics. Symphony introduces **KVFS (KV Cache File System)**, virtualizing the KV cache as persistent file-like resources, enabling LIPs to perform operations such as `open`, `fork`, `merge`, and `extract`‚Äîjust like regular files. This allows developers to design optimal cache reuse strategies tailored to specific use cases (e.g., multi-turn conversations, Tree-of-Thought reasoning), significantly improving resource utilization and response speed.\n\n> ‚úÖ Innovation: First to expose KV cache as a first-class programmable resource to user programs.\n\n### (2) **Externalized Generation Logic and Programmable Sampling**\n\nExisting systems hardcode the autoregressive generation loop within the service, allowing users only to adjust parameters like temperature or top-p. Symphony provides the `pred()` system call, returning full token distributions, enabling LIPs to implement advanced generation strategies autonomously, such as:\n- Structured output (JSON/regex constraints)\n- Parallel generation (Tree-of-Thought)\n- Speculative decoding\n- Dynamic termination conditions\n\n> ‚úÖ Innovation: Opens the \"generation control flow\" as a programmable interface, breaking the black-box generation model.\n\n### (3) **Built-in Execution Environment Eliminates Client Mediation Overhead**\n\nFor function calling or multi-agent communication, traditional approaches require frequent round-trips to the client for parsing and execution, causing significant latency. Symphony allows LIPs to initiate HTTP requests or execute Python code snippets (within sandboxes) directly on the server, without leaving the service context.\n\n> ‚úÖ Innovation: Enables integrated \"reasoning + action\" execution, reducing end-to-end latency and enhancing efficiency for multi-step tasks.\n\n### (4) **Operating System-Inspired Resource Scheduling Architecture**\n\nSymphony treats each LIP as a \"process,\" using POSIX thread APIs to support concurrent generation branches. A two-level scheduler (CPU thread scheduling + GPU batch scheduling) coordinates heterogeneous workloads, maintaining high GPU utilization while supporting complex control flows.\n\n> ‚úÖ Innovation: Applies OS design principles to LLM serving, pioneering an early form of an \"LLM Operating System.\"\n\n---\n\n## 3. Startup Project Ideas Based on This Paper\n\nBuilding upon Symphony‚Äôs vision of \"programmable LLM services,\" several high-value startup opportunities emerge. Below are promising entrepreneurial concepts:\n\n---\n\n### üöÄ Project One: **LIPHub ‚Äî Open-Source LIP Marketplace & Collaboration Platform**\n\n#### Positioning  \nCreate the world‚Äôs first open-source community and distribution platform for LLM Inference Programs (LIPs)‚Äîa \"GitHub for AI Generation Logic.\"\n\n#### Core Features  \n- LIP template library (e.g., legal Q&A caching strategies, parallel math solvers, medical diagnosis decision trees)  \n- Version control, performance benchmarking, and security scanning for LIPs  \n- One-click deployment to private/public Symphony clusters  \n- Developer ecosystem with plugin extensions (connectors, DSL compilers)\n\n#### Business Model  \n- Enterprise-grade private repositories + SaaS hosting  \n- Commission on commercial LIP transactions  \n- Partnership with cloud providers to offer certified LIP runtimes\n\n#### Why It Works?  \nAs AI applications grow in complexity, enterprises demand more than parameter tuning‚Äîthey need reusable \"generation logic assets.\" LIPHub can become the next-generation knowledge repository for AI engineering.\n\n---\n\n### üåê Project Two: **GenOS ‚Äî Programmable LLM Cloud Service Platform**\n\n#### Positioning  \nNext-generation high-performance LLM PaaS, optimized for complex AI workflows‚Äîcomparable to AWS SageMaker but focused on programmable generation logic.\n\n#### Core Capabilities  \n- Hosted Symphony-compatible runtime supporting user-uploaded custom LIPs  \n- Visual LIP IDE (drag-and-drop generation workflow builder)  \n- Built-in optimization modules: KV cache warm-up, automatic parallelization, retry strategies  \n- Intelligent scheduler: dynamically adjusts batching based on LIP behavior\n\n#### Target Customers  \n- Tech companies building autonomous agent systems (autonomous driving, financial advisory)  \n- SaaS providers requiring low-latency, high-concurrency services (chatbots, code completion)  \n- Research institutions developing novel inference algorithms\n\n#### Competitive Edge  \nDeep integration of hardware (GPU) and software (LIP scheduling), creating dual barriers in both *performance* and *flexibility*.\n\n---\n\n### üîí Project Three: **SafeLIP ‚Äî LIP Security Sandbox & Compliance Audit Tool**\n\n#### Pain Point  \nAllowing arbitrary user code execution poses major risks (resource abuse, model leakage, jailbreak attacks). Securely executing untrusted LIPs is critical for real-world adoption.\n\n#### Solution  \nDevelop a dedicated secure execution environment:\n- Lightweight isolation via WASM/seL4 micro-virtual machines  \n- Fine-grained resource quotas (GPU time, memory, network egress)  \n- Real-time behavioral monitoring and anomaly detection (e.g., attempts to extract weight distributions)  \n- Automated compliance reporting aligned with GDPR/SOC2 standards\n\n#### Product Forms  \n- Embedded SDK security runtime  \n- Standalone audit gateway (sidecar mode)  \n- Cloud-native policy management center\n\n#### Market Opportunity  \nFinancial institutions, government agencies, and healthcare sectors have stringent requirements for AI safety and control, and are willing to pay a premium for trusted execution.\n\n---\n\n### üìà Summary: Startup Opportunity Matrix\n\n| Project Name | Technology Layer | Target Market | Key Success Factors |\n|--------------|------------------|---------------|----------------------|\n| **LIPHub**   | Ecosystem Platform | Developer Community | Community engagement, standardization |\n| **GenOS**    | Infrastructure     | Enterprise Clients | Performance optimization, cloud integration |\n| **SafeLIP**  | Security Middleware | Compliance-Sensitive Industries | Security assurance, certification |\n\n> üí° Together, these projects form a new ecosystem around *programmable LLM services*, potentially becoming the next technological wave after Prompt Engineering. **Future AI competitiveness will depend not just on models themselves, but on who masters the most efficient ways to organize generation logic.**",
    "github": "",
    "hf": ""
}