{
    "id": "2509.26534",
    "title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework",
    "summary": "This article reconsiders the lifecycle management of AI data centers, covering three stages: construction, hardware updates, and operations, and proposes an integrated framework to reduce total cost of ownership.",
    "abstract": "The rapid rise of large language models (LLMs) has been driving an enormous demand for AI inference infrastructure, mainly powered by high-end GPUs. While these accelerators offer immense computational power, they incur high capital and operational costs due to frequent upgrades, dense power consumption, and cooling demands, making total cost of ownership (TCO) for AI datacenters a critical concern for cloud providers. Unfortunately, traditional datacenter lifecycle management (designed for general-purpose workloads) struggles to keep pace with AI's fast-evolving models, rising resource needs, and diverse hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme across three stages: building, hardware refresh, and operation. We show how design choices in power, cooling, and networking provisioning impact long-term TCO. We also explore refresh strategies aligned with hardware trends. Finally, we use operation software optimizations to reduce cost. While these optimizations at each stage yield benefits, unlocking the full potential requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle management framework that coordinates and co-optimizes decisions across all three stages, accounting for workload dynamics, hardware evolution, and system aging. Our system reduces the TCO by up to 40\\% over traditional approaches. Using our framework we provide guidelines on how to manage AI datacenter lifecycle for the future.",
    "category1": "Application Implementation",
    "category2": "Software Engineering",
    "category3": "Non-Agent",
    "authors": "Jovan Stojkovic,Chaojie Zhang,Íñigo Goiri,Ricardo Bianchini",
    "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Hardware Architecture (cs.AR)",
        "Distributed, Parallel, and Cluster Computing (cs.DC)"
    ],
    "comments": "",
    "keypoint": "The paper introduces a holistic lifecycle management framework for AI datacenters that reduces TCO by up to 40% compared to traditional approaches.\nThe framework rethinks the AI datacenter lifecycle across three stages: building, hardware refresh, and operation.\nDesign choices in power, cooling, and networking provisioning significantly impact long-term TCO.\nFlatter power delivery architectures reduce stranded power and lower TCO by enabling better utilization across larger domains.\nHybrid cooling (liquid for AI racks, air for others) achieves a 9% TCO reduction over full lifecycle compared to traditional air cooling.\nHierarchical networking (NVLink intra-server, InfiniBand intra-rack, Ethernet inter-rack) reduces TCO by 6% compared to flat high-performance networks.\nAI hardware refresh should be flexible rather than fixed-cycle, adapting to performance gains and cost-efficiency of new generations.\nSkipping intermediate GPU generations (e.g., B100/B200) can be more cost-effective when efficiency gains are marginal.\nExtending the lifespan of older GPUs is beneficial when newer models are small or sparse, allowing reuse for less demanding workloads.\nSoftware optimizations such as model migration smoothing, quantization, KV-cache management, and disaggregation reduce operational costs.\nDisaggregating prefill and decode phases across different hardware improves efficiency and extends useful life of heterogeneous fleets.\nHeterogeneity-aware scheduling maps workloads to optimal hardware generations, deferring refresh costs.\nInfrastructure-aware scheduling exploits headroom in power and cooling envelopes to improve infrastructure efficiency.\nStage-specific optimizations reduce TCO by 15% (build), 23% (IT provisioning), and 19% (operation).\nCross-stage coordination yields up to 40% TCO reduction, surpassing the sum of individual stage optimizations.\nThe framework uses workload trends, hardware roadmaps, and cost models to project future scenarios and guide decision-making.\nMonte Carlo simulations capture uncertainty in demand growth, model scaling, hardware availability, and cost projections.\nFuture model scaling trends (linear, sub-linear, exponential) influence optimal build, refresh, and operation strategies.\nFor fast-growing models, aggressive software optimizations and high-capacity infrastructure are justified.\nFor slow-growing models, cost-effective infrastructure (e.g., InfiniBand) and extended hardware lifetimes are optimal.\nEmerging opportunities include co-designing accelerators for long-term compatibility with existing infrastructure.\nRack-level provisioning with mixed-generation accelerators can reduce interconnect bottlenecks and power fragmentation.\nAccelerators combining high-power SMs for prefill and PIM units for decode can improve phase-specific efficiency.\nCross-stage planning should anticipate shifts in compute-memory balance due to new architectures like MoEs or SSMs.",
    "date": "2025-10-03",
    "paper": "Rearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\nJovan Stojkovic\nUniversity of Illinois\nUrbana-Champaign, USA\njovans2@illinois.edu\nChaojie Zhang\nMicrosoft Azure Research\nRedmond, USA\nchaojiezhang@microsoft.com\nÍñigo Goiri\nMicrosoft Azure Research\nRedmond, USA\ninigog@microsoft.com\nRicardo Bianchini\nMicrosoft Azure\nRedmond, USA\nricardob@microsoft.com\nAbstract\nThe rapid rise of large language models (LLMs) has driven\nan enormous demand for AI inference infrastructure, mainly\npowered by high-end GPUs. While these accelerators offer\nimmense computational power, they incur high capital and\noperational costs due to frequent upgrades, dense power\nconsumption, and cooling demands, making total cost of\nownership (TCO) for AI datacenters a critical concern for\ncloud providers.\nUnfortunately, traditional datacenter lifecycle manage-\nment (designed for general-purpose workloads) struggles to\nkeep pace with AI’s fast-evolving models, rising resource\nneeds, and diverse hardware profiles. In this paper, we re-\nthink the AI datacenter lifecycle scheme across three stages:\nbuilding, hardware refresh, and operation. We show how de-\nsign choices in power, cooling, and networking provisioning\nimpact long-term TCO. We also explore refresh strategies\naligned with hardware trends. Finally, we use operation soft-\nware optimizations to reduce cost.\nWhile these optimizations at each stage yield benefits, un-\nlocking the full potential requires rethinking the entire lifecy-\ncle. Thus, we present a holistic lifecycle management frame-\nwork that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware\nevolution, and system aging. Our system reduces the TCO\nby up to 40% over traditional approaches. Using our frame-\nwork we provide guidelines on how to manage AI datacenter\nlifecycle for the future.\n1\nIntroduction\nGenerative LLMs are reshaping industries, from education [9]\nand healthcare [90] to software development [28] and sci-\nentific research [117]. Their rapid adoption is driven by the\nability to perform complex reasoning, summarization, and\ninteractive tasks with minimal supervision, creating unprece-\ndented demand for scalable AI inference infrastructure [58].\nModern LLM inference relies on high-end GPUs (e.g.,\nNVIDIA’s A100 [77] and H100 [76]) which offer strong per-\nformance but come with steep financial and infrastructure\ncosts. For example, a single NVIDIA DGX H100 server can ex-\nceed $200,000 [18] and draw up to 10.2kW [89, 100], pushing\ndemands for power and cooling capacity far beyond those\nof traditional CPU servers [75]. To support these workloads,\ncloud providers have built specialized datacenters for high-\nthroughput inference, making AI-serving one of the most\nresource-intensive and costly datacenter operations [74].\nResearchers have proposed software and hardware tech-\nniques that improve the performance [2, 19, 57, 71, 89, 113,\n119, 121] or energy-efficiency [93, 97, 99, 100] of LLM infer-\nence clusters. However, for providers, the key challenge is\nminimizing the TCO over the datacenter lifecycle, spanning\nCapEx (e.g., infrastructure build-out) and OpEx (e.g., energy)\nwhile meeting performance expectations from their users.\nTraditional datacenter practices (e.g., regular refresh cy-\ncles [41] and conservative provisioning [46, 61, 98, 115]) fall\nshort for AI workloads, where models grow rapidly in com-\nplexity and scale [24], hardware has higher cost and infras-\ntructure demands [18, 50], and inference is highly sensitive\nto latency and model quality [99, 100].\nOur Work. To address this challenge, we first break down\ndatacenter lifecycle into stages: build, IT provisioning, and op-\neration, each with distinct costs and optimization opportuni-\nties. The build stage covers initial infrastructure setup, includ-\ning provisioning decisions about power topology (e.g., flat [7]\nvs. hierarchical [26, 115]), cooling technology (e.g., air vs. liq-\nuid [32, 43]), and network configurations (e.g., NVLink [85]\nvs. Ethernet). The IT provisioning stage governs when and\nhow to decommission and upgrade hardware. The operation\nstage focuses on runtime management: workload placement,\nscheduling, and software optimizations.\nWe introduce a framework that rethinks and rearchitects\nthe entire lifecycle for AI datacenters, by exploring alter-\nnative strategies across all stages and identifies the most\ncost-effective combination. In build, we compare emerging\ninfrastructure designs to understand and balance long-term\nscalability, efficiency, and performance. In IT provisioning,\nwe evaluate when to adopt new hardware and retire old\nsystems, assessing the impact on TCO given AI’s distinct\nmodel and hardware characteristics. In operation, we assess\nthe impact of software techniques (e.g., model migration,\nLLM inference disaggregation, and workload scheduling)\nover lifecycle TCO.\n1\narXiv:2509.26534v1  [cs.AI]  30 Sep 2025\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nFigure 1. Hosting AI workloads from models to hardware\nand supporting datacenter infrastructure.\nThese stages are interdependent and our framework cap-\ntures cross-stage interactions to support lifecycle-aware de-\ncisions. Leveraging workload growth trends, hardware road\nmaps, and cost models, it projects future scenarios and se-\nlects strategies that satisfy performance, fault-tolerance, and\naccuracy requirements. For example, investing in a larger\npower-sharing domain is more expensive at build time but\nprovides flexibility for future IT provisioning and improves\nutilization during operation.\nWe build our TCO model using open-source LLMs, public\nhardware specs, and detailed cost data from public sources.\nStage-specific optimizations reduce TCO by 15% (build), 23%\n(IT provisioning), and 19% (operation). Our cross-stage strat-\negy achieves up to a 40% reduction. Looking ahead, we iden-\ntify emerging cross-stage opportunities and provide guide-\nlines for adapting AI datacenter lifecycle management to\nfuture model and hardware trajectories.\nSummary. This paper makes three main contributions:\n• Characterization of LLM workload and GPU performance-\npower interactions across datacenter lifecycle stages.\n• Evaluation of stage-wise strategies that improve efficiency\nand cross-generation compatibility.\n• The first-of-its-kind framework for lifecycle-aware, cross-\nstage optimization of AI datacenters.\n2\nHosting AI Workloads\nFigure 1 shows the stack required to host AI workloads\nwithin a cloud provider: from datacenter infrastructure and\nspecialized hardware to the workloads. To minimize the TCO\nof AI datacenters, we start at the top of this stack by ana-\nlyzing AI workloads and reasoning about the demands they\nplace on the underlying hardware and infrastructure.\n2.1\nAI Workloads\nNowadays, cloud providers host a wide range of AI work-\nloads, spanning large language models (LLMs), vision and\nmultimodal models, speech, recommendation systems, and\nclassical deep neural networks (DNNs) [16, 42, 55]. These\n2016\n2018\n2020\n2022\n2024\nYear\n1M\n1B\n1T\nModel Parameters (log scale)\n+151.6%/yr\n+139.4%/yr\n+158.0%/yr\nDeepSeek\nGPT-3\nBERT\nYOLO\nP99\nP50\nAVG\nFigure 2. The P50, P99, and average size of the most popular\nAI models published in the last decade.\nworkloads differ substantially in compute complexity, mem-\nory footprint, performance and accuracy targets, and in-\nput modalities. The largest difference is between training\nand inference workloads: training demands high-bandwidth\nmemory, fast interconnects, and fault-tolerant checkpoint-\ning, while inference workloads range from latency-sensitive,\nmemory-bound LLMs at small batch sizes to throughput-\noriented vision and recommendation pipelines.\nIn this paper, we focus on LLM inference, which is rapidly\nbecoming the dominant workload in AI datacenters [16, 55,\n88, 89]. For this workload, the most critical factors for data-\ncenter build and provisioning are the size and architecture\nof the models (which drive compute and memory needs) and\nthe user demand (the load that the system must sustain).\nAI Model Trends. These workloads have rapidly evolved\nin scale, architecture, and demand over the past decade.\nScale. Models have grown dramatically in size, demanding\nfar more compute, memory, and interconnect bandwidth.\nLarger models drive higher FLOP requirements for inference\nand require larger on-device memory with greater band-\nwidth to store weights and intermediate results. Once models\nexceed the capacity of a single GPU, multi-GPU setups be-\ncome necessary, connected by high-bandwidth, low-latency\nlinks to exchange activations efficiently. Figure 2 shows this\ntrend: model sizes have increased exponentially over the past\ndecade, from Google Neural Machine Translation’s ≈200M\nparameters in 2016 [110], to GPT-3’s 175B parameters in\n2020 [10], and most recently to Llama 4 Behemoth, which\nexceeds 2T parameters [69].\nHowever, recent trends indicate a slowdown, with growth\nshifting to linear [25] and potentially becoming sublinear\nor even flat in the medium term. This plateau reflects di-\nminishing returns from traditional LLM scaling and reduced\ngains in training efficiency. Several emerging approaches\nare attracting attention as alternatives to pure model scaling.\nFor example, distillation transfers knowledge into smaller,\nmore efficient models [112]; and reasoning models leverage\ntime-test-compute to enhance capabilities without relying\non parameter growth [13, 39].\nArchitecture. Model architecture largely determines how much\ncomputation and memory bandwidth an inference workload\n2\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\nneeds. Transformer-based models dominate current deploy-\nments [104]. Their attention layers scale quadratically in\ncompute and memory with sequence length, while large-\nkernel matrix multiplications (GEMM) demand high FLOP\nthroughput and sustained memory bandwidth. Emerging\nalternatives like state-space models (SSMs) [38] replace at-\ntention with convolution-like operations, reducing memory\nfootprint and improving scalability for long contexts.\nMixture-of-Experts (MoE) [95] activate only a subset of\nexperts per query, lowering the average compute cost but\nincreasing the memory and network demands due to ex-\npert sharding. Complementing these designs, fine-tuning\ntechniques such as low-rank adaptation (LoRA) [47] update\nonly a small subset of parameters, reducing memory and\nbandwidth needs for model updates.\nDespite architectural differences, the core computation\nacross modern AI models reduces to large matrix multipli-\ncations. This commonality enables a unified performance\nmodeling framework for AI inference—unlike traditional\ngeneral-purpose datacenters, where heterogeneous work-\nloads (e.g., databases, web services, and scientific applica-\ntions) made unified modeling impractical. Even as emerging\napplications adopt agentic systems that orchestrate multi-\nple LLMs to solve complex, multi-step tasks [1, 12], they\nstill rely on foundational models for each component stage,\npreserving a consistent underlying computational structure.\nUser Demand. The global AI market is projected to grow\nfrom $638B in 2024 to over $3.68T by 2034 [123], with U.S.\ngenerative AI expected to see a 36.3% CAGR through 2030 [33].\nThis growth is driving increased inference workloads, which\nalready dominate AI operational costs [88]. Unlike training,\ninference incurs higher cumulative costs due to continuous,\nlarge-scale deployment serving millions of queries daily [58].\nCloud providers like Microsoft, Amazon, and Google report\n15–25% year-over-year growth in AI workloads [56, 91, 108],\nreflecting rising user demand and the shift toward scalable,\ncost-efficient inference infrastructure.\n2.2\nHardware for AI Workloads\nAccelerators. AI inference workloads are both compute-\nand memory-intensive, relying heavily on GEMM and atten-\ntion mechanisms. These kernels require high floating-point\nthroughput and high-bandwidth memory (HBM) to sustain\nperformance. Modern accelerators (e.g., GPUs [4, 76, 77],\nTPUs [55], and NPUs [73, 111]) combine massive parallelism\nwith optimized memory systems that offer both large capac-\nity and sufficient bandwidth to feed compute efficiently.\nInterconnects. AI servers rely on high-speed interconnects\nto fully utilize accelerators. These links—electrical or opti-\ncal, over copper, active optical cables, or co-packaged op-\ntics—differ in bandwidth, latency, and energy efficiency. Intra-\nnode connections (PCIe, NVLink [85]) enable fast device\ncommunication within a server, while inter-node networks\n(InfiniBand, RoCE [86]) are critical for scaling large models\nacross servers. Efficient collective operations (e.g., all-reduce,\nall-to-all) are essential to synchronize activations, gradients,\nand KV-cache data without stalling computation [30, 81].\n2.3\nDatacenter Infrastructure for AI Hardware\nThe high performance of modern AI accelerators comes with\nhigh demands on supporting infrastructure. Power, cooling,\nand networking requirements for large-scale AI deployments\nfar exceed those of traditional datacenters.\nPower. High-end accelerators consume hundreds of watts\nper device. For example, an NVIDIA DGX H100 server with\neight GPUs has a thermal design power (TDP) of 10.2kW [76].\nAs a result, rack densities can range from several kilowatts\nto over 100 kW per rack [17]. Sustaining such loads requires\nrobust power delivery systems, including high-capacity un-\ninterruptible power supplies (UPS) and power distribution\nunit (PDU) topologies, often with oversubscription to bal-\nance utilization and provisioning costs [115]. In addition,\nrapid workload fluctuations can induce large transient cur-\nrents [14, 62], requiring careful electrical design and moni-\ntoring to maintain stability.\nCooling. The heat output of dense AI clusters quickly ex-\nceeds the practical limits of traditional air cooling [99]. To\nmaintain performance and reliability, operators increasingly\nadopt advanced cooling solutions such as rear-door heat ex-\nchangers, direct-to-chip liquid cooling, and, in ultra-dense\ndeployments, full liquid cooling [79]. While these technolo-\ngies enable sustained operation, they also require specialized\nfacility layouts, coolant distribution systems, and higher\nmaintenance complexity.\nNetworking. Large-scale AI inference also stresses network-\ning infrastructure. Models using tensor, pipeline, or expert\nparallelism rely on low-latency, high-bandwidth communi-\ncation between thousands of accelerators [55]. This drives\nadoption of specialized network topologies (e.g., fat-tree,\ndragonfly) and high-performance interconnect technologies\nsuch as InfiniBand and RoCE [86], with precise bandwidth\nprovisioning to prevent collective communication from be-\ncoming the bottleneck. The capital cost of network switches,\noptical modules, and cabling for such deployments can rep-\nresent a significant share of overall system expenditure.\nIn combination, these infrastructure requirements make\nAI datacenter builds substantially more complex and capital-\nintensive than traditional deployments.\n3\nDatacenter Lifecycle\nBased on these AI workloads, hardware, and infrastructure\ntrends, we examine the datacenter lifecycle to identify op-\nportunities for reducing TCO. Table 1 breaks the lifecycle\ninto: build, IT provisioning, and operate. These stages help us\nexplore how traditional lifecycle policies must be revisited\nto address the scale, density, and performance demands of\n3\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nStage\nDescription\nTimeline\nBuild\nSite selection and facility construction\n15–30 years\nIT provision\nIT hardware deployment and upgrades\n4–6 years\nOperate\nWorkload scheduling, resource management\nPer inference\nTable 1. Lifecycle stages for datacenter infrastructure.\nAI. On this foundation, we develop a TCO model to evaluate\ncosts across design choices over the datacenter’s lifetime.\n3.1\nStages of a Datacenter Lifecycle\nBuild. This stage is where the datacenter facility itself is de-\nsigned and constructed. Decisions made here set long-lived\nconstraints on utility capacity, substation feeds, power distri-\nbution topology (e.g., flat vs. hierarchical), cooling technology\n(air vs. liquid), floor space, and network fabric. These choices\ndetermine the maximum achievable rack density, define fault\ndomains, and affect how easily the facility can accommo-\ndate future upgrades such as liquid cooling or higher-voltage\npower buses. Networking decisions (e.g., Ethernet vs. Infini-\nBand, optical link reach, and oversubscription ratios) impact\njob scaling efficiency and the cost of east–west traffic, which\nis critical for large AI models distributed across accelerators.\nIT provisioning. This stage defines when and how to intro-\nduce new accelerators and retire or repurpose older ones.\nThese decisions balance several factors, including performance-\nper-watt improvements from newer hardware, cost of newer\nhardware, software maturity and compatibility, depreciation\nschedules, and risk of underutilized power or cooling. IT\nprovisioning may involve mixed-generation GPU pools with\nplacement constraints or repurposing older GPUs to give\nthem a “second life” on workloads with lower performance\nrequirements (e.g., fine-tuning or batch analytics).\nOperate. In this stage, decisions focus on where to place\nmodel instances, how to schedule queries to instances, and\nhow to execute queries efficiently. Placement strategies ac-\ncount for hardware heterogeneity, ensuring that each model\nruns on the accelerator generation that provides the best\nperformance-to-cost tradeoff. Scheduling considers factors\nsuch as service-level objectives (SLOs), model routing based\non query complexity, and price-aware policies that shift flex-\nible workloads across time or geography. Execution lever-\nages AI-specific optimizations, including dynamic batching,\nquantization, speculative decoding, distillation, and model\ndisaggregation, to reduce cost per query while meeting SLOs.\nTraditional Approach. Table 2 outlines the lifecycle of\ngeneral-purpose datacenters, which are primarily powered\nby CPUs and designed to support diverse workloads.\nBuild. Traditional datacenters rely on a conservative, uni-\nform infrastructure. Power distribution usually follows a\nhierarchical topology: from the colo-level to rows, and then\nto individual racks, with each level having its own power\nStage\nTraditional Approach Characteristics\nBuild\nHierarchical power distrib; Air cooling; Ethernet network.\nIT provision\nFixed per-server lifecycle; New server generations released\nevery 2–3 years; Gradual replacement.\nOperate\nServices tied to fixed hardware configurations; Instances\nmigrated to new hardware when released; Legacy applica-\ntions remain on old servers.\nTable 2. Overview of the traditional approach to manage\nthe lifecycle of a traditional datacenter for general-purpose\nserving CPU-based workloads.\ncaps [115]. Cooling is primarily air-based, and networking\nis implemented with standard Ethernet [8, 34].\nIT provisioning. Servers follow a fixed lifecycle, with each\ngeneration operating for a defined period before decommis-\nsioning. New hardware is typically released every 2–3 years,\nand operators replace older servers in a phased manner ac-\ncording to this schedule.\nOperate. Services run on servers with a specific hardware con-\nfiguration. When new hardware is introduced, new services\nmigrate to the latest generation, while legacy applications\nremain on older servers. This approach prioritizes stability\nand predictability but reduces flexibility to exploit hardware\nheterogeneity or tailor performance to specific workloads.\nRearchitecting for AI. As shown in Section 2, AI workloads\nchallenge traditional datacenter design. Modern accelerators\nhave higher power and thermal demands, increasing the\nvalue of liquid cooling and high-density racks. Space and\ndensity constraints needs favor scale-up architectures like\nNVLink Switch-based designs [85].\nMemory capacity and bandwidth have grown to support\nlarger model contexts, enabling more complex workloads but\nraising per-node costs. Efficient memory provisioning is es-\nsential. Similarly, interconnect performance across servers is\ncritical for parallel efficiency (low-bandwidth or high-latency\nlinks can severely limit scaling).\nTrade-offs between cost and performance must be eval-\nuated both within each stage (build, IT provisioning, and\noperate) and across them. For example, separating prefill\nand decode across servers [89] enables using heterogeneous\nhardware and influences build and refresh strategies.\n3.2\nTotal Cost of Ownership (TCO) Model\nWe use a comprehensive TCO model that enables automated,\nend-to-end lifecycle analysis by capturing workload dynam-\nics, hardware evolution, and system aging. It accounts for\ncosts across lifecycle stages and supports exploration of how\nworkload trends, hardware roadmaps, and infrastructure\ndecisions impact total cost.\nModel overview. Table 3 summarizes the components of\nthe TCO, breaking them down into CapEx and OpEx:\nTCO = CapEx + OpEx\n4\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\nCategory Component\nDescription\nExample Cost ($)\nCapEx\nIT\nServers, racks, accelerators, storage\n$375k/server [18]\nNetworking\nFabric switches, optics, structured cabling\n$2000/server [59, 85]\nBuilding\nSite preparation, building shell, land, electrical and mechanical base infrastructure\n$0.5/ft2 [29]\nPower\nPower infrastructure (Switchgear, transformers, UPS, PDUs, busbars, rack distribution)\n$7.0/W [26, 116]\nCooling\nCooling infrastructure (chillers, CRAH/CRAC units, pumps, piping, liquid loops, airflow)\n$2.5/W [103, 116]\nOpEx\nNetworking\nPort licenses, optics replacement, networking component power\n$600/server [59, 82, 84]\nEnergy\nIT load scaled by PUE, utility tariffs, demand charges\n$20–40/MWh [35]\nMaintenance\nSpares, repairs, monitoring, water/treatment, field-replaceable units, failure-rate\n$5000/server [102]\nSoftware\nLicenses, support contracts\n$200/server\nTable 3. TCO components for an example datacenter with DGX H100 [18] servers.\n0\n5\n10\n15\n20\n25\n30\n35\nCost (Million $ Per Year)\nIT\nNetworking\nBuilding\nPower\nCooling\nEnergy\nMaintenance\nFigure 3. TCO breakdown for a 10MW AI datacenter.\nCapEx =\nCapExFacility+CapExPower+CapExCool+CapExNet+CapExIT\nOpEx = OpExenergy +OpExM&R +OpExnetwork +OpExsoftware\nFor the annualized TCO, CapEx amortizes long-term in-\nfrastructure and IT over their useful lives, OpEx captures\nvariable and recurring operational costs over a full year.\nFigure 3 shows the annual datacenter TCO breakdown,\ndivided into CapEx and OpEx categories for a representative\nuser demand and model size projected for 2025. At 75% aver-\nage utilization, this 10MW datacenter [115] sustains roughly\n500 H100 servers, consuming 70 GWh of energy per year\nfor operation. GPU servers drive IT CapEx and dominate\ncosts, followed by energy-related OpEx. On the other hand,\nbuilding construction and maintenance contribute the least.\nCapital Expenses (CapEx). This is the upfront costs for\nacquiring, building, or upgrading long-term datacenter as-\nsets, including facility, IT equipment (such as servers and\nracks), and networking infrastructure. Facility, network, and\nIT assets are amortized over 15–30, 7–10, and 3–5 years\nrespectively, using straight-line or declining-balance depre-\nciation. CapEx is often normalized per delivered kW or per\naccelerator to facilitate design comparisons.\nIT Infrastructure. It consists of compute servers equipped\nwith CPUs and accelerators such as GPUs, TPUs, or NPUs,\nalong with racks and storage devices like NVMe. Modern\nservers may also include CXL-attached memory to expand\ncapacity. These components provide the core computational\nand storage resources of the datacenter.\nNetworking Infrastructure. It connects servers, storage, and\nother datacenter resources. Fabric switches route traffic be-\ntween racks, while optical transceivers and structured ca-\nbling provide high-bandwidth, low-latency connections across\nthe facility. This infrastructure ensures efficient communica-\ntion for distributed workloads.\nBuilding. It includes the physical infrastructure required to\nsupport a datacenter. These foundational elements ensure a\nsafe and efficient environment for all equipment.\nPower Infrastructure. Electrical systems deliver reliable power\nto racks and IT equipment, including all power devices and\nconnections to ensure stable and redundant power distribu-\ntion throughout the datacenter.\nCooling Infrastructure. Mechanical systems cool datacenters\nto maintain safe operating temperatures, with core infras-\ntructure such as chillers and pumps. Together, these systems\nprevent overheating and enable high-performance operation.\nOperational Expenses (OpEx). This is the ongoing costs of\nrunning and maintaining a datacenter. We model utilization-\nsensitive costs (e.g., energy) based on workload mix and\nscheduling policies, reflecting how activity levels impact\noverall consumption. In contrast, utilization-insensitive costs\n(e.g., fixed maintenance, software contracts, and leases) are\ntreated as per-rack or per-site constants, providing a stable\nbaseline that does not fluctuate with workload intensity.\nNetwork Operations. Covers the ongoing costs of maintaining\nthe datacenter network, including licensing fees for switch\nports and the replacement of failed components. Costs are\ninfluenced by network size, redundancy, and utilization pat-\nterns, as higher traffic and denser topologies can increase\nwear and require more frequent upgrades or maintenance.\nPeak power. Datacenters often pay monthly fees based on\ntheir highest instantaneous power draw. Utilities charge this\nway because the grid must be provisioned for these peaks:\ngenerators, transformers, and transmission lines all need\ncapacity for the maximum load, even if brief. Peaks can also\nthreaten grid stability and require fast-response resources\nlike spinning reserves. By tying costs to peak usage, utilities\nincentivize datacenters to smooth load spikes, easing stress\non the grid and lowering overall infrastructure costs.\nEnergy. They include the electricity consumed by IT equip-\nment as well as the supporting infrastructure (e.g., cooling\nand power distribution systems). These costs, typically billed\non a monthly basis, account for datacenter’s IT hardware\n5\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nutilization and power usage effectiveness (PUE), which mea-\nsures the ratio of total facility energy to energy used by IT\nequipment. A higher PUE indicates more energy spent on\noverheads such as cooling and power conversion.\nMaintenance & Repairs. Includes both preventive and correc-\ntive maintenance of mechanical, electrical, and IT systems.\nCosts are primarily driven by the expected failure rates of\ncomponents such as cooling and power components, servers,\nstorage devices, and networking equipment. This category\nalso covers other maintenance ensuring that the datacenter\nremains operational and reliable over time.\nOther. Recurring expenses such as software licenses, support\ncontracts, and land or lease payments. These costs are largely\nfixed and do not vary with workload or utilization.\n4\nTCO-Driven Lifecycle Framework\nTo evaluate the long-term economics of AI datacenters, we\ndevelop a cross-stage, TCO-driven framework that spans the\n15-year datacenter lifecycle, covering build, IT provisioning,\nand operation. Our framework couples workload dynamics,\nmodel evolution, hardware road maps, and infrastructure\ncosts to project scenarios and assess alternative policies,\nallowing us to identify the best policies both within each\nstage and across stages. We validate our methodology against\nreal-world observations.\n4.1\nModeling Assumptions\nTimeline. We model a lifecycle of 15 years starting from\n2015 and ending in 2030. The methodology generalizes to\nlonger or shorter horizons, but uncertainty increases the fur-\nther we project. For validation, we fit model outputs against\ncurrent-day trends (i.e., 2025), then forecast costs and fleet\ncomposition for the following five years.\nWorkload. We focus on LLM inference as the dominant AI\ndatacenter workload [11]. Training workloads would follow\na similar methodology but differ in modeling, as they are\nheavier in both computation and communication. We use\ninput traces from DynamoLLM [100], which exhibit diurnal\npatterns, and assume a baseline of 100K requests per second.\nFollowing Section 2.1, we apply a 15% annual growth rate [56,\n91, 108], which implies over 200K RPS after five years.\nAI Models. Based on 2015–2025 parameter scaling trends\n(Section 2.1), we assume linear growth in model size through\n2030, with alternative scenarios for accelerated (exponen-\ntial) or slowed (sub-linear) scaling. Providers are assumed to\nadopt new models via smooth migration, gradually transition-\ning workloads rather than abrupt cutovers [45, 80]. We also\nassume future LLMs follow the LLaMA design lineage [67],\ni.e., decoder-only transformer architectures with consistent\nlayer organization, attention mechanisms, and parameteri-\nzation strategies. This assumption reflects the convergence\nof recent frontier models, which primarily differ in scale.\nHardware. Hardware projections include FLOPS, memory\nbandwidth, TDP, and cost, with linear growth trends [44].\nWe also model delays between the announcement of a new\nGPU [78] and its actual mass availability in cloud providers [6,\n15, 94] (e.g., B200 had a delay between 6 months and a year).\nPerformance. We develop a roofline model tailored to LLM\ninference across diverse hardware configurations. Our val-\nidation against known model==hardware pairings shows\nthat it aligns with prior work [40, 114] and with profiling\nresults from real workloads. The model captures how hard-\nware limits (compute throughput and memory bandwidth)\ninteract with workload characteristics (arithmetic intensity\nand memory footprint) during LLM inference).\nOur model derives the arithmetic intensity and memory\nfootprint analytically from the LLM architecture and param-\neter count, making the predictions reproducible, validated\nagainst profiling-based approaches. Extending the model to\nnew GPUs requires only the peak FLOPs and memory band-\nwidth (GB/s) of the device, while applying it to new LLMs\nrequires recomputing theoretical compute and memory re-\nquirements from the model’s structure. The roofline model\npredicts the time-to-first-token (TTFT) and time-between-\ntokens (TBT) latencies for a given hardware, model, and\nrequest load (e.g., H200 running Llama3 at 10 requests per\nsecond will have a TTFT of 200 ms and a TBT of 50 ms).\nWe then increase the load until requests exceed an SLO\nof 400 ms for TTFT and 100 ms for TBT [100]. The resulting\ngoodput is the maximum request rate (RPS) the system can\nsustain without violating the SLO. This approach identifies,\nfor each hardware–model pair, the utilization point at which\nlatency begins to degrade. Using this SLO, we provision the\nminimal number of GPUs required to serve the load and\ncalculate the corresponding GPU utilization.\n4.2\nOptimization Goal\nCost. The framework integrates both CapEx (IT hardware,\nnetworking, building, power, cooling) and OpEx (networking,\nenergy, maintenance, software). CapEx is amortized over\nuseful lifetimes (e.g., 15–30 years for facilities, 3–5 years for\nGPUs), while OpEx captures recurring operational costs. This\nenables evaluation of trade-offs such as upfront investment\nin cooling vs. deferred savings in refresh.\nCarbon. A similar methodology applies to carbon emissions,\nincluding embodied carbon from construction and hardware,\nand operational carbon from electricity use. We leave this\nextension to future work.\n4.3\nLifecycle Evaluation\nBaseline Timeline. Figure 4 shows the simulated deploy-\nment timeline of an AI fleet under the baseline policy, which\nfollows the traditional datacenter lifecycle approach (Table 2).\nThe figure shows how model release cycles and hardware\navailability shape the fleet composition over time, with the\n6\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\n2021\n2024\n2027\n2030\nYear\n0\n10\n20\n30\n40\n50\n# Active Servers (K)\nGPT-3\nSwitch\nMinerva\nPanGu\nDeepSeek\nLlama4\nfuture \nP100\nV100\nA100\nH100\nH200\nB100\nB200\nGPU'27\nGPU'28\nGPU'29\nGPU'30\nGPU'31\nFigure 4. Server count by GPU type over time in an AI fleet\nfollowing the traditional baseline in Table 2. Includes the\nrelease dates of major AI models.\nrelease dates of notable large models marked for reference.\nThe simulation begins in 2015 with 50 P100 GPU servers\nsustaining an initial steady-state load of 100K RPS. At this\npoint, the total annual TCO of the AI datacenter is ≈$0.2M.\nAs user demand and model size grow (i.e., 15% year-over-\nyear), the fleet scales gradually. By 2024, traffic reaches 350K\nRPS, coinciding with the release of DeepSeek V3, a 671B-\nparameter model [21, 64], which triggers a major hardware\nrefresh with H200 GPUs. This refresh increases the total\nserver count to 25K to meet performance targets, and the\nannual TCO rises to ≈$0.3B, reflecting the added hardware,\nexpanded datacenter infrastructure, and higher operating\nexpenses. Peaks in server deployments align with major\nLLM launches, highlighting the close connection between\nAI model roadmaps and datacenter economics.\nSolution Approach. We run Monte Carlo simulations [70]\nto capture uncertainty in workload growth, model scaling,\nhardware availability, and cost projections. Each simulation\nsamples these distributions, yielding a range of outcomes\nfor TCO under different policies (e.g., aggressive vs. delayed\nserver refresh). We use the traditional approach in Table 2\nas a baseline. By comparing distributions, we identify which\ndecisions dominate long-term cost and where flexibility pro-\nvides the greatest value.\n5\nBuilding Efficient AI Infrastructure\nThe first stage in the datacenter lifecycle is building the in-\nfrastructure, which includes: (1) the physical building that\nhouses the datacenter, (2) the power delivery that supplies\nelectricity to servers and other equipment, (3) the cooling\nthat removes the heat generated by servers, and (4) the net-\nworking that interconnects servers.\nThe physical building is relatively static, while power, cool-\ning, and networking requirements [26] must anticipate shifts\nin workload demand and hardware capabilities over decades\nof operation. Emerging AI workloads, with their extreme\npower and thermal densities and distinct communication\npatterns, are already reshaping the design space across these\ndimensions. We revisit these design choices using our TCO-\ndriven framework to build a holistic understanding over the\nfull lifecycle and validate our approach comparing it to the\ndirections already being pursued in practice.\n2016\n2018\n2020\n2022\n2024\nYear\n0\n2\n4\n6\n8\n10\n12\nPer-Server TDP (kW)\nP100\nV100\nA100\nH100\nB100\nMI250\nMI300X\nHaswell\nCascade\nIce Lake\nEmerald\nIntel CPU\nAMD CPU\nNVIDIA GPU\nAMD GPU\nFigure 5. Per-server TDP across Intel and AMD CPUs, and\nNVIDIA and AMD GPUs over the years.\nPower domain\nFeature\nPer-PDU\nPer-UPS\nPer-DC\nCapEx Stranding\nLower\nMedium\nHigher\nComplexity\nHigher\nMedium\nLower\nOpEx\nMaintenance\nHigher\nMedium\nLower\nOther Fault isolation\nExcellent\nGood\nPoor\nTable 4. Comparison of power delivery infrastructure de-\nsigns. Green: good, yellow: moderate, red: poor.\n5.1\nPower Infrastructure\nTraditional Approach. Traditional datacenters adopt a hi-\nerarchical power distribution, balancing fault isolation, main-\ntainability, and power stranding [107, 109, 115]. An Auto-\nmatic Transmission Switch (ATS) directs power from the\ngrid to multiple Uninterruptible Power Supplies (UPS) with\nredundancy. Each UPS supports a fraction of the total load\nand feeds downstream into multiples Power Distribution\nUnits (PDUs), distributing power to server rows and racks.\nServer and rack provisioning must respect capacity limits\nwithin each power sharing domain defined by the hierarchy.\nWhen the budget at a power domain (e.g., per-PDU) is 𝑋\nand each server consumes 𝑌, only ⌊𝑋/𝑌⌋servers can be\ndeployed. The residual capacity, 𝑋−𝑌× ⌊𝑋/𝑌⌋, becomes\nstranded power (i.e., power fragmentation).\nRearchitecting for AI. AI accelerators (e.g., GPUs and\nTPUs) push power density far beyond historical norms. Fig-\nure 5 shows that the TDP of high-end GPU servers has sig-\nnificantly outgrown the power demand of high-end CPU\nservers. For example, while NVIDA DGX H100 server with 8\nGPUs needs 10.2kW [76], Intel Emerald Rapids server with\n64 cores needs only 385W [50].\nWhile this surge exacerbates power fragmentation, flat-\nter power distribution architectures can pool power across\nbroader domains to reduce stranding [7, 92]. However, these\ndesigns involve complicated trade-offs in fault tolerance, de-\nsign and maintenance complexity, and TCO, we introduce\nsimplifications by removing second-order effects and prac-\ntical constraints (e.g. supply chains) and summarize these\ntrade-offs qualitatively in Table 4. For example, while a flat\n7\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nPer-PDU\nPer-UPS\nPer-DC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized TCO\n(a) TCO vs. power infrastructure.\nAir\nLiquid\nHybrid\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) TCO vs. cooling infrastructure.\nEthernet\nInfiniB\nNVLink\nHierar.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIT\nPower\nCooling\nNetworking\nFacility\nEnergy(IT)\nEnergy(non-IT)\nMaintenance\n(c) TCO vs. networking infrastructure.\nFigure 6. TCO vs. infrastructure designs during the build stage.\nFeature\nAir\nHybrid\nLiquid\nCapEx Complexity\nLower\nMedium\nHigher\nOpEx\nEnergy efficiency\nLower\nMedium\nHigher\nMaintenance\nLower\nMedium\nHigher\nOther High-dense racks\nLower\nMedium\nHigher\nNoise level\nHigher\nMedium\nLower\nTable 5. Comparison of cooling infrastructure designs.\nper-DC power delivery minimizes stranded capacity, it de-\nmands greater redundancy and more intricate maintenance\nplanning. Though a flat per-DC power delivery architec-\nture might not be feasible in practice today, our TCO-driven\nanalysis in Figure 6a shows that, with simplified modeling\nof associated costs and hardware requirements, it reduces\nlifecycle TCO by 4.2% compared to conventional hierarchi-\ncal designs [115], motivating new designs enabling a flatter\npower sharing domains in real-world distribution schemes.\n5.2\nCooling Infrastructure\nTraditional Approach. Most datacenters rely on air-based\ncooling systems [20, 23, 48, 66, 99], where chillers or adia-\nbatic cooling units deliver cold air through air handling units\n(AHUs). These AHUs circulate the air via raised floors or hot/-\ncold aisle containment using fans. Cooling capacity is pro-\nvisioned conservatively to meet peak thermal loads. Power\nusage effectiveness (PUE) is optimized through careful air-\nflow management and economization. Modern datacenters\nachieve typical PUE values between 1.1 and 1.3 [3, 31, 72].\nRearchitecting for AI. High-density GPU racks generate\nfar more heat than CPU-based system (often 4–8× higher per\nrack [79]). Meeting these loads requires significantly higher\nairflow rates, lower inlet temperatures, and more fan energy,\npushing air cooling beyond its physical and economic limits.\nAs a result, liquid cooling (e.g., cold plates or immer-\nsion [32, 53]) is increasingly considered for future dense\nGPU deployments [7, 79]. While upfront complexity and\nCapEx are higher, OpEx is reduced through improved heat\ntransfer, reduced chiller load, and lower fan power, sum-\nmarized in Table 5. Hybrid designs, combining liquid for AI\nracks and air for low-density racks, balance cost, density, and\nmaintainability. Specifically, a hybrid design could be a mix\nof 75% cold plates and 25% air cooling [43], corresponding\nto the high and low density loads with a resulting Power Us-\nage Effectiveness (PUE) of 1.15. Our evaluation in Figure 6b\nFeature\nEthernet\nInfiniBand\nNVLink\nHierarchical\nCapEx Cost\nLower\nMedium\nHigher\nMedium\nOpEx\nEnergy\nLower\nMedium\nHigher\nHigher\nMaintain\nLower\nMedium\nHigher\nMedium\nPerf\nBandwidth\nLower\nHigher\nHigher\nHigher\nLatency\nHigher\nLower\nLower\nMedium\nTable 6. Comparison of networking infrastructure designs.\nshows that hybrid cooling yields the lowest TCO with 9%\nreduction over the full lifecycle.\n5.3\nNetworking Infrastructure\nTraditional Approach. General-purpose datacenters use\nmulti-tier Ethernet fabrics (e.g., leaf–spine) with moderate\noversubscription at higher tiers [8, 34]. This design is cost-\neffective for CPU-based workloads with modest inter-node\nbandwidth and latency needs.\nRearchitecting for AI. AI workloads impose far greater\nnetwork demands than general-purpose datacenters. Emerg-\ning practice starts adopting hierarchical designs for AI in-\nference workloads to match communication patterns [60]:\nNVLink [85] for intra-server to support tensor parallelism\n(TP) [96], and lower-cost networks for pipeline parallelism [96],\nwhich exchanges data less frequently and is employed across\nservers. We evaluate four network designs: (1) all-accelerator\nconnectivity over Ethernet, (2) all over InfiniBand [5], (3) all\nover NVLink [85], and (4) a hierarchical approach (NVLink\nwithin a server, InfiniBand within a rack, and Ethernet across\nracks). Table 6 shows different cost–performance trade-offs\nacross networking strategies. Figure 6c shows that the hier-\narchical design delivers the best balance, reducing TCO by\n6% relative to a flat high-performance network to achieve\nthe workload latency requirements. By aligning intercon-\nnect performance with workload communication patterns,\nhierarchical networking ensures scalability without over-\nprovisioning expensive, low-latency links.\n5.4\nLessons\nAI workloads fundamentally challenge legacy datacenter\ndesigns, creating unprecedented demands on power, cool-\ning, and networking. As accelerator power densities increase\nand communication patterns grow more complex, traditional\nhierarchical power distribution, air-based cooling, and uni-\nform network fabrics become increasingly inadequate; both\n8\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\n2016\n2018\n2020\n2022\n2024\nYear\n0\n200\n400\n600\nTFLOPS\nMI250\nMI300X\nA100\nB100\nH100\nP100\nV100\n0\n2\n4\n6\n8\n10\nMem. Bandwidth (TB/s)\nAMD TFLOPS\nNVIDIA TFLOPS\nAMD BW\nNVIDIA BW\nFigure 7. Evolution of AMD and NVIDIA GPUs showing\nTFLOPS (left axis) and memory bandwidth (right) over time.\nin terms of cost efficiency and workload performance. These\nrapidly evolving requirements have spurred a wave of in-\nfrastructure innovations. By evaluating design choices from\nthe ground up using our TCO-driven framework over the\nfull datacenter lifecycle, we show how emerging solutions\n(e.g., flatter power delivery architectures, hybrid cooling sys-\ntems, and hierarchical network designs) can meet long-term\nefficiency and cost objectives.\n6\nProvisioning AI Hardware\nOnce the datacenter infrastructure is built, the next step is\nmanaging the hardware lifecycle: deciding when and how to\nretire outdated components and introduce new ones. Care-\nful refresh planning ensures the system continues to meet\nperformance, efficiency, and scalability demands.\n6.1\nCurrent Hardware AI Trends\nGPU Release Cycle. GPUs from vendors like NVIDIA and\nAMD have evolved significantly. Starting with the NVIDIA\nP100 (Pascal), which offered modest FP16/FP32 throughput,\nmodern GPUs such as the B200 deliver vastly higher perfor-\nmance through advanced tensor cores, increased memory\nbandwidth, and optimizations in sparsity and mixed preci-\nsion. Figure 7 shows the progression of compute capability\n(TFLOPS) and memory bandwidth of NVIDIA datacenter\nGPUs from 2016 to 2025. Compute throughput has increased\nnearly 12×, while memory bandwidth (critical for large-scale\nand sparse model inference) has grown over 5×. These trends\nare even more pronounced for AMD GPUs [4].\nGPU vendors are aggressively releasing new architectures\nevery year (even multiple generations per year) to meet the\nrapidly evolving demands of AI. This pace contrasts the\ntraditional 2–3 year cadence of CPU releases. Similar trends\nhold for other specialized AI hardware such as TPUs [55]\nand LPUs [37], which also follow fast-paced release cycles.\nCost. The cost of GPU servers has also grown substantially.\nFor example, the NVIDIA P100 was priced at around $9K\nper GPU (roughly $90K per server), whereas the modern\nH100 costs about $30K per GPU (over $350K per server) [18].\nAMD’s MI series accelerators follow a similar trend, with\nnewer generations carrying significantly higher price tags. In\ncontrast, CPU server costs have risen more moderately over\n1B\n3B\n8B\n70B 405B\nModel Size\n0\n2\n4\n6\n8\n10\nNormalized TTFT\n1B\n3B\n8B\n70B 405B\nModel Size\nNormalized TBT\nH100\nA100\nV100\nT4\nFigure 8. TTFT and TBT latencies for different sizes of\nLlama-3 LLM across GPU generations normalized to H200.\nthe same period. For example, from around $7K per server\nfor Intel’s Haswell [51] systems in 2014 to approximately\n$12K for the latest Granite Rapids [49] generation in 2025.\nPerformance. We evaluate how the performance of AI infer-\nence workloads with different model sizes and architectures\nscales across GPU generations. We ran experiments using\nreal hardware on vLLM [57] and compare against the roofline\nmodel, which shows within 5% of errors for Llama3 [68] mod-\nels. All runs use a 2K sequence length and batch size of 8.\nWe measure TTFT and TBT in milliseconds, normalized to\nthe H200 baseline per model. We also combine performance\nwith cost and power (i.e., Perf/$ and Perf/Watt).\nModel Size. We evaluate scalability using Llama3 [68] models\nfrom 1B to 405B parameters on NVIDIA GPUs: T4, V100,\nA100, H100, and H200. We configure TP [96] with the small-\nest value that fits each model across most GPUs (e.g., TP1 for\n1B/3B, TP4 for 8B, TP8 for 70B/405B). Some setups fail on\nolder GPUs due to memory limits. Figure 8 shows that older\nGPUs remain viable for small models. The decode phase\n(TBT) is less sensitive to GPU generation than the prefill\nphase, reflecting its lower compute demands [89].\nModel Architecture. We evaluate the impact of model spar-\nsity by comparing sparse Qwen3 models (30B A3B and 235B\nA22B) with dense Llama3 models of similar sizes. Figure 9\nshows that sparse models scale better on older GPUs, main-\ntaining competitive accuracy while outperforming dense\ncounterparts in latency. For example, Qwen3-235B-A22B\nmatches Llama3-70B in accuracy but degrades less on older\nhardware (though it requires nearly twice the GPU mem-\nory). This highlights the value of sparsity-aware designs for\nextending the utility of legacy hardware.\nFigure 10 compares transformer-based (Llama3-3B) with\nstate-space-based (Mamba-2.8B). State-space models are more\nhardware-efficient: for 2K sequences using TP1, Llama3 runs\n7.7× slower on V100 than on H200, while Mamba slows down\nby only 3.6×. This shows the architectural compatibility of\nstate-space models with older or less performant GPUs.\nModel Efficiency. We combine model performance and hard-\nware cost and power usage to evaluate model efficiency on\ndifferent generations of hardware. While the goodput per\n9\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nQ-30B\nQ-235B\nL-3B\nL-8B\nL-70B\nL-405B\n0\n1\n2\nNormalized TTFT\nQ-30B\nQ-235B\nL-3B\nL-8B\nL-70B\nL-405B\n0\n1\n2\nNormalized TBT\nQ-30B\nQ-235B\nL-3B\nL-8B\nL-70B\nL-405B\n0\n20\n40\n60\nAccuracy (%)\nFigure 9. Latencies and accuracies for dense (Llama3) and\nsparse models (Qwen3) across GPU generations normalized\nto H200. Pink and blue bars for H100 and A100, respectively.\nL1B\nL3B\nM0.8B M2.8B\nModel Architecture\n0\n2\n4\n6\n8\n10\nNormalized TTFT\nL1B\nL3B\nM0.8B M2.8B\nModel Architecture\nNormalized TBT\nH100\nA100\nV100\nT4\nFigure 10. Latencies for transformer (Llama3) and state\nspace (Mamba) models across GPUs normalized to H200.\nWatt of large models degrades substantially on older hard-\nware, the gap is much smaller for smaller models. For in-\nstance, running Llama3-70B on an A100 yields roughly 3×\nlower goodput per Watt compared to an H100, whereas run-\nning Llama-1B on an A100 is only about 18% lower. In terms\nof goodput per Watt per dollar, the A100 actually outper-\nforms the H100 by 8–23% for the 1B, 3B, and 8B models, but\ndelivers about 2× lower efficiency for Llama-70B. In fact,\nfor smaller models, even the V100 is on par with the H100,\nachieving performance per Watt per dollar within 5% of it.\n6.2\nRefresh Policies\nTraditional Approach. General-purpose datacenters typi-\ncally follow a steady CPU refresh cycle, with servers remain-\ning in service for about five years before replacement [106].\nThis baseline policy strikes a balance between capital and\noperational efficiency.\nAlternative strategies include extending server lifetimes\nto reduce CapEx (at the expense of higher energy use and\nmaintenance) or shortening lifetimes to deploy more efficient\nhardware sooner, increasing capital costs but improving en-\nergy and space efficiency. Skipping intermediate generations\nis another option when current hardware meets workload\nneeds and newer gains are marginal.\nWe use our framework to evaluate the policies to provi-\nsion and refresh the servers in an AI datacenter. Figure 11a\nshows the TCO distribution of all feasible refresh policies\nnormalized to baseline policy value. Most of the policies lie\non the right side of the distribution, indicating the 5-year\n1.0\n1.2\n1.4\nNormalized TCO\n0\n5\n10\n15\n20\n25\n% of Policies\n(a) General-purpose datacenters.\n0.8\n0.9\n1.0\n1.1\n1.2\nNormalized TCO\n0\n20\n40\n60\n80\n100\nCDF (%)\n(b) AI datacenters.\nFigure 11. TCO distribution for various hardware refresh\npolicies in general-purpose and AI datacenters.\nFigure 12. TCO changing refresh policy for each hardware\ngeneration in general-purpose and AI datacenters.\nbaseline remains among the most cost-effective strategies\nfor general-purpose datacenters.\nFigure 12 shows the same data split by hardware gener-\nation. The top of Figure 12 presents a per-CPU-generation\nview, showing how varying the refresh policies of each in-\ndividual hardware generation, from 0 years (skipping that\ngeneration entirely) to 10 years, impacts normalized TCO.\nFor general-purpose datacenters, except for Skylake (where\nskipping was preferable), the refresh policies of 4–6 years\nlifetime have the lowest TCOs.\nRearchitecting for AI. The dynamics of AI accelerators and\nworkloads differ substantially from those of general-purpose\ndatacenters, rendering the traditional refreshes sub-optimal.\nFigure 11b shows that many alternative refresh strategies\ncan reduce TCO by 15–20% compared to the baseline.\nHardware Trend. Figure 12 shows the TCO changing the\nrefresh cycle (and skipping) for each hardware generation.\nThree dominant hardware-driven trends emerge. First, newer\nis much better: when new GPUs provide substantial effi-\nciency gains, early decommissioning of older hardware is\njustified, and datacenters benefit from upgrading as soon as\nthe next generation is available. For example, transitioning\nfrom NVIDIA V100 to A100 GPUs is beneficial.\n10\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\n2021\n2024\n2027\n2030\nYear\n0\n10\n20\n30\n40\n50\n# Active Servers (K)\nGPT-3\nSwitch\nMinerva\nPanGu\nDeepSeek\nLlama4\nfuture \nP100\nV100\nA100\nH100\nH200\nGPU'27\nGPU'28\nGPU'29\nGPU'30\nGPU'31\nFigure 13. Server count by GPU type over time in an AI fleet\nfollowing the optimal refresh policy for minimizing TCO.\nSecond, older is competitive: The generation breakdown\nshows that for most cases extending their hardware lifespan\nbeyond 6 years is cost-effective. This was the case for all\ngenerations before B100 GPUs.\nThird, newer is similar or worse: when new GPUs offer\nmarginal or negative efficiency gains, it is better to extend the\nlife of existing hardware and skip intermediate generations.\nFor example, it is more cost effective to skip B100/B200 GPUs.\nWorkload Evolution. If models grow significantly year over\nyear (as seen in past GPT-family releases [87]) refresh cycles\nmust be accelerated to provision more efficient hardware\nthat can handle the increased compute needs. Conversely, if\nmodel sizes stabilize or decrease, extending hardware life-\ntime becomes more cost-effective.\nExample Timeline. Figure 13 shows the deployment of GPU\nserver generations under the optimal refresh policy to min-\nimize TCO. The policy skips some generations (e.g., B100,\nB200) entirely, as extending the service life of H100 and H200\nGPUs proves more cost-effective. When a newer generation\ndelivers substantial performance and efficiency gains, the\npolicy triggers earlier decommissioning and demand-driven\npurchases to match workload growth and model sizes. Com-\npared to the baseline policy in Figure 4, this approach yields\na smoother, more balanced mix of old and new hardware,\nand at times even a modest reduction in total server count\ndue to improved GPU efficiency (e.g., in early 2027).\n6.3\nLessons\nFixed refresh intervals are no longer sufficient for AI work-\nloads. Unlike traditional datacenters, where hardware effi-\nciency and workload demands change gradually, AI accel-\nerators and models evolve at a much faster pace. Some gen-\nerations deliver dramatic performance gains, while others\nbring only marginal improvements. These shifts are further\namplified by rapid increases in power and thermal densi-\nties, hardware costs, and release frequency, changing the\ntrade-offs between raw performance and efficiency.\nTherefore, AI datacenters must adopt flexible hardware\nrefresh strategies that respond to evolving hardware effi-\nciency and workload trends. This may involve aggressively\nretiring older GPUs when new generations deliver signif-\nicant efficiency gains, while extending the life of existing\n2016\n2018\n2020\n2022\n2024\nRelease Year\n0\n5\n10\n15\nNorm. Performance\nHaswell\nCascade\nSapphire\nPerformance\nPerf per Watt\n0\n5\n10\n15\nNorm. Perf/Watt\nFigure 14. Performance and performance/Watt for DCPerf\napplications [101] across generations of Intel servers.\nhardware (or skipping intermediate generations) when im-\nprovements are limited. By aligning refresh decisions with\nmodel scaling and operational cost trends, infrastructure can\nevolve more intelligently, maximizing performance while\ncontrolling costs.\n7\nOperating an AI Datacenter\nOnce hardware is provisioned, the challenge becomes sus-\ntaining high utilization while meeting SLOs across a diverse,\nevolving fleet. This demands software techniques to orches-\ntrate diverse workload types, hardware generations, and\ndifferent performance-cost trade-offs, all shaped by earlier\nbuilding and refreshing decisions.\nTraditional Approach. In general-purpose datacenters,\nworkloads typically run on homogeneous hardware pools\nwith uniform deployment strategies, from bare-metal servers\nand VMs to microservices and serverless. Most workloads\nare generally deployed on the latest hardware, while some\nworkloads may remain on legacy systems until decommis-\nsioned [36]. Software stacks are often tuned for predictable,\nstable performance, requiring little adaptation to hardware\ndiversity or rapid workload changes.\nTo quantify this, we use the DCPerf benchmark suite [101],\nwhich includes representative production-grade applications\n(e.g., key-value stores, databases, and video processing) to\nevaluate performance and power efficiency across multiple\nIntel server generations. Figure 14 shows aggregated per-\nformance and performance per Watt, indicating that both\nthroughput and power efficiency scale nearly linearly with\nnewer servers. This justifies the common practice of migrat-\ning workloads to the latest generation.\nRearchitecting for AI. As discussed in Section 6.1, AI work-\nloads and hardware trends diverge significantly from those\nof general-purpose datacenters. Unlike CPUs, we do not ob-\nserve uniform or linear performance improvements of AI in-\nference workloads across GPU generations, and the benefits\nof new hardware vary considerably across different models\nand use-cases. Hence, applying the same direct-migration\nstrategy used in traditional datacenters can be suboptimal\nfor AI workloads. Instead, achieving operational efficiency\nfor AI requires software optimizations that bridge the gap\n11\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nOptimization Technique\nDescription\nTCO Impact\nSmooth Model Migration [45, 80]\nGradual migration from old to newer models upon releases\nAvoid rapid hardware procurement\nModel Quantization [63, 118]\nLower precision to reduce compute/memory\nLower hardware demand and cost per inference\nKV-Cache Management [27, 83]\nOptimize storage and reuse of KV cache\nIncrease older hardware reuse\nDisaggregation [83, 89, 105, 120, 122]\nSplit distinct phases onto different hardware\nExtend useful life of heterogeneous generation\nAlternative Architectures\nMixture-of-Experts [95], State-Space-Models [38]\nIncrease older hardware reuse\nModel Routing [22, 52]\nDirect workloads to the most efficient model variant\nIncrease older hardware reuse\nHeterogeneity-Aware Scheduling [54, 65]\nMap workloads to optimal/available hardware generation\nDefer refresh costs\nInfrastructure-Aware Scheduling [99, 100]\nExploit headroom within infra capacity envelopes\nImprove infrastructure efficiency\nTable 7. Operation stage software optimizations that introduce new cross-stage optimization opportunities.\nMigrate\nQuantize\nKVCache\nDisaggre\nAlternArch\nRouting\nHeterogen\nInfraSched\nAll\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized TCO\n(a) TCO vs. software optimization.\nBuild   \nRefresh   \nOperate   \nB+R   \nB+O   \nR+O   \nB+R+O   \n(b) TCO vs. stages.\nFigure 15. TCO for optimization during operation and for\nstages, normalized to the baseline without optimizations.\nbetween evolving models, growing user demands, and a het-\nerogeneous fleet, while controlling cost and sustaining SLOs.\nOptimizations. Table 7 summarizes some key operation opti-\nmizations that influence build and refresh strategies. Model\nmigration, quantization, and KV-cache management reduce\ncompute and memory pressure. Disaggregation leverages fast\ninterconnects and aligns workload phases with suitable hard-\nware generations. Heterogeneity-aware routing, placement,\nand scheduling adapt workloads in real time to match hard-\nware with workload characteristics. Infrastructure-aware sched-\nuling optimizes for datacenter constraints (e.g., power and\ncooling) via software controls (e.g., dynamic sharding), link-\ning operational policies to build-time decisions and enabling\nlifecycle-wide optimization.\nTCO savings. These optimizations extend the life of older\nhardware, defer costly refreshes, and improve infrastruc-\nture value. Figure 15a shows TCO reductions of 12–39%\nper strategy compared to the baseline. Smoothen model mi-\ngration delivers the largest gains by reducing post-release\ncompute load. Disaggregation and infrastructure-aware sched-\nuling achieve strong savings without modifying load, by\nleveraging heterogeneous fleets and aligning software with\ndatacenter constraints. Combining all optimizations yields\nover 60% TCO reduction. While not fully additive due to\noverlaps, the cumulative impact is substantial.\n7.1\nLessons\nFixed, uniform operational strategies are insufficient for\nrapidly evolving models and heterogeneous hardware gen-\nerations. Software-driven techniques (e.g., model migration\nsmoothing, disaggregation, and heterogeneity-aware sched-\nuling) shift operations from a reactive process to a proactive,\ncost-optimized stage of the datacenter lifecycle. These opti-\nmizations extend the useful life of older hardware, leverage\nheterogeneous fleets, and dynamically align workloads with\navailable infrastructure to capture efficiency gains. By clos-\ning the loop between operations, hardware refresh, and build\nstages, these techniques turn rigid hardware timelines into\nadaptive, lifecycle-aware policies.\n8\nCross-Stage Optimizations\nOptimizing individual stages for AI reduces TCO, but the\nlargest gains come from cross-stage rearchitecting the end-\nto-end AI datacenter lifecycle. We outline the current cross-\nstage strategies and explore emerging opportunities.\n8.1\nExisting Cross-Stage Optimizations\nIT provisioning →Build. AI hardware trends are reshap-\ning the datacenter build. Power hierarchies are flattening to\nsupport high-density accelerators, cooling is shifting from\nair to liquid [79], and networking is moving beyond Ethernet\nto InfiniBand and NVLink fabrics [85]. These choices raise\ninitial build costs but ease future refreshes, extending the\nusable lifetime of each deployment.\nOperate →IT provisioning. Heterogeneity-aware sched-\nuling helps repurposing older GPUs for workloads better\nsuited to their capabilities: compute-intensive phases (e.g.,\nprefill or large models) run on newer GPUs, while memory-\nor bandwidth-bound phases (e.g., decode or smaller mod-\nels) are offloaded to older generations [65, 89]. This strategy\nsmooths refresh costs and maintains high utilization, turning\nhardware upgrades into opportunities for redistribution and\ncontinued value rather than premature hardware retirement.\nBuild →Operate. Infrastructure decisions made at build\ntime shape operational flexibility. Scheduling frameworks\ntranslate these choices into software controls that smooth de-\nmand, enable safe hardware derating, and sustain efficiency.\nConversely, coordinated derating of servers and power de-\nvices within the power hierarchy allows oversubscription\n12\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\nand denser deployments; effectively “upgrading” infrastruc-\nture at runtime without new physical buildouts [88, 99].\nCompound TCO Benefits. Figure 15b shows that cross-\nstage strategies amplify impact across the lifecycle. Optimiz-\ning individual stages yields 20–30% savings, while combining\nbuild and refresh policies pushes savings beyond 35%. A holis-\ntic approach can reduce TCO by over 40%. Assuming hard-\nware and model sizes grow linearly (center cells in Table 8),\nthe best infrastructure to build includes flat power delivery,\nhybrid liquid-air cooling, and a hierarchical networking with\nEthernet, Infiniband, and NVLink. For hardware refresh, it is\nbest to extend server lifetimes to five years and adopt new\nhardware as it becomes available. During operation, the best\nstrategy is to combine all available optimizations.\n8.2\nOpportunities for Cross-Stage Optimizations\nLooking ahead, several opportunities emerge when infras-\ntructure, hardware, and software are explicitly co-designed\nwith lifecycle interplay in mind.\nInfrastructure. Today’s software supports heterogeneous\nfleets, but build and refresh strategies can better leverage het-\nerogeneity. Rack-level provisioning with mixed-generation\naccelerators or general-purpose compute reduces intercon-\nnect bottlenecks and power fragmentation. Combined with\nheterogeneous derating, these setups adapt efficiently to\nworkload demands. While they require upfront investment\nin fine-grained telemetry and control, they offer substantial\nlong-term efficiency gains.\nHardware. Emerging AI accelerators have traditionally shaped\ndatacenter infrastructure design. Looking ahead, future accel-\nerators should be designed not only for performance but also\nfor long-term compatibility with the existing infrastructure.\nLower power density and moderated TDP simplify power\ndelivery and cooling, reducing fragmentation. For example,\naccelerators could combine high-power SMs to handle the\ncompute-bound prefill phase with Processing-in-Memory\n(PIM) units tailored for the memory-bound decode phase,\nenabling more efficient execution within the same server.\nOperation. Techniques such as KV-cache management or\nnew model architectures (e.g., MoEs) shift the balance be-\ntween compute and memory needs, reshaping both refresh\npriorities and placement strategies. Cross-stage planning\nanticipates these shifts by provisioning memory or storage\nservers that support multiple GPU generations.\n8.3\nFuture Trends\nAs AI models and hardware keep evolving, lifecycle manage-\nment must adapt not only to these trends individually but\nalso to their compounded effects. We analyze model scaling\nand hardware trajectories to offer guidance for future AI\ndatacenter lifecycles. Table 8 shows the optimal strategies\nacross build, IT provisioning, and operate stages, tailored to\nprojected growth patterns in both hardware and models. We\nModel Growth\nSlow →\nMedium\nFast ↑\nHardware Growth\n→\nPer-PDU\nAir\nIB\nPer-PDU\nAir\nNVLink\nPer-PDU\nAir\nNVLink\nFlat\nHybrid\nIB\nFlat\nHybrid\nHierarchy\nFlat\nLiquid\nHierarchy\n↑\nFlat\nLiquid\nIB\nFlat\nHybrid\nIB\nFlat\nHybrid\nHierarchy\n→\n8 years\nSkip\n8 years\nSkip\n8 years\nSkip\n4 years\nBuy new\n5 years\nBuy new\n4 years\nSkip\n↑\n4 years\nBuy new\n4 years\nBuy new\n3 years\nBuy new\n→\nAll Optimizations\nMigration + Quantization\nMigration + Quantization\nDisaggregation\nAll Optimizations\nMigration + Quantization\n↑\nDisagg. + Hetero.\nDisaggregation\nAll Optimizations\nTable 8. Optimal cross-stage strategies based on model and\nhardware trends under exponential user demand growth.\nThe rows represent build, refresh, and operate stages. Color\ngradient shows degree of lifecycle adaptation required.\nexplore slow (sub-linear), medium (linear), and fast (expo-\nnential) growth of model size/complexity and hardware.\nAI Hardware. When hardware performance scales rapidly,\nfrequent refresh cycles are advantageous, justifying earlier\nadoption and earlier decommissioning. However, if perfor-\nmance gains slow or power efficiency declines, it becomes\nmore beneficial to extend refresh intervals and design infras-\ntructure for long-term use. Lower-power, lower-density hard-\nware can also extend infrastructure lifetimes by reducing the\nneed for costly upgrades to power and cooling systems.\nAI Model Size. So far, we have assumed that AI models\nwill grow linearly. Larger models amplify compute, memory,\nand networking requirements, accelerating refresh cadence\nand demanding infrastructure explicitly designed for modu-\nlar expansion. If model size growth slows down, operators\ncan rely on more cost-effective infrastructures (e.g., Infini-\nBand networking). Conversely, if models continue to scale\nrapidly, more expensive infrastructures become necessary\n(e.g., NVLink), alongside more aggressive software optimiza-\ntions (e.g., model migration and quantization).\n8.4\nLessons\nCross-stage optimization reframes lifecycle management as\na continuum rather than a sequence. Build-time choices, re-\nfresh strategies, and operation policies compound to drive\nlong-term efficiency. Looking ahead, future datacenters should\nbe increasingly co-designed across infrastructure, hardware,\nand software; with lifecycle-aware thinking at the core of\nscalable, cost-effective AI.\n9\nConclusion\nThe rapid growth of AI workloads has outpaced traditional\ndatacenter management. AI datacenters need flexible designs,\nadvanced cooling, and software for heterogeneous hardware.\nWhile stage-specific improvements (build, IT provisioning,\noperate) help, the biggest gains, up to 40% TCO reduction,\ncome from a holistic, cross-stage approach that anticipates\nworkload dynamics and hardware trends, enabling cloud\nproviders to scale efficiently and control costs.\n13\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nReferences\n[1] Deepak Bhaskar Acharya, Karthigeyan Kuppan, and B Divya. 2025.\nAgentic ai: Autonomous intelligence for complex goals–a compre-\nhensive survey. IEEe Access (2025).\n[2] Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatam-\nifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, and\nMehrdad Farajtabar. 2024.\nLLM in a flash: Efficient Large Lan-\nguage Model Inference with Limited Memory.\narXiv preprint\narXiv:2312.11514 (2024).\n[3] Amazon AWS. 2025. Power usage effectiveness. https://sustainability.\naboutamazon.com/products-services/aws-cloud.\n[4] AMD. 2025.\nAMD Instinct™GPUs.\nhttps://www.amd.com/en/\nproducts/accelerators/instinct.html.\n[5] InfiniBand Trade Association. 2001. InfiniBand Architecture Specifi-\ncation. In InfiniBand Trade Association. https://www.infinibandta.\norg/specs/ Version 1.0.\n[6] Microsoft Azure. 2025.\nMicrosoft and NVIDIA accelerate AI\ndevelopment and performance.\nhttps://azure.microsoft.com/en-\nus/blog/microsoft-and-nvidia-accelerate-ai-development-and-\nperformance/.\n[7] Ricardo Bianchini, Christian Belady, and Anand Sivasubramaniam.\n2024. Datacenter power and energy management: past, present, and\nfuture. IEEE Micro (2024).\n[8] Kashif Bilal, Samee U Khan, Limin Zhang, Hongxiang Li, Khizar\nHayat, Sajjad A Madani, Nasro Min-Allah, Lizhe Wang, Dan Chen,\nMajid Iqbal, et al. 2013. Quantitative comparisons of the state-of-the-\nart data center architectures. Concurrency and Computation: Practice\nand Experience 25 (2013).\n[9] Jairus Bowne. 2024. Using Large Language Models in Learning and\nTeaching.\nhttps://biomedicalsciences.unimelb.edu.au/study/dlh/\nassets/documents/large-language-models-in-education/llms-in-\neducation.\n[10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot\nLearners. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/2005.14165\n[11] Foundation Capital. 2024.\nWhy 2024 Will Be the Year of Infer-\nence. https://foundationcapital.com/insights/why-2024-will-be-the-\nyear-of-inference Accessed: 2025-09-26.\n[12] Gohar Irfan Chaudhry, Esha Choukse, Haoran Qiu, Íñigo Goiri, Ro-\ndrigo Fonseca, Adam Belay, and Ricardo Bianchini. 2025. Murakkab:\nResource-Efficient Agentic Workflow Orchestration in Cloud Plat-\nforms. arXiv preprint arXiv:2508.18298 (2025).\n[13] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan,\nPeng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang\nChe. 2025. Towards reasoning era: A survey of long chain-of-thought\nfor reasoning large language models. arXiv preprint arXiv:2503.09567\n(2025).\n[14] Esha Choukse, Brijesh Warrier, Scot Heath, Luz Belmont, April Zhao,\nHassan Ali Khan, Brian Harry, Matthew Kappel, Russell J Hewett,\nKushal Datta, et al. 2025. Power stabilization for AI training datacen-\nters. arXiv preprint arXiv:2508.14318 (2025).\n[15] Google Cloud. 2025. Introducing A4X VMs powered by NVIDIA\nGB200. https://cloud.google.com/blog/products/compute/new-a4x-\nvms-powered-by-nvidia-gb200-gpus.\n[16] Joel Coburn, Chunqiang Tang, Sameer Abu Asal, Neeraj Agrawal,\nRaviteja Chinta, Harish Dixit, Brian Dodds, Saritha Dwarakapuram,\nAmin Firoozshahian, Cao Gao, Kaustubh Gondkar, Tyler Graf, Jun-\nhan Hu, Jian Huang, Sterling Hughes, Adam Hutchin, Bhasker Jakka,\nGuoqiang Jerry Chen, Indu Kalyanaraman, Ashwin Kamath, Pankaj\nKansal, Erum Kazi, Roman Levenstein, Mahesh Maddury, Alex Mas-\ntro, Siji Medaiyese, Pritesh Modi, Jack Montgomery, Satish Nadathur,\nAmit Nagpal, Ashwin Narasimha, Maxim Naumov, Eleanor Ozer,\nJongsoo Park, Poorvaja Ramani, Harikrishna Reddy, David Reiss,\nDeboleena Roy, Sathish Sekar, Arushi Sharma, Pavan Shetty, Aravind\nSukumaran-Rajam, Eran Tal, Mike Tsai, Shreya Varshini, Richard\nWareing, Olivia Wu, Xiaolong Xie, Jinghan Yang, Hangchen Yu, Tan-\nmay Zargar, Zitong Zeng, Feixiong Zhang, Ajit Matthews, Xun Jiao,\nJiyuan Zhang, Emmanuel Menage, Truls Edvard Stokke, and Mo-\nhammed Sourouri. 2025. Meta’s Second Generation AI Chip: Model-\nChip Co-Design and Productionization Experiences. In Proceedings of\nthe 52nd Annual International Symposium on Computer Architecture\n(ISCA ’25). Association for Computing Machinery, New York, NY,\nUSA, 1689–1702. https://doi.org/10.1145/3695053.3731409\n[17] CoreSite. 2025. AI and the Data Center: Driving Greater Power\nDensity. https://www.coresite.com/blog/ai-and-the-data-center-\ndriving-greater-power-density.\n[18] Cyfuture Cloud. 2025. What is the Price of an NVIDIA DGX H100 Sys-\ntem? https://cyfuture.cloud/kb/gpu/what-is-the-price-of-an-nvidia-\ndgx-h100-system.\n[19] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher\nRé. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention\nwith IO-Awareness. arXiv preprint arXiv:2205.14135 (2022).\n[20] Hafiz M Daraghmeh and Chi-Chuan Wang. 2017. A review of current\nstatus of free cooling in datacenters. Applied Thermal Engineering\n114 (2017), 1224–1239.\n[21] DeepSeek. 2025.\nIntroducing DeepSeek-V3.\nhttps://api-docs.\ndeepseek.com/news/news1226.\n[22] Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata\nMukherjee, Victor Ruhle, Laks VS Lakshmanan, and Ahmed Hassan\nAwadallah. 2024. Hybrid llm: Cost-efficient and quality-aware query\nrouting. arXiv preprint arXiv:2404.14618 (2024).\n[23] Energy.gov. 2024. Evaporative Coolers. https://www.energy.gov/\nenergysaver/evaporative-coolers.\n[24] Epoch AI. 2024. Data on Notable AI Models. https://epoch.ai/data/\nnotable-ai-models Accessed: 2025-06-02.\n[25] Ege Erdil. 2024.\nFrontier language models have become\nmuch smaller. https://epoch.ai/gradient-updates/frontier-language-\nmodels-have-become-much-smaller.\n[26] Xiaobo Fan, Wolf-Dietrich Weber, and Luiz Andre Barroso. 2007.\nPower provisioning for a warehouse-sized computer. In Proceedings\nof the 34th Annual International Symposium on Computer Architecture\n(ISCA ’07). https://doi.org/10.1145/1250662.1250665\n[27] Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic,\nJunbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. Cost-\nEfficient Large Language Model serving for multi-turn conversations\nwith CachedAttention. In 2024 USENIX Annual Technical Conference\n(USENIX ATC 24). 111–126.\n[28] GitHub. 2024. The world’s most widely adopted AI developer tool.\nhttps://github.com/features/copilot.\n[29] Inigo Goiri, Kien Le, Jordi Guitart, Jordi Torres, and Ricardo Bianchini.\n2011. Intelligent Placement of Datacenters for Internet Services.\nIn Proceedings of the 31st International Conference on Distributed\nComputing Systems. https://doi.org/10.1109/ICDCS.2011.19\n[30] Raja Gond, Nipun Kwatra, and Ramachandran Ramjee. 2025. Token-\nWeave: Efficient Compute-Communication Overlap for Distributed\nLLM Inference. arXiv:2505.11329 [cs.DC] https://arxiv.org/abs/2505.\n11329\n[31] Google. 2025. Growing the internet while reducing energy consump-\ntion. https://datacenters.google/efficiency/.\n[32] Google Cloud. 2025. Enabling 1 MW IT racks and liquid cooling at\nOCP EMEA Summit. https://cloud.google.com/blog/topics/systems/\nenabling-1-mw-it-racks-and-liquid-cooling-at-ocp-emea-summit.\n14\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\nAccessed on August 20, 2025.\n[33] Grand View Research. 2025.\nU.S. Generative AI Market Size &\nTrends. https://www.grandviewresearch.com/industry-analysis/us-\ngenerative-ai-market-report.\n[34] Albert Greenberg, James Hamilton, David A Maltz, and Parveen Patel.\n2008. The cost of a cloud: research problems in data center networks.\nIn SIGCOMM.\n[35] GridStatus.io Team. 2025. GridStatus: Real-Time Power Grid Status\nMonitoring. https://www.gridstatus.io/. Accessed: 2025-09-29.\n[36] Tim Grieser, Joeph Pucciarelli, Jean Bozman, and Randy Perry. 2010.\nManaging the Server Migration Process: The HP Approach to Reducing\nOperational Costs. Technical Report. HP.\n[37] Groq. 2025. What is a Language Processing Unit? https://groq.com/\nblog/the-groq-lpu-explained.\n[38] Albert Gu, Karan Goel, and Christopher Ré. 2022.\nEffi-\nciently Modeling Long Sequences with Structured State Spaces.\narXiv:2111.00396 [cs.LG] https://arxiv.org/abs/2111.00396\n[39] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang,\nRunxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025.\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Rein-\nforcement Learning. arXiv:2501.12948 (2025).\n[40] Wenzhe Guo, Joyjit Kundu, Uras Tos, Weijiang Kong, Giuliano Sisto,\nTimon Evenblij, and Manu Perumkunnil. 2025. System-performance\nand cost modeling of Large Language Model training and inference.\narXiv:2507.02456 [cs.AR] https://arxiv.org/abs/2507.02456\n[41] Udit Gupta, Mariam Elgamal, Gage Hills, Gu-Yeon Wei, Hsien-Hsin S.\nLee, David Brooks, and Carole-Jean Wu. 2022. ACT: designing sus-\ntainable computer systems with an architectural carbon modeling\ntool. In Proceedings of the 49th Annual International Symposium on\nComputer Architecture (ISCA ’22).\n[42] Bowen He, Xiao Zheng, Yuan Chen, Weinan Li, Yajin Zhou, Xin Long,\nPengcheng Zhang, Xiaowei Lu, Linquan Jiang, Qiang Liu, Dennis\nCai, and Xiantao Zhang. 2023. DxPU: Large-scale Disaggregated\nGPU Pools in the Datacenter. ACM Trans. Archit. Code Optim. 20, 4,\nArticle 55 (Dec. 2023), 23 pages. https://doi.org/10.1145/3617995\n[43] Ali Heydari, Bahareh Eslami, Vahideh Radmard, Fred Rebarber, Tyler\nBuell, Kevin Gray, Sam Sather, and Jeremy Rodriguez. 2022. Power\nUsage Effectiveness Analysis of a High-Density Air-Liquid Hybrid\nCooled Data Center (International Electronic Packaging Technical\nConference and Exhibition, Vol. ASME 2022 International Technical\nConference and Exhibition on Packaging and Integration of Electronic\nand Photonic Microsystems). V001T01A014. https://doi.org/10.1115/\nIPACK2022-97447\n[44] Marius Hobbhahn, Lennart Heim, and Gökçe Aydos. 2023. Trends\nin Machine Learning Hardware. https://epoch.ai/blog/trends-in-\nmachine-learning-hardware.\n[45] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,\nYasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and\nTomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger\nLanguage Models with Less Training Data and Smaller Model Sizes.\narXiv:2305.02301 [cs.CL] https://arxiv.org/abs/2305.02301\n[46] Chang-Hong Hsu, Qingyuan Deng, Jason Mars, and Lingjia Tang.\n2018. SmoothOperator: Reducing Power Fragmentation and Improv-\ning Power Utilization in Large-Scale Datacenters. In Proceedings of\nthe 23rd International Conference on Architectural Support for Pro-\ngramming Languages and Operating Systems (ASPLOS).\n[47] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi\nLi, Lu Wang, and Weizhu Chen. [n. d.]. LoRA: Low-Rank Adaptation\nof Large Language Models. arXiv:2106.09685 ([n. d.]).\n[48] Urs Hölzle. 2022. Our commitment to climate-conscious data center\ncooling. https://blog.google/outreach-initiatives/sustainability/our-\ncommitment-to-climate-conscious-data-center-cooling/.\n[49] Intel.\n2025.\nIntel®\nXeon®\n6980P\nProcessor.\nhttps:\n//www.intel.com/content/www/us/en/products/sku/240777/intel-\nxeon-6980p-processor-504m-cache-2-00-ghz/specifications.html.\n[50] Intel.\n2025.\nIntel®\nXeon®\nPlatinum\n8593Q\nProcessor.\nhttps://www.intel.com/content/www/us/en/products/sku/\n237252/intel-xeon-platinum-8593q-processor-320m-cache-2-\n20-ghz/specifications.html.\n[51] Intel. 2025.\nIntel® Xeon® Processor E7-8890 v3.\nhttps://www.\nintel.com/content/www/us/en/products/sku/84685/intel-xeon-\nprocessor-e78890-v3-45m-cache-2-50-ghz/specifications.html.\n[52] Kunal Jain, Anjaly Parayil, Ankur Mallick, Esha Choukse, Xiaoting\nQin, Jue Zhang, Íñigo Goiri, Rujia Wang, Chetan Bansal, Victor Rühle,\net al. 2024. Intelligent Router for LLM Workloads: Improving Per-\nformance Through Workload-Aware Load Balancing. arXiv preprint\narXiv:2408.13510 (2024).\n[53] Majid Jalili, Ioannis Manousakis, Íñigo Goiri, Pulkit A. Misra, Ashish\nRaniwala, Husam Alissa, Bharath Ramakrishnan, Phillip Tuma, Chris-\ntian Belady, Marcus Fontoura, and Ricardo Bianchini. 2021. Cost-\nEfficient Overclocking in Immersion-Cooled Datacenters. In Pro-\nceedings of the 48th Annual International Symposium on Computer\nArchitecture (ISCA).\n[54] Youhe Jiang, Ran Yan, Xiaozhe Yao, Yang Zhou, Beidi Chen, and\nBinhang Yuan. 2024. HexGen: Generative Inference of Large Lan-\nguage Model over Heterogeneous Environment.\narXiv preprint\narXiv:2311.11514 (2024).\n[55] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan,\nLifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian\nTowles, Clifford Young, Xiang Zhou, Zongwei Zhou, and David A\nPatterson. 2023. TPU v4: An Optically Reconfigurable Supercomputer\nfor Machine Learning with Hardware Support for Embeddings. In\nProceedings of the 50th Annual International Symposium on Computer\nArchitecture (ISCA ’23).\n[56] Keith Kirkpatrick and Daniel Newman. 2025.\nMicrosoft\nQ3 FY 2025: Azure and AI Services Drive Strong Growth.\nhttps://futurumgroup.com/insights/microsoft-q3-fy-2025-\nearnings-beat-on-strong-cloud-and-ai-services-growth/.\n[57] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin\nZheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica.\n2023. Efficient Memory Management for Large Language Model\nServing with PagedAttention. In Proceedings of the Symposium on\nOperating Systems Principles (SOSP).\n[58] Marina Lammertyn. 2024. 60+ ChatGPT Statistics And Facts You\nNeed to Know in 2024. https://blog.invgate.com/chatgpt-statistics.\n[59] Chuan Li. 2025. Tesla A100 Server Total Cost of Ownership Analysis.\nhttps://lambda.ai/blog/tesla-a100-server-total-cost-of-ownership.\n[60] Jinhao Li, Jiaming Xu, Shan Huang, et al. 2025. Large Language Model\nInference Acceleration: A Comprehensive Hardware Perspective.\narXiv preprint arXiv:2410.04466 (2025).\nhttps://arxiv.org/abs/2410.\n04466\n[61] Shaohong Li, Xi Wang, Xiao Zhang, Vasileios Kontorinis, Sreekumar\nKodakara, David Lo, and Parthasarathy Ranganathan. 2020. Thun-\nderbolt: Throughput-Optimized, Quality-of-Service-Aware Power\nCapping at Scale. In Proceedings of the 14th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI).\n[62] Yuzhuo Li, Mariam Mughees, Yize Chen, and Yunwei Ryan Li. 2024.\nThe Unseen AI Disruptions for Power Grids: LLM-Induced Transients.\narXiv:2409.11416 [cs.AR] https://arxiv.org/abs/2409.11416\n[63] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan\nXiao, Chuang Gan, and Song Han. 2024. Qserve: W4a8kv4 quantiza-\ntion and system co-design for efficient llm serving. arXiv preprint\narXiv:2405.04532 (2024).\n[64] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda\nLu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\net al. 2024. DeepSeek-V3 Technical Report. arXiv:2412.19437 (2024).\n[65] Yixuan Mei, Yonghao Zhuang, Xupeng Miao, Juncheng Yang, Zhihao\nJia, and Rashmi Vinayak. 2025. Helix: Serving large language models\n15\nJovan Stojkovic, Chaojie Zhang, Íñigo Goiri, and Ricardo Bianchini\nover heterogeneous gpus and network via max-flow. In Proceedings of\nthe International Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS).\n[66] Meta. 2024. 2024 Sustainability Report. https://sustainability.atmeta.\ncom/2024-sustainability-report/.\n[67] Meta. 2024. Llama2-70B. https://huggingface.co/meta-llama/Llama-\n2-70b.\n[68] Meta. 2024. Llama3-70B. https://huggingface.co/meta-llama/Meta-\nLlama-3-70B-Instruct.\n[69] Meta. 2025. The Llama 4 herd: The beginning of a new era of na-\ntively multimodal AI innovation. https://ai.meta.com/blog/llama-4-\nmultimodal-intelligence/.\n[70] Nicholas Metropolis and Stanislaw Ulam. 1949. The Monte Carlo\nMethod. J. Amer. Statist. Assoc. 44, 247 (1949), 335–341.\n[71] Xupeng Miao, Chunan Shi, Jiangfei Duan, Xiaoli Xi, Dahua Lin, Bin\nCui, and Zhihao Jia. 2024. SpotServe: Serving Generative Large\nLanguage Models on Preemptible Instances. In Proceedings of the\nInternational Conference on Architectural Support for Programming\nLanguages and Operating Systems (ASPLOS).\n[72] Microsoft. 2025. Measuring datacenter energy and water use to im-\nprove Microsoft Cloud sustainability. https://datacenters.microsoft.\ncom/sustainability/efficiency/.\n[73] Seungjae Moon, Junseo Cha, Hyunjun Park, and Joo-Young Kim. 2025.\nHybe: GPU-NPU Hybrid System for Efficient LLM Inference with\nMillion-Token Context Window. In Proceedings of the 52nd Annual\nInternational Symposium on Computer Architecture (ISCA ’25).\n[74] Jesse\nNoffsinger,\nMaria\nGoodpaster,\nMark\nPatel,\nHaley\nChang,\nPankaj\nSachdeva,\nand\nArjita\nBhan.\n2025.\nThe\ncost of compute: A $7 trillion race to scale data centers.\nhttps://www.mckinsey.com/industries/technology-media-and-\ntelecommunications/our-insights/the-cost-of-compute-a-7-trillion-\ndollar-race-to-scale-data-centers.\n[75] NVIDIA. 2024.\nCooling and Airflow Optimization.\nhttps:\n//docs.nvidia.com/dgx-superpod/design-guides/dgx-superpod-\ndata-center-design-h100/latest/cooling.html.\n[76] NVIDIA. 2024. DGX H100: AI for Enterprise. https://www.nvidia.\ncom/en-us/data-center/dgx-h100/.\n[77] NVIDIA. 2024.\nIntroduction to the NVIDIA DGX A100 Sys-\ntem. https://docs.nvidia.com/dgx/dgxa100-user-guide/introduction-\nto-dgxa100.html.\n[78] NVIDIA. 2024.\nNVIDIA Blackwell Platform Arrives to Power a\nNew Era of Computing. https://nvidianews.nvidia.com/news/nvidia-\nblackwell-platform-arrives-to-power-a-new-era-of-computing.\n[79] NVIDIA. 2025. Chill Factor: NVIDIA Blackwell Platform Boosts Wa-\nter Efficiency by Over 300x. https://blogs.nvidia.com/blog/blackwell-\nplatform-water-efficiency-liquid-cooling-data-centers-ai-\nfactories/.\n[80] NVIDIA. 2025. LLM Model Pruning and Knowledge Distillation with\nNVIDIA NeMo Framework. https://developer.nvidia.com/blog/llm-\nmodel-pruning-and-knowledge-distillation-with-nvidia-nemo-\nframework/.\n[81] NVIDIA. 2025. NVIDIA Collective Communications Library (NCCL).\nhttps://developer.nvidia.com/nccl.\n[82] NVIDIA. 2025.\nNVIDIA ConnectX-7 Adapter Cards User Man-\nual.\nhttps://docs.nvidia.com/networking/display/connectx7vpi/\nspecifications.\n[83] NVIDIA. 2025. NVIDIA Dynamo Platform. https://developer.nvidia.\ncom/dynamo.\n[84] NVIDIA. 2025. NVIDIA Quantum-2 InfiniBand Platform. https:\n//www.nvidia.com/en-eu/networking/quantum2/.\n[85] NVIDIA. 2025. NVLink and NVLink Switch. https://www.nvidia.\ncom/en-us/data-center/nvlink/.\n[86] NVIDIA. 2025.\nRDMA over Converged Ethernet (RoCE).\nhttps://docs.nvidia.com/networking/display/winofv55054000/\nrdma+over+converged+ethernet+(roce).\n[87] OpenAI. 2025. Models. https://platform.openai.com/docs/models.\n[88] Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh\nWarrier, Nithish Mahalingam, and Ricardo Bianchini. 2024. Charac-\nterizing Power Management Opportunities for LLMs in the Cloud.\nIn Proceedings of the 29th ACM International Conference on Archi-\ntectural Support for Programming Languages and Operating Systems\n(ASPLOS).\n[89] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo\nGoiri, Saeed Maleki, and Ricardo Bianchini. 2024. Splitwise: Efficient\ngenerative LLM inference using phase splitting. In Proceedings of\nthe 51st Annual International Symposium on Computer Architecture\n(ISCA).\n[90] Cheng Peng, Xi Yang, Aokun Chen, Kaleb Smith, Nima PourNe-\njatian, Anthony Costa, Cheryl Martin, Mona Flores, Ying Zhang,\nTanja Magoc, Gloria Lipori, Mitchell Duane, Naykky Ospina, Mustafa\nAhmed, William Hogan, Elizabeth Shenkman, Yi Guo, Jiang Bian,\nand Yonghui Wu. 2023. A study of generative large language model\nfor medical research and healthcare. npj Digital Medicine (2023).\n[91] Sundar Pichai. 2025.\nQ2 earnings call: CEO’s remarks.\nhttps://blog.google/inside-google/message-ceo/alphabet-earnings-\nq2-2025/#introduction.\n[92] Varun Sakalkar, Vasileios Kontorinis, David Landhuis, Shaohong Li,\nDarren De Ronde, Thomas Blooming, Anand Ramesh, James Kennedy,\nChristopher Malone, Jimmy Clidaras, et al. 2020. Data Center Power\nOversubscription with a Medium Voltage Power Plane and Priority-\nAware Capping. In Proceedings of the International Conference on\nArchitectural Support for Programming Languages and Operating Sys-\ntems (ASPLOS).\n[93] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam\nMichaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh\nTiwari, and Vijay Gadepally. 2023. From Words to Watts: Bench-\nmarking the Energy Costs of Large Language Model Inference. In\nProceedings of the High Performance Extreme Computing Conference\n(HPEC).\n[94] Amazon Web Services. 2025. Amazon EC2 P6e-GB200 UltraServers\nand P6-B200 instances. https://aws.amazon.com/blogs/aws/new-\namazon-ec2-p6e-gb200-ultraservers-powered-by-nvidia-grace-\nblackwell-gpus-for-the-highest-ai-performance/.\n[95] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,\nQuoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously Large\nNeural Networks: The Sparsely-Gated Mixture-of-Experts Layer.\narXiv:1701.06538 [cs.LG] https://arxiv.org/abs/1701.06538\n[96] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper, and Bryan Catanzaro. 2019. Megatron-LM: Training\nMulti-Billion Parameter Language Models Using Model Parallelism.\narXiv:1909.08053 (2019).\n[97] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Íñigo Goiri, and Josep\nTorrellas. 2024. Towards Greener LLMs: Bringing Energy-Efficiency\nto the Forefront of LLM Inference. arXiv preprint arXiv:2403.20306\n(2024).\n[98] Jovan Stojkovic, Pulkit Misra, Íñigo Goiri, Sam Whitlock, Esha\nChoukse, Mayukh Das, Chetan Bansal, Jason Lee, Zoey Sun, Haoran\nQiu, Reed Zimmermann, Savyasachi Samal, Brijesh Warrier, Ashish\nRaniwala, and Ricardo Bianchini. 2024. SmartOClock: Workload-\nand Risk-Aware Overclocking in the Cloud. In Proceedings of the\nInternational Symposium on Computer Architecture (ISCA).\n[99] Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Esha Choukse, Haoran\nQiu, Rodrigo Fonseca, Josep Torrellas, and Ricardo Bianchini. 2025.\nTAPAS: Thermal-and power-aware scheduling for LLM inference in\ncloud platforms. In ASPLOS. 1266–1281.\n[100] Jovan Stojkovic, Chaojie Zhang, Íñigo Goiri, Josep Torrellas, and Esha\nChoukse. 2025. DynamoLLM: Designing LLM Inference Clusters\nfor Performance and Energy Efficiency. In Proceedings of the IEEE\n16\nRearchitecting Datacenter Lifecycle for AI:\nA TCO-Driven Framework\nInternational Symposium on High Performance Computer Architecture\n(HPCA).\n[101] Wei Su, Abhishek Dhanotia, Carlos Torres, Jayneel Gandhi, Neha\nGholkar, Shobhit Kanaujia, Maxim Naumov, Kalyan Subramanian,\nValentin Andrei, Yifan Yuan, and Chunqiang Tang. 2025. DCPerf:\nAn Open-Source, Battle-Tested Performance Benchmark Suite for\nDatacenter Workloads. In Proceedings of the 52nd Annual International\nSymposium on Computer Architecture (ISCA’25).\n[102] Team Uvation. 2025.\nBreaking Down the AI server data cen-\nter cost. https://uvation.com/articles/breaking-down-the-ai-server-\ndata-center-cost#maintenance-amp-support-contracts.\n[103] Wendy Torell. 2025. Liquid vs. Air Cooling. Which is the Capex win-\nner? https://blog.se.com/datacenter/architecture/2020/02/24/liquid-\nvs-air-cooling-which-is-the-capex-winner/.\n[104] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is All You Need. Advances in Neural Information Processing\nSystems (NeurIPS) (2017).\n[105] Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng\nHu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, et al.\n2025. Step-3 is Large yet Affordable: Model-system Co-design for\nCost-effective Decoding. arXiv preprint arXiv:2507.19427 (2025).\n[106] Jaylen Wang, Daniel S Berger, Fiodar Kazhamiaka, Celine Irvene,\nChaojie Zhang, Esha Choukse, Kali Frost, Rodrigo Fonseca, Brijesh\nWarrier, Chetan Bansal, et al. 2024. Designing cloud servers for lower\ncarbon. In ISCA.\n[107] Xiaorui Wang, Ming Chen, Charles Lefurgy, and Tom W Keller. 2009.\nSHIP: Scalable Hierarchical Power Control for Large-Scale Data Cen-\nters. In Proceedings of the 18th International Conference on Parallel\nArchitectures and Compilation Techniques (PACT).\n[108] Kitty Wheeler. 2025.\nHow Amazon Achieved Rocketing Sales\nand Growth From AI. https://aimagazine.com/news/how-amazon-\nachieved-rocketing-sales-and-growth-from-ai.\n[109] Qiang Wu, Qingyuan Deng, Lakshmi Ganesh, Chang-Hong Hsu,\nYun Jin, Sanjeev Kumar, Bin Li, Justin Meza, and Yee Jiun Song.\n2016. Dynamo: Facebook’s Data Center-Wide Power Management\nSystem. In Proceedings of the International Symposium on Computer\nArchitecture (ISCA).\n[110] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad\nNorouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaob-\ning Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo,\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei\nWang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol\nVinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016.\nGoogle’s Neural Machine Translation System: Bridging the Gap be-\ntween Human and Machine Translation. CoRR abs/1609.08144 (2016).\nhttp://arxiv.org/abs/1609.08144\n[111] Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Meng-\nwei Xu, and Xuanzhe Liu. 2025. Fast On-device LLM Inference with\nNPUs. In Proceedings of the 30th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Sys-\ntems, Volume 1 (ASPLOS ’25).\n[112] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng,\nJinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. 2024. A survey\non knowledge distillation of large language models. arXiv preprint\narXiv:2402.13116 (2024).\n[113] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and\nByung-Gon Chun. 2022. Orca: A Distributed Serving System for\nTransformer-Based Generative Models. In Proceedings of the 16th\nUSENIX Symposium on Operating Systems Design and Implementation\n(OSDI).\n[114] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe\nZhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae\nLee, Yan Yan, Beidi Chen, Guangyu Sun, and Kurt Keutzer. 2024.\nLLM Inference Unveiled: Survey and Roofline Model Insights.\narXiv:2402.16363 [cs.CL] https://arxiv.org/abs/2402.16363\n[115] Chaojie Zhang, Alok Gautam Kumbhare, Ioannis Manousakis, Deli\nZhang, Pulkit A. Misra, Rod Assis, Kyle Woolcock, Nithish Ma-\nhalingam, Brijesh Warrier, David Gauthier, Lalu Kunnath, Steve\nSolomon, Osvaldo Morales, Marcus Fontoura, and Ricardo Bianchini.\n2021. Flex: High-Availability Datacenters with Zero Reserved Power.\nIn Proceedings of the 48th Annual International Symposium on Com-\nputer Architecture (ISCA).\n[116] Mary Zhang. 2025. How Much Does it Cost to Build a Data Cen-\nter. https://dgtlinfra.com/how-much-does-it-cost-to-build-a-data-\ncenter/.\n[117] Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei\nWang, and Jiawei Han. 2024. A Comprehensive Survey of Scientific\nLarge Language Models and Their Applications in Scientific Discov-\nery. arXiv:2406.10833 [cs.CL] https://arxiv.org/abs/2406.10833\n[118] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size\nZheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris\nKasikci. 2024. Atom: Low-bit quantization for efficient and accurate\nllm serving. Proceedings of Machine Learning and Systems 6 (2024),\n196–209.\n[119] Youpeng Zhao Zhao, Di Wu Wu, and Jun Wang. 2024. ALISA: Ac-\ncelerating Large Language Model Inference via Sparsity-Aware KV\nCaching. In Proceedings of the 51st Annual International Symposium\non Computer Architecture (ISCA).\n[120] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xu-\nanzhe Liu, Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating\nPrefill and Decoding for Goodput-optimized Large Language Model\nServing. In 18th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI).\n[121] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. 2022. PetS:\nA Unified Framework for Parameter-Efficient Transformers Serving.\nIn Proceedings of the USENIX Annual Technical Conference (USENIX\nATC).\n[122] Ruidong Zhu, Ziheng Jiang, Chao Jin, Peng Wu, Cesar A Stuardo,\nDongyang Wang, Xinlei Zhang, Huaping Zhou, Haoran Wei, Yang\nCheng, et al. 2025. MegaScale-Infer: Serving Mixture-of-Experts\nat Scale with Disaggregated Expert Parallelism.\narXiv preprint\narXiv:2504.02263 (2025).\n[123] Shivani Zoting. 2025. Artificial Intelligence (AI) Market Size and\nGrowth 2025 to 2034. https://www.precedenceresearch.com/artificial-\nintelligence-market.\n17\n",
    "content": "# *Reimagining the AI Data Center Lifecycle: A TCO-Driven Framework* – Paper Summary and Analysis\n\n## 1. Core Content and Key Contributions\n\nThe central objective of this paper is **to systematically reframe traditional data center lifecycle management paradigms to meet the unique demands of large language model (LLM) inference workloads, with minimizing Total Cost of Ownership (TCO) as the primary optimization goal**.\n\nConventional data center lifecycle management—spanning construction, hardware refresh, and operations—was designed for general-purpose computing. However, today’s AI inference clusters, built around high-power GPUs, exhibit rapidly escalating demands in performance, energy consumption, cooling, and networking. This renders traditional approaches inefficient and cost-prohibitive. The paper proposes a novel, end-to-end integrated framework specifically tailored to address these challenges across the entire lifecycle.\n\n### Key Contributions:\n\n- **Comprehensive Characterization of LLM Workloads and GPU Power-Performance Dynamics**: The authors analyze trends over the past decade—including explosive growth in model parameters, advancements in GPU compute and bandwidth, and rising user demand—to understand how these factors influence data center design and operations across lifecycle stages.\n\n- **Evaluation of Stage-Specific Optimization Strategies**: The lifecycle is divided into three critical phases—**Build**, **IT Hardware Provisioning**, and **Operation**—with an assessment of optimization strategies available at each stage and their impact on TCO.\n\n- **First Holistic, Cross-Stage Optimization Framework for AI**: This is the paper’s most significant contribution. The authors introduce a **unified, cross-phase, TCO-driven framework** that jointly optimizes decisions across all three lifecycle stages. By accounting for workload dynamics, hardware evolution, and technological obsolescence, and using simulation to forecast future scenarios, the framework achieves up to **40% reduction in TCO**.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nThe paper’s novelty and impact span multiple dimensions:\n\n### (1) Fundamental Reconceptualization of the Lifecycle Perspective\n- **From \"Linear Process\" to \"Closed-Loop Collaboration\"**: Traditional methods treat build, provisioning, and operation as isolated phases. This paper emphasizes their strong interdependencies—for instance, initial power topology design affects future hardware flexibility and runtime resource utilization. This “full-lifecycle co-optimization” represents a paradigm shift.\n\n### (2) Introduction and Validation of Disruptive Technical Strategies\n- **Power Infrastructure**: Challenges conventional tiered power delivery by proposing a flatter, **data-center-wide power architecture (per-DC)**, significantly reducing stranded capacity due to power fragmentation and lowering TCO.\n- **Cooling Solutions**: Advocates for **hybrid cooling**, combining liquid cooling for high-density AI racks and air cooling for low-density equipment, achieving the optimal balance between capital expenditure (CapEx) and operational expenditure (OpEx)—proven to yield the lowest TCO.\n- **Network Architecture**: Proposes **hierarchical networking**, integrating NVLink (intra-server), InfiniBand (intra-rack), and Ethernet (inter-rack) to precisely match LLM communication patterns (e.g., tensor and pipeline parallelism), avoiding costly over-provisioning of high-speed links.\n\n### (3) Flexible, Data-Driven Hardware Refresh Strategy\n- **Moving Beyond Fixed Refresh Cycles**: Recognizing rapid AI hardware iteration, the paper advocates for **dynamic refresh policies**. If a new GPU generation offers substantial efficiency gains, early retirement of old hardware is justified; otherwise, older hardware should be retained or even skip intermediate generations—contrasting sharply with rigid 5-year refresh cycles.\n\n### (4) Software as the Bridge Across Lifecycle Stages\n- The paper highlights that **software techniques during operation**—such as model disaggregation, heterogeneous scheduling, quantization, and KV cache management—are not just efficiency tools but essential enablers of cross-stage optimization. For example, **prefill-decode phase disaggregation** allows use of mixed-generation hardware, which in turn influences procurement and infrastructure planning.\n\n### (5) Quantitative Validation and Forward-Looking Guidance\n- The authors developed a detailed TCO model, validated with public data and Monte Carlo simulations, quantifying the benefits: **15–23% savings per individual optimization**, and up to **40% overall TCO reduction** with full integration.\n- They provide a decision-making guide (Table 8) based on projected trends in model and hardware advancement (fast/medium/slow), offering practical, actionable insights for industry stakeholders.\n\n---\n\n## 3. Startup Ideas Inspired by the Paper\n\nGiven the vast cost-saving potential and innovative directions revealed in the paper, here are several promising startup concepts:\n\n### 🚀 Startup Idea 1: Full-Cycle AI Data Center TCO Optimization – Consulting & SaaS Platform\n\n**Name**: LifecycleAI / TCOOptima\n\n**Core Concept**: Productize the paper’s framework into a comprehensive service offering TCO optimization from planning through operations for cloud providers, enterprises, and AI startups.\n\n**Products & Services**:\n- **SaaS Decision Engine**: A cloud platform where users input business forecasts, model roadmaps, and budget constraints. The system applies the paper’s models to recommend optimal strategies across build, refresh, and operation (e.g., cooling type, GPU procurement timing, generational skipping).\n- **Digital Twin Simulation**: Build a virtual replica of the client’s data center to simulate TCO, PUE, and carbon emissions under various scenarios, enabling robust \"what-if\" analysis.\n- **Expert Advisory Services**: Offer consulting from seasoned data center architects to help implement complex optimizations.\n\n**Market Positioning**: Target organizations investing heavily in AI infrastructure but lacking in-house expertise, helping them save tens or even hundreds of millions in TCO.\n\n---\n\n### 🚀 Startup Idea 2: Intelligent Orchestration System for Heterogeneous AI Clusters\n\n**Name**: HeteroFlow / GenieScheduler\n\n**Core Concept**: As the paper identifies intelligent scheduling in heterogeneous environments as key to TCO reduction, develop an OS-level scheduler specifically optimized for AI, surpassing Kubernetes’ native capabilities.\n\n**Key Features**:\n- **Deep Performance-Aware Scheduling**: Goes beyond GPU identification to understand performance-per-watt-per-dollar profiles of different models (LLMs, MoEs, SSMs) across GPU generations, automatically assigning workloads to the most cost-efficient hardware.\n- **Native Phase Disaggregation Support**: Integrates prefill-decode splitting natively—automatically running compute-heavy prefill on high-end GPUs (e.g., H200) and memory-intensive decode on older, cheaper GPUs (e.g., A100/V100).\n- **Infrastructure-Aware Load Balancing**: Dynamically adjusts workload placement based on real-time PUE, thermal hotspots, and power availability to avoid bottlenecks.\n\n**Technical Barrier**: Requires deep expertise in systems programming, distributed systems, and AI hardware to build a low-latency, high-throughput scheduler core.\n\n**Business Model**: Open-source core with premium enterprise subscription (advanced features, support, monitoring dashboards).\n\n---\n\n### 🚀 Startup Idea 3: \"Green\" Secondary Market for AI Hardware and Reuse-as-a-Service\n\n**Name**: GPURecycle / AI-Hardware-as-a-Service (AHAAS)\n\n**Core Concept**: Since the paper shows older GPUs (e.g., A100, V100) remain cost-effective for smaller models or specific phases (decode), create a platform to facilitate the resale, refurbishment, and efficient reuse of decommissioned AI hardware.\n\n**Business Model**:\n- **B2B Marketplace**: Connect cloud providers retiring GPUs with AI startups and research labs needing affordable compute.\n- **Refurbishment & Certification**: Offer professional testing, repair, cleaning, and performance validation to ensure reliability and trust in used hardware.\n- **Compute Leasing Service**: Launch an **AI-as-a-Service** model where customers rent access to optimized heterogeneous clusters (mix of new and legacy GPUs), with backend scheduling maximizing cost efficiency.\n- **Software Integration**: Bundle with orchestration tools that support heterogeneous workloads, enabling seamless adoption.\n\n**Social Impact**: Reduces e-waste and carbon footprint, aligning with ESG goals while creating economic value.\n\n---\n\n## Conclusion\n\nThis paper is more than a high-caliber academic study—it functions as a **strategic blueprint for building data centers in the AI era**. Each optimization insight it reveals opens doors to significant commercial opportunities. Entrepreneurs can draw inspiration to build the “brain” (decision intelligence platforms), the “muscle” (execution engines like schedulers), or the “circular economy” (hardware reuse marketplaces). In the coming wave of AI infrastructure investment, these ideas represent clear paths to untapped blue oceans.",
    "github": "",
    "hf": ""
}