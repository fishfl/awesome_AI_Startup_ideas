{
    "id": "2509.08827",
    "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
    "summary": "This paper reviews the latest advancements in using reinforcement learning to enhance the reasoning capabilities of large language models, and explores the challenges and strategies involved in further extending reinforcement learning to achieve artificial superintelligence.",
    "abstract": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github:this https URL",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Kaiyan Zhang,Yuxin Zuo,Bingxiang He,Youbang Sun,Runze Liu,Che Jiang,Yuchen Fan,Kai Tian,Guoli Jia,Pengfei Li,Yu Fu,Xingtai Lv,Yuchen Zhang,Sihang Zeng,Shang Qu,Haozhan Li,Shijie Wang,Yuru Wang,Xinwei Long,Fangfu Liu,Xiang Xu,Jiaze Ma,Xuekai Zhu,Ermo Hua,Yihao Liu,Zonglin Li,Huayu Chen,Xiaoye Qu,Yafu Li,Weize Chen,Zhenzhao Yuan,Junqi Gao,Dong Li,Zhiyuan Ma,Ganqu Cui,Zhiyuan Liu,Biqing Qi,Ning Ding,Bowen Zhou",
    "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
    ],
    "comments": "",
    "keypoint": "• Rule-based rewards provide scalable and reliable training signals for RL, especially in math and code tasks, by leveraging accuracy and format checks.\n• Verifier’s law highlights that tasks with clear and automatic verification enable efficient RL optimization, while subjective tasks remain challenging.\n• Generative Reward Models (GenRMs) extend RL to subjective, non-verifiable domains by providing nuanced, text-based feedback.\n• Model-based verifiers and generative rewards are used to address limitations of rule-based systems in handling complex reasoning tasks.\n• Token-level and step-level rewards offer fine-grained credit assignment and improve training efficiency and optimization stability.\n• Unsupervised rewards help bypass human annotation bottlenecks by using the model’s internal knowledge or external data.\n• Reward shaping enriches sparse signals into stable gradients by combining verifiers with reward models and using group baselines.\n• Policy gradient objective is central to RL optimization, aiming to maximize expected reward through gradient estimation.\n• Critic-based algorithms like PPO and VCPPO are used to train models with a critic that provides value signals.\n• Critic-free algorithms like REINFORCE and GRPO simplify training by only requiring sequence-level rewards.\n• Off-policy optimization decouples data collection from policy learning, improving sample efficiency.\n• Regularization objectives like KL and entropy regularization balance exploration and exploitation.\n• Dynamic sampling strategies enhance training efficiency and manage exploration-exploitation trade-offs.\n• Sampling hyper-parameter adjustments like temperature tuning and length budgeting are crucial for stable RL training.\n• RL sharpens existing reasoning capabilities rather than discovering entirely new ones, relying on verifiable tasks.\n• RL differs from SFT in generalizing reasoning rather than memorizing data.\n• Weak model priors benefit more from RL compared to strong priors, where training methods like mid-training improve performance.\n• Training recipes with RL involve balancing various techniques, distinguishing between effective strategies and potential pitfalls.\n• Process-based rewards offer detailed feedback during reasoning steps, contrasting with outcome-based rewards.\n• Static datasets support training with curated, verifiable data across domains like math and code.\n• Dynamic environments provide interactive settings for RL training, enhancing realism and scalability.\n• RL infrastructure frameworks like OpenRLHF and veRL facilitate scalable and efficient training pipelines.\n• RL is applied to coding tasks to improve code generation and software engineering through automated testing.\n• Agentic tasks leverage RL for autonomous decision-making in areas like search and tool use.\n• Multimodal tasks benefit from RL in integrating text, image, and video reasoning.\n• Multi-agent systems use RL for coordinated and strategic interactions between multiple LLMs.\n• Robotics tasks apply RL to enhance autonomous manipulation and navigation capabilities.\n• Medical tasks use RL for both understanding and generating clinical insights, leveraging verifiable rewards.\n• Future directions include continual and memory-based RL, model-based approaches, and co-design of architecture and algorithms.\n• RL for LLMs pre-training and diffusion models represents emerging areas with potential for scalable learning.\n• RL contributes to scientific discovery by optimizing reasoning processes in specialized domains.\n• RL infrastructure and frameworks continue to evolve to support large-scale training and deployment.",
    "date": "2025-09-14",
    "paper": "A Survey of Reinforcement Learning for Large Reasoning Models\n2025-09-11\nA Survey of Reinforcement Learning for Large\nReasoning Models\nKaiyan Zhang1∗†, Yuxin Zuo1∗†, Bingxiang He1∗, Youbang Sun1∗, Runze Liu1∗, Che Jiang1∗, Yuchen Fan2,3∗,\nKai Tian1∗, Guoli Jia1∗, Pengfei Li2,6∗, Yu Fu9∗, Xingtai Lv1∗, Yuchen Zhang2,4∗, Sihang Zeng7∗, Shang Qu1,2∗,\nHaozhan Li1∗, Shijie Wang2∗, Yuru Wang1∗, Xinwei Long1, Fangfu Liu1, Xiang Xu5, Jiaze Ma1, Xuekai Zhu3,\nErmo Hua1,2, Yihao Liu1,2, Zonglin Li2, Huayu Chen1, Xiaoye Qu2, Yafu Li2, Weize Chen1, Zhenzhao Yuan1,\nJunqi Gao6, Dong Li6, Zhiyuan Ma8, Ganqu Cui2, Zhiyuan Liu1, Biqing Qi2‡, Ning Ding1,2‡, Bowen Zhou1,2‡\n1 Tsinghua University\n2 Shanghai AI Laboratory\n3 Shanghai Jiao Tong University\n4 Peking University\n5 University of Science and Technology of China\n6 Harbin Institute of Technology\n7 University of Washington\n8 Huazhong University of Science and Technology\n9 University College London\n† Project Lead.\n∗Core Contributors.\n‡ Corresponding Authors.\n# zhang-ky22@mails.tsinghua.edu.cn\n§ TsinghuaC3I/Awesome-RL-for-LRMs\nAbstract | In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large\nLanguage Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities,\nparticularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged\nas a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further\nscaling of RL for LRMs now faces foundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of\nthis domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial\nSuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning\nabilities, especially since the release of DeepSeek-R1, including foundational components, core problems,\ntraining resources, and downstream applications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for broader reasoning models.\nSection 3\nFoundational Components\nReward Design\nSection 4\nFoundational Problems\nRL’s Role\nModel Prior\nTraining Recipes\nRL vs. SFT\nReward Type\nStatic Corpus\nMath\nCode\nSTEM\nAgent\nMixture\nDynamic Environment\nRule\nCode\nGame\nEnsemble\nRL Infrastructure & \nFrameworks\ne.g.,OpenRLHF/veRL/AReaL/slime/TRL\nSection 6\nApplications\nAgentic Tasks\nCoding Tasks\nMultimodal \nTasks\nMulti-Agent \nSystems\nMedical \nTasks\nRobotics \nTasks\nRewards \nShaping\nDense \nRewards\nUnsupervised \nRewards\nSharpening  Discovery\nVS\nWeak\nStrong\nVS\nTricks\nTraps\nVS\nGeneralize\nMemorize\nVS\nProcess\nOutcome\nVS\nSection 5\nTraining Resource\nPolicy Optimization\nCritic-Based \nAlgorithms\nPolicy \nGradient\nCritic-Free \nOff-Policy  Regularization \nAlgorithms Optimization Objectives\nSampling Strategy\nDynamic \nSampling \nSampling  Hyper-Parameters\nModel\nVerifiable \nRewards\nGenerative \nRewards\nCompute\nIntelligence\nMore Scalable\nEvolution (Training Step)\nInteraction (Episode)\nAgent\nEnvironment\nReward\nrt\nAction\nat\nFigure 1 | Overview of the survey. We introduce the foundational components of RL for LRMs,\nalong with open problems, training resources, and applications. Central to this survey is a focus on\nlarge-scale interactions between language agents and environments throughout long-term evolution.\narXiv:2509.08827v1  [cs.CL]  10 Sep 2025\nContents\n1\nIntroduction\n4\n2\nPreliminaries\n5\n2.1\nBackground . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nFrontier Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nRelated Surveys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nFoundational Components\n10\n3.1\nReward Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.1.1\nVerifiable Rewards\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.1.2\nGenerative Rewards\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n3.1.3\nDense Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.1.4\nUnsupervised Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n3.1.5\nRewards Shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.2\nPolicy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.2.1\nPolicy Gradient Objective\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.2.2\nCritic-based Algorithms\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.2.3\nCritic-Free Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n3.2.4\nOff-policy Optimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n3.2.5\nRegularization Objectives\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n3.3\nSampling Strategy\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n3.3.1\nDynamic and Structured Sampling . . . . . . . . . . . . . . . . . . . . . . . .\n28\n3.3.2\nSampling Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n4\nFoundational Problems\n30\n4.1\nRL’s Role: Sharpening or Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n4.2\nRL vs. SFT: Generalize or Memorize . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n4.3\nModel Prior: Weak and Strong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n4.4\nTraining Recipes: Tricks or Traps\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n4.5\nReward Type: Process or Outcome\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n5\nTraining Resources\n37\n5.1\nStatic Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n5.2\nDynamic Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n5.3\nRL Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n6\nApplications\n46\n6.1\nCoding Tasks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n6.2\nAgentic Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n6.3\nMultimodal Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n2\nA Survey of Reinforcement Learning for Large Reasoning Models\n6.4\nMulti-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n6.5\nRobotics Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n6.6\nMedical Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n7\nFuture Directions\n59\n7.1\nContinual RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n7.2\nMemory-based RL for LLMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n7.3\nModel-based RL for LLMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n7.4\nTeaching LRMs Efficient Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n7.5\nTeaching LLMs Latent Space Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n7.6\nRL for LLMs Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n7.7\nRL for Diffusion-based LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n7.8\nRL for LLMs in Scientific Discovery\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n7.9\nRL for Architecture-Algorithm Co-Design . . . . . . . . . . . . . . . . . . . . . . . . .\n62\n8\nConclusion\n63\nAuthor Contributions\n64\n3\nA Survey of Reinforcement Learning for Large Reasoning Models\n1. Introduction\nReinforcement Learning (RL) [Sutton et al., 1998] has repeatedly demonstrated that narrow, well-\nspecified reward signals can drive artificial agents to superhuman competence on complex tasks.\nLandmark systems such as AlphaGo [Silver et al., 2016] and AlphaZero [Silver et al., 2017], which\nlearned exclusively through self-play and reward feedback, surpassed world champions in Go, chess,\nshogi and Stratego [Perolat et al., 2022, Schrittwieser et al., 2020, Silver et al., 2018], establishing\nRL as a practical and promising technology for high-level problem solving. In the era of Large\nLanguage Models (LLMs) [Zhao et al., 2023a], RL initially rose to prominence as a post-training\nstrategy for human alignment [Ouyang et al., 2022]. Widely adopted methods such as Reinforcement\nLearning from Human Feedback (RLHF) [Christiano et al., 2017] and Direct Preference Optimization\n(DPO) [Rafailov et al., 2023] finetune pre-trained models to follow instructions and reflect human\npreferences, markedly improving helpfulness, honesty, and harmlessness (3H) [Bai et al., 2022b].\nMore recently, a new trend has emerged: RL for Large Reasoning Models (LRMs) [Xu et al., 2025a],\nwhich aims not merely to align behavior but to incentivize reasoning itself. Two recent milestones\n(i.e., OpenAI o1 [Jaech et al., 2024] and DeepSeek-R1 [Guo et al., 2025a]) demonstrate that training\nLLMs using reinforcement learning with verifiable rewards (RLVR), such as answer correctness for\nmathematics or unit-test pass rates for code, can enable models to perform long-form reasoning,\nincluding planning, reflection, and self-correction. OpenAI reports [Jaech et al., 2024] that o1’s\nperformance improves smoothly with both additional RL (increased train-time compute) and more\ntime spent “thinking” at inference (test-time compute) [Brown et al., 2024, Liu et al., 2025l, Snell\net al., 2024], revealing a new scaling axis beyond pre-training alone [Aghajanyan et al., 2023, Kaplan\net al., 2020]. DeepSeek-R1 [Guo et al., 2025a] employs explicit, rule-based accuracy rewards for\nmathematics, as well as compiler- or test-based rewards for coding tasks. This approach demonstrates\nthat large-scale reinforcement learning, specifically, Group Relative Policy Optimization (GRPO), can\ninduce sophisticated reasoning behaviors even in base models prior to subsequent alignment stages.\nThis shift reframes reasoning as a capability that can be explicitly trained and scaled [OpenAI,\n2025a,b]: LRMs allocate significant test-time compute to generate, evaluate, and revise intermediate\nchain-of-thought [Wei et al., 2022], and their performance rises as this compute budget increases.\nThis dynamic introduces a complementary path to capability gains, orthogonal to data and parameter\nscaling during pre-training [Aghajanyan et al., 2023, Kaplan et al., 2020], while leveraging a reward\nmaximization objective [Silver et al., 2021], automatically checkable rewards wherever reliable\nverifiers exist (e.g., competition mathematics [Guo et al., 2025a, Jaech et al., 2024], competitive\nprogramming [El-Kishky et al., 2025], and selected scientific domains [Bai et al., 2025]). Furthermore,\nRL can overcome data limitations [Shumailov et al., 2024, Villalobos et al., 2022] by enabling self-\ngenerated training data [Silver et al., 2018, Zhao et al., 2025a]. As a result, RL is increasingly\nregarded as a promising technology for achieving Artificial SuperIntelligence (ASI) on a broader range\nof tasks through continual scaling.\nAt the same time, further scaling of RL for LRMs introduces new constraints, not only in compu-\ntational resources, but also in algorithm design, training data, and infrastructure. How and where\nRL for LRMs can be scaled to achieve high-level intelligence and generate real-world value remain\nunresolved issues. Therefore, we argue that it is timely to revisit the development of this domain and\nexplore strategies to enhance the scalability of RL toward artificial superintelligence. In summary,\nthis survey reviews recent work on RL for LRMs as follows:\n• We introduce the preliminary definitions of RL modeling in the context of LRMs (§ 2.1) and\noutline the development of frontier reasoning models since the release of OpenAI o1 (§ 2.2).\n• We review recent literature on the foundational components of RL for LRMs, including reward\n4\nA Survey of Reinforcement Learning for Large Reasoning Models\nGPT-3.5, GPT-4\nLlama 3, Qwen 2.5\nReward-based\nReward-free\nRL from Human\nFeedback \nDirect Preference\nOptimization\no1, DeepSeek-R1\nRule-based\nRL with Verifiable\nReward \n \n \n2023\n2025\n2022\nTask Solving Capacity\n  RLHF\nDPO\nRLVR\nOpen-ended RL\nFigure 2 | RLHF and DPO have been the two predominant RL methodologies for human alignment in\nrecent years. In contrast, RLVR represents an emerging trend in RL for LRMs, significantly enhancing\ntheir capacity for complex task solving. The next stage of scaling RL for LLMs remains an open\nquestion, with open-ended RL presenting a particularly challenging and promising direction.\ndesign (§ 3.1), policy optimization (§ 3.2), and sampling strategies (§ 3.3), comparing the\ndifferent research directions and technical approaches for each component.\n• We discuss foundational and still controversial problems in RL for LRMs (§ 4), such as the role of\nRL (§ 4.1), RL versus Supervised Fine-Tuning (SFT) (§ 4.2), model priors (§ 4.3), training recipes\n(§ 4.4), and reward definitions (§ 4.5). We argue that these issues warrant further exploration to\nenable continued scaling of RL.\n• We examine training resources for RL (§ 5), including static corpora (§ 5.1), dynamic environments\n(§ 5.2), and training infrastructure (§ 5.3). While these resources are reusable in both research\nand production, further standardization and development are needed.\n• We review applications of RL to a wide range of tasks (§ 6), such as coding tasks (§ 6.1), agentic\ntasks (§ 6.2), multimodal tasks (§ 6.3), multi-agent systems (§ 6.4), robotics tasks (§ 6.5), and\nmedical applications (§ 6.6).\n• Finally, we discuss future directions in RL for language models (§ 7), covering novel algorithms,\nmechanisms, features, and additional research avenues.\n2. Preliminaries\n2.1. Background\nIn this subsection, we introduce the basic components of RL and describe how language models can be\nconfigured as agents within RL frameworks. As shown in Figure 3, RL provides a general framework\nfor sequential decision making, in which an agent interacts with an environment by taking actions\nto maximize cumulative reward. In classical RL, the problem is typically formulated as a Markov\n5\nA Survey of Reinforcement Learning for Large Reasoning Models\nAgent\nstate\nst\nrt+1\nrt+1\nreward\nrt\naction\nat\nEnvironment\nstate\ny1  y2\nx1  x2 x3\n······\nxT-1 xT\nreward\naction\nLanguage Model\nR(x,y)\nFigure 3 | Basic components of RL and language models (LMs) as agents. The agent selects actions,\nwhile the environment provides states and rewards at each turn. In the context of LMs, completion\ntokens are treated as actions, which are concatenated with the context to form the state. Rewards are\ntypically assigned at the level of the entire response.\nDecision Process (MDP) [Sutton et al., 1998], which is defined by a tuple (S, A, P, 𝑅, 𝛾). The main\ncomponents include a state space S, an action space A, transition dynamics P : S ×A ↦→S, a reward\nfunction 𝑅: S × A ↦→ℝ, and a discount factor 𝛾∈[0, 1]. At each step, the agent observes a state 𝑠𝑡,\nselects an action 𝑎𝑡according to its policy 𝜋𝜃parameterized by 𝜃, receives a reward 𝑟𝑡, and transits to\nthe next state 𝑠𝑡+1. When applying RL to language models, these concepts can be naturally mapped\nto the language domain with minimal adaptation. The mapping is summarized as follows:\n• Prompt/Task (𝑥): Corresponds to the initial state or environment context, drawn from a data\ndistribution and corresponding to the dataset D.\n• Policy (𝜋𝜃): Represents the language model, which generates a sequence of tokens 𝑦= (𝑎1, . . . , 𝑎𝑇)\nin response to the prompt, where 𝑇= |𝑦| denotes the sequence length.\n• State (𝑠𝑡): Defined as the prompt together with the sequence generated so far, i.e., 𝑠𝑡= (𝑥, 𝑎1:𝑡−1).\n• Action (𝑎𝑡): The token chosen at step 𝑡from the vocabulary V, which is also A.\n• Transition Dynamics (P): The state transition is usually deterministic in the context of LLM\nsince 𝑠𝑡+1 = [𝑠𝑡, 𝑎𝑡], where [·, ·] denotes string concatenation.\n• Reward (𝑅(𝑥, 𝑦) or 𝑟𝑡): Typically assigned at the end of the sequence (sequence-level), denoted\n𝑅(𝑥, 𝑦), but may also be decomposed into token-level rewards 𝑟𝑡with process supervision.\n• Return (𝐺): The cumulative (optionally discounted) reward accrued over the whole trajectory 𝑦\nfor prompt 𝑥. With sequence-level feedback it reduces to the single scalar 𝑅(𝑥, 𝑦); with token-level\nfeedback it aggregates the per-token rewards 𝑟𝑡(typically with 𝛾=1 for finite 𝑇).\nIn this setting, the learning objective [Sutton et al., 1998] is to maximize the expected reward\nover the data distribution D, that is,\nmax\n𝜃\nJ (𝜃) := 𝔼𝑥∼D,𝑦∼𝜋𝜃(𝑥) [𝑅(𝑥, 𝑦)].\n(1)\nIn practice, it is common to regularize the learned policy towards a reference policy 𝜋ref, often\nimplemented as KL-divergence constraints to stabilize training and maintain language quality. In the\nfollowing sections, we present various algorithms that build upon this fundamental formulation.\n6\nA Survey of Reinforcement Learning for Large Reasoning Models\n2.2. Frontier Models\nIn this subsection, we provide an overview of state-of-the-art large reasoning models trained with\nRL-like methods, organized roughly chronologically along three major directions: LRMs, agentic\nLRMs, and multimodal LRMs.\nOver the past year, RL has progressively expanded the frontier of reasoning models and their\napplications. The first large reasoning models, OpenAI’s o1 [Jaech et al., 2024] series, established the\neffectiveness of scaling both train-time RL and test-time compute towards more powerful reasoning\nabilities, achieving leading results on mathematics, coding, and science benchmarks. DeepSeek’s\nflagship model R1 [Guo et al., 2025a] followed as the first open-source model to match o1’s per-\nformance across benchmarks. It employs a multi-stage training pipeline to ensure well-rounded\nmodel abilities, and explores the route of pure RL without supervised finetuning (i.e., Zero RL).\nOther proprietary model releases promptly followed: Claude-3.7-Sonnet [Anthropic, 2025a] featured\nhybrid reasoning, Gemini 2.0 and 2.5 [Comanici et al., 2025] introduced longer context lengths,\nSeed-Thinking 1.5 [Seed et al., 2025b] featured generalization across domains, and the o3 [OpenAI,\n2025b] series showcased increasingly advanced reasoning abilities. Recently, OpenAI introduced their\nfirst open-source reasoning model gpt-oss-120b [Agarwal et al., 2025a], and subsequently GPT5 [Ope-\nnAI, 2025a], their most capable AI system to date, which flexibly switches between an efficient model\nand a deeper reasoning model GPT-5 thinking. Parallel open-source efforts continued to expand\nthe landscape. Within the Qwen family, QwQ-32B [Team, 2025g] matched R1’s performance, and\nwas followed by the Qwen3 [Yang et al., 2025a] series, with the representative model Qwen3-235B\nfurther improving benchmark scores. The Skywork-OR1 [He et al., 2025d] suite of models were\nbased on R1-distilled models, and achieved scalable RL training through effective data mixtures and\nalgorithmic innovations. Minimax-M1 [Chen et al., 2025a] was the first model to introduce hybrid\nattention to scale RL efficiently. Other works include Llama-Nemotron-Ultra [Bercovich et al., 2025],\nwhich aimed to balance accuracy and efficiency; Magistral 24B [Rastogi et al., 2025], trained through\nRL from scratch without distillation from prior models; and Seed-OSS [Team, 2025a], emphasizing\nlong-context reasoning abilities.\nModel reasoning improvements have in turn extended their use cases in coding and agentic\nscenarios. The Claude series has been known for their leading performance on agentic coding tasks,\nand this was exemplified by Claude-4.1-Opus [Anthropic, 2025b], which further pushed forward\nthe state-of-the-art results on SWE-bench [Jimenez et al., 2023]. Kimi K2 [Team, 2025d] is a recent\nrepresentative agentic model which was specifically optimized for agentic tasks, forging large-scale\nagentic training data synthesis and a general RL procedure that accommodates non-verifiable rewards.\nShortly after, both the GLM4.5 [Zeng et al., 2025a] and DeepSeek-V3.1 releases emphasized tool-use\nand agentic tasks, showing substantial improvements on relevant benchmarks.\nMultimodality is a key component behind the widespread adoption of reasoning models. Most\nfrontier proprietary models, including GPT-5, o3, Claude, and Gemini families, are natively multimodal.\nGemini-2.5 [Comanici et al., 2025] notably emphasized strong performance across text, images, video,\nand audio. On the open-source side, Kimi 1.5 [Team, 2025d] represents an early effort towards\nmultimodal reasoning, highlighting long context scaling as well as joint reasoning over text and vision\ndomains. QVQ [Qwen Team, 2025] excels in visual reasoning and analytical thinking, while Skywork\nR1V2 [Wang et al., 2025j] balances reasoning and general abilities through hybrid RL, using both\nMPO and GRPO. As notable additions to the InternVL series, InternVL3 [Zhu et al., 2025c] adopted\na unified native multimodal pretraining phase, and later InternVL3.5 [Wang et al., 2025n] used a\ntwo-stage cascade RL framework, achieving improved efficiency and versatility. More recently, the\nIntern-S1 [Bai et al., 2025] model focused on multimodal scientific reasoning across diverse domains,\nbenefiting from a mixture-of-rewards design during online RL to facilitate simultaneous training on a\n7\nA Survey of Reinforcement Learning for Large Reasoning Models\n1\n3\n5\n7\n2\n4\n2024\n2025\nPublicly Available \nPublicly Unavailable\nDeepSeek-R1 671B\nKimi 1.5\nORZ 0.5-32B\nLlama-Nemotron-Ultra 253B \nINTELLECT-2 32B\nMiMo 7B\nQwen3 0.6-235B\nSkywork OR-1 7/32B \nKimi k2 1T\nStep 3 321B\nIntern-S1 241B\nClaude 3.7 Sonnet\no3-mini\nGemini 2.0 Flash\no3\no4-mini\no1-mini\no1-2024-12-17\no1-preview\nMultimodal\nLegend\nGemini 2.5 Flash-Lite\nQWQ 32B\nGrok 4\nDeepSeek-R1-0528 671B\nQwen3-2507-Thinking 4-235B\nQVQ-Max\nGLM-4.1V-Thinking 9B\nSeed-Thinking 1.5\nPhi-4 Reasoning 14B\n   \n \n \nInternV\nSkywork-R1V2 38B\nL3 1-78B\n8\nGPT-5\nClaude Opus 4.1\n  \ngpt-oss 21/117B\nGLM-4.5 355B\nDeepSeek-V3.1 671B\nGLM-4.5V 106B\n InternVL3.5 1-241B\n6\nMagistral 24B\nMinimax-M1 456B\no3-pro\nGemini 2.5 Flash\nGemini 2.5 Pro\nSeed-Thinking 1.6\nHunyuan-TurboS\nClaude 4 \nFigure 4 | Timeline of representative open-source and closed-source reasoning models trained with\nRL, including language models, multimodal models, and agentic models.\nwide range of tasks. Other recent models include Step3 [Wang et al., 2025a], designed for efficient\ntraining and minimizing decoding costs, and GLM-4.5V [Team et al., 2025a], with state-of-the-art\nperformance across most visual multimodal benchmarks.\nIn addition to the aforementioned models, we provide a comprehensive list of reasoning models\nin Figure 4 and detailed information on open-source models in Table 1.\n2.3. Related Surveys\nIn this subsection, we compare recent surveys related to RL and LLMs. Several surveys focus primarily\non RL itself, covering both classical RL and its recent extensions. Ghasemi et al. [2024] present\na general RL survey covering algorithms and real-world challenges, Huh and Mohapatra [2023]\nfocuse on multi-agent RL, Zhang et al. [2024b] review self-play techniques, and Wu et al. [2025h]\nsurvey RL in computer vision tasks. While these works offer broad perspectives on RL, they do not\nexplicitly address its application to LLMs. In contrast, other surveys center on LLMs and their emerging\ncapabilities, such as long chain-of-thought reasoning [Chen et al., 2025m, Li et al., 2025u, Xia et al.,\n2024] and adaptive behaviors [Feng et al., 2025c, Sui et al., 2025], where RL is often introduced\nas a key method to support these advances. Zhao et al. [2023a] provide a broad overview of LLM\narchitectures and applications, while more recent works concentrate specifically on reasoning abilities.\nZhang et al. [2025a] survey replication studies on reasoning LLMs in the wake of DeepSeek-R1, Chen\net al. [2025m] examine long chain-of-thought reasoning, and Li et al. [2025u] analyze the transition\nfrom System 1 to System 2 reasoning. These studies highlight RL-based methods such as RLHF and\nRLVR as useful tools, but treat them as only one element among a wide range of reasoning strategies.\nSun et al. [2025b] offer a broader, structured take on reasoning via foundation models. It highlights\nkey foundation models that are either proposed or adapted specifically for reasoning, as well as\nrecent progress across diverse reasoning tasks, methodologies, and benchmarks. Zhang et al. [2025b]\n8\nA Survey of Reinforcement Learning for Large Reasoning Models\nTable 1 | Comparison of representative open-source models trained with RL. OPMD denotes Online\nPolicy Mirror Descent; MPO denotes Mixed Preference Optimization; CISPO denotes Clipped IS-\nweight Policy Optimization. T, I, and V indicate Text, Image, and Video modalities, respectively.\nDate\nModel\nOrganization\nArchitecture\nParameters\nAlgorithm\nModal\nLink\n2025.01\nDeepSeek-R1\nDeepSeek\nMoE/MLA\n671B\nGRPO\nText\n§\n[Guo et al., 2025a]\n2025.03\nORZ\nStepAI\nDense\n0.5-32B\nPPO\nText\n§\n[Hu et al., 2025b]\n2025.03\nQwQ\nAlibaba Qwen\nDense\n32B\n-\nText\n§\n[Team, 2025g]\n2025.04\nPhi-4 Reasoning\nMicrosoft\nDense\n14B\nGRPO\nText\n§\n[Abdin et al., 2025]\n2025.04\nSkywork-R1V2\nSkywork\nDense\n38B\nMPO/GRPO\nT/I\n§\n[Wang et al., 2025j]\n2025.04\nInternVL3\nShanghai AI Lab\nDense\n1-78B\nMPO\nT/I/V\n§\n[Zhu et al., 2025c]\n2025.04\nMiMo\nXiaomi\nDense\n7B\nGRPO\nText\n§\n[Xiaomi et al., 2025]\n2025.04\nQwen3\nAlibaba Qwen\nMoE/Dense\n0.6-235B\nGRPO\nText\n§\n[Yang et al., 2025a]\n2025.05\nLlama-Nemotron-Ultra\nNVIDIA\nDense\n253B\nGRPO\nText\n§\n[Bercovich et al., 2025]\n2025.05\nINTELLECT-2\nIntellect AI\nDense\n32B\nGRPO\nText\n[Team et al., 2025b]\n2025.05\nHunyuan-TurboS\nTencent\nHybrid MoE\n560B\nGRPO\nText\n§\n[Team et al., 2025c]\n2025.05\nSkywork OR-1\nSkywork\nDense\n7B/32B\nGRPO\nText\n§\n[He et al., 2025d]\n2025.05\nDeepSeek-R1-0528\nDeepSeek\nMoE/MLA\n671B\nGRPO\nText\n§\n[Guo et al., 2025a]\n2025.06\nMagistral\nMistral AI\nDense\n24B\nGRPO\nText\n[Rastogi et al., 2025]\n2025.06\nMinimax-M1\nMinimax\nHybrid MoE\n456B\nCISPO\nText\n§\n[Chen et al., 2025a]\n2025.07\nIntern-S1\nShanghai AI Lab\nMoE\n241B\nGRPO\nT/I/V\n§\n[Bai et al., 2025]\n2025.07\nKimi K2\nKimi\nMoE\n1T\nOPMD\nText\n§\n[Team, 2025c]\n2025.07\nStep 3\nStep AI\nMoE\n321B\n-\nT/I/V\n§\n[Wang et al., 2025a]\n2025.07\nQwen3-2507\nAlibaba Qwen\nMoE/Dense\n4-235B\nGSPO\nText\n§\n[Yang et al., 2025a]\n2025.07\nGLM-4.1V-Thinking\nZhipu AI\nDense\n9B\nGRPO\nT/I/V\n§\n[Team et al., 2025a]\n2025.07\nGLM-4.5\nZhipu AI\nMoE\n355B\nGRPO\nText\n§\n[Zeng et al., 2025a]\n2025.07\nSkywork-R1V3\nSkywork\nDense\n38B\nGRPO\nT/I\n§\n[Shen et al., 2025b]\n2025.08\ngpt-oss\nOpenAI\nMoE\n117B/21B\n-\nText\n§\n[Agarwal et al., 2025a]\n2025.08\nSeed-OSS\nBytedance Seed\nDense\n36B\n-\nText\n§\n[Team, 2025a]\n2025.08\nGLM-4.5V\nZhipu AI\nMoE\n106B\nGRPO\nT/I/V\n§\n[Team et al., 2025a]\n2025.08\nInternVL3.5\nShanghai AI Lab\nMoE/Dense\n1-241B\nMPO/GSPO\nT/I/V\n§\n[Wang et al., 2025n]\n2025.09\nERNIE-4.5-Thinking\nBaidu\nMoE\n21B-A3B\n-\nText\n[Baidu-ERNIE-Team, 2025]\n9\nA Survey of Reinforcement Learning for Large Reasoning Models\nexamine how RL can endow LLMs with autonomous decision-making and adaptive agentic capabilities.\nXu et al. [2025a] move closer to our focus by discussing reinforced reasoning for LLMs, emphasizing\nhow trial-and-error optimization can improve complex reasoning. Wu [2025] complement this view by\nsurveying reward models and strategies for learning from feedback. Nevertheless, these works remain\noriented towards reasoning performance or reward design, rather than offering a systematic treatment\nof RL methods as a whole for LLMs. Srivastava and Aggarwal [2025] represent a more recent attempt\nto bridge the two fields by reviewing RL algorithms for LLM alignment and enhancement, primarily\nthrough methods such as RLHF [Christiano et al., 2017], RLAIF [Lee et al., 2024b], and DPO [Rafailov\net al., 2023]. It remains primarily focused on alignment rather than reasoning capabilities.\nUnlike previous surveys that cover either general RL or reasoning in LLMs, we place RL at the\ncenter and provide a systematic synthesis of its role throughout the LLM training lifecycle, including\nreward design, policy optimization, and sampling strategies. Our aim is to identify new directions for\nscaling reinforcement learning in LRMs toward ASI, focusing on long-term interactions and evolution.\n3. Foundational Components\nIn this section, we review the foundational components of RL for LRMs, including reward design\n(§ 3.1), policy optimization algorithms (§ 3.2), and sampling strategies (§ 3.3). The taxonomy of the\nfoundational components are shown in Figure 5.\n3.1. Reward Design\nIn this subsection, we provide a comprehensive examination of reward design in RL for LRMs. We\nbegin in § 3.1.1 with verifiable rewards, which offer a natural starting point. There are substantial\nadvances in this direction, exemplified by the success of DeepSeek-R1, which demonstrated the\nscalability of RL through verifiable reward mechanisms. In contrast, § 3.1.2 examines generative\nrewards, wherein the model is engaged to either verify or directly generate reward signals. However,\nboth verifiable and generative rewards are typically expressed as sparse numerical feedback. An\nimportant complementary dimension lies in the density of the reward signal. § 3.1.3 accordingly\nexamines approaches that incorporate dense rewards. A further axis of categorization pertains to\nwhether rewards are computed from external ground truth or instead estimated directly by the model.\nThis distinction motivates our discussion of unsupervised rewards in § 3.1.4. Building upon these four\ncategories, we then turn in § 3.1.5 to reward shaping, where we analyze strategies for combining or\ntransforming diverse reward signals to facilitate learning.\n3.1.1. Verifiable Rewards\nTakeaways\n• Rule-based rewards provide scalable and reliable training signals for RL, especially in math\nand code tasks, by leveraging accuracy and format checks.\n• Verifier’s law highlights that tasks with clear and automatic verification enable efficient RL\noptimization, while subjective tasks remain challenging.\nRule-based Rewards. The reward serves as the training signal of RL, determining the optimization\ndirection [Guo et al., 2025a]. Recently, rule-based verifiable rewards have been predominantly em-\nployed to train LRMs in large-scale RL. Such rewards enable the reliable enhancement of mathematical\nand coding reasoning abilities by encouraging longer and more reflective chain-of-thought [Guo et al.,\n10\nA Survey of Reinforcement Learning for Large Reasoning Models\nFoundational\nComponents § 3\nReward Design § 3.1\nGenerative\nRewards § 3.1.2\nModel-based Verifiers for Verifiable Tasks: e.g., TinyV [Xu et al., 2025g]; xVer-\nify [Chen et al., 2025b]; CompassVerifier [Liu et al., 2025m]; General\nReasoner [Ma et al., 2025c]; Seed-Thinking-Verifier [Seed et al., 2025a]\nReasoning Reward Models: e.g., AIR [He et al., 2025a]; DeepSeek-GRM [Liu\net al., 2025w]; Pairwise-RL [Xu et al., 2025c]; J1 [Whitehouse et al., 2025];\nRM-R1 [Chen et al., 2025p]; RRM [Guo et al., 2025b]; Think-RM [Hong et al.,\n2025b]; GRAM [Wang et al., 2025c]; OMNI-THINKER [Li et al., 2025b]; LI-\nBRA [Zhou et al., 2025c]; TP-GRPO [He et al., 2025f]; CAPO [Xie et al., 2025b]\nRubric-based Rewards: e.g., RaR [Gunjal et al., 2025]; Rubicon [Huang et al.,\n2025f]; RLCF [Viswanathan et al., 2025]; Writing-Zero [Jia et al., 2025]; ProxyRe-\nward [Guo et al., 2025e]; ReviewRL [Zeng et al., 2025c]; RuscaRL [Zhou et al., 2025f]\nCo-Evolving Systems: e.g., self rewarding GRM [Yuan et al., 2024]; RL Tango [Zha et al.,\n2025]; PAG [Jiang et al., 2025e]; URPO [Lu et al., 2025e]; PCL [Fei et al., 2025b];\nK2 [Team, 2025c]; Cooper [Hong et al., 2025a]; Critique-GRPO [Zhang et al., 2025l]\nDense\nRewards § 3.1.3\nToken-level: e.g., Implicit PRM [Yuan et al., 2025d]; PRIME [Cui et al., 2025a]; SRPO [Fei\net al., 2025a]; Entropy Advantage [Cheng et al., 2025a]; HICRA [Wang et al., 2025g]\nStep-level: e.g., PURE [Cheng et al., 2025b]; Tango [Zha et al., 2025];\nVinePPO [Kazemnejad et al., 2025]; SPO [Guo et al., 2025c]; TreeRPO [Yang\net al., 2025f]; TreeRL [Hou et al., 2025]; TreePO [Li et al., 2025q]\nTurn-level: e.g., TLCA [Zeng et al., 2025d]; LLM-RD [Lee et al., 2025]; ToolRL [Qian\net al., 2025]; MUA-RL [Zhao et al., 2025d]; ESDP [Zhu et al., 2024]; SWEET-\nRL [Zhou et al., 2025g]; GELI [Lee et al., 2024a]; SPA-RL [Wang et al., 2025e]\nUnsupervised\nRewards § 3.1.4\nModel-Specific: e.g., TTRL [Zuo et al., 2025b]; ETTRL [Liu et al., 2025c]; EMPO [Zhang\net al., 2025h]; Spurious Rewards [Shao et al., 2025]; Absolute Zero [Zhao et al.,\n2025a]; RLIF [Zhao et al., 2025e]; R-Zero [Huang et al., 2025a]; SRT Shafayat et al.\n[2025]; EM-RL [Agarwal et al., 2025b]; SeRL [Fang et al., 2025a]; RLSC [Li et al.,\n2025g]; Co-Reward [Zhang et al., 2025v]; CoVo [Zhang et al., 2025g]; CAGSR [Kiru-\nluta et al., 2025]; RENT [Prabhudesai et al., 2025]; RLSF [van Niekerk et al., 2025];\nSSR-Zero [Yang et al., 2025e]; MINIMO [Poesia et al., 2024]; SQLM [Chen et al., 2025i]\nModel-Agnostic: e.g., SEAL [Zweiger et al., 2025];\nRPT [Dong et al., 2025c]; Xin et al. [2025]\nRewards\nShaping § 3.1.5\nRule-based Reward Shaping: e.g., Qwen2.5-Math [Yang et al.,\n2024a];DeepSeek-R1 [Guo et al., 2025a];Laser [Liu et al., 2025o]\nStructure-based Reward Shaping: e.g., GRPO [Shao et al., 2024]; RLOO [Ahmadian et al.,\n2024]; PKPO [Walder and Karkhanis, 2025]; Pass@k Training [Chen et al., 2025v]\nPolicy Algo-\nrithm § 3.2\nCritic-based\nAlgorithms § 3.2.2\ne.g., PPO [Schulman et al., 2017a]; VCPPO [Yuan et al.,\n2025f]; VAPO [Yue et al., 2025c]; PRIME [Cui et al., 2025a]\nCritic-Free\nAlgorithms § 3.2.3\ne.g., REINFORCE [Cheng et al., 2025d]; ReMax [Li et al., 2023c]; RLOO [Cheng et al.,\n2025d]; REINFORCE++ [Hu, 2025]; GRPO [Shao et al., 2024]; DAPO [Yu et al., 2025d];\nCISPO [Chen et al., 2025a]; Dr.GRPO [Liu et al., 2025t]; GSPO [Zheng et al., 2025a];\nVinePPO [Kazemnejad et al., 2025]; MDPO [Team, 2025d]; LitePPO [Liu et al., 2025v]\nRegularization\nObjectives § 3.2.5\nKL Regularization: e.g., SimpleRL [Zeng et al., 2025e]; ProRL [Liu\net al., 2025i]; OREAL [Lyu et al., 2025]; Phi4-reasoning [Abdin\net al., 2025]; Kimi-K1.5 [Team, 2025d], Archer [Wang et al., 2025h]\nEntropy Regularization: e.g., DAPO [Yu et al., 2025d]; KL-Cov/Clip-\nCov [Cui et al., 2025b]; HighEntropy-RL [Wang et al., 2025l];\nLength Penalty: e.g., ALP [Xiang et al., 2025]; LASER [Liu et al., 2025o];\nL1 [Aggarwal and Welleck, 2025]; O1-pruner [Luo et al., 2025a]\nOff-Policy\nOptimization § 3.2.4\nOff-policy: e.g., SPO [Cohen et al., 2025];\nTOPR [Roux et al., 2025]; ReMix [Liang et al., 2025a]\nMix-policy: e.g., HPT [Lv et al., 2025]; LUFFY [Yan et al., 2025a]; Re-\nLIFT [Ma et al., 2025a]; UFT [Liu et al., 2025j]; BREAD [Zhang et al.,\n2025o]; SRFT [Fu et al., 2025c]; Prefix-RFT [Huang et al., 2025g]\nSampling Strat-\negy § 3.3\nDynamic and\nStructured Sam-\npling § 3.3.1\nDynamic Sampling: e.g., PRIME [Cui et al., 2025a]; DAPO [Yu et al., 2025d]; K1.5 [Team,\n2025d]; SEC [Chen et al., 2025o]; E2H [Parashar et al., 2025]; AdaRFT [Shi et al.,\n2025b]; SPaRFT [Do et al., 2025]; ARPO [Dong et al., 2025b]; DARS [Yang et al., 2025g];\nHyper-Parameters\nAdjustment § 3.3.2\ne.g., DeepScaleR [Luo et al., 2025c]; E3-RL4LLMs [Liao et al., 2025b]; Pro-RL [Liu et al.,\n2025i]; AceReason-Nemotron [Chen et al., 2025q]; T-PPO [Fan et al., 2025b]; POLARIS\n[An et al., 2025]; Confucius3-Math [Wu et al., 2025e]; GFPO [Shrivastava et al., 2025]\nFigure 5 | Taxonomy of foundational components and representative works for each direction.\n11\nA Survey of Reinforcement Learning for Large Reasoning Models\n2025a, Team, 2025c, Yu et al., 2025d]. This paradigm was formalized as RLVR in the Tülu 3 [Lambert\net al., 2024], which replaces a learned reward model with a programmatic verifier (e.g., answer\ncheckers or unit tests). Such verifiers provide binary, checkable signals in domains with objectively\nverifiable outcomes. Similar rule-based approaches to verifiable reward design were subsequently\nintegrated into DeepSeek’s training pipeline. For instance, DeepSeek-V3 [Liu et al., 2024] explicitly\nincorporated a rule-based reward system tailored to deterministic tasks, while DeepSeek-R1 [Guo\net al., 2025a] further employed accuracy-based and format-based rewards. Rule-based rewards stand\nin contrast to outcome-based or process-based Reward Models (RMs), such as standard RLHF with\na learned reward model trained on human preference rankings [Ouyang et al., 2022] and Process\nReward Models (PRMs) trained on step-level annotations [Setlur et al., 2024, Sun et al., 2025c, Yuan\net al., 2025d]. DeepSeek-V3 and DeepSeek R1 demonstrate that RMs may suffer from reward hacking\nwhen scaled to large-scale RL settings, but by leveraging rule-based rewards wherever possible, we\nensure greater reliability by making the system resistant to manipulation and exploitation [Guo et al.,\n2025a, Liu et al., 2024]. In practice, two kinds of rule-based verifiable rewards are widely used:\n• Accuracy rewards: For tasks with deterministic outcomes (e.g., math), the policy must produce\nthe final solution within a prescribed delimiter (commonly \\boxed{...}). An automatic checker\nthen compares this output to the ground truth. For coding tasks, unit tests, or compilers provide\nthe pass/fail signal [Albalak et al., 2025, Chen et al., 2025r, Guo et al., 2025a].\n• Format rewards: These impose a structural constraint requiring the model to place its private\nchain-of-thought between <think> and </think>, and to output the final answer in a separate\nfield (e.g., <answer>...</answer>). This improves reliable parsing and verification in large-\nscale RL [Guo et al., 2025a, Lambert et al., 2024].\nRule-based Verifier. Rule-based rewards are typically derived from rule-based verifiers. These rely\non a large collection of manually written equivalence rules to determine whether a predicted answer\nmatches the ground truth. Currently, widely used mathematical verifiers are primarily built on\nthe Python libraries Math-Verify1 and SymPy2. In addition, some works such as DAPO [Yu et al.,\n2025d] and DeepScaleR [Luo et al., 2025c], also provide open-source and well-established verifiers.\nRecently, Huang et al. [2025e] highlight the distinctive limitations associated with both rule-based\nand model-based verifiers, to inform the design of more reliable reward systems.\nIn practice, tasks such as mathematical problem solving and code generation are difficult to solve\nyet comparatively easy to verify, thereby satisfying the main criteria for efficient RL optimization [Guo\net al., 2025a, He et al., 2025d]: the existence of clear ground truth, the availability of rapid automated\nverification, the scalability of evaluating many candidate solutions, and a reward signal that is closely\naligned with correctness. By contrast, tasks lacking fast or objective verification (e.g., open-ended\nquestion answering or free-form writing) remain challenging for outcome-based RL, as they rely on\nnoisy learned reward models or subjective human feedback [Yu et al., 2025e, Zhou et al., 2025e].\nVerifier’s Law posits that the ease of training AI systems to perform a task is proportional to the degree\nto which the task is verifiable3. It emphasizes that once a task can be equipped with robust automated\nfeedback, it becomes amenable to rapid improvement via RL. The successful applications discussed\nin §6 substantiate this principle, as their central challenge lies in the design of reliable verifiable\nfeedback. Conversely, many of the open problems highlighted in §7 arise precisely from the absence\nof dependable automated rewards.\n1https://github.com/huggingface/Math-Verify\n2https://www.sympy.org/\n3https://www.jasonwei.net/blog/asymmetry-of-verification-and-verifiers-law\n12\nA Survey of Reinforcement Learning for Large Reasoning Models\n3.1.2. Generative Rewards\nTakeaways\n• Generative Reward Models (GenRMs) extend RL to subjective, non-verifiable domains by\nproviding nuanced, text-based feedback, overcoming the limitations of rule-based systems.\n• A dominant trend is training RMs to reason before judging, often using structured rubrics\nto guide evaluation or co-evolving them with the policy model in a unified RL loop.\nWhile rule-based rewards provide reliable signals for verifiable tasks, as discussed previously\n(§ 3.1.1), their applicability is limited. Many complex reasoning tasks, particularly in open-ended or\ncreative domains, lack objective ground truth, making them intractable for simple verifiers. To bridge\nthis gap, GenRMs have emerged as a powerful alternative. Instead of outputting a simple scalar score,\nGenRMs leverages the generative capabilities of LRMs to produce structured critiques, rationales,\nand preferences, providing a more interpretable and nuanced reward signal [Mahan et al., 2024,\nZhang et al., 2024a]. This approach addresses two key challenges: first, it improves the robustness of\nverification for verifiable tasks that are difficult to parse; second, and more importantly, it enables the\napplication of RL to subjective, non-verifiable domains.\nModel-based Verifiers for Verifiable Tasks. A primary challenge with rule-based systems is their\nbrittleness; they often produce false negatives when a model generates a correct answer in an\nunexpected format. To mitigate this, one line of research uses Specification-Based GenRMs as flexible,\nmodel-based verifiers. These models are trained to semantically assess the equivalence between a\nmodel’s free-form output and a reference answer. This approach has been used to develop lightweight\nverifiers that augment existing rule-based systems [Xu et al., 2025g], as well as more comprehensive,\nmulti-domain verifiers capable of handling diverse data types and reasoning tasks [Chen et al., 2025b,\nLiu et al., 2025m, Ma et al., 2025c, Seed et al., 2025a]. By replacing or supplementing rigid string\nmatching with learned semantic judgment, these verifiers provide more accurate reward signals for\nRL in verifiable domains.\nGenerative Rewards for Non-Verifiable Tasks. Another core application of GenRMs is Assessment-\nBased GenRMs, which enable RL for tasks where Verifier’s Law does not hold. This paradigm has\nevolved from using powerful LLMs as zero-shot evaluators to sophisticated, co-evolving systems. We\ncan categorize these approaches based on their core design principles.\n• Reasoning Reward Models (Learning to Think): A major advancement beyond simple prefer-\nence prediction is to train RMs to explicitly reason before rendering a judgment. This approach,\nfoundational to the LLM-as-a-Judge concept [Li et al., 2023b, Zheng et al., 2023], involves\nprompting the RM to generate a CoT critique or rationale. For instance, CLoud RMs first generate\na natural language critique and then use it to predict a scalar reward [Ankner et al., 2024]. This\nprinciple of formulating reward modeling as a reasoning task is now central to state-of-the-art\nRMs, which are trained to produce detailed rationales before assigning a score or preference [Chen\net al., 2025p, Guo et al., 2025b, Hong et al., 2025b, Liu et al., 2025w, Wang et al., 2025c, Zhou\net al., 2025c]. To further improve their judgment capabilities, these reasoning RMs are often\ntrained with RL themselves, using simple, verifiable meta-rewards based on the correctness of\ntheir final verdict [Chen et al., 2025l, Whitehouse et al., 2025]. This line of work also explores\ndifferent reward formats, such as deriving soft rewards from token probabilities [Mahan et al.,\n2024, Su et al., 2025c, Zhang et al., 2024a] and weighing the trade-offs between pointwise and\npairwise scoring schemes [He et al., 2025a, Xu et al., 2025c].\n• Rubric-based Rewards (Structuring Subjectivity): To anchor the evaluation of subjective tasks\n13\nA Survey of Reinforcement Learning for Large Reasoning Models\nin more consistent criteria, many frameworks employ structured rubrics. Unlike rule-based\napproaches that rely on hard-coded logic for objective, verifiable tasks, rubric-based methods\nleverage natural language descriptions to capture nuanced evaluation criteria for subjective,\nnon-verifiable domains where traditional binary rules would be insufficient. This involves using\nan LLM to either generate or follow a checklist of principles to guide its assessment. Frameworks\nlike RaR [Gunjal et al., 2025], Rubicon [Huang et al., 2025f], and RLCF [Viswanathan et al.,\n2025] use such rubrics to produce fine-grained, multi-faceted rewards. This concept extends\nto decomposing high-level tasks into a set of verifiable proxy questions [Guo et al., 2025e] or\ngenerating domain-specific principles, such as for creative writing [Jia et al., 2025] or scientific\nreviews [Zeng et al., 2025c]. Furthermore, rubrics can serve a dual purpose as both instructional\nscaffolds to guide policy exploration and as criteria for the final reward [Zhou et al., 2025f].\n• Co-Evolving Systems (Unifying Policy and Reward): The most advanced paradigm moves\nbeyond a static policy-reward relationship and toward dynamic systems where the generator and\nverifier improve together. This can occur through:\n– Self-Rewarding, where a single model generates its own training signals. This was notably\ndemonstrated in Self-Rewarding Language Models [Yuan et al., 2024] and has been opera-\ntionalized in frameworks where a model alternates between policy and verifier roles [Jiang\net al., 2025e], performs self-correction based on its own critique [Team, 2025c, Xiong et al.,\n2025b, Zhang et al., 2025l], or internalizes the reward function via post-completion learn-\ning [Fei et al., 2025b].\n– Co-Optimization, where the policy and a separate reward model are trained concurrently.\nFor example, RL Tango jointly trains the generator and a process-level GenRM using a shared\noutcome-level reward [Zha et al., 2025]. Similarly, Cooper co-optimizes both models to\nenhance robustness and mitigate reward hacking [Hong et al., 2025a]. Other works unify\nthe policy (“player”) and reward (“referee”) functions within a single model trained via a\nunified RL loop [Lu et al., 2025e].\nThis evolution from static judges to dynamic, co-evolving systems is often supported by hybrid\nreward schemes that combine rule-based and generative signals [Li et al., 2025b, Seed et al., 2025a].\nAdditionally, GenRMs are being adapted to provide more granular, process-level feedback to address\nthe credit assignment problem in complex reasoning chains [He et al., 2025f, Xie et al., 2025b, Zhao\net al., 2025b]. In essence, generative rewards are proving indispensable for scaling RL to the full\nspectrum of tasks targeted by general-purpose LRMs.\n3.1.3. Dense Rewards\nTakeaways\n• Dense rewards (e.g., process reward models) provide fine-grained credit assignment and\nimprove training efficiency and optimization stability in RL.\n• Scaling remains challenging for tasks like open-domain text generation due to the difficulty\nof defining dense rewards or using verifiers.\nIn classical RL such as gaming and robotic manipulation tasks [Liu et al., 2022, Schrittwieser\net al., 2020, Sun et al., 2025d], dense rewards provide frequent feedback at (nearly) every decision\nstep. Such shaping shortens the credit assignment horizon and often improves sample efficiency and\noptimization stability, but it also risks mis-specification and reward hacking if the signal is poorly\n14\nA Survey of Reinforcement Learning for Large Reasoning Models\nTable 2 | Definitions of action and reward granularity in RL for language models (𝑧(𝑢) is the environ-\nment feedback at turn 𝑢).\nGranularity\nAction\nReward\nReturn (𝐺)\nTrajectory\nEntire sequence 𝑦= (𝑎1, . . . , 𝑎𝑇)\nScalar 𝑅(𝑥, 𝑦)\n𝑅(𝑥, 𝑦)\nToken\nEach token 𝑎𝑡∈V\n𝑟𝑡= 𝑅(𝑥, 𝑎1:𝑡)\nÍ𝑇\n𝑡=1 𝛾𝑡−1𝑟𝑡\nStep\nSegment 𝑦(𝑘) (e.g., sentence)\n𝑟𝑘= 𝑅(𝑥, 𝑦(1:𝑘))\nÍ𝐾\n𝑘=1 𝛾𝑘−1𝑟𝑘\nTurn (Agent)\nAgent response 𝑦(𝑢) per turn\n𝑟𝑢= 𝑅(𝑥, 𝑦(1:𝑢), 𝑧(1:𝑢))\nÍ𝑈\n𝑢=1 𝛾𝑢−1𝑟𝑢\ndesigned [Hadfield-Menell et al., 2017]. As for LLM reasoning, dense rewards are typically process-\nbased signals that supervise intermediate steps rather than only outcomes, and they have been found\neffective, often outperforming outcome-based rewards [Lightman et al., 2024, Uesato et al., 2022].\nBased on the definitions in § 2.1, we further formalize sparse/outcome and dense rewards in the\ncontext of LLM RL, according to the action and reward granularity, as shown in Table 2.\nToken-Level Rewards. DPO [Rafailov et al., 2023] and its subsequent work [Rafailov et al., 2024]\nshow that token-level rewards can be computed as log-likelihood ratios between the policy and the\nreference model. Implicit PRM [Yuan et al., 2025d] further shows that token-level rewards can be\nobtained by training an ORM and using the parameterization of Rafailov et al. [2024]. PRIME [Cui\net al., 2025a] integrates ORM learning into RL training and uses implicit token-level rewards to\ntrain the policy. SRPO [Fei et al., 2025a] removes the ORM in PRIME and improves advantage\nestimation. Another line of works focus on using internal feedback as token-level rewards, such as\ntoken entropy [Cheng et al., 2025a, Tan and Pan, 2025] and strategic grams [Wang et al., 2025g].\nStep-Level Rewards. Approaches to step-level rewards fall into two classes: model-based and sampling-\nbased. Early works rely on human experts to annotate step-level dense rewards [Lightman et al.,\n2024, Uesato et al., 2022], which is costly and difficult to scale.\n• Model-based: To reduce annotation cost, Math-Shepherd [Wang et al., 2024b] uses Monte Carlo\nestimation to obtain step-level labels and demonstrates that process verification with trained\nPRMs is effective in RL. PAV [Setlur et al., 2024] further improves process rewards via advantage\nmodeling. To mitigate reward hacking with model-based step-level rewards, PURE [Cheng et al.,\n2025b] adopts min-form credit assignment rather than sum-form, while Tango [Zha et al., 2025]\nand AIRL-S [Jin et al., 2025c] jointly train the policy and PRMs. With the strong verification\ncapabilities of generative PRMs [Zhao et al., 2025b] (discussed in § 3.1.2), ReasonFlux-PRM [Zou\net al., 2025], TP-GRPO [He et al., 2025f], and CAPO [Xie et al., 2025b] leverage them to provide\nstep-level rewards for RL training. Nevertheless, model-based dense rewards are vulnerable to\nreward hacking, and training PRMs online is expensive.\n• Sampling-based: Another line of works use Monte Carlo sampling for online process reward\nestimation [Guo et al., 2025c, Hou et al., 2025, Kazemnejad et al., 2025, Li et al., 2025q,\nYang et al., 2025f, Zheng et al., 2025c]. VinePPO [Kazemnejad et al., 2025] improves PPO via\nMonte Carlo estimation. To improve step segmentation, SPO [Guo et al., 2025c], TreeRL [Hou\net al., 2025], and FR3E [Zheng et al., 2025c] use low-probability or high-entropy tokens as\ndivision points. To improve sample efficiency and advantage estimation, SPO [Guo et al., 2025c],\nTreeRPO [Yang et al., 2025f], TreeRL [Hou et al., 2025] and TreePO [Li et al., 2025q] explore\ntree-based structures for fine-grained process reward computation. MRT [Qu et al., 2025b],\nS-GRPO [Dai et al., 2025a], VSRM [Yue et al., 2025a], and SSPO [Xu et al., 2025f] force the\nLLM to terminate the thinking process at intermediate positions to estimate step-level rewards\n15\nA Survey of Reinforcement Learning for Large Reasoning Models\nefficiently. PROF [Ye et al., 2025a] utilizes the consistency between outcome rewards and process\nrewards to filter noisy data for RL training.\nTurn-Level Rewards. Turn-level rewards evaluate each complete agent-environment interaction, such\nas a tool call and its result, providing feedback at the granularity of a single turn in multi-turn tasks.\nResearch on turn-level rewards can be broadly divided into two lines: direct per-turn supervision and\nderiving turn-level signals from outcome-level rewards.\n• For direct per-turn supervision, works provide explicit feedback at each turn. For example,\nEmotion-sensitive dialogue policy learning [Zhu et al., 2024] exploits user emotions as per-turn\nrewards to guide policy optimization, showing how turn-level feedback can enhance interaction\nquality in conversational agents. Similarly, ToolRL [Qian et al., 2025] designs structured rewards\non format and correctness that are provided at each tool invocation step, offering dense turn-level\nsignals for learning. Zeng et al. [2025d] further leverage verifiable signals with explicit turn-level\nadvantage estimation to improve multi-turn tool use during RL. In addition, SWEET-RL [Zhou\net al., 2025g] learns a step/turn-level critic that provides per-turn rewards and credit assignment,\nthereby supplying explicit turn-level supervision. More recently, MUA-RL [Zhao et al., 2025d]\nincorporates simulated user interactions into the RL loop, where each multi-turn exchange\nproduces per-turn feedback, allowing the agent to iteratively refine its policy under realistic\nuser-agent dynamics. G-RA [Sun et al., 2025g] extends this line of work by introducing gated\nreward aggregation, where dense turn-level rewards (e.g., action format, tool call validity, tool\nchoice) are only accumulated if higher-priority outcome-level conditions are satisfied.\n• For deriving turn-level signals from outcome-level rewards, the idea is to decompose or redis-\ntribute outcome-based supervision into finer-grained units. Aligning Dialogue Agents with Global\nFeedback [Lee et al., 2025] transforms session-level scores into turn-level pseudo-rewards, and\nGELI [Lee et al., 2024a] exploits multimodal cues such as prosody and facial expressions to\nrefine session-level feedback into local turn-level signals. Similarly, SPA-RL [Wang et al., 2025e]\nredistributes outcome-based rewards into per-step or per-turn contributions through progress\nattribution. ARPO [Dong et al., 2025b] follows this line by attributing step/turn-level advantages\nfrom trajectory-level outcomes (e.g., after tool use), effectively converting global returns into\nlocalized signals.\nOverall, turn-level rewards, whether directly assigned at each interaction or derived from outcome\ndecomposition, serve as a bridge between process- and outcome-based supervision, and play a central\nrole in stabilizing and improving optimization in multi-turn agent RL, with more details in § 6.2.\n3.1.4. Unsupervised Rewards\nTakeaways\n• Unsupervised rewards eliminate the human annotation bottleneck, enabling reward signal\ngeneration at the scale of computation and data, not human labor.\n• Main approaches include deriving signals either from the model’s own processes (Model-\nSpecific: consistency, internal confidence, self-generated knowledge) or from automated\nexternal sources (model-agnostic: heuristics, data corpora).\nFrontier language models excel at a wide range of tasks, including many that are exceptionally\nchallenging [Glazer et al., 2024, Jimenez et al., 2023, Li et al., 2024b, Phan et al., 2025]. However, a\n16\nA Survey of Reinforcement Learning for Large Reasoning Models\nkey limitation in advancing these models is the reliance on human-generated reward signals for RL\n(§ 3.1.1–3.1.3). For tasks requiring superhuman expertise, human feedback is often slow, expensive,\nand impractical [Burns et al., 2023]. To address this, a promising approach is Unsupervised RL, which\nuses automatically generated, verifiable reward signals instead of ground-truth labels. This method\nis fundamental to achieving scalable RL for LLMs. This section surveys these unsupervised reward\nmechanisms, categorizing them into two types based on their source: those derived from the model\nitself (Model-Specific) and those from external, non-human sources (Model-Agnostic).\nModel-Specific Rewards. This paradigm uses an LLM’s internal knowledge as the sole source of\nsupervision. It operates on the assumption that a high-performing model will generate consistent,\nconfident, or evaluatively sound outputs. This method is highly scalable, requiring only the model\nand computational resources to generate a virtually infinite amount of “labeled” data. However, its\nclosed-loop nature risks reward hacking and model collapse.\n• Rewards from Output Consistency: This approach posits that correct answers will form a dense,\nconsistent cluster among multiple generated outputs. Foundational works like EMPO [Zhang\net al., 2025h] and Test-Time Reinforcement Learning (TTRL) [Zuo et al., 2025b] operationalize\nthis via clustering and majority voting, respectively. Subsequent methods aim to refine this by\nimproving efficiency (ETTRL [Liu et al., 2025c]), incorporating reasoning trajectories (CoVo\n[Zhang et al., 2025g]), or using contrastive agreement to combat reward hacking (Co-Reward\n[Zhang et al., 2025v]).\n• Rewards from Internal Confidence: An alternative is to derive rewards directly from the\nmodel’s internal states, using confidence as a proxy for correctness. Signals can be based on\ncross-attention (CAGSR [Kiruluta et al., 2025]), negative entropy (EM-RL [Agarwal et al., 2025b],\nRENT [Prabhudesai et al., 2025]), or generation probabilities (Intuitor [Zhao et al., 2025e], RLSC\n[Li et al., 2025g], RLSF [van Niekerk et al., 2025]). The success of these methods often depends\non the base model’s initial quality [Gandhi et al., 2025] and can be brittle [Press et al., 2024,\nShumailov et al., 2023], as they rely on priors like low-density separation between correct and\nincorrect paths [Chapelle and Zien, 2005, Lee et al., 2013].\n• Rewards from Self-Generated Knowledge: This paradigm uses the model’s knowledge to\ncreate learning signals, either by acting as a judge (self-rewarding) or a problem proposer (self-\ninstruction). In self-rewarding, the model evaluates its own outputs to generate a reward, a\nconcept framed by Yuan et al. [2024] and Wu et al. [2024] and applied in works like SSR-Zero\n[Yang et al., 2025e] and MINIMO [Poesia et al., 2024]. In self-instruction, a proposer model\ngenerates a curriculum for a solver. The proposer is often rewarded for creating tasks of optimal\ndifficulty [Chen et al., 2025i, Huang et al., 2025a, Zhao et al., 2025a], while the solver’s reward\ncan be model-agnostic (e.g., from a code executor in AZR [Zhao et al., 2025a]) or Model-Specific\n(e.g., via majority voting in SQLM [Chen et al., 2025i] and SeRL [Fang et al., 2025a]).\nModel-Agnostic Rewards. In contrast to Model-Specific methods, this paradigm derives rewards from\nexternal, automated sources. This approach grounds the learning process in external information,\neliminating the need for human labels. Its core principle is that these external signals are readily\naccessible and do not require manual effort. However, since precise feedback is often unavailable, the\nquality of the proxy reward is critical, and the risk of reward hacking persists.\n• Heuristic Rewards: This approach constitutes another form of rule-based reward, employing\nsimple, predefined rules based on output properties such as length or format as proxies for quality.\nIt represents a specific case discussed in § 3.1.1. This was pioneered by DeepSeek-R1 [Guo et al.,\n17\nA Survey of Reinforcement Learning for Large Reasoning Models\n2025a] and later refined with techniques like dynamic reward scaling [Yu et al., 2025d]. While\nscalable, these heuristics can be gamed by the model, leading to superficial improvements without\nadvancing true capability [Liu et al., 2025s, Xin et al., 2025].\n• Data-Centric Rewards: This approach derives reward signals from the structure of large, unla-\nbeled corpora. Analogous to next-word prediction for large-scale pre-training, RPT [Dong et al.,\n2025c] reframes next-token prediction as an RL task, turning web-scale datasets into millions of\ntraining examples. At a meta-level, SEAL [Zweiger et al., 2025] allows a model to generate its\nown training data and hyperparameters, using downstream performance as the reward.\nIn summary, unsupervised reward design is essential for creating scalable RL systems for LLMs. The\nModel-Specific paradigm facilitates self-improvement by leveraging the model’s internal knowledge,\nwhereas the Model-Agnostic paradigm grounds learning in external, automated feedback. While\nboth approaches effectively bypass the human annotation bottleneck, they remain susceptible to\nreward hacking [Zhang et al., 2025p]. The future of scalable RL will likely involve hybrid systems\nthat strategically combine these methods, for instance, using data-centric rewards for pre-training,\nModel-Specific self-rewarding for fine-tuning on complex reasoning, and minimal human oversight\nfor safety and alignment.\n3.1.5. Rewards Shaping\nTakeaways\n• Reward shaping enriches sparse signals into stable, informative gradients for LLM training.\n• Combine verifiers with reward models, and use group baselines plus Pass@K-aligned\nobjectives to stabilize training, expand exploration, and match evaluation metrics at scale.\nAs noted, the primary learning objective of agents in RL is to maximize cumulative rewards,\nmaking the design of the reward function particularly critical [Sutton et al., 1998]. In previous\nsections, we introduced various reward functions, such as verifiable rewards (§ 3.1.1), generative\nrewards (§ 3.1.2), dense rewards (§ 3.1.3) and even unsupervised rewards (§ 3.1.4). Beyond reward\nengineering, it is equally important to consider how the reward function can be modified or augmented\nto encourage behaviors that drive progress toward the desired solution. This process, known as reward\nshaping [Goyal et al., 2019, Gupta et al., 2022, Hu et al., 2020, Xie et al., 2023], can be categorized\ninto rule-based and structured-based reward shaping.\nRule-based Reward Shaping. The simplest and most commonly adopted approach to reward shaping\nin LLM-based RL involves combining rewards from both a rule-based verifier and a reward model to\ngenerate the overall reward signal, as demonstrated in Qwen2.5 Math [Yang et al., 2024a]. Typically,\na constant coefficient is used to balance the contributions of the reward model and the rule-based\ncomponent. Rather than assigning identical rewards to all correct responses, this method allows for\nfurther ranking of responses based on the scores from the reward model. This approach is particularly\nuseful for more challenging samples and helps to avoid cases where all reward values are 0 or 1, which\nwould otherwise lead to ineffective learning gradients [Yu et al., 2025d]. This heuristic combination\nstrategy is widely employed in open-domain tasks, where integrating rule-based rewards and reward\nmodels [Guo et al., 2025b, Liao et al., 2025a, Liu et al., 2025w] results in more informative and\neffective reward signals for the RL of LLM [Su et al., 2025c, Zeng et al., 2025c, Zhang et al., 2024a].\nAnother approach involves combining rule-based rewards, such as outcome-level rewards and format\nrewards, as implemented in DeepSeek-R1 [Guo et al., 2025a], which enables LLMs to learn long\nchain-of-thought reasoning. These rewards include format-based [Xin et al., 2025] and length-based\n18\nA Survey of Reinforcement Learning for Large Reasoning Models\ncomponents [Liu et al., 2025o] to address various exceptions in the outputs of LLMs. Recent work also\nexplores multi-role RL training and assigns different rewards for different roles with different reward\nfunctions, such as solver and critic [Li et al., 2025h]. Typically, these rewards are combined using\nmanually set constants. Recent works have also explored multi-role RL training [Li et al., 2025h,i],\nassigning distinct reward functions to different roles to encourage diverse behaviors and objectives [Li\net al., 2025h], such as solver and critic.\nStructure-based Reward Shaping. In contrast to rule-based reward shaping, which relies solely on\nindividual samples, structure-based reward shaping computes rewards across a group of candidates\nby leveraging list-wise or set-level baselines. One influential method is GRPO [Shao et al., 2024],\nwhich uses the group mean of responses to the same question G as a baseline (or variants such\nas leave-one-out [Ahmadian et al., 2024] or ranking) and constructs advantages accordingly for\nPPO-style updates [Schulman et al., 2017b]. Recent works have further modified the optimization\nobjective or credit allocation strategies to promote stronger exploration and achieve closer alignment\nwith evaluation metrics, such as Pass@K [Yue et al., 2025b]. For example, Walder and Karkhanis\n[2025] perform a joint transformation on the final reward, making the optimization directly equivalent\nto set-level objectives like Pass@K, and provide low-variance, unbiased gradient estimation. Chen et al.\n[2025v] directly target Pass@K in deriving and analyzing advantages and efficient approximations,\ndecomposing set-level targets back into individual sample credit allocation. Reward shaping methods\nin this direction aim to stabilize training and encourage the policy to explore more extensively, thereby\nreducing the risk of premature convergence to suboptimal local solutions.\n3.2. Policy Optimization\nIn this subsection, we first provide a technical overview of the mathematical formulation of the policy\ngradient objective (§ 3.2.1). Next, we divide the on-policy optimization algorithms in RL into two\ncategories based on how the reward is generated for the gradient calculation process: critic-based\n(§ 3.2.2) and critic-free (§ 3.2.3). In addition, we discuss recent studies that combine on-policy RL\nwith offline datasets for more sophisticated post-training (i.e., off-policy) optimization (§ 3.2.4), as\nwell as various regularization techniques such as entropy and KL (§ 3.2.5).\n3.2.1. Policy Gradient Objective\nAs introduced in § 2.1, the context in RL for LLMs is treated as the environment, and the probability\ndistribution of the next-level prediction is treated as a policy. For an RL system, the objective of the\nsystem is to find an optimal policy such that the expected cumulative reward generated by the system\nis maximized. The RL policy optimization algorithms for LLMs are mostly first-order gradient-based\nalgorithms, due to the large number of parameters in the LLMs. In general, RL algorithms seek to\noptimize network parameters such that the expected reward is maximized. Below, we present a\ngeneral formulation for LLM gradient calculation of RL algorithms.\nNotations. Although we have introduced the relevant symbols in § 2.1, we revisit these definitions\nhere for the sake of comparative clarity. Let 𝑥∼D be a prompt (initial state 𝑠1 = 𝑠). A stochastic\npolicy 𝜋𝜃generates a sequence 𝑦= (𝑎1, · · · , 𝑎𝑇), we denote the total sequence length of 𝑦as |𝑦|, with\nstates defined by 𝑠𝑡+1 = (𝑥, 𝑠≤𝑡). We assume a primarily sequence-level reward 𝑅(𝑥, 𝑦), optionally\ndecomposed into token-level rewards 𝑟𝑡. We collect 𝐺≥1 responses per prompt using a behavior\npolicy 𝜋𝑏(also denoted as 𝜋old, referring to an earlier version of the current policy). Optionally, a\nreference policy 𝜋ref (e.g., base, finetuned or instructed models) may be used for regularization.\nWe revisit the MDP defined in § 2.1. In MDPs, we denote the expected cumulative reward given\n19\nA Survey of Reinforcement Learning for Large Reasoning Models\nthe current state 𝑠as the V (value) function\n𝑉(𝑠) = 𝔼𝑎𝑡∼𝜋𝜃(𝑠𝑡),𝑠𝑡+1∼P(𝑠,𝑎) [\n𝑇\n∑︁\n𝑡=0\n𝛾𝑡𝑟(𝑠𝑡, 𝑎𝑡)|𝑠0 = 𝑠],\n(2)\nand the expected cumulative reward for the current state-action pair is denoted as Q (quality) function\n𝑄(𝑠, 𝑎) = 𝔼𝑎𝑡∼𝜋𝜃(𝑠𝑡),𝑠𝑡+1∼P(𝑠,𝑎) [\n𝑇\n∑︁\n𝑡=0\n𝛾𝑡𝑟(𝑠𝑡, 𝑎𝑡)|𝑠0 = 𝑠, 𝑎0 = 𝑎].\n(3)\nThen the objective of RL can be formulated as a maximization problem for the expected cumulative\nreward. To optimize the objective function, it is a common practice to use the Policy Gradient\nalgorithm [Sutton et al., 1999, Williams, 1992] for gradient estimation:\n∇𝜃J (𝜃) = 𝔼𝑥∼D,𝑦∼𝜋𝜃\n\" 𝑇\n∑︁\n𝑡=1\n∇𝜃𝜋𝜃(𝑦𝑡|𝑦<𝑡)𝑄𝑡\n#\n.\n(4)\nThe policy gradient can be justified by the intuition that an algorithm following the policy\ngradient should maximize the probability of better-than-average actions and minimize the probability\nof worse-than-average actions. This notion led to the introduction of the 𝐴(advantage) function\n𝐴(𝑠, 𝑎) = 𝑄(𝑠, 𝑎) −𝑉(𝑠). The advantage measures how much the current action improves upon the\nexpected total reward compared to the existing policy. The advantage can be estimated in many ways.\nIf we only have rewards for the full trajectory, the vanilla REINFORCE algorithm [Williams, 1992]\ndirectly defines 𝐴𝑡= 𝑅(𝑥, 𝑦).\nFor the case of training LLMs, the vanilla policy gradient algorithms often suffer from stability\nissues. Instead, the training is often done with the PPO algorithm [Schulman et al., 2017b]. For an\nalgorithm with 𝑁samples, we define a general objective with PPO-style updates as follows:\nJ (𝜃) = 𝔼data\n\"\n1\n𝑍\n𝑁\n∑︁\n𝑖=1\n𝑇𝑖\n∑︁\n𝑡=1\nmin\n\u0010\n𝑤𝑖,𝑡(𝜃) ˆ𝐴𝑖,𝑡, clip(𝑤𝑖,𝑡(𝜃), 1 −𝜖low, 1 + 𝜖high) ˆ𝐴𝑖,𝑡\n\u0011#\n,\n(5)\nwhere:\n• 𝑤𝑖,𝑡(𝜃) is the importance ratio;\n• ˆ𝐴𝑖,𝑡is the advantage (either token-wise or sequence-level);\n• 𝑇𝑖is the number of tokens or responses per sample;\n• 𝑁is the total number of samples under the given prompt;\n• 𝑍is the normalization factor (e.g., total tokens, group size, etc.).\nThe PPO algorithm [Schulman et al., 2017b] was first proposed as a computationally efficient\napproximation for the TRPO algorithm [Schulman et al., 2015a]. PPO excels when vanilla policy\ngradient methods suffer from poor data efficiency and robustness issues. In addition, PPO is shown to\nbe much simpler to implement, more general, and has better sample complexity compared to TRPO.\nHowever, since the complex and long CoT nature of LLMs, the exact objective function, gradient\nestimation, and update techniques can take a wide range of different forms as shown in Table 3.\n3.2.2. Critic-based Algorithms\n20\nA Survey of Reinforcement Learning for Large Reasoning Models\nTable 3 | Comparison of representative RL algorithms for reasoning models training.\nDate\nAlgorithm\nAdvantage Estimate\nImportance Sampling\nLoss Agg.\n2017.01\nPPO\nCritic-GAE\nPPO-Style\nToken-Level\n2023.10\nReMax\nGreedy Baseline\nN/A\nToken-Level\n2024.02\nRLOO\nLeave-One-Out\nN/A\nToken-Level\n2025.01\nRF++\nNegative KL + Batch Relative\nPPO-Style\nSequence-level\n2024.02\nGRPO\nGroup Relative\nPPO-Style\nSequence-level\n2025.01\nPRIME\nOutcome + Implicit PRM\nPPO-Style\nToken-Level\n2025.03\nVAPO\nValue Adjusted GAE\nClip-Higher\nToken-Level\n2025.03\nDr. GRPO\nGroup Baseline\nPPO-Style\nToken-Level\n2025.04\nDAPO\nGroup Relative\nClip-Higher\nToken-Level\n2025.05\nClip-Cov\nGroup Relative\nPPO-Style\nSequence-level\n2025.05\nKL-Cov\nGroup Relative\nPPO-Style\nSequence-level\n2025.06\nCISPO\nGroup Relative\nClipped IS-weight\nToken-Level\n2025.07\nGSPO\nGroup Relative\nPPO-Style\nSequence-level\n2025.08\nGMPO\nGroup Relative\nClip-Wider\nGeometric-Avg\n2025.08\nGFPO\nFilter + Group Relative\nPPO-Style\nToken-level\n2025.08\nLitePPO\nGroup-level mean, Batch-level std\nPPO-Style\nToken-level\n2025.08\nFlashRL\nGroup Relative\nTruncated IS\nToken-level\nTakeaways\n• The critic model is trained on a small subset of labeled data, and provides scalable token-\nlevel value signals for unlabeled roll-out data.\n• The critic is required to run and update alongside the LLM, resulting in a significant\ncomputational overhead and scales unfavorably for complex tasks.\nThe first LLM-related works in RL focus on how to effectively align the LLM policy to the external\nsupervision, to make LLMs have better instruction following capabilities while ensuring the models\nare helpful, honest, and harmless. The most common approach for LLM alignment is RLHF [Bai et al.,\n2022a, Christiano et al., 2017, Ouyang et al., 2022, Stiennon et al., 2020]. This technique utilizes\nhumans as a critic for the learning algorithm; the exact steps are as follows. First, a selection of model\noutputs is generated by the LLM and labeled by humans to create a dataset. The dataset is then\nused to train a reward model to predict which response would be preferred by humans. Lastly, the\nreward model is used to train the LLM along with a value function, acting as the critic in the system.\nThe training is often done with the PPO algorithm [Schulman et al., 2017b]. The PPO algorithm\nformulates the objective in the following form:\nJPPO(𝜃) = 𝔼𝑥∼D,𝑦∼𝜋𝜃old (·|𝑥)\n\"\n1\n|𝑦|\n| 𝑦|\n∑︁\n𝑡=1\nmin\n\u0010\n𝑤𝑡(𝜃) ˆ𝐴𝑡, clip(𝑤𝑡(𝜃), 1 −𝜖, 1 + 𝜖) ˆ𝐴𝑡\n\u0011#\n,\n(6)\nwhere ˆ𝐴𝑡is a value-model-based advantage and\n𝑤𝑡(𝜃) = 𝜋𝜃(𝑦𝑡|𝑥, 𝑦<𝑡)\n𝜋𝜃old(𝑦𝑡|𝑥, 𝑦<𝑡) .\n(7)\nWe note that PPO is proposed as a clipped surrogate objective of TRPO, which preserves the con-\nservative policy iteration of TRPO while being unconstrained and having a computational complexity\n21\nA Survey of Reinforcement Learning for Large Reasoning Models\nclose to traditional policy gradient methods. Due to the discrepancy between the current policy and\nthe sampling distribution, the advantage in TRPO is multiplied by 𝑤𝑡, the importance sampling factor\nin Equation 6. PPO maximizes the same objective as TRPO, but removes the trust region constraint.\nFurthermore, PPO adds a clipping mechanism and a KL regularization factor to ensure the current\npolicy does not diverge too far from the rollout policy 𝑝𝑖𝜃𝑜𝑙𝑑.\nIn critic-based approaches, the scalability of RL is achieved by the introduction of a critic model.\nAfter the reward model is sufficiently trained on the manually labeled small subset of generated data,\nit can be used to construct the critic model, generating token-level value signals on a much larger\nscale for the vast majority of unlabeled generated data for RL. However, these works require a critic\nmodel to run and optimize along the target LLM, and create a significant computational overhead.\nIn PPO, the critic model adapts the Generalized Advantage Estimator (GAE) [Schulman et al.,\n2015b] from the RL literature. GAE is typically constructed with the temporal difference error\n𝛿𝑡= 𝑟𝑡+ 𝛾𝑉(𝑦𝑡+1) −𝑉(𝑦𝑡),\n(8)\nwhich is then accumulated across time steps:\nˆ𝐴𝐺𝐴𝐸,𝑡=\n𝑇\n∑︁\n𝑙=𝑡\n(𝛾𝜆)𝑙𝛿𝑡+𝑙,\n(9)\nwhere 𝛾is the discount factor of the MDP and 𝜆is a parameter that controls the bias-variance tradeoff.\nRecent work has argued that the decay factor scales unfavorably for complex reasoning tasks that\nrequire long CoT and proposed a Value-Calibrated PPO [Yuan et al., 2025f] and VAPO [Yue et al.,\n2025c], VRPO [Zhu et al., 2025a] proposed novel mechanisms for enhancing the robustness of the\ncritic model under noisy reward signals.\nIn addition, critic-based algorithms [Hu et al., 2025b] have also demonstrated steady scalability\nproperties for Monte-Carlo estimation with rule-based rewards. Similar approaches have been adapted\nwith fixed external models [Lu et al., 2024, Wang et al., 2024b] by the implementation of PRMs.\nAnother approach to introduce critic models is done with the introduction of Implicit PRM [Yuan\net al., 2025d]. This approach is also able to provide token-level supervision for scalable RL training.\nDifferent from the GAE approach, methods such as Implicit PRM [Yuan et al., 2025d] and PRIME [Cui\net al., 2025a] adapted a specific reward model formulation to directly generate token-level rewards.\n3.2.3. Critic-Free Algorithms\nTakeaways\n• Critic-free algorithms only require sequence-level rewards for training, making them more\nsufficient and scalable.\n• For RLVR tasks, rule-based training signals reliably prevent critic-related issues such as\nreward hacking.\nApart from the critic-based models, which provide token-level feedback signals for model training,\nmany recent works have stated that the response-level rewards are sufficient for scalable reasoning\ntasks with RL. These critic-free algorithms apply the same rule-based or model-generated response-\nlevel reward for all tokens in the response and demonstrate their effectiveness across various tasks.\nCompared to the critic-based algorithms, critic-free approaches do not require a separate critic model,\n22\nA Survey of Reinforcement Learning for Large Reasoning Models\nsignificantly reducing the computational requirement and simplifying training. Moreover, when\ntraining LLMs in rule-based environments where the reward for any response can be clearly defined,\ncritic-free algorithms can avoid reward hacking issues that may arise from an ill-trained critic model.\nThis property makes critic-free algorithms more scalable than critic-based approaches in such settings.\nThe classic REINFORCE [Williams, 1992] algorithm was among the first algorithms developed\nfor RL. It was applied to the LLM problem in [Ahmadian et al., 2024]. The exact formulation for\nREINFORCE is as follows,\nJREINFORCE(𝜃) = 𝔼𝑥∼D,{ 𝑦}∼𝜋old(·|𝑥) [𝑅(𝑥, 𝑦)∇𝜃log(𝜋𝜃(𝑦|𝑥))] ,\n(10)\nwhere 𝑅(𝑥, 𝑦) usually takes the form of ±1 for RLVR tasks. This naive formulation takes the entire\nsequence as a single action and considers the response task as a bandit. However, the vanilla algorithm\nusually suffers from severe instability issues due to high variance. ReMax [Li et al., 2023c] introduced\na variance reduction mechanism to REINFORCE with a greedy baseline estimation. Ahmadian et al.\n[2024] also introduced RLOO, which further provides an unbiased baseline with more stable results.\nREINFORCE++ [Hu, 2025] adapts techniques such as clipping and global advantage normalization\nfrom PPO and GRPO style algorithms to provide a more accurate advantage and gradient estimations.\nOne of the most popular critic-free approaches for RL is GRPO [Shao et al., 2024]. The objective\nformulation for GRPO is as follows,\nJGRPO(𝜃) = 𝔼𝑥∼D,{ 𝑦𝑖}𝐺\n𝑖=1∼𝜋𝜃old (·|𝑥)\n\"\n1\n𝐺\n𝐺\n∑︁\n𝑖=1\n1\n|𝑦𝑖|\n| 𝑦𝑖|\n∑︁\n𝑡=1\nmin\n\u0010\n𝑤𝑖,𝑡(𝜃) ˆ𝐴𝑖,𝑡, clip(𝑤𝑖,𝑡(𝜃), 1 −𝜖, 1 + 𝜖) ˆ𝐴𝑖,𝑡\n\u0011#\n,\n(11)\n𝑤𝑖,𝑡(𝜃) = 𝜋𝜃(𝑦𝑖,𝑡|𝑥, 𝑦𝑖,<𝑡)\n𝜋𝜃old(𝑦𝑖,𝑡|𝑥, 𝑦𝑖,<𝑡) ,\nˆ𝐴𝑖,𝑡= ˆ𝐴𝑖=\n𝑅(𝑥, 𝑦𝑖) −mean({𝑅(𝑥, 𝑦𝑖)}𝐺\n𝑖=1)\nstd({𝑅(𝑥, 𝑦𝑖)}𝐺\n𝑖=1)\n,\n(12)\nwhere all the tokens in 𝑦𝑖share the same advantage as ˆ𝐴𝑖.\nGRPO is a critic-free modification of PPO, where instead of GAE provided by a critic, the entire\nsequence uses the same advantage estimate, which is calculated by a group-relative normalization\nas a better estimation than the binary rule-based reward. Compared to PPO and REINFORCE-style\nmethods, the group-based advantage calculation of GRPO effectively reduces variance from training\nsignals and has been shown to speed up the training process. Other recent approaches, including\nDAPO [Yu et al., 2025d], CISPO [Chen et al., 2025a], Dr. GRPO [Liu et al., 2025t], LitePPO [Liu\net al., 2025v], made further modifications to GRPO with careful tuning of sampling strategy, clipping\nthreshold, and loss normalization to further enhance the stability of the RL training process. Another\nrecent approach, GSPO [Zheng et al., 2025a], replaces the token-wise clipped importance sampling\nratio with a sequence-level clipping.\nApart from REINFORCE and GRPO-related algorithms, there are other critic-free approaches.\nVinePPO modifies PPO by replacing the learned critic with a Monte Carlo advantage estimation.\nCPGD [Liu et al., 2025y] proposed a novel policy gradient objective, along with a drift regularization\nmechanism. K1.5 [Team, 2025d] utilizes RL with an adaptation of mirror descent in the training of\nfoundational models, which successfully enhanced the long-context reasoning capabilities of LLMs. Lv\net al. [2025] have recently introduced a unified policy gradient estimator with a hybrid post-training\nalgorithm, providing a unified framework for policy gradient estimation for RL in LLMs.\nImportance Sampling for Policy Optimization. Due to the rollout-reward-training cycle for RL, it is\ngenerally computationally intractable to ensure the rollout data follows the exact policy distribution\nof the current model. Therefore, importance sampling was introduced to reduce bias in training. The\nfirst version of importance sampling in RL was introduced in TRPO, where a token-wise importance\n23\nA Survey of Reinforcement Learning for Large Reasoning Models\nratio 𝑤𝑖,𝑡was introduced into the objective. This approach is widely adopted among recent works, such\nas GRPO. This approach is restricted to the token-wise importance ratio since the actual distribution\nratio can not be effectively calculated over the long context of CoT. However, token-level importance\nsampling introduces another bias into RL algorithms, since the actual sampling distribution given\npolicy is defined with respect to the state-action pair, whereas the token-level approach only considers\nthe current action. GMPO [Zhao et al., 2025f] seeks mitigation by introducing a geometric averaging\nto increase training robustness for tokens with extreme importance sampling ratios. In the recent\nwork of GSPO [Zheng et al., 2025a], a sequence-level importance sampling factor was calculated.\nGSPO adds a unique normalization factor to ensure that the probability ratio can be calculated, but\nthis approach is also a biased estimation of the actual importance sampling factor. A promising new\ndirection is to move beyond the theoretical framework of standard on-policy policy gradient methods\nand instead derive inherently off-policy algorithms directly from supervised learning theory [Chen\net al., 2025c]. We will provide a detailed introduction to off-policy optimization in the next section.\n3.2.4. Off-policy Optimization\nTakeaways\n• Off-policy RL boosts sample efficiency by decoupling data collection from policy learning,\nenabling training from historical, asynchronous, or offline datasets.\n• Modern practice mixes off-policy, offline, and on-policy methods (e.g., SFT+RL or large-\nscale offline learning) to improve stability and performance.\nIn RL, off-policy methods address the scenario where the policy being learned (the target policy)\ndiffers from the policy generating the data (the behavior policy). This core distinction allows an\nagent to learn about an optimal course of action without having to follow it during data collection.\nThis flexibility is a key advantage, often leading to more sample-efficient algorithms than on-policy\ncounterparts, which require new data sampled directly from the current policy for each update. A\ncore challenge in these methods is correcting for the distributional shift between the behavior policy\nand the target policy, often addressed using importance sampling with a weighted objective function:\nLpolicy(𝜃) = −𝔼𝑥∼D,𝑦∼𝜋b(𝑦|𝑥)\n\u0014𝜋𝜃(𝑦|𝑥)\n𝜋b(𝑦|𝑥) · 𝑟(𝑥, 𝑦)\n\u0015\n,\n(13)\nwhere the fraction 𝜋𝜃(𝑦|𝑥)\n𝜋𝑏(𝑦|𝑥) serves as the importance weight between the target policy 𝜋𝜃and the\nbehavior policy 𝜋b.\nIn practical large-scale model training, off-policy learning often manifests in different forms. Recent\nworks can be broadly grouped into three aspects: 1) training–inference precision discrepancies, where\nmodels are trained with high precision but deployed in lower precision, creating a gap between the\ntarget and behavior policies; 2) asynchronous experience replay mechanisms, which enhance efficiency\nand stability by reusing past trajectories during learning; and 3) broader off-policy optimization\napproaches, including optimizer-level improvements, data-level offline learning, and hybrid methods\nthat combine supervised fine-tuning with RL.\nTraining-Inference Precision Discrepancy. A notable off-policy scenario arises from the difference\nin parameter precision between the training model and the inference model, a common consequence\nof using different frameworks for training and inference [Yao et al., 2025a] (e.g., vLLM vs. FSDP), or\nof model quantization to accelerate inference [Lin et al., 2016]. It is common practice to train a model\nusing high-precision parameters (e.g., 32-bit floating point) and then deploy a quantized version with\nlower-precision parameters (e.g., 8-bit integers) [Liu et al., 2025h]. This creates a discrepancy where\n24\nA Survey of Reinforcement Learning for Large Reasoning Models\nthe deployed, low-precision model acts as the behavior policy, generating real-world interaction data,\nwhile the high-precision model remains the target policy being updated during training. While this\nmismatch establishes an off-policy learning problem, research indicates that the policy divergence\ndue to quantization is often minimal. Consequently, this difference can be effectively managed with\nsimple correction techniques, such as truncated importance sampling (TIS) [Ionides, 2008, Yao et al.,\n2025a], allowing for stable training while retaining the benefits of accelerated inference.\nAsynchronous Off-Policy Training. Asynchronous training pairs naturally with off-policy RL for\nLLMs. Many actors generate trajectories concurrently and append them to a shared replay buffer,\nwhile a centralized learner samples mini-batches from this buffer to update the target policy. Building\non this view, several recent methods deliberately reuse past trajectories to improve efficiency and\nstability. One example is Retrospective Replay [Dou et al., 2025], which enhances exploration for LLM\nreasoning by selectively replaying earlier reasoning traces to guide current policy updates. Similarly,\nEFRame [Wang et al., 2025b] adopts an exploration-filter-replay mechanism, interleaving filtered\nresponses with fresh rollouts to encourage deeper reasoning. In the domain of code generation,\nPossibility- and Pass-rate Prioritized Experience Replay (P2Value) [Chen et al., 2024c] takes this\nfurther by prioritizing high-value code samples in the buffer, leading to more stable optimization.\nExtending these ideas to multimodal interaction, ARPO [Lu et al., 2025b] applies replay to GUI\nagents, where successful trajectories are reused to provide reliable learning signals under sparse\nrewards. Finally, RLEP [Zhang et al., 2025c] anchors exploration with an experience buffer of verified\nsuccessful trajectories from earlier runs, which are blended with new rollouts to balance reliability\nwith discovery. Together, these approaches illustrate how replay buffers have become a cornerstone\nof modern, asynchronous off-policy training for LLM-based agents.\nOff-Policy Optimization. Recent advancements in fine-tuning LLMs have explored sophisticated\noptimization strategies beyond traditional on-policy RL. These methods, broadly categorized as off-\npolicy and mixed-policy optimization, aim to improve sample efficiency, training stability, and overall\nperformance by creatively using data from various sources. We introduce this topic below:\n• Optimizer-Level Off-Policy Methods: These approaches focus on improving the optimization\nprocedure itself, emphasizing stability and efficiency in policy updates. For example, SPO [Cohen\net al., 2025] introduces a soft policy optimization method that enables stable online, off-policy\nRL, while TOPR [Roux et al., 2025] proposes a tapered off-policy REINFORCE algorithm for\nimproved stability and efficiency. ReMix [Liang et al., 2025a] further highlights this by focusing\non efficiently leveraging off-policy data to maximize the utility of available information.\n• Data-Level Off-Policy Methods: A class of off-policy algorithms learns entirely from large-scale,\nexternal offline data [Zhang et al., 2025f]. For instance, the Decision Field Theory (DFT) frame-\nwork [Wu et al., 2025i] adapts methodologies from other fields to learn from these complex\ndatasets. Similarly, techniques like Implicit Fine-Tuning (IFT) [Hua et al., 2024] are being\nexplored to refine pre-trained models using this external data, aiming to enhance their perfor-\nmance on specific downstream tasks. Another relevant method is DPO [Rafailov et al., 2023],\nwhich directly optimizes a policy from preference data through a simple classification objective.\nThese methodologies collectively represent a move towards more data-centric approaches in RL,\nenabling the development of sophisticated policies from vast and diverse sources of offline data.\n• Mix-Policy Methods: In parallel with reusing past data more efficiently, mixed-policy optimization\nrepresents another significant trend, which combines the strengths of SFT and RL. This hybrid\napproach leverages the stability from SFT on expert data while using RL to optimize for specific\nreward functions, integrating the supervised data in two primary ways. One strategy is at the\nloss-level, where SFT and RL objectives are combined directly in the loss function [Lv et al., 2025,\n25\nA Survey of Reinforcement Learning for Large Reasoning Models\nXiao et al., 2025b, Zhang et al., 2025j]. Methods like UFT [Liu et al., 2025j], SRFT [Fu et al.,\n2025c], LUFFY [Yan et al., 2025a], RED [Guan et al., 2025], and ReLIFT [Ma et al., 2025a] all\nexemplify this by creating unified or single-stage training processes that learn from both expert\ndemonstrations and RL feedback simultaneously. A second strategy operates at the data level,\nusing expert data to structure the generation process itself. Here, high-quality data serves as\na prefix or anchor to guide the model’s exploration [Guo et al., 2025d]. For instance, BREAD\n[Zhang et al., 2025o] generates branched rollouts from expert anchors, and Prefix-RFT [Huang\net al., 2025g] blends the training regimes via prefix sampling. By mixing policies at either the loss\nor data level, these methods prevent reward hacking and ensure the model retains knowledge\nfrom SFT, leading to more robust and capable models for complex reasoning.\n3.2.5. Regularization Objectives\nTakeaways\n• Objective-specific regularization helps balance exploration and exploitation, boosting RL\nefficiency and policy performance.\n• The optimal choice and form of KL, entropy, and length regularization remain open\nquestions, each affecting policy optimization and scalability.\nAs introduced in previous sections, ensuring stability and preventing catastrophic policy drift is\nparamount. In particular, for long-horizon training, techniques such as KL regularization and entropy\nregularization are widely employed.\nKL Regularization. The role of KL divergence regularization is a highly controversial topic in this\narea. In most studies, KL regularization is applied to 1). current policy 𝜋𝜃and the reference policy\n𝜋ref, 2). current policy 𝜋𝜃and the old policy 𝜋old. We provide a unified formulation in Equation 14.\nLKL = 𝛽\n| 𝑦|\n∑︁\n𝑡=1\n𝐾𝐿(𝜋𝜃(·|𝑦𝑡)||𝜋ref/old(·|𝑦𝑡)).\n(14)\n• For the former, this is a commonly used technique in RLHF [Ouyang et al., 2022, Touvron et al.,\n2023]. It was initially introduced to prevent the model from being destructively updated. Prior\nwork argues that incorporating a KL penalty is essential for maintaining stability and avoiding\nentropy collapse over thousands of training steps. To reduce the risk of the KL term excessively\nconstraining progress, Liu et al. [2025i] use this method combined with a periodic reference\npolicy reset, in which the reference model is updated to a recent snapshot of the training policy.\nTo simultaneously maintain knowledge and enhance reasoning capabilities, Wang et al. [2025h]\napply stronger KL regularization to low-entropy tokens and weaker regularization to high-entropy\ntokens. However, in the context of RL for reasoning with LLMs, which is more challenging\nthan standard RLHF, the necessity of this kind of KL regularization needs to be reconsidered.\nRecently, many studies have identified that the policy is expected to explore freely during training,\nthus may diverge significantly from its initialization to discover new CoT structures, making the\nKL constraint an unnecessary restriction. Thus, a majority of other recent works advocate for\nremoving the KL penalty entirely [An et al., 2025, Arora and Zanette, 2025, Chen et al., 2025q,\nCui et al., 2025a, Fan et al., 2025b, He et al., 2025d, Liao et al., 2025b, Liu et al., 2025t, Yan\net al., 2025a, Yu et al., 2025d] to simplify implementation, reduce memory cost and achieve\nmore scalable GRPO.\n• For the latter case, it can serve as a substitute for the clip form of the policy loss [Schulman\n26\nA Survey of Reinforcement Learning for Large Reasoning Models\net al., 2017b]. Zhang et al. [2025q] discusse the differences between forward KL, reverse KL,\nnormalized KL, and Normalized forms. This approach has also been adopted in Cui et al. [2025b],\nLyu et al. [2025], Team [2025d], demonstrating its potential across different RL training scales.\nNevertheless, its deeper mechanisms and its significance for scalable RL remain under exploration.\nEntropy Regularization. In the RL literature, preserving policy entropy is widely considered a critical\naspect of many algorithms [Eysenbach and Levine, 2021, Williams, 1992, Williams and Peng, 1991].\nTo this end, policy entropy is actively controlled through regularization techniques [Haarnoja et al.,\n2018, Schulman et al., 2017b, Ziebart et al., 2008].\nLent = −𝛼\n| 𝑦|\n∑︁\n𝑡=1\n𝐻[𝜋𝜃(·|𝑦𝑡)] = 𝛼\n| 𝑦|\n∑︁\n𝑡=1\n|V|\n∑︁\n𝑣=1\n𝜋𝜃(𝑦𝑣\n𝑡|𝑦𝑡) log 𝜋𝜃(𝑦𝑣\n𝑡|𝑦𝑡).\n(15)\nHowever, in RL for LLMs, directly applying entropy regularization is neither common nor effec-\ntive [Cui et al., 2025b, He et al., 2025d]. The use of an explicit entropy regularization term in the\nloss function remains a point of contention. While some find it beneficial, using either a standard\ncoefficient [Shrivastava et al., 2025] or a targeted loss function [Wu et al., 2025e], others argue\nagainst it, finding it can lead to instability or even training collapse, especially with sparse rewards [An\net al., 2025, Liao et al., 2025b]. Many studies have shown the phenomenon of entropy collapse when\nno intervention is applied [Cheng et al., 2025a, Cui et al., 2025b, Yu et al., 2025d], which hinders\neffective policy exploration during training. To address it, He et al. [2025d] dynamically adjust the\ncoefficient of the entropy loss, Yu et al. [2025d] employs the clip-higher technique to involve more\nlow-probability tokens in the policy update, Wang et al. [2025l] directly train on 20% high-entropy\ntokens, Cheng et al. [2025a] and Chen et al. [2025j] emphasize entropy through incorporate it\ninto the advantage computation. Beyond these techniques, which explicitly maximize entropy, Cui\net al. [2025b] provide a theoretical explanation for the underlying mechanism of entropy dynamics,\nidentifying the covariance between an action’s output probability and its advantage as the entropy\n“driver”. Built on this insight, Clip-Cov and KL-Cov are proposed to regulate entropy by selectively\nconstraining a small portion of tokens exhibiting exceptionally high covariance.\nLength Penalty. Recent successes of LRMs on complex tasks have validated the effectiveness of long-\nCoT reasoning. Yet longer reasoning traces incur higher inference costs. To balance the reasoning\nbudget and performance [Agarwal et al., 2025a, He et al., 2025e], many works seek to reduce the\nreasoning cost while retaining the model performance [Aggarwal and Welleck, 2025, Liu et al., 2025o,\nLuo et al., 2025a, Su et al., 2025b, Xiang et al., 2025]. For example, Aggarwal and Welleck [2025]\ncontrol reasoning length by ensuring adherence to user-specified length constraints, while Yuan et al.\n[2025a] and Luo et al. [2025a] design relative-length regularization and an accuracy-preservation\nconstraint to the optimization objective, Xiang et al. [2025] and Liu et al. [2025o] propose to apply\nadaptive length penalties conditioned on problem difficulty to preserve the model ability.\n3.3. Sampling Strategy\nUnlike static datasets, RL depends on actively curated rollouts, where decisions about what and\nhow to sample directly influence learning efficiency, stability, and the quality of acquired reasoning\nbehaviors. Effective sampling strategies not only ensure diverse and informative training signals\nbut also align the learning process with the intended reward structure and policy objectives. In\nthis subsection, we survey recent advances in dynamic and structured sampling (§ 3.3.1), as well\nas hyperparameter adjustment techniques that further optimize sampling and policy improvement\n(§ 3.3.2).\n27\nA Survey of Reinforcement Learning for Large Reasoning Models\n3.3.1. Dynamic and Structured Sampling\nTakeaways\n• High-quality, diverse rollouts stabilize RL training and enhance overall performance by\nexposing agents to a broader range of meaningful experiences.\n• Balancing the exploration of diverse trajectories with maintaining high sampling efficiency\npresents a fundamental trade-off in RL.\nSampling has become a first-class lever in RL fine-tuning for reasoning LLMs, serving as an efficient\nand adaptive mechanism to maximize data utilization, reduce wasted computation, and enhance\ntraining effectiveness or a control and a guidance for LLMs to sample in a structured format.\nDynamic Sampling. Dynamic sampling adapts both the selection of prompts for rollout and the\ncomputational budget allocated to each, based on online learning signals such as success rate,\nadvantage, uncertainty, or estimated difficulty. The primary goal is to concentrate computing on\ninformative examples while avoiding saturated or unproductive ones. Existing methods generally fall\ninto two categories:\n• Efficiency-oriented Sampling: Some works use online-filtering to concentrate training on\nquestions of medium difficulty to ensure training effectiveness and efficiency. A representative\ndesign is PRIME [Cui et al., 2025a], which applies an online filter to drop out too easy or too\ndifficult problems. Another example is DAPO [Yu et al., 2025d], which over-samples and filters\nprompts whose rollouts are saturated (all-correct) or degenerate (all-wrong), then repeatedly\nsamples until each mini-batch contains prompts with non-zero advantage, focusing on medium-\ndifficulty cases to maintain informative gradients. Building on this foundation, prioritized schemes\nallocate rollout budget toward under-mastered items by sampling proportional to failure rates,\nas 𝑝(𝑖) ∝(1 −𝑠𝑖) rule [Team, 2025d]. Curriculum learning approaches operate at multiple scales:\ncategory-level selection [Chen et al., 2025o] uses non-stationary bandits, while E2H [Parashar\net al., 2025] follows easy-to-hard schedules with convergence guarantees for small models.\nEfficiency methods include pre-rollout selection to skip unhelpful prompts and difficulty-based\nonline selection with rollout replay [Sun et al., 2025e, Zheng et al., 2025b]. POLARIS [An\net al., 2025] formalizes this via offline difficulty estimation, constructing “mirror-J” distributions\nby model scale, continuously removing mastered items, and applying in-batch information\nreplacement. Extending these efficiency gains, recent advances use lightweight controllers for\nadaptive sampling [Do et al., 2025, Shi et al., 2025b] without modifying algorithms, while\nexperience replay with random reshuffling [Fujita, 2025] reduces variance through balanced\nutilization, and enhanced prioritized methods [Li et al., 2024a] dynamically adjust priority weights\nbased on experience pool features. Sampling efficiency can also be improved by structuring the\ngeneration process with expert data: high-quality demonstrations are used as prefix anchors to\nbias exploration toward promising regions of the search space [Guo et al., 2025d, Huang et al.,\n2025g, Zhang et al., 2025o]. The field shifts from uniform sampling to model-aware strategies\ncombining item-, category-, and difficulty-level choices for stronger learning signals per rollout.\n• Exploration-oriented Sampling: There are other works aiming for exploration using dynamic\nrollout. ARPO [Dong et al., 2025b] is proposed to implement entropy-guided rollout to ensure\nhigh uncertainty so that the model will call external tools, improving diversity. DARS [Yang et al.,\n2025g] proposes a rollout mechanism to dynamically assign sample numbers for questions of\ndifferent difficulty. Zhou et al. [2025f] propose RuscaRL by providing the policy with different\nrubrics during rollout to enhance exploration. Different from above, G2RPO-A [Guo et al., 2025d]\n28\nA Survey of Reinforcement Learning for Large Reasoning Models\ndoes not drop all-wrong questions, but add a guidance during the thinking process to generate\ncorrect samples for hard questions. Besides, Li et al. [2025s] utilize the latest 𝑘checkpoints to\ngenerate 𝑘responses to prevent forgetting during training.\nStructured Sampling. Structured sampling controls not only what is sampled but also the topology\nof reasoning traces, aligning generation, credit assignment, and compute reuse with the underlying\nstructure of problem solving. By organizing rollouts as trees or through shared and segmented\nprefixes, these methods enable node-level rewards, improved reuse of partial computations (e.g.,\nKV caches), and greater sample efficiency under memory and budget constraints. We highlight two\nrepresentative approaches:\n• Search-driven Tree Rollouts: Other works leverage Monte Carlo Tree Search (MCTS) for tree-\nformat response generation using the classic phases: initialization, selection, expansion, and\nbackpropagation. They view a single inference as a tree rather than a single chain, and assign\nrewards at the node level, which can produce a more dense/fine-grained process signal. Hou\net al. [2025] propose TreeRL, an on-policy tree search framework that outperforms traditional\nChain-of-Thought RL (ChainRL) while substantially reducing computational overhead through\nmore efficient search strategies. Concurrently, ToTRL [Wu et al., 2025c] introduces a Tree-\nof-Thought–guided training paradigm in synthetic puzzle environments, enabling emergent\ngeneralization to out-of-distribution tasks such as mathematical reasoning. Additionally, Yang\net al. [2025f] integrate MCTS into training pipelines to generate rule-based, fine-grained process\nrewards, improving reward signal granularity and fidelity in policy optimization.\n• Shared-prefix or Segment-wise Schemes: While these tree search methods enrich exploration\nand provide fine-grained rewards, their sample efficiency remains a limitation. Some works\ndesign segmented/shared prefix sampling to improve generation efficiency [Guo et al., 2025c,\nHou et al., 2025, Li et al., 2025q, Yang et al., 2025f]. SPO [Guo et al., 2025c], TreeRPO [Yang\net al., 2025f], TreeRL [Hou et al., 2025], FR3E [Zheng et al., 2025c], and ARPO [Dong et al.,\n2025b] conduct additional sampling starting from previously generated prefix. TreePO [Li et al.,\n2025q] implements a segment-wise tree sampling algorithm that alleviates the KV cache burden,\nreducing the GPU hours for training, and improving sampling efficiency.\n3.3.2. Sampling Hyper-parameters\nTakeaways\n• Careful hyperparameter tuning is essential for scalable RL, as naive settings can lead to\ninefficiency and unstable training (e.g., entropy collapse).\n• Scalable RL relies on a holistic combination of strategies to balance cost and stability, such\nas staged context lengthening and dynamic exploration controls.\nThis subsection summarizes the hyperparameter adjustment strategies for sampling from recent\nworks. Effective RL training requires a delicate balance between several competing objectives, and\nrecent literature has focused on techniques across two primary axes: 1) managing the exploration-\nexploitation trade-off to ensure the model discovers and refines effective reasoning paths; 2) efficiently\nmanaging sequence length to balance reasoning depth with computational cost.\nExploration and Exploitation Dynamics. A central challenge is balancing exploration (discovering\nnovel reasoning strategies) with exploitation (refining high-reward solutions). The primary levers\nfor this are temperature, entropy regularization, and PPO’s clipping mechanism. For temperature,\n29\nA Survey of Reinforcement Learning for Large Reasoning Models\nstrategies vary significantly. Some works propose a dynamic approach, such as staged temperature\nincreases (e.g., 1.40 →1.45 →1.50 for a 4B model, 0.7 →1.0 →1.1 for a 7B model) to gradually\nexpand trajectory diversity as training progresses [An et al., 2025], or using a scheduler to dynamically\nadjust temperature to maintain a stable entropy level [Liao et al., 2025b]. A more prescriptive approach\nrecommends tuning the training temperature to keep the post-scaling entropy around a target of 0.3,\nwhich is found to strike an optimal balance [Liu et al., 2025u, Wu et al., 2025e]. Other works simply\nadvocate for a high, fixed temperature (e.g., 1.0 or 1.2) to encourage initial exploration, while noting\nit is insufficient on its own to prevent long-term entropy decline [Arora and Zanette, 2025, Liu et al.,\n2025i, Shrivastava et al., 2025].\nLength Budgeting and Sequence Management. Nearly all works grapple with managing the length\nof generated responses to balance performance and cost. The most prevalent strategy is staged context\nlengthening [Luo et al., 2025c]. This involves starting RL with a short context window (e.g., 8𝑘)\nbefore progressively increasing it to 16𝑘, 24𝑘, or 32𝑘in later stages [Chen et al., 2025q, Liu et al.,\n2025i,u, Luo et al., 2025c]. The initial short-context stage is considered essential, as it forces the\nmodel to learn more concise and token-efficient reasoning patterns [Chen et al., 2025q, Liu et al.,\n2025u, Luo et al., 2025c]. An alternative to training on very long contexts is to apply inference-time\nlength extrapolation techniques like Yarn at inference time, allowing a model trained on shorter\nsequences to generalize to longer ones [An et al., 2025]. For handling responses that exceed the length\nbudget, there is no consensus. Some works apply a soft, linear penalty as the response approaches the\nmaximum length [Yu et al., 2025d] or a tunable penalty (𝛼) directly in the reward function [Arora\nand Zanette, 2025]. A more nuanced, stage-dependent strategy is to filter (mask the loss of) overlong\nsamples when the length budget is short (8𝑘-16𝑘) but to penalize them when the budget is large\n(32𝑘), as filtering can become detrimental at very long contexts [Liu et al., 2025u, Wu et al., 2025e].\nAcross these works, effective hyperparameter adjustment emerges as the joint tuning of explo-\nration (temperature, entropy targets, clipping), efficiency (staged length curricula), and sequence\nmanagement (overlength filters, penalties, or inference-time extrapolation). These methods are\ndirectly applicable to most GRPO/PPO-style RL pipelines for LLMs.\n4. Foundational Problems\nHaving reviewed the key components of RL pipelines for LLMs, we now turn to several foundational\nproblems that remain central and often unresolved in the field. In this section, we articulate the\ncore issues, present contrasting perspectives, and summarize recent progress on each open question.\nSpecifically, we discuss challenges such as the fundamental role of RL (sharpening versus discovery) in\n§ 4.1, the boundary between RL and SFT (generalization versus memorization) in § 4.2, the selection\nof model priors (weak versus strong models) in § 4.3, the effectiveness of training algorithms (tricks\nversus traps) in § 4.4, and the granularity of reward signals (process versus outcome) in § 4.5. By\nhighlighting these open questions, we aim to clarify the current landscape and motivate further\ninvestigation into the foundational underpinnings of RL for LRMs.\n4.1. RL’s Role: Sharpening or Discovery\nWe begin by summarizing the two prevailing perspectives on the role of RL: Sharpening and Discovery.\nThese perspectives appear to be in direct opposition. The Sharpening view suggests that RL does not\ncreate genuinely novel patterns, but instead refines and reweights correct responses already contained\nwithin the base model. By contrast, the Discovery view claims that RL is capable of uncovering new\npatterns that the base model does not acquire during pre-training and would not generate through\nrepeated sampling.\n30\nA Survey of Reinforcement Learning for Large Reasoning Models\nThe divergence between the Sharpening and Discovery perspectives can be understood through\nmultiple theoretical lenses. First, from the KL divergence optimization viewpoint, SFT typically\noptimizes the forward KL divergence 𝐷𝐾𝐿(𝑝𝑑𝑎𝑡𝑎||𝑝𝑚𝑜𝑑𝑒𝑙), exhibiting mode-covering behavior: the model\nattempts to cover all modes in the data distribution. In contrast, RL methods optimize the reverse\nKL divergence 𝐷𝐾𝐿(𝑝𝑚𝑜𝑑𝑒𝑙||𝑝𝑟𝑒𝑤𝑎𝑟𝑑), which exhibits mode-seeking behavior: concentrating probability\nmass on high-reward regions [Ji et al., 2024, Sun, 2024]. Recent theoretical advances have further\nenriched this understanding. Xiao et al. [2025b] demonstrate that RLHF can be viewed as implicit\nimitation learning on preference data, establishing a deep connection between RL-based alignment\nand behavioral cloning. Similarly, Sun [2024] frames SFT itself as a form of inverse RL, revealing\nthat even supervised approaches implicitly involve reward modeling. These perspectives suggest\nthat the Sharpening vs. Discovery debate may be addressing different aspects of a unified learning\nprocess: while the mode-seeking nature of RL provides a mechanism for sharpening, the implicit\nreward learning and compositional capabilities could enable discovery through extended training.\n• Initially, DeepSeek-R1 [Guo et al., 2025a] demonstrated promising “Aha” behaviors through RLVR,\ninspiring lightweight reproductions such as TinyZero [Pan et al., 2025c], which reported similar\nphenomena with simplified training recipes and minimal code. Domain-specific adaptations soon\nfollowed, including Logic-RL [Xie et al., 2025c], which showcased rule-based RL that fosters\nreflection and verification skills with transfer to mathematical reasoning.\n• However, Limit-of-RLVR [Yue et al., 2025b] provides a sharpening-oriented counterargument:\nPass@K evaluations indicate that RL enhances Pass@1 performance, yet tends to underperform\nrelative to base models when sampling broadly at large-𝑘Pass@K. This suggests that RL predomi-\nnantly narrows the search space rather than uncovering fundamentally novel solution trajectories.\nConcurrent debates questioned whether the observed “Aha” behaviors were genuinely induced\nby RL or merely latent capabilities already embedded during pre-training [Liu et al., 2025s,\nSetlur et al., 2025]. Mechanistic analyses further argued that RL gains often arise from entropy\nshaping or reward proxies. For instance, high-entropy “forking” tokens appear to dominate\nimprovements [Wang et al., 2025l]; maximizing model confidence (RENT) and TTRL enhance\nreasoning without relying on external rewards [Prabhudesai et al., 2025, Zuo et al., 2025b]; and\neven spurious or random reward signals can shift Qwen models [Shao et al., 2025], implying\nthat RL often surfaces pre-trained reasoning features rather than learning entirely new ones. A\nparallel line of work frames test-time search and compute as a meta-RL problem, proposing MRT\nto densify progress signals and yield better scaling of “thinking time” than outcome-only RL [Qu\net al., 2025b]. Data-efficiency studies have also shown that even extreme cases such as 1-shot\nRLVR can substantially improve mathematical reasoning, again aligning with the sharpening\nview of eliciting latent capabilities [Wang et al., 2025q]. Complementing these perspectives, a\nsystematic study of exploration in RLVR [Deng et al., 2025a] formalizes Pass@K as a measure of\nexploration boundaries and uncovers nuanced entropy–performance trade-offs across training,\ninstance, and token levels, thereby situating the sharpening view within a unified analytic frame-\nwork. Recently, Shenfeld et al. [2025] introduce the principle of “RL’s Razor,” demonstrating that\nonline RL preserves prior knowledge significantly better than supervised fine-tuning. They show\nthat RL’s advantages stem from its ability to maintain existing capabilities while adapting to new\ntasks, rather than discovering entirely novel behaviors.\n• Recently, however, several works have reopened the case for discovery. ProRL [Liu et al., 2025i]\nreports that sufficiently prolonged and stabilized RL can extend a base model’s reasoning frontier,\nimproving both Pass@1 and Pass@K. Continued scaling evidence is provided by ProRL v2 [Liu et al.,\n2025i], which incorporates engineering advances and demonstrates stronger results. Meanwhile,\ncritiques of Pass@K metrics have led to alternatives such as CoT-Pass@𝑘, supported by theoretical\n31\nA Survey of Reinforcement Learning for Large Reasoning Models\narguments that RLVR implicitly incentivizes correct reasoning paths rather than merely rewarding\nlucky endpoints [Wen et al., 2025c]. Complementary approaches sustain RLVR’s benefits by\nemploying self-play problem synthesis to preserve entropy and enhance Pass@K [Liang et al.,\n2025c], or by directly optimizing Pass@K through novel policy objectives [Chen et al., 2025v,\nWalder and Karkhanis, 2025]. Yuan et al. [2025c] further provide compelling evidence for the\ndiscovery view by demonstrating that LLMs can learn new skills in RL through the composition of\nexisting capabilities, suggesting that RL enables emergent behaviors beyond simple refinement of\npre-existing patterns.\nThe apparent dichotomy between Sharpening and Discovery may be reconciled through recent\ntheoretical advances that reveal deeper connections between different alignment paradigms. The\nwork of Xiao et al. [2025b] shows that RLHF implicitly performs imitation learning, while Sun [2024]\ndemonstrates that SFT can be understood as inverse RL. These insights suggest that both supervised\nand RL approaches are operating within a shared theoretical framework of distribution matching\nand reward optimization. The key distinction lies not in whether these methods can discover new\ncapabilities, but rather in how they navigate the trade-off between exploration and exploitation.\nThe mode-seeking property of reverse KL in RL provides a mechanism for efficient convergence to\nhigh-performance regions (Sharpening), while the implicit reward learning and sequential decision-\nmaking aspects enable the composition of existing capabilities into novel behaviors (Discovery) when\ngiven sufficient training time and appropriate regularization [Liu et al., 2025i, Yuan et al., 2025c].\nThis unified perspective suggests that the debate should shift from “Sharpening or Discovery” to\nunderstanding the conditions under which each phenomenon dominates.\n4.2. RL vs. SFT: Generalize or Memorize\nIn this subsection, we discuss the roles of RL and supervised fine-tuning, focusing on the interplay\nbetween generalization and memorization. There are two primary approaches to post-training LLMs:\nSFT and RL. Current debates focus on two main questions: 1) Which method better enables out-of-\ndistribution generalization? 2) Does behavior cloning via SFT set an upper bound on generalization\ncapabilities? Recently, significant research attention has been devoted to this topic. Notably, Chu et al.\n[2025a] provide a direct conclusion across both textual and vision environments, stating that “SFT\nmemorizes, RL generalizes.”\nTwo recent studies sharpen this contrast. Huan et al. [2025] find that RL on math tasks (RL-\non-math) tends to preserve, or even enhance, performance on non-math tasks and instruction\nfollowing, whereas supervised fine-tuning on math (SFT-on-math) often leads to negative transfer and\ncatastrophic forgetting. Their diagnostic analyses based on latent-space PCA and token-distribution\n(KL) measures, as well as those by Mukherjee et al. [2025], suggest that SFT induces representation\nand output drift (memorization), while RL better preserves the base-domain structure (generalization).\nComplementarily, Zhou et al. [2025d] dissect five math problem-solving training routes and observe\nthat 1) continual pretraining on math text provides only modest transfer, 2) conventional short-CoT\nSFT frequently harms generalization, yet 3) long-CoT SFT and rule-based RL (with format/correctness\nrewards) expand reasoning depth and self-reflection and thus improve broader reasoning; moreover,\nan SFT warmup before RL stabilizes the policy and further boosts cross-domain transfer. These results\nsuggest that on-policy objectives and longer, self-reflective traces foster transferable patterns that\nremain robust under distribution shift, whereas short-CoT SFT tends to overfit to surface patterns,\nmirroring the classic RL-vs.-SFT divide between generalization and memorization. There are three\nmain research directions on this topic:\n• RL demonstrates superior generalization: Chu et al. [2025a] show that RL outperforms SFT\n32\nA Survey of Reinforcement Learning for Large Reasoning Models\nin terms of Out-of-Distribution (OOD) performance, while SFT tends to memorize data on the\nGeneralPoints and V-IRL tasks. Previous studies [Kirk et al., 2023] have also indicated that RLHF,\nparticularly under greater distribution shifts, can generalize more effectively than SFT, though\nthis may come at the cost of reduced output diversity. Additionally, DeepSeek-R1 [Guo et al.,\n2025a] demonstrates that pure RL training can lead to the spontaneous emergence of advanced\nreasoning behaviors, such as reflection and verification.\n• RL is not a panacea: The generalization ability of RL is strongly influenced by the initial\ndata distribution and the design of verification rewards. Jin et al. [2025d] find that RL can\npartially mitigate overfitting; however, it remains ineffective in cases of severe overfitting or\nabrupt distributional shifts, as observed in OOD “24 points” and spectrum analysis tasks. The\nprimary value of RL lies in its ability to facilitate “proper learning” [Swamy et al., 2025]. SFT\ncan significantly improve generalization when appropriate reweighting, trust-region constraints,\nor dynamic rescaling are applied, and it often better prepares models for subsequent RL [Qin\nand Springenberg, 2025]. In practice, SFT may serve as a lower bound for sparse reward RL.\n• Unified or alternating paradigms of SFT and RL: Yan et al. [2025a] present a framework\nthat enhances RLVR by incorporating off-policy reasoning traces. Liu et al. [2025j] integrates\nSFT and RL into a single-stage target, theoretically overcoming the bottleneck of long-horizon\nsample complexity and empirically demonstrating superiority over using either approach alone.\nFu et al. [2025c] propose a joint single-stage integration of demonstration imitation (SFT) and\nstrategy improvement (RL) using entropy perception weights. Zhang et al. [2025o] provide\ntheoretical evidence that in scenarios involving small models, high difficulty, or sparse successful\ntrajectories, the traditional from SFT to RL two-stage approach may fail entirely. They address\nthis by employing a branch rollout mechanism that begins from expert anchors to effectively link\nthe two stages. Ma et al. [2025a] find that RL excels at consolidating and enhancing existing\nabilities, whereas SFT is more effective at introducing new knowledge or novel model capabilities.\nHowever, several challenges remain unresolved. One major issue is distinguishing between\ngenuine problem-solving ability and mere memorization of answers, while simultaneously avoiding\ndata contamination [Satvaty et al., 2024]. There is still a lack of standardized, reproducible out-of-\ndistribution benchmarks. Additionally, RL training is highly sensitive to the initial data distribution;\nwhen SFT induces significant representation drift, the ability of RL to recover and generalize is\nlimited [Jin et al., 2025d]. To address these challenges, there is a need to promote frameworks\nsuch as UFT [Liu et al., 2025j], SRFT [Fu et al., 2025c], and Interleaved [Ma et al., 2025a], which\nmechanize the integration of SFT for incorporating new knowledge with RL for amplification and\nrobustness. Lv et al. [2025] also explore automated scheduling strategies to determine when to switch\nbetween SFT and RL and how to allocate their proportions effectively.\nIn conclusion, RL tends to achieve “true generalization” on verifiable tasks and under substantial\ndistribution shifts, but it is not a panacea. Modified SFT can help bridge the remaining gaps in\ngeneralization. Consequently, best practices are converging towards unified or alternating hybrid\nparadigms that combine the strengths of both approaches [Chen et al., 2025c,h, Liu et al., 2025j, Lv\net al., 2025, Wu et al., 2025i, Zhu et al., 2025e].\n4.3. Model Prior: Weak and Strong\nRecent studies have shown that RL can now perform well across a wide range of tasks when coupled\nwith sufficiently powerful model priors and verifiable reward signals, thereby shifting the primary\nbottleneck from scale to the design of environments and evaluation protocols4. From this perspective,\n4https://ysymyth.github.io/The-Second-Half/\n33\nA Survey of Reinforcement Learning for Large Reasoning Models\nRL serves chiefly to resharpen latent competencies already encoded during pretraining, rather than\nto generate novel abilities entirely from scratch.\nIn this subsection, we examine three key dimensions of this dependency: the comparative ad-\nvantages of applying RL to base versus instruction-tuned models, the substantial variations in RL\nresponsiveness across different model families (particularly between Qwen and Llama architectures),\nand the emerging strategies that can enhance RL outcomes for both weak-prior and strong-prior\nmodels, including mid-training and curriculum design.\nBase vs. Instruct Models. DeepSeek-R1 first introduced a discussion on applying RL to either base\nmodels or instruct-tuned models, and it introduced two viable paradigms for post-training: 1) R1-Zero,\nwhich applies large-scale rule-based RL directly to a base model, yielding emergent long-horizon\nreasoning; and 2) R1, which incorporates a brief cold-start SFT stage to stabilize output format and\nreadability prior to RL. Independently, Open-Reasoner-Zero [Hu et al., 2025b] demonstrated that a\nminimalist training recipe applied to base Qwen models is sufficient to scale both response length\nand benchmark accuracy, mirroring the training dynamics of R1-Zero. These findings suggest that\nbase model priors are better suited to RL than those of instruct models, often producing smoother\nimprovement trajectories than those observed when starting from heavily aligned Instruct models,\nwhere entrenched formatting and obedience priors may interfere with reward shaping.\nModel Family Differences. More recent studies highlight that the choice of base model can critically\nshape RL outcomes. For instance, One-shot RLVR [Wang et al., 2025q] shows that introducing a single,\ncarefully selected mathematical example can more than double MATH500 accuracy for Qwen2.5-\nMath-1.5B, delivering substantial average improvements across multiple benchmarks. Yet, Spurious\nRewards [Shao et al., 2025] uncovers a contrasting pattern: Qwen-family models register significant\ngains even under random or spurious reward signals, whereas Llama and OLMo models often do\nnot. This divergence underscores the influence of model priors and emphasizes the importance of\nvalidating RL claims across models with differing priors. The observed asymmetries suggest differences\nin pretraining exposure to reasoning patterns (e.g., mathematical or code CoT). Qwen models, having\nbeen extensively exposed to such distributions, tend to be more “RL-friendly”, whereas comparable\nLlama models often exhibit brittleness when subjected to the same RLVR procedure.\nMid-training Solutions. In practice, researchers have found that this performance gap can be ad-\ndressed through mid-training or annealing training strategies. In recent LLM research, annealing\ndenotes a late-stage pre-training phase during which the learning rate decays while the data distribu-\ntion is reweighted to emphasize smaller, high-quality sources such as code, mathematics, and curated\nQA corpora. Llama 3 [Grattafiori et al., 2024] explicitly names this phase Annealing Data, describing\nboth a shift in the data mixture and a linear LR decay to zero. They further report that injecting\nsmall amounts of high-quality math and code at this stage substantially improves reasoning-oriented\nbenchmarks. Earlier, MiniCPM [Hu et al., 2024b] articulated a comparable two-stage curriculum,\ntermed stable-then-decay. During the decay (annealing) stage, they interleave SFT-style, high-quality\nknowledge and skill data with standard pre-training corpora, observing larger improvements than\napplying the same SFT only after pre-training. Similarly, OLMo 2 [OLMo et al., 2024] makes pub-\nlic a modern mid-training recipe: pre-training is split into a long, web-heavy stage followed by a\nshorter mid-training phase that up-samples high-quality and domain-specific sources, especially\nmathematics, while linearly decaying the LR to zero. More generally, contemporary mid-training\nstrategies treat the joint design of learning rate schedules and data distribution switches as a first-class\nconcern. For instance, Parmar et al. [2024] show that optimal continued-pretraining requires: 1) a\ntwo-distribution curriculum that emphasizes the target capabilities during the late stage, and 2 an\nannealed, non-rewarmed LR schedule where the timing of the distribution switch is determined by\nthe LR fraction rather than a fixed token count. A recent systematic study extends this line of work,\n34\nA Survey of Reinforcement Learning for Large Reasoning Models\ndemonstrating that a stable-then-decay mid-training curriculum that injects high-quality mathematics\nand chain-of-thought QA corpora makes Llama models substantially more scalable under RL-based\nfine-tuning, effectively narrowing the performance gap with Qwen models [Wang et al., 2025t]. Taken\ntogether, these findings suggest a practical recipe for weak-prior model families: strengthen reasoning\npriors through mid-training, and subsequently apply RLVR.\nStrong Model Improvements. While many replications favor base models, there is mounting evidence\nthat RL can further improve strong distilled/Instruct models when curriculum, verification, and length\ncontrol are carefully designed. For example, AceReason-Nemotron [Chen et al., 2025q] reports\nconsistent gains from math-first then code-only RL atop distilled Qwen models, with analyses showing\nimprovements in both Pass@1 and Pass@K regimes. These findings nuance a simplistic “base-only”\nnarrative: with the right constraints, Instruct/distilled starts can also benefit, but optimization is\nless forgiving. A parallel line evaluates the controllability of reasoning models. MathIF [Fu et al.,\n2025a] highlights a systematic tension: scaling up reasoning capabilities frequently undermines\ninstruction-following performance, particularly in the context of long-form outputs. Complementary\nevidence shows that explicit CoT prompting can reduce instruction-following accuracy and proposes\nselective-reasoning mitigations [Li et al., 2025k]. Together, these works motivate multi-objective\ntraining (format, brevity, obedience) alongside correctness/verifiability in RL.\nWe can summarize how model priors fundamentally shape RL outcomes in LLM training from\nthree perspectives: 1) Base models consistently outperform instruct-tuned models as RL starting\npoints, with DeepSeek-R1 and Open-Reasoner-Zero demonstrating emergent reasoning from minimal\nrecipes; 2) Model families exhibit asymmetric RL responsiveness: Qwen models show gains even under\nspurious rewards while Llama/OLMo models require careful mid-training with annealed learning\nrates and high-quality math/code data injection; 3) Strong distilled models can benefit from RL but\ndemand more sophisticated curriculum design and multi-objective optimization.\nAs RL increasingly serves to resharpen latent pretraining competencies rather than create novel\nabilities, the focus shifts toward optimizing the pretraining-to-RL pipeline holistically rather than\ntreating these stages independently.\n4.4. Training Recipes: Tricks or Traps\nRL training for large models has primarily evolved from the PPO [Schulman et al., 2017b] series,\nmaintaining stability through a variety of engineering techniques [Huang et al., 2022] such as\ntrimming, baseline correction, normalization, and KL regularization. In the context of RL for LLM\nreasoning, DeepSeek-Math and DeepSeek-R1 introduce critic-free GRPO [Shao et al., 2024], which\nsimplifies the training process by reducing complexity. Despite these advances, challenges related\nto training stability and efficiency persist, motivating a range of new methods, including dynamic\nsampling, various importance sampling ratios, and multi-level normalization.\nA more widely adopted technique to boost exploration is to use decoupled PPO clipping (“Clip-\nHigher”), where the upper clipping bound is set higher than the lower one (e.g., 𝜖low = 0.2, 𝜖high = 0.28)\nto allow the probabilities of unlikely but potentially useful tokens to increase more freely [An et al.,\n2025, Liu et al., 2025i, Yu et al., 2025d].\n• Minimalism in Data and Sampling: Xiong et al. [2025a] decompose GRPO and finds that the\nlargest performance gains come from discarding all incorrect samples, rather than relying on\ncomplex reward normalization techniques. They propose that methods like RAFT [Dong et al.,\n2023] or “Reinforce-Rej” [Liu et al., 2023a] can achieve stability and KL efficiency comparable to\nGRPO/PPO using much simpler mechanisms. DAPO [Yu et al., 2025d] systematizes “dynamic\n35\nA Survey of Reinforcement Learning for Large Reasoning Models\nsampling + decoupled pruning” into a reproducible large-scale approach, and incorporates\ndecoupled PPO clipping (“Clip-Higher”) where the upper clipping bound is set higher than the\nlower one (e.g., 𝜖low = 0.2, 𝜖high = 0.28) to allow the probabilities of unlikely but potentially useful\ntokens to increase more freely, demonstrating state-of-the-art results on strong baselines for the\nAIME24 benchmark. Similarly, GRESO [Zheng et al., 2025b] shows that pre-filtering can speed\nup rollout time by 2.4× and overall training by 2.0× with minimal loss in performance.\n• Structural Modification of the Objective Function: GSPO [Zheng et al., 2025a] shifts ratio\nand cropping operations to the sequence level, resulting in improved stability and efficiency over\nGRPO, especially for stable RL training of Mixture-of-Experts (MoE) models. S-GRPO [Dai et al.,\n2025a] further reduces redundant reasoning, mitigating the tendency for longer and unnecessary\nreasoning chains and shortening sequence length by 35–61% across multiple benchmarks, with\nslight improvements in accuracy.\n• The Struggle Between De-biasing and Normalization: Dr. GRPO [Liu et al., 2025t] identifies a\nkey deviation in GRPO where “the longer it’s wrong, the more wrong it gets,” and introduces\nminor algorithmic modifications to improve token efficiency. At the same time, other studies (e.g.,\nBNPO [Xiao et al., 2025a]) revisit the importance of reward normalization from an adaptive\ndistribution perspective, proposing new normalization families. The evidence from these two\ncamps is contradictory, indicating that viewing normalization as a universal solution may be\nmisleading.\nLiu et al. [2025v] present a recent review with unified evaluation, incorporating common tech-\nniques into a single open-source framework [Wang et al., 2025m] to enable isolated and reproducible\nexperiments. This work provides a roadmap outlining “which techniques are effective under what\nsettings” and demonstrates that a minimalist combination of methods can outperform GRPO and\nDAPO across multiple configurations. Crucially, it highlights the field’s most pressing challenges:\ninconsistent experimental settings, incomplete reporting, and conflicting conclusions. This constitutes\na fundamental limitation in the current application of RL within the research community. In summary,\nwhile practical “tricks” are valuable for stabilizing RL training, the essence of “scientific training” lies\nin verification and scalability. Progress in the field requires unified experimental protocols, verifiable\nreward structures, and explicit scalability–performance–cost curves [Nimmaturi et al., 2025] to show\nthat a method remains effective as it scales, rather than only at specific data or models.\n4.5. Reward Type: Process or Outcome\nIn standard RL, the objective of the policy is to maximize the expected cumulative reward [Sutton\net al., 1998]. The “Reward is Enough” hypothesis [Bowling et al., 2023, Silver et al., 2021] further\nposits that appropriately designed rewards are sufficient and that maximizing returns can, in principle,\ngive rise to all aspects of intelligence. In the context of RL for LLMs, the core challenge is how to\nprovide meaningful rewards, such as training a reward model or verifier to score outputs and using\nthese scores for RL or search. Common approaches include outcome rewards, which evaluate only\nthe final result (e.g., correctness or passing individual tests), and process rewards, which provide\nstep-by-step scoring through dense feedback on intermediate steps [Lightman et al., 2024].\n• As shown in § 3.1.1, when task answers are verifiable, outcome rewards are the simplest and\nmost scalable for challenging mathematical and coding tasks. However, outcome-only approaches\nmay tacitly encourage unfaithful chain-of-thought [Arcuschin et al., 2025], such as “answer first,\nhallucinate later,” and reward speculation. Recent research [Baker et al., 2025] indicates that\nstate-of-the-art models also exhibit unfaithful reasoning and post-hoc rationalization in real-world\n36\nA Survey of Reinforcement Learning for Large Reasoning Models\nscenarios. Other work has highlighted that rule-based RL is prone to reward hacking and the\ndevelopment of reasoning illusions [Sun et al., 2025h].\n• PRMs [Zhang et al., 2025e] naturally facilitate long-chain credit assignment. Lightman et al.\n[2024] clearly compare the two reward approaches: for mathematical reasoning, PRMs trained\nwith process supervision are more stable and reliable, significantly outperforming those supervised\nsolely by results. Nevertheless, step-wise annotation is extremely costly, and quality often declines\nacross different domains [Zhang et al., 2025t]. Relevant studies suggest that heuristic or Monte\nCarlo–based synthesis approaches tend to generalize poorly and introduce bias [Yin et al., 2025].\nOverall, outcome rewards provide “scalable goal alignment with automated verification”, while\nprocess rewards offer “interpretable dense guidance.” Combining the two, for example via implicit\nprocess modeling [Cui et al., 2025a] or generative verifiers [Zhang et al., 2024a], may represent a\npromising future direction in reward design.\n5. Training Resources\nEffective RL for LLMs depends not only on algorithms and objective design, but also on the quality\nand structure of the underlying training resources. The selection of resources ranging from static\ncorpora to dynamic environments and specialized RL infrastructure, profoundly influences both the\nstability and scalability of large-scale training. In this section, we survey the key categories of training\nresources leveraged in current practice. We first examine the role and limitations of static corpora as a\nfoundation for RL (§ 5.1), then discuss the growing importance of dynamic, interactive environments\nthat provide richer learning signals and more realistic task distributions (§ 5.2). Finally, we review\nthe RL infrastructure that enables scalable and efficient training pipelines for LLMs (§ 5.3).\n5.1. Static Corpus\nTakeaways\n• RL reasoning datasets are moving from large-scale raw data to higher-quality, verifi-\nable supervision using distillation, filtering, and automated evaluation to boost sample\neffectiveness and process fidelity.\n• Data coverage has expanded beyond single domains (math/code/STEM) to include search,\ntool use, and agentic tasks with traceable, plan–act–verify trajectories.\nThis section surveys static corpora for RL with LLMs. Data construction is shifting from “scale-first”\nto “quality- and verifiability-first”, explicitly to support verifiable rewards (see § 3.1.1). As shown in\nTable 4, the dataset coverage spans four major tracks: mathematics, coding, STEM, and agentic tasks\n(e.g., search and tool use). All corpora are directly compatible with RLVR, enabling process-aware\nevaluation. These datasets support key components of the RL pipeline, including policy pretraining,\nreward modeling, and difficulty-aware sampling.\nMath-focused RL datasets coalesce around three construction pipelines, including annotation/ver-\nification, distillation, and multi-source merging, while widely exposing intermediate reasoning traces\nand spanning sizes from hundreds to millions of examples. Compact, carefully curated sets such as\nLIMO [Ye et al., 2025d] and LIMR [Li et al., 2025o] emphasize high-quality problems with explicit\nprocess feedback; annotated/verified resources like DAPO [Yu et al., 2025d], Big-MATH [Albalak\net al., 2025], and DeepMath [He et al., 2025h] deliver reliable solution trajectories suitable for reward\n37\nA Survey of Reinforcement Learning for Large Reasoning Models\nTable 4 | Static datasets for RL training of LLMs, including Math, Code, STEM, and Agent domains.\nFor data acquisition methods, “Distil” and “Anno” indicate distillation and annotation, respectively.\n“Merge” indicates the integration of existing datasets, including difficulty and quality filtering.\nDomain\nDate\nName\n#Sample\nFormat\nType\nLink\nMath\n2025.02\nDAPO\n17k\nQ-A\nAnno\n§\n2025.02\nPRIME\n481k\nQ-A\nMerge&Distil\n§\n2025.02\nBig-MATH\n47k\nQ-A\nAnno\n2025.02\nLIMO\n800\nQ-C-A\nAnno\n§\n2025.02\nLIMR\n1.39k\nQ-A\nAnno\n§\n2025.02\nDeepScaleR\n40.3k\nQ-C-A\nDistil\n2025.02\nNuminaMath 1.5\n896k\nQ-C-A\nAnno\n§\n2025.02\nOpenReasoningZero\n72k\nQ-A\nMerge&Distil\n§\n2025.02\nSTILL-3-RL\n90k\nQ-A\nMerge&Distil\n§\n2025.02\nOpenR1-Math\n220k\nQ-C-A\nDistil\n§\n2025.03\nLight-R1\n79.4k\nQ-C-A\nMerge\n2025.04\nDeepMath\n103k\nQ-C-A\nDistil&Anno\n§\n2025.04\nOpenMathReasoning\n5.5M\nQ-C-A\nDistil\n§\n2025.07\nMiroMind-M1-RL-62K\n62k\nQ-A\nMerge\n§\nCode\n2024.12\nSWE-Gym\n2.4k\nQ-A\nAnno\n§\n2025.01\ncodeforces-cots\n47.8k\nQ-C-A\nDistil\n2025.01\nSWE-Fixer\n110k\nQ-A\nAnno\n§\n2025.03\nKodCode\n268k\nQ-A\nDistil\n§\n2025.03\nCode-R1\n12k\nQ-A\nMerge\n§\n2025.04\nZ1\n107k\nQ-C-A\nDistil\n§\n2025.04\nLeetCodeDataset\n2.9k\nQ-A\nAnno\n§\n2025.04\nOpenCodeReasoning\n735k\nQ-C-A\nDistil\n2025.04\nDeepCoder\n24k\nQ-A\nMerge\n§\n2025.05\nrStar-Coder\n592k\nQ-C-A\nDistil&Anno\n§\nSTEM\n2025.01\nSCP-116K\n182k\nQ-C-A\nDistil\n2025.02\nNaturalReasoning\n2.15M\nQ-C-A\nDistil\n2025.05\nChemCoTDataset\n5k\nQ-C-A\nDistil\n2025.06\nReasonMed\n1.11M\nQ-C-A\nDistil\n§\n2025.07\nMegaScience\n2.25M\nQ-C-A\nMerge&Distil\n2025.09\nSSMR-Bench\n16k\nQ-A\nAnno\n§\nAgent\n2025.03\nSearch-R1\n221K\nQ-A\nAnno\n2025.03\nToRL\n28K\nQ-A\nMerge\n§\n2025.03\nToolRL\n4K\nQ-C-A\nDistil\n§\n2025.05\nZeroSearch\n170K\nQ-A\nAnno\n§\n2025.07\nWebShaper\n0.5K\nQ-A\nAnno\n2025.08\nMicroThinker\n67.2K\nQ-A\nAnno\n2025.08\nASearcher\n70K\nQ-A\nAnno\nMix\n2025.01\ndolphin-r1\n300k\nQ-C-A\nDistil\n2025.02\nSYNTHETIC-1/2\n2M/156K\nQ-C-A\nDistil\n2025.04\nSkyWork OR1\n14k\nQ-A\nMerge\n§\n2025.05\nLlama-Nemotron-PT\n30M\nQ-C-A\nDistil\n2025.06\nAM-DS-R1-0528-Distilled\n2.6M\nQ-C-A\nDistil\n§\n2025.06\nguru-RL-92k\n91.9k\nQ-A\nDistil\nmodeling and value alignment; at larger scale, NuminaMath 1.5 [Li et al., 2024b] extends process-rich\nsamples; distillation-centric corpora including DeepScaleR [Luo et al., 2025c], OpenR1-Math [Hug-\nging Face, 2025], and OpenMathReasoning [Moshkov et al., 2025] inherit strong-teacher or “R1-style”\nlong-chain reasoning, supporting policy pretraining and RL-stage selection; merge-and-distill collec-\ntions such as PRIME [Cui et al., 2025a], OpenReasoningZero [Hu et al., 2025b], and STILL-3-RL [Chen\net al., 2025u] integrate open problems with self-generated candidates, offering difficulty stratification\n38\nA Survey of Reinforcement Learning for Large Reasoning Models\nand high-quality filtering signals; community-leaning releases like Light-R1 [Wen et al., 2025b] and\nMiroMind-M1-RL-62K [Li et al., 2025m] package lightweight, RL-ready formats for rapid iteration\nunder compute constraints. Collectively, these resources span basic computation to competition-level\nproblems and provide both final answers and measurable intermediate steps, enabling scalable policy\nlearning, reward modeling, and process-based reinforcement.\nCode-oriented RL datasets primarily fall into three categories: program repair/editing, algo-\nrithmic competition problems, and general code synthesis with reasoning. These datasets typically\nprovide executable unit tests and intermediate execution traces, facilitating reward shaping and\nprocess-level evaluation. Interactive, test-driven resources such as SWE-Gym [Pan et al., 2024] target\nfine-grained editing policies; human-verified repair pairs like SWE-Fixer [Xie et al., 2025a] and\nLeetCodeDataset [Xia et al., 2025c] support value alignment and reward modeling. For competition-\nstyle and algorithmic reasoning, codeforces-cots [Penedo et al., 2025], Z1 [Yu et al., 2025f], and\nOpenCodeReasoning [Ahmad et al., 2025] emphasize long-chain trajectories and difficulty stratifica-\ntion. In large-scale, “R1-style” distillation for general code generation, KodCode [Xu et al., 2025h]\nand rStar-Coder [Liu et al., 2025p] provide process-rich samples that aid policy pretraining and\nRL-stage selection. Lightweight, merge-centric releases such as Code-R1 [Liu and Zhang, 2025]\nand DeepCoder [Luo et al., 2025b] are convenient for rapid iteration under compute constraints.\nCollectively, these corpora span single-function repair through competition-level problem solving,\noffering both automatically checkable end artifacts and stepwise plans/edits, thereby enabling scalable\npolicy learning, reward modeling, and process-based reinforcement for code agents.\nSTEM-oriented RL datasets generally converge on three themes: textbook or curriculum extrac-\ntion, cross-disciplinary large-scale reasoning, and domain-specialized corpora (e.g., chemistry and\nmedicine) featuring merge-and-distill pipelines. These datasets commonly release chain-of-thought\nrationales and evidence-aligned signals, enabling process-level rewards. SCP-116K [Lu et al., 2025a]\ntargets undergraduate-to-doctoral science with automatically extracted problem–solution pairs plus\nmodel-generated reasoning. NaturalReasoning [Yuan et al., 2025e] offers multi-discipline questions\ndecontaminated from popular benchmarks with extracted reference answers. ChemCoTDataset [Li\net al., 2025c] contributes chemistry-specific CoT exemplars spanning molecular editing/optimization\nand reaction prediction. ReasonMed [Sun et al., 2025f] provides multi-agent–distilled medical QA\nwith multi-step CoT rationales and concise summaries. SSMR-Bench [Wang et al., 2025u] program-\nmatically synthesizes music-theory-grounded sheet-music reasoning questions in both textual (ABC\nnotation) and visual formats, releasing 16k training pairs per modality, and supporting evaluation\nas well as RL with verifiable rewards. MegaScience [Fan et al., 2025a] aggregates public scientific\ncorpora via ablation-based selection and annotates step-by-step solutions for most constituent sets,\nforming a large training pool for RL on scientific reasoning.\nMixed-domain RL datasets unify math, code, and scientific reasoning through distillation-first\nand merge-centric pipelines, while broadly releasing chain-of-thought traces, verifier signals, and\nmulti-trajectory candidates that enable process rewards and difficulty-aware selection. In R1-style\nmixtures, dolphin-r1 [Team, 2025b] blends DeepSeek-R1, Gemini-thinking, and curated chat data\nfor general reasoning. The SYNTHETIC suite couples large-scale SFT-style traces with RL-ready multi-\ntrace samples: SYNTHETIC-1 [Mattern et al., 2025] aggregates DeepSeek-R1 reasoning with diverse\nverifiers, and SYNTHETIC-2-RL [Mattern et al., 2025] provides multi-domain tasks with multiple\ntrajectories for preference/reward learning. SkyWork OR1-RL-Data [He et al., 2025d] emphasizes\nverifiable math and code problems with difficulty labels, serving as a lightweight RL pool. Llama-\nNemotron Post-Training [Bercovich et al., 2025] compiles instruction/R1-style data spanning math,\ncode, STEM, general reasoning, and tool use for post-training. AM-DeepSeek-R1-0528-Distilled [a-m\nteam, 2025] offers cross-domain distilled traces with documented quality filtering, and guru-RL-\n92k [Cheng et al., 2025d] curates six high-intensity reasoning domains via a five-stage pipeline\n39\nA Survey of Reinforcement Learning for Large Reasoning Models\noptimized for RL formats. Collectively, these corpora provide verifiable endpoints and stepwise\nrationales across domains, supporting scalable policy learning, reward modeling, and process-based\nreinforcement.\nAgent-centric RL datasets concentrate on two complementary capabilities, search-as-action and\ntool use, while releasing verifiable process signals such as search/browse traces, evidence URLs,\nand tool-execution logs that enable process rewards and offline evaluation. Search-R1 [Jin et al.,\n2025b] builds on NQ/HotpotQA to train interleaved reasoning–search behavior. ToRL [Li et al.,\n2025p] scales tool-integrated RL from base models to learn when and how to invoke computational\ntools. ToolRL [Qian et al., 2025] studies fine-grained reward design for learning tool selection\nand application. ZeroSearch [Sun et al., 2025a] formulates offline information-seeking tasks that\nincentivize search without real web calls. WebShaper [Tao et al., 2025] synthesizes information-\nseeking data via an “Expander Agent”, covering diverse task forms and reasoning structures with URL\nevidence. MicroThinker [Team, 2025f] contributes full rollout trajectories and rich tool-use logs for\nmulti-step agents. ASearcher [Gao et al., 2025a] releases Apache-2.0-licensed training splits for long-\nhorizon search agents with question/answer fields and source annotations. Collectively, these corpora\nspan planning, retrieval, tool orchestration, evidence verification, and answer generation, supporting\nscalable policy learning, reward modeling, and process-based reinforcement for web/search and\ntool-using agents.\n5.2. Dynamic Environment\nTakeaways\n• Static RL training datasets are increasingly insufficient for advanced and generalizable\nreasoning abilities.\n• Scalable RL for LLMs needs to turn to synthesized or generated data and interactive\nenvironments, such as various gyms and world models.\nExisting static RL corpora, whether manually annotated, semi-automatically labeled, or scraped\nfrom the Web, are increasingly insufficient for training models that require more advanced and gener-\nalizable reasoning abilities. A growing number of works are now leveraging “Dynamic Environments”\nto jointly ensure both scalability and verifiability, two essential properties for effective model training\n[Wei, 2025].\nUnlike traditional reasoning corpora, these dynamic environments represent a paradigm shift.\nThey enable either the automated and limitless synthesis of data, or provide step-level, multi-turn\nfeedback on a model’s entire reasoning process. As shown in Table 5, based on the methods used for\nsynthesis and interaction, these environments can be categorized, serving as the interaction objects\nfor the RL process. Given our focus on resources for training, this subsection’s organization of datasets\nand environments will exclude benchmarks intended solely for evaluation.\nRule-based Environment. Relying solely on feedback like “Exact Match” can lead models to shortcut\nto memorization rather than actual reasoning. To counteract this, some environments offer complex\nand diverse tasks that require deterministic rule-based operations as a verifier. AutoLogi [Zhu et al.,\n2025d] generates open-ended logic puzzles with controllable difficulty by building code that checks\nthe correctness of logical constraints based on a fixed model output format. Logic-RL [Xie et al.,\n2025c] uses a scalable Knights and Knaves puzzle to create a rule-based RL environment, which\ngeneralized the reasoning capabilities of a 7B model to the mathematical domain. Projects like\nSynLogic [Liu et al., 2025g], Reasoning Gym [Stojanovski et al., 2025], and Enigmata [Chen et al.,\n40\nA Survey of Reinforcement Learning for Large Reasoning Models\nTable 5 | Dynamic RL Environments for RL Training of LLMs. Data source legend: RD = Read Data,\nRS = Rule-based Synthesis, MS = Model-based Synthesis. Scale legend: Training/Test set.\nCategory\nDate\nName\nData Source\nInteractive\nScale\nMultimodal\nLink\nRule-based\n2025.02\nAutoLogi\nRD + MS\n×\n2458/6739 puzzles\n×\n§\n2025.02\nLogic-RL\nRS\n×\n5𝑘samples\n×\n§\n2025.05\nReasoning Gym\nRS\n×\n104 tasks\n×\n§\n2025.05\nSynLogic\nRS\n×\n35 tasks\n×\n§\n2025.06\nProtoReasoning\nRD + MS\n×\n6620 samples\n×\n-\n2025.06\nEnigmata\nRD + RS\n×\n36 tasks\n×\n§\n2025.07\nStepFun-Prover\nRD + RS\n×\n36 tasks\n×\n§\n2025.08\nFTRL\nRD + MS\n✓\n2215/200 samples\n×\n§\nCode-based\n2024.07\nAppWorld\nRD + RS\n✓\n750 tasks\n×\n§\n2025.02\nAgentCPM-GUI\nRD + RS\n✓\n55𝑘trajectories\n✓\n§\n2025.02\nMLGym\nRD + RS\n✓\n13 tasks\n×\n§\n2025.03\nReCall\nRD + MS\n✓\n10010 samples\n×\n§\n2025.04\nR2E-Gym\nRD + MS\n✓\n8135 cases\n×\n§\n2025.05\nMLE-Dojo\nRD + RS\n✓\n202 tasks\n✓\n§\n2025.05\nSWE-rebench\nRD + MS\n✓\n21336 cases\n×\n§\n2025.05\nZeroGUI\nMS\n✓\n-\n✓\n§\n2025.06\nMedAgentGym\nRD\n✓\n72, 413 cases\n×\n§\nGame-based\n2020.10\nALFWorld\nRS\n✓\n6 tasks\n✓\n§\n2022.03\nScienceWorld\nRS\n✓\n30 tasks\n×\n§\n2025.04\nCross-env-coop\nRS\n✓\n1.16𝑒17 cases\n×\n§\n2025.05\nlmgame-BENCH\nRD + RS\n✓\n6 games\n✓\n§\n2025.05\nG1(VLM-Gym)\nRD + RS\n✓\n4 games\n✓\n§\n2025.06\nCode2Logic (GameQA)\nRD + MS\n×\n140𝑘QA\n✓\n§\n2025.06\nPlay to Generalize\nRS\n✓\n36𝑘samples × 2 games\n✓\n§\n2025.06\nKORGym\nRS\n✓\n51 games\n✓\n§\n2025.06\nOptimus-3\nRS\n✓\n6 tasks\n✓\n§\n2025.08\nPuzzleJAX\nRS\n✓\n∼900 games\n✓\n§\nModel-based\n2025.03\nSweet-RL\nRD + MS\n✓\n10𝑘/1𝑘tasks\n×\n§\n2025.04\nTextArena\nRS\n✓\n99 games\n×\n§\n2025.05\nAbsolute Zero\nMS\n✓\n-\n×\n§\n2025.06\nSwS\nRD + MS\n×\n40𝑘samples\n×\n§\n2025.07\nSPIRAL\nRS\n✓\n3 games\n×\n§\n2025.08\nGenie 3\nMS\n✓\n-\n✓\n§\nEnsemble-based\n2025.06\nInternBootcamp\nRD + RS\n✓\n1060 tasks\n×\n§\n2025.07\nSynthetic-2\nRD + MS\n✓\n19 tasks\n×\n2025d] expand the task diversity further. They identify the key parameters that control the difficulty\nfor each task, allowing for the unlimited generation of data across various logic-related reasoning\nchallenges. In contrast, ProtoReasoning [He et al., 2025b] operates on the hypothesis that a model’s\ngeneralization ability comes from shared abstract reasoning prototypes. It normalizes different task\ntypes into a consistent format, like Prolog questions or PDDL tasks, and then automatically verifies\nthe model’s output using an interpreter.\nCode-based Environment. An important application area for LLM reasoning is software engineering\nand code development. A key characteristic of these environments is that models must interact\nwith a compilable code environment during training. Therefore, how to scalably construct code-\nbased task environments remains a significant research direction. To teach agents to use tools,\nReCall [Chen et al., 2025k] leverages advanced LLMs to construct a Python-based tool interaction\nenvironment, autonomously synthesizing its own SynTool data for RL training. In the field of AutoML,\nMLGym [Nathani et al., 2025] was among the first to support an interactive environment for iterative\nexperimentation and training. It isolates each task’s execution environment using Docker containers.\nThough its tasks are largely fixed, it offers less scalability. MLE-Dojo [Qiang et al., 2025] offers\nmore scalability as it is easier for users to integrate new tasks. In a similar vein, MedAgentGym [Xu\n41\nA Survey of Reinforcement Learning for Large Reasoning Models\net al., 2025b] is an efficient and scalable interactive training environment for the medical domain.\nIn software engineering, R2E-Gym [Jain et al., 2025] reduces the reliance on manually authored\nGitHub issues and test cases by programmatically generating environments directly from GitHub\ncommit histories, integrating with OpenHands for interactive capabilities. Similarly, SWE-rebench\n[Badertdinov et al., 2025] extends the original static SWE-bench by proposing a scalable pipeline for\nconstructing software engineering tasks. This pipeline includes complex, interactive tasks that simulate\nreal-world software development scenarios, ensuring data freshness and avoiding data contamination.\nIn the field of computer use, AgentCPM-GUI [Zhang et al., 2025u] constructs an interactive GUI\nenvironment during the RFT phase to provide feedback on the model’s actions. Similarly, AppWorld\n[Trivedi et al., 2024] uses an environment comprising various mobile application APIs. ZeroGUI\n[Yang et al., 2025b] takes this a step further by using existing advanced VLMs to construct tasks for\nboth Ubuntu and Android. During training, a GUI agent interacts with the environment, and the\nfeedback is then provided to the VLM to give rewards, all without the need for manual data curation.\nGame-based Environment. Game environments are characterized by their clear and complex state\nspaces, where an AI’s behavior is tightly coupled with the environment’s state.This leads to a more\nmulti-step and continuous interaction process compared to the environments mentioned previously,\nand such environments naturally support dense rewards in § 3.1.3, making RL training more efficient\nand stable. Early works on interactive environments for training agents, such as ALFWorld [Shridhar\net al., 2020] and ScienceWorld [Wang et al., 2022], remain influential in the agent planning field.\nCode2Logic [Tong et al., 2025b] utilized game code and Q&A templates to automatically generate\nmultimodal reasoning data, resulting in the GameQA dataset. This dataset is not only scalable but\nalso tests a model’s multimodal reasoning capabilities with graduated difficulty. lmgame-Bench [Hu\net al., 2025c], in a different approach, directly selects classic games and interacts with an LLM via a\nunified API. The game environment updates its state and provides a reward based on the LLM’s action,\nwhich the LLM then uses to adjust its strategy. Similarly, Play to Generalize [Xie et al., 2025d] used a\nsimple, scalable game environment for RL to train a 7B-parameter MLLM. The research found that\nthe reasoning skills acquired by the model could generalize to unseen games and multidisciplinary\nreasoning tasks. The work G1 [Chen et al., 2025g] introduced the VLM-Gym, an RL environment that\nsupports the parallel execution of multiple game states, facilitating large-scale training. KORGym\n[Shi et al., 2025a] further expands the number of supported simple games, offering interactive and\ndifficulty-configurable RL environments. PuzzleJAX [Earle et al., 2025] takes a different approach by\naccelerating games generated from the PuzzleScript language using JAX. This not only speeds up the\ngame environment to support RL-based training but also provides access to a community of game\ndevelopers with a source of unlimited games. To learn general cooperative skills, Cross-environment\nCooperation [Jha et al., 2025] leverages the game Overcooked and maximizes environmental diversity\nwithin a self-play framework. For more complex, high-degree-of-freedom games like Minecraft, the\nOptimus series of work [Li et al., 2025t] leverages knowledge graphs to interact with the game\nenvironment, constructing data to evaluate a model’s long-term planning ability.\nModel-based Environment. This paradigm facilitates the creation of highly flexible and diverse RL\nenvironments through model-to-model interaction or self-play. SwS [Liang et al., 2025b] utilizes a\nmodel’s failed training cases to abstract key concepts and generate new problems, thus enhancing its\nreasoning abilities in a targeted manner. SPIRAL [Liu et al., 2025a] uses three zero-sum games for\nself-play to prevent overfitting to a static policy. For model-to-model interaction, Sweet-RL [Zhou\net al., 2025g] uses a prover-verifier-like training framework, where an agent interacts and collaborates\nwith an LLM-based human simulator to solve front-end design and back-end programming tasks.\nTextArena [Guertler et al., 2025] proposes using adversarial text games combined with a ranking\nsystem, which overcomes the bottleneck of human scoring by allowing models to interact directly to\nrelatively measure their abilities. Absolute Zero [Zhao et al., 2025a] goes a step further by completely\n42\nA Survey of Reinforcement Learning for Large Reasoning Models\nmoving away from human-defined evaluation tasks, utilizing three reasoning modes for a model to\nautonomously generate its own tasks and improve its reasoning capabilities through self-evolution.\nIn the visual domain, Genie 3 [Ball et al., 2025] generates near-realistic and interactive 3D virtual\nenvironments, laying the foundation for future multimodal environment-interactive RL. While some\nexisting world models have already enabled RL-based model training [Dedieu et al., 2025, Hafner\net al., 2023, Russell et al., 2025], and we have listed works that train LRMs using model-based\nenvironments above, there is still no sufficiently scalable solution to support RL training of LRMs\nbased on world models. The ultimate form of such dynamic environments, we posit, would be an\noracle world model capable of simulating a complete, self-contained world.\nEnsemble-based Environment. There are also works that involve significant engineering effort\nthat integrate various tasks and datasets to form interactive environments and training data for RL.\nInternBootcamp [Li et al., 2025f] is a large-scale, extensible library of environments designed to train\nLRMs. It supports over 1000 general reasoning tasks across eight domains by providing difficulty-\ncontrollable generators and rule-based verifiers. A key contribution is its empirical demonstration\nof “Task Scaling,” showing that increasing the number of training tasks significantly boosts both\nreasoning performance and training efficiency. Synthetic-2 [PrimeIntellect, 2025] contributes to this\napproach by providing a massive, open dataset of four million verified reasoning traces. These traces\nwere collaboratively generated via a “planetary-scale, pipeline-parallel, decentralized inference run,”\nshowcasing a highly scalable method for creating verified training data for complex RL tasks.\n5.3. RL Infrastructure\nTakeaways\n• Modern RL infrastructure centers on flexible pipelines and communication layers that\nallocate resources between agent rollout and policy training, typically implemented as\nwrappers over mature distributed training frameworks and inference engines.\n• Specialized variants (agentic workflows, multi-agent, and multimodal) commonly support\nasynchronous rollouts/training and standardized environment interfaces.\nIn this subsection, we introduce the open-source RL infrastructure that promotes the development\nnot only in algorithmic research but also in downstream applications. We begin by presenting primary\ndevelopment frameworks, which mainly provide basic wrappers around LLM training and inference\nframeworks. Next, we introduce secondary development frameworks, which are built upon these\nprimary frameworks and further adapted to various downstream applications, including agentic\nRL, coding RL, multi-agent RL, and multimodal RL, distributed RL, and others. We compare these\nopen-source RL frameworks in Table 6 and introduce the main frameworks below..\nPrimary Development. Current RL infrastructure relies heavily on mature training frameworks\nand inference engines designed for LLMs. Frameworks such as DeepSpeed [Rasley et al., 2020],\nMegatron [Shoeybi et al., 2019], and Fully Sharded Data Parallel (FSDP) [Zhao et al., 2023b] are\noptimized for both pre-training and post-training of LLMs. In terms of inference, vLLM [Kwon et al.,\n2023] and SGLang5 are tailored for efficient inference, incorporating advanced schedulers and flash\nattention mechanisms. These optimizations enable significantly faster inference compared to direct\nforward computation on PyTorch models. Many open-source RL frameworks are built upon plug-and-\nplay training and inference frameworks, most of which are implemented on distributed computing\nengines such as Ray6. Here, we review RL frameworks that are directly developed based on the\n5https://github.com/sgl-project/sglang\n6https://github.com/ray-project/ray\n43\nA Survey of Reinforcement Learning for Large Reasoning Models\nTable 6 | Open-source RL infrastructure for LLM post-training. Status legend: ✓= native, × =\nunsupported, P = partial.\nDate\nFramework\nRuntime\nServing\nTraining\nAsync\nAgents\nMulti-Agents\nMultimodal\nvLLM\nSGLang\nDeepSpeed\nMegatron\nFSDP\nPrimary development\n2020.03\nTRL\n×\n×\n×\nP\n✓\n×\n✓\n×\n✓\n2023.11\nOpenRLHF\n✓\n✓\n×\n×\n✓\n×\n✓\n×\n×\n2024.11\nveRL\n✓\n✓\n×\nP\n✓\n✓\n×\n✓\n✓\n2025.03\nAReaL\n✓\n✓\n×\nP\n✓\n✓\n✓\n✓\n✓\n2025.05\nNeMo-RL\nP\nP\n×\n✓\n✓\n×\n×\n✓\n✓\n2025.05\nROLL\n✓\n✓\n×\n✓\n✓\n✓\n✓\n✓\n×\n2025.07\nslime\n✓\nP\n×\n×\n×\n✓\n×\n✓\n×\nSecondary development\n2025.02\nrllm\nP\n✓\n×\n×\n✓\n✓\n×\n×\n✓\n2025.02\nVLM-R1\n×\n×\n×\n✓\n✓\n×\n✓\n×\n×\n2025.03\nEasyR1\n×\n×\n×\n✓\n✓\n×\n×\n×\n✓\n2025.03\nverifiers\n✓\n✓\n×\n×\n✓\n×\n✓\n×\n✓\n2025.05\nprime-rl\n✓\n×\n×\n×\n✓\n×\n×\n×\n✓\n2025.05\nMARTI\nP\n✓\n✓\n×\n✓\n×\n✓\n×\n×\n2025.05\nRL-Factory\n✓\n✓\n×\n✓\n✓\n✓\n✓\n✓\n✓\n2025.06\nverl-agent\n✓\n✓\n×\n✓\n✓\n✓\n✓\n✓\n✓\n2025.08\nagent-lightning\n✓\n✓\nP\n×\n✓\n×\n×\n✓\n✓\naforementioned backbone training and inference frameworks.\n• TRL [von Werra et al., 2020]: TRL focuses on trainer-centric post-training with SFT, PPO/GRPO,\nDPO, and a dedicated RewardTrainer (plus recent online variants), rather than a bespoke dis-\ntributed runtime. It integrates vLLM for online methods (server or colocated modes) but does\nnot natively target SGLang or TensorRT-LLM. Scaling is delegated to accelerate, which natively\nsupports DDP, DeepSpeed ZeRO, and FSDP; Megatron is not a backend. Reward modeling is\nsupported out-of-the-box through the RewardTrainer, and the library provides clear APIs for\nGRPO/DPO/online rollouts.\n• OpenRLHF [Hu et al., 2024a]: OpenRLHF provides distributed implementations of PPO, GRPO,\nREINFORCE++ (and its baseline variant) and RLOO, and also includes preference-learning\nbaselines such as DPO/IPO/cDPO and KTO. Its runtime supports both asynchronous pipeline\nRLHF and asynchronous agentic RL modes, exposing a class-based agent API for multi-turn\nsettings. For serving, OpenRLHF integrates tightly with vLLM for high-throughput rollouts.\nTraining is organized around DeepSpeed ZeRO-3 with Auto Tensor Parallelism (AutoTP), without\nrequiring Megatron or FSDP. The framework ships recipes for RMs and PRMs training and\nintegrates PRM signals into rollouts.\n• Verl [Sheng et al., 2025]: Verl offers one of the broadest algorithm menus (PPO, GRPO, GSPO,\nReMax, REINFORCE++, RLOO, PRIME, DAPO/DrGRPO, and more) together with multi-turn\ntraining and tool use. Its runtime is centered on the HybridFlow controller and adds agentic\nRL rollout and prototypes for disaggregated asynchronous training (with “Async and off-policy\narchitecture” on the public roadmap). Verl supports vLLM and SGLang for serving, and provides\nboth FSDP and Megatron-LM training backends. Reward options include model-based and\nfunction/verifiable rewards (e.g., math/coding), with multi-GPU LoRA-RL support.\n• AReaL [Fu et al., 2025b]: AReaL targets high-throughput RL for large reasoning models with\na fully asynchronous design that decouples generation from training via interruptible rollout\n44\nA Survey of Reinforcement Learning for Large Reasoning Models\nworkers, a replay buffer, and a parallel reward service (e.g., unit-test–based code rewards),\nstabilized by a staleness-aware PPO objective. Empirically, the system reports up to 2.77× training\nspeedups at matched or better final accuracy on math/code benchmarks and scales near-linearly\nto 512 GPUs. The open-source stack emphasizes SGLang-based rollout serving and Ray launchers\nfor single-node to ∼1K-GPU clusters, with PyTorch FSDP as the main training backend (Megatron\nalso available); the newer “AReaL-lite” adds an algorithm-first API with GRPO examples and\nsupport for multi-turn agentic RL/RLVR workflows.\n• NeMo-RL [NVIDIA-NeMo, 2025]: NVIDIA’s NeMo stack now exposes a dedicated “NeMo RL”\nlibrary and the earlier NeMo-Aligner toolkit for alignment. Algorithmically, NeMo covers SFT\nand preference training (DPO/RPO/IPO/REINFORCE) as well as full RLHF with PPO and GRPO,\nincluding multi-turn variants. The runtime emphasizes scalable, production-oriented orchestration\nand extensive parallelism; training is built on Megatron Core (tensor/data/pipeline/expert\nparallelism) for 100B-scale models and multi-node clusters. For serving, the NeMo framework\ndocuments deployment with TensorRT-LLM and vLLM. Reward-model training is first-class in the\nRLHF tutorials, with end-to-end pipelines from RM fitting to PPO.\n• ROLL [Wang et al., 2025m]: ROLL targets large-scale RL for LLMs with GRPO/PPO/REIN-\nFORCE++ and additional recipes (e.g., TOPR/RAFT++/GSPO), and explicitly supports asyn-\nchronous training and agentic RL pipelines. The runtime follows a Ray-based multi-role design\nand integrates SGLang and vLLM for rollout serving. Training is built primarily around Megatron-\nCore, with FSDP2 listed on the public roadmap; DeepSpeed is acknowledged as a dependency.\nReward handling is modular via Reward Workers (e.g., verifiers, sandbox tools, LLM-as-judge)\nand pluggable environments. A technical report details the system and scaling considerations.\n• slime [THUDM, 2025]: Slime is positioned as an SGLang-native post-training framework for RL\nscaling, connecting SGLang on the rollout side with Megatron on the training side. It emphasizes\ninfrastructure over algorithm breadth, but ships examples for dense and MoE models, and includes\nmulti-turn + tool-calling (“Search-R1 lite”). The runtime supports asynchronous training and\nagentic workflows; serving is first-class via SGLang. Training uses Megatron-LM with Ray for\ncluster launch; reward modeling per se is not the primary focus, although verifier/“reward”\nsignals can be produced on the rollout plane.\nSecondary Development. In this part, we introduce several representative frameworks that are built\nupon primary development frameworks and extend their features to support a broader range of\ndownstream applications. We primarily focus on frameworks for agentic RL, multimodal RL, and\nmulti-agent RL. Although some primary frameworks already offer partial support for these areas, we\nhighlight specialized frameworks designed for specific domain studies.\n• Agentic RL: This area focuses on training LLMs to utilize external tools in a variety of scenar-\nios, such as search engines [Jin et al., 2025b], Python interpreters [Feng et al., 2025a], web\nbrowsers [Li et al., 2025e], and more. Primary frameworks like veRL [Sheng et al., 2025] and\nAReaL [Fu et al., 2025b] have been updated or specifically designed to support these capabilities.\nA core feature of agentic RL is asynchronous generation and training, which significantly reduces\ncomputational time during long-term interactions between LLMs and external environments. The\nsecondary frameworks are mostly built upon veRL to integrate additional tools and environments,\nand their new features are gradually incorporated back into veRL. More details about Agentic RL\nwill be discussed in § 6.1 and 6.2.\n• Multimodal RL: Although the primary development frameworks were originally designed for\ntraining language models, they are typically based on transformers, which support both inference\n45\nA Survey of Reinforcement Learning for Large Reasoning Models\nand training of vision language models. The main challenges in this area involve data process-\ning and loss function design. Notable frameworks such as VLM-R1 [Shen et al., 2025a] and\nEasyR1 [Zheng et al., 2025d] have been developed for training vision-language models based\non veRL. For multimodal generation, certain frameworks have been specifically developed for\nRL training of diffusion-based models, such as DanceGRPO [Xue et al., 2025]. However, these\napproaches are beyond the scope of this paper, and readers may refer to recent RL surveys focused\non vision models for further details [Wu et al., 2025h]. More details about Multimodal RL will\nbe discussed in § 6.3.\n• Multi-Agent RL: Frameworks for agentic RL primarily focus on implementing dynamic workflows\nfor asynchronous rollouts and training. While most of these frameworks are still limited to single-\nagent applications, LLM-based MARL remains an area under active exploration. Zhang et al.\n[2025d] propose the first high-performance, open-source framework for LLM-based multi-agent\nreinforced training and inference, enabling centralized interactions and distributed policy training.\nIn addition, recent frameworks such as Agent-Lightning [Luo et al., 2025e] have implemented\ndisentanglement of training and inference, making it easier to support multi-agent training. More\ndetails about Multi-Agent RL will be discussed in § 6.4.\n6. Applications\nAdvancements in RL for LLMs are best understood through their practical impact across a variety\nof domains. In this section, we review recent progress and challenges associated with applying\nRL-trained language models to real-world tasks. We highlight how RL-driven methods have improved\ncapabilities in coding tasks (§ 6.1), enabled more autonomous and adaptive agentic behaviors (§ 6.2),\nand extended LLMs to multimodal reasoning across text, vision, and beyond (§ 6.3). Further, we\ndiscuss applications in multi-agent systems (§ 6.4), robotics (§ 6.5), and medicine (§ 6.6), illustrating\nboth the broad potential and unique requirements of each area. We provide the overall taxonomy of\napplications along with corresponding related works in Figure 6.\n6.1. Coding Tasks\nTakeaways\n• RL has advanced LLMs’ reasoning and code generation in competitive programming and\ndomain-specific tasks, driving progress toward agentic, closed-loop coding.\n• However, scalability, cross-task generalization, and robust automation in large-scale soft-\nware settings remain open challenges.\nRecently, numerous studies have demonstrated that RL offers significant advantages in verifiable\ntasks. Given the inherent verifiability and practical importance of coding tasks, RL has become\na core approach for improving code reasoning and continues to attract substantial attention. To\nsystematically review the field, we categorize existing research into three directions: code generation,\nsoftware engineering assistance, and agentic coding, based on task complexity and developmental\ntrend, from simpler verifiable tasks toward more complex, autonomous agentic coding.\nCode Generation. The primary objective of this direction is to generate correct and executable\ncode. Research focuses on using RL to adjust LLM generation distributions to meet the requirements\nof diverse coding tasks. Following the demonstration of RL’s potential for complex reasoning in\nDeepSeek-R1, an increasing number of studies have applied RL to code generation.\n46\nA Survey of Reinforcement Learning for Large Reasoning Models\nApplications § 6\nCoding Tasks § 6.1\nCode Generation\nCompetitive-Code: e.g., Code-R1 [Liu and Zhang, 2025]; Open-R1 [Face,\n2025]; DeepCoder [Luo et al., 2025b]; AceReason-Nemotron [Chen\net al., 2025q]; SkyWork OR1 [He et al., 2025d]; AReaL [Fu et al., 2025b]\nDomain-Specific-Code: e.g., Reasoning-SQL [Pourreza et al., 2025]; ReEX-SQL\n[Dai et al., 2025b]; CogniSQL-R1-Zero [Gajjar et al., 2025]; Kimina-Prover\n[Wang et al., 2025d]; DeepSeek-Prover-v2 [Ren et al., 2025]; StepFun-Prover\n[Shang et al., 2025]; Leanabell-Prover-V2 [Ji et al., 2025a]; MedAgentGym [Xu\net al., 2025b]; VeriReason [Wang et al., 2025r]; CodeV-R1 [Zhu et al., 2025f]\nSoftware Engineering\nCode-Quality-Improvement: e.g., RePaCA [Fuster-Pena et al.,\n2025]; Repair-R1 [Hu et al., 2025a]; CURE [Wang et al.,\n2025p]; Afterburner [Du et al., 2025a]; REAL [Yao et al., 2025b]\nRepository-Level-Code-Generation: e.g., RLCoder [Wang et al., 2024c]; RepoGenReflex\n[Wang et al., 2024a]; SWE-RL [Wei et al., 2025c]; Satori-SWE [Zeng et al., 2025b]\nAgentic Tasks § 6.2\nAgentic Coding\ne.g., SWE-RL [Wei et al., 2025c]; Satori-SWE [Zeng et al., 2025b]; Kimi K2\n[Team, 2025d]; Qwen3 Coder [Yang et al., 2025a]; GLM4.5 [Zeng et al., 2025a];\nARPO [Dong et al., 2025b]; AutoTIR [Wei et al., 2025b]; CoRT [Li et al., 2025a];\nToRL [Li et al., 2025p]; FormaRL [Huang et al., 2025d]; MLE-bench [Chan\net al., 2024]; MLE-STAR [Nam et al., 2025]; ML-Agent [Liu et al., 2025r]\nSearch & Deep\nResearch\ne.g., Search-R1 [Jin et al., 2025b]; R1-Searcher [Song et al., 2025a]; Deep-\nResearcher [Zheng et al., 2025e]; ZeroSearch [Sun et al., 2025a]; SSRL [Fan\net al., 2025c]; R1-Searcher++ [Song et al., 2025b]; 𝑂2-searcher [Mei et al.,\n2025]; ReZero [Dao and Le, 2025]; S3 [Jiang et al., 2025d]; WebSailor [Li\net al., 2025e]; WebShaper [Tao et al., 2025]; WebThinker [Li et al., 2025l]; We-\nbGPT [Nakano et al., 2021]; Web-RL [Qi et al., 2024]; WebAgent-R1 [Wei et al.,\n2025d]; WebDancer [Wu et al., 2025d]; Kimi-Searcher [AI, 2025]; Jan-nano\n[Dao and Vu, 2025]; MicroThinker [Team, 2025e]; Webwatcher [Geng et al.,\n2025]; Atom-Searcher [Deng et al., 2025b]; MedResearcher-R1 [Yu et al., 2025a]\nGUI & Compute-use\ne.g., UI-R1 [Lu et al., 2025f]; GUI-R1 [Luo et al., 2025d]; GUI-Critic-R1 [Wanyan et al.,\n2025]; GUI-G1 [Zhou et al., 2025h]; InfiGUI-R1 [Liu et al., 2025q]; GUI-Reflection [Wu\net al., 2025g]; UIShift [Gao et al., 2025b]; ZeroGUI [Yang et al., 2025b]; WEBAGENT-\nR1 [Wei et al., 2025d]; ARPO [Lu et al., 2025b]; Computer-RL [Lai et al., 2025]\nOther-tasks\ne.g., AdLlama [Jiang et al., 2025a]; Shop-R1 [Zhang et al.,\n2025r]; LaviPlan [Oh, 2025]; Drive-R1 [Li et al., 2025r]; OpenTab-\nR1 [Qiu, 2025]; TooRL [Qian et al., 2025]; K2 [Team, 2025d]\nMultimodal\nTasks § 6.3\nMultimodal Un-\nderstanding\nImage: e.g., Vision-R1 [Huang et al., 2025c]; VLM-R1 [Shen et al.,\n2025a]; Taco [Kan et al., 2025]; Visionary-r1 [Xia et al., 2025a]; Visual-\nRFT [Liu et al., 2025x]; Deepeyes [Zheng et al., 2025f]; CoF [Zhang\net al., 2025m]; Ground-r1 [Cao et al., 2025]; Grit [Fan et al., 2025d]\nVideo: e.g., Video-R1 [Feng et al., 2025b]; Focused Thinking [Dang et al., 2025];\nVQ-Insight [Zhang et al., 2025n]; Ego-R1 [Tian et al., 2025]; Long-RL [Chen\net al., 2025t]; Video-RFT [Wang et al., 2025k]; VideoChat-R1 [Li et al., 2025n]\n3D: e.g., MetaSpatial [Pan and Liu, 2025]; Spatial-\nMLLM [Wu et al., 2025b]; SpaceR [Ouyang et al., 2025]; 3D-\nR1 [Huang et al., 2025b]; R1-Zero-VSI [Liao et al., 2025c]\nMultimodal\nGeneration\nImage: e.g., DanceGRPO [Xue et al., 2025]; Flow-GRPO [Liu et al., 2025d]; Qwen-\nImage [Wu et al., 2025a]; MixGRPO [Li et al., 2025d]; TempFlow-GRPO [He et al.,\n2025g]; SimpleAR [Wang et al., 2025i]; FocusDif [Pan et al., 2025e]; RePrompt [Wu\net al., 2025f]; T2I-R1 [Jiang et al., 2025b]; GoT-R1 [Duan et al., 2025]; ReasonGen-\nR1 [Zhang et al., 2025s]; CoRL [Jiang et al., 2025c]; DSR [Hong et al., 2025c]\nVideo: e.g., DanceGRPO [Xue et al., 2025]; In-\nfLVG [Fang et al., 2025b]; Phys-AR [Lin et al., 2025b]\nMulti-Agent\nSystems § 6.4\nLLM-based MAS\ne.g., LLaMAC [Zhang et al., 2023a]; CTRL [Xie et al., 2025e]; MAPoRL [Park et al., 2025];\nMAGRPO [Liu et al., 2025n]; ReMA [Wan et al., 2025]; JoyAgents-R1 [Han et al., 2025]\nRobotics Tasks § 6.5\nDual/Single-Arm\nManipulation\ne.g., SimpleVLA-RL [SimpleVLA-RL Team, 2025]; VLA-RL [Lu et al.,\n2025c]; VLA RL Generalization [Liu et al., 2025f]; RIPT-VLA [Tan\net al., 2025a]; ConRFT [Chen et al., 2025s]; RLinf [Team, 2025h]\nMedical Tasks § 6.6\nMedical Un-\nderstanding\ne.g., Med-U1 [Zhang et al., 2025k]; MED-RLVR [Zhang et al., 2025i]; Open-\nMedical-R1 [Qiu et al., 2025]; Gazal-R1 [Arora et al., 2025]; ProMed [Ding\net al., 2025]; MedVLM-R1 [Pan et al., 2025d]; MedGround-R1 [Xu and Nie,\n2025]; ARMed [Liu and Wei, 2025]; MMedAgent-RL [Xia et al., 2025b];\nMedical Generation\ne.g., FineMedLM-o1 [Yu et al., 2025c]; Med-REFL [Yang et al., 2025h];\nDOLA [Nusrat, 2025]; LA-CDM [Bani-Harouni, 2025]; PPME [Sun et al.,\n2025i]; Baichuan-M1 [Inc., 2025] MORE-CLEAR [Yooseok Lim, 2025]\nFigure 6 | Taxonomy of applications, including research directions and representative works.\n47\nA Survey of Reinforcement Learning for Large Reasoning Models\n• Competitive Programming: Competitive programming, one of the earliest benchmarks, has in-\nspired studies including Code-R1 [Liu and Zhang, 2025], Open-R1 [Face, 2025], DeepCoder [Luo\net al., 2025b], AceReason-Nemotron [Chen et al., 2025q], SkyWork-OR1 [He et al., 2025d], and\nAReaL [Fu et al., 2025b], which replicate DeepSeek-R1 results in code tasks. To address RL train-\ning instabilities and slow inference, DeepCoder [Luo et al., 2025b] and SkyWork OR1 [He et al.,\n2025d] adopted staged RL training, progressively increasing the context length to stabilize the\nlearning process; DeepCoder [Luo et al., 2025b] and AReaL [Fu et al., 2025b] further employed\nasynchronous rollouts to decouple training from inference and accelerate learning. Regarding\ncross-task generalization, AceReason-Nemotron [Chen et al., 2025q] observed a positive transfer\neffect from mathematical reasoning tasks to competitive programming.\n• Domain-Specific Code: Due to domain-specific differences in code requirements, RL is increas-\ningly applied to specialized tasks. In data retrieval, Reasoning-SQL [Pourreza et al., 2025],\nReEX-SQL [Dai et al., 2025b], and CogniSQL-R1-Zero [Gajjar et al., 2025] applied the GRPO\nalgorithm to Text-to-SQL tasks, achieving notable performance on corresponding benchmarks. In\nformal proofs, Kimina-Prover [Wang et al., 2025d] and DeepSeek-Prover-v2 [Ren et al., 2025]\nunified informal and formal proofs by combining natural language with Lean, while StepFun-\nProver [Shang et al., 2025] developed an end-to-end tool-integrated training pipeline, and\nLeanabell-Prover-V2 [Ji et al., 2025a] directly optimized reasoning trajectories via multi-round\nverifier feedback, further advancing RL’s capabilities in this field. In other domains, MedAgent-\nGym [Xu et al., 2025b] provided an executable coding environment for large-scale trajectory\ngeneration to improve LLM-based medical reasoning; VeriReason [Wang et al., 2025r] and\nCodeV-R1 [Zhu et al., 2025f] extended RLVR to the field of electronic design automation (EDA),\naccelerating LLM-driven hardware design. Additionally, chart-to-code generation enables agents\nto process structured or visual inputs and translate them into executable code, exemplifying\ncross-modal domain-specific code generation [Chen et al., 2025e].\nSoftware Engineering. Despite progress in competitive programming and domain-specific tasks,\nthese studies often fall short of real-world software development environments. Consequently, RL\nresearch also focuses on real-world software engineering, including code repair, quality optimization,\nand repository-level generation.\n• Code Quality Improvement: Automated code repair and quality improvement enhance software\nreliability while preserving functionality. RL significantly improves repair effectiveness and\ngeneralization, enabling models to handle unseen defects. RePaCA [Fuster-Pena et al., 2025]\nmitigates APR patch overfitting by guiding LLMs with chain-of-thought reasoning and GRPO-\nbased fine-tuning, while Repair-R1 [Hu et al., 2025a] jointly optimizes test-case generation and\nrepair, reducing reliance on post-hoc validation. Beyond bug fixing, RL enhances code efficiency,\nmaintainability, readability, and security. CURE [Wang et al., 2025p] evolves code and unit\ntests via encoder-tester interactions without ground-truth supervision, and Afterburner [Du\net al., 2025a] leverages execution feedback, raising Pass@1 from 47% to 62% and surpassing\nhuman-level efficiency. REAL [Yao et al., 2025b] integrates program analysis and unit testing as\nhybrid rewards to improve scalability and quality, achieving high-quality code generation without\nhuman intervention.\n• Repository-Level Code Generation: Beyond function- and snippet-level tasks, recent work\nexplores repository-level code generation and maintenance, emphasizing consistency and main-\ntainability across complex cross-file and cross-module dependencies. RLCoder [Wang et al.,\n2024c] combines Retrieval-Augmented Generation (RAG) with RL to train a retriever and improve\ncode completion accuracy. RepoGenReflex [Wang et al., 2024a] further introduces a reflection\n48\nA Survey of Reinforcement Learning for Large Reasoning Models\nmechanism to evaluate generated results and provide feedback, continuously optimizing gen-\neration strategies and improving generalization. By integrating RL with automated testing and\ncontinuous integration, this approach aligns LLM optimization with real-world development\nprocesses, advancing software engineering automation.\n6.2. Agentic Tasks\nTakeaways\n• Agentic RL enables advanced behaviors but faces scalability issues from high computational\ncosts and long rollout times within environments.\n• Asynchronous rollouts and memory agents help reduce latency and manage context, but\nfurther progress relies on better training data.\nTool use is considered a fundamental ability of language models [Schick et al., 2023]. Recent\nworks leverage RL to help LLMs master tools and complete more complex problems [Dong et al.,\n2025a, Team, 2025d]. We group them into Coding Agent, Simple Search Agent, Browser-use\nAgent, DeepResearch, GUI & Computer-use Agent, and Other Tasks.\nCoding Agent. The integration of RL and agent paradigms has advanced code generation from single-\nstep outputs to multi-round interactions and autonomous iteration, endowing LLMs with execution\nand verification capabilities for closed-loop optimization.\n• Code Agents: A common practice is to integrate RL into code agents equipped with execution\nand verification capabilities, and evaluate them on realistic benchmarks such as SWE-Bench.\nSWE-RL [Wei et al., 2025c] applies GRPO to the patch generation–execution–correction loop,\nenabling continuous policy optimization and improving mathematical reasoning, general code\ngeneration, and cross-domain tasks. EvoScale (Satori-SWE) [Zeng et al., 2025b] allows agents\nto autonomously enhance patch quality without external verifiers. RL-enhanced models such\nas Kimi-K2 [Team, 2025d], Qwen3-Coder, and GLM-4.5 demonstrate stronger agentic behavior,\npromoting greater autonomy and scalability. These developments suggest that combining RL with\nagentic coding is driving a shift from “single-step generation” toward “autonomous iteration.”\n• Tool-Integrated Reasoning: Another emerging application of RL lies in Tool-Integrated Reasoning\n(TIR), which enhances LLMs’ code reasoning capabilities by tightly coupling natural language\nreasoning with external tool execution environments. This approach enables models to generate,\nexecute, and verify intermediate code or program outputs, reducing errors and improving\nverifiability. Representative works such as ARPO [Dong et al., 2025b], AutoTIR [Wei et al.,\n2025b], CoRT [Li et al., 2025a], and ToRL [Li et al., 2025p] adopt similar strategies: models\nare post-trained with SFT or RL (mainly GRPO or variants), and outputs are structured (e.g.,\n<code>...</code>) to trigger tool execution, feeding results back into the reasoning loop.\nThis tight integration provides explicit RL reward signals, guiding models to produce logically\nconsistent outputs and iteratively refine them through verifiable computation. Additionally,\nautoformalization approaches such as FormaRL [Huang et al., 2025d] extend TIR to Lean-\nbased formal proof generation by integrating compiler-based syntax checks and LLM consistency\nevaluation with minimal labeled data, further improving reliability and correctness.\n• Automated ML Programming: RL shows promise in automated machine learning (AutoML),\nexpanding code agents into ML engineering agents (MLE agents) capable of autonomous data\nprocessing, model building, and optimization. MLE-bench [Chan et al., 2024] evaluates ML\n49\nA Survey of Reinforcement Learning for Large Reasoning Models\nagent capabilities; MLE-STAR [Nam et al., 2025] proposes a search- and optimization-based ML\nengineering agent; ML-Agent [Liu et al., 2025r] shows RL-driven autonomous ML engineering.\nSimple Search Agent. LLMs can be trained to function as search agents through structured prompting,\nmulti-turn generation, and integration with either online search engines (e.g., Google) or static local\ncorpora such as Wikipedia [Jin et al., 2025a,b, Song et al., 2025a]. However, training with online\nsearch engines often incurs substantial API costs, making this approach prohibitively expensive. To\naddress this challenge, Sun et al. [2025a] propose simulating a search engine during the training of\nsearch-capable LLMs, significantly reducing costs while maintaining or even improving performance.\nOther works such as R1-Search++ [Song et al., 2025b] and SEM [Sha et al., 2025] leverage the\ninternal knowledge of LLMs to reduce training budgets while yielding better performance. Specifically,\nSSRL [Fan et al., 2025c] proposes training models in fully-simulated environments that can be\nseamlessly adapted to real scenarios through Sim2Real Generalization. Meanwhile, diverse reward\nsignals can be developed for specific applications. Dao and Le [2025], Mei et al. [2025] employ\ndiversity rewards to encourage comprehensive yet accurate information gathering. Wang et al. [2025v]\nleverage step-level rewards to further enhance the performance of search agents. S3 [Jiang et al.,\n2025d] utilizes gains beyond RAG to achieve better performance with fewer data. To enhance LLMs’\ncapabilities on more challenging queries, such as those in benchmarks like GAIA [Mialon et al., 2023]\nand BrowseComp [Wei et al., 2025a], WebSailor [Li et al., 2025e] constructs training data from\nknowledge graphs, enabling models to search and browse open web environments to solve obscure\nproblems. WebShaper [Tao et al., 2025] introduces a formalized data construction framework aimed\nat improving general AI assistants’ problem-solving abilities.\nBrowser-use Agent. Besides using search engines, other browser-user agents leverage web-browsing\nas well. WebGPT [Nakano et al., 2021] uses textual web description to train a model to possess the\nability to browse websites. Web-RL [Qi et al., 2024] employs a curriculum strategy along with ORM to\nconvert LLMs into web agents. DeepResearcher [Zheng et al., 2025e] leverages another LLM to serve\nas a summarizer when browsing to help the search process. Vattikonda et al. [2025] bootstrap to\ntrain a student model using a variety of hyperparameters for stable training and better performance.\nWebAgent-R1 [Wei et al., 2025d] proposes a multi-turn asynchronous GRPO to train an end-to-end\nweb browse agent, achieving strong performance. WebDancer [Wu et al., 2025d] conducts SFT and\nRL to enable in-depth information seeking and multi-step reasoning by web searching and browsing.\nBesides, other tasks are calling for a web agent, e.g., Academic Browse [Zhou et al., 2025b].\nDeepResearch Agent. DeepResearch is introduced for gathering information from various sources\nonline to help complete real-world problems, e.g., report generation. WebThinker [Li et al., 2025l],\ntrained with iterative DPO, leverages the long-cot abilities of LRMs, using deep web explorer along\nwith an LLM writer to finish challenging tasks. Kimi-Searcher [AI, 2025] identifies the dilemma of\nmulti-agent, and automatically constructs intensive tool-use data to end-to-end train a single agent\nmodel, achieving great performance on HLE [Prabhudesai et al., 2025]. Jan-nano [Dao and Vu,\n2025] eliminates the need for cold-start or SFT by taking multi-stage RLVR, focusing on tool calling,\nanswering quality, and extending response length, respectively. MicroThinker [Team, 2025e] uses SFT\nand DPO to train Qwen3 [Wu et al., 2025a], enhancing its performance in real-world applications.\nRecently, WebWatcher is proposed [Geng et al., 2025] which is a multi-modal deepresearch-model\ncapable of using external tools and visual information to solve extremely complex problems. Atom-\nSearhcer [Deng et al., 2025b] leverages an LRM as a PRM to provide fine-grained reward signals\nduring training, achieving better performance. ASearcher [Gao et al., 2025a] scales the interaction\nturns to more than 10 turns to elicit the reasoning capability of the deep research agent. Besides\ngeneral QA tasks, MedResearcher-R1 [Yu et al., 2025a] is proposed to solve clinical questions.\n50\nA Survey of Reinforcement Learning for Large Reasoning Models\nGUI & Computer-use Agent. UI-R1 [Lu et al., 2025f] is the first work to apply rule-based RL to\ngraphical user interface (GUI) tasks. It introduces a novel rule-based action reward and is optimized\nusing a small, human-curated training set. Building on this practice, GUI-R1 [Luo et al., 2025d],\nGUI-Critic-R1 [Wanyan et al., 2025], and so on [Du et al., 2025b, Lin et al., 2025a], carefully design\nfine-grained rule-based rewards tailored to specific objectives of GUI tasks, such as action accuracy,\nargument correctness, and step-level status. GUI-G1 [Zhou et al., 2025h] presents an empirical\nanalysis of prior methods, identifying issues such as length bias, difficulty bias, and susceptibility to\nreward hacking, and reformulates the reward normalization scheme to mitigate these limitations.\nFurthermore, recent studies [Gu et al., 2025, Shi et al., 2025c] have attempted to obtain feedback\nfrom online GUI environments to better simulate real-world operating conditions. GUI-Reflection [Wu\net al., 2025g] and UIShift [Gao et al., 2025b] derive binary rewards based on changes of UI elements\nto indicate action success or failure. Liu et al. [2025q] propose a two-stage training paradigm that\nexplicitly enhances planning and reflective reasoning capabilities. ZeroGUI [Yang et al., 2025b]\nintroduces an automated pipeline for generating challenging tasks and estimates rewards solely based\non online environmental feedback, eliminating the need for human annotation. Different from the\nabove step-level methods, there is a growing trend towards applying end-to-end asynchronous RL\nframeworks to train agents for mobile [Lu et al., 2025b,d, Ye et al., 2025b], and computer [Lai et al.,\n2025] use, which optimize the model using only rule-based task-level completion rewards without\nrequiring step-wise reward signals. UI-TARS [Wang et al., 2025f] learns from mistakes and adapts to\nunforeseen situations through iterative training and reflection tuning. UI-TARS 2 [Qin et al., 2025]\nfeatures with enhanced capabilities in GUI, Game, Code and Tool Use with end-to-end RL.\nOther Tasks. Beyond search and GUI agents, RL has also been successfully applied to a variety of other\nagentic tasks. For example, Jiang et al. [2025a] improve ad copy generation by leveraging historical\nperformance metrics, such as click-through rates, as reward signals to guide RL-based optimization.\nIn the e-commerce domain, Shop-R1 [Zhang et al., 2025r] introduces a composite reward function\nthat combines internal model logits with external hierarchical feedback to better simulate human-like\ndecision-making in shopping environments. For autonomous driving, LaviPlan [Oh, 2025] aligns\nperceptual vision capabilities with context-aware decision-making, enabling more robust navigation\nunder dynamic conditions. Similarly, Drive-R1 [Li et al., 2025r] is designed to balance reasoning and\nplanning abilities for complex driving scenarios, improving both strategic and reactive behavior. In\nstructured data interaction, OpenTab-R1 [Qiu, 2025] employs a two-stage training framework to\nenhance LLMs’ proficiency in table-based question answering. Furthermore, general-purpose agentic\nmodels such as those in Qian et al. [2025] and Team [2025d] demonstrate the ability to master\nmultiple commonly used tools (e.g., calculators, APIs, and databases) to solve diverse real-world tasks,\nshowcasing the scalability of RL in building versatile, tool-augmented agents.\n6.3. Multimodal Tasks\nTakeaways\n• RL strengthens multimodal models to address challenges such as limited-data settings,\nlong-video reasoning, and numerically or attribute-sensitive cross-modal generation.\n• Exploring unified RL frameworks for understanding and generation is an urgent task.\nThe success of RL is evident not only in language models, but also in fostering notable progress in\nmultimodal tasks. Specific optimization has been developed to enhance capabilities such as spatial\nperception [Su et al., 2025e] and cross-modal controllability [Wu et al., 2025h]. In the following, we\ndiscuss RL applications in multimodal tasks in terms of understanding and generation.\n51\nA Survey of Reinforcement Learning for Large Reasoning Models\nMultimodal Understanding. Compared to the language scenario, multimodal understanding de-\nmands powerful spatial perception and semantic alignment cross-modalities. Recently, a surge\nof research has employed RL to enhance reasoning ability across images, videos, and 3D spaces,\ndemonstrating significant improvements in understanding capability.\n• RL in Image Understanding: Vision-R1 [Huang et al., 2025c], VLM-R1 [Shen et al., 2025a],\nand Visual-RFT [Liu et al., 2025x] represent the first attempt to extend the DeepSeek-R1 styled\nRFT from math and code domains to multimodal perception tasks. These methods mark a\nshift in training paradigm: moving from data scaling in SFT toward the strategic design of\nverifiable reward functions tailored to task-specific objectives. They achieve strong performance\non several detection and grounding benchmarks, demonstrating the advanced generalization\nability of Reinforced Fine-Tuning (RFT) even with limited training data. Subsequently, several\nvisual reasoning models [Kan et al., 2025, Xia et al., 2025a] adopt a similar thinking-answer\nformat in an attempt to learn through trial and error. These methods enhance reasoning abilities\nvia outcome-reward-driven optimization, eliminating the need for costly step-wise supervision\nor CoT training data. Recently, Deepeyes [Zheng et al., 2025f], CoF [Zhang et al., 2025m],\nand others [Cao et al., 2025, Fan et al., 2025d, Su et al., 2025a] have extended beyond pure\ntext-based CoT to explicit multimodal-interleaved reasoning chains. These methods attempt to\niteratively identify regions of interest in images using off-the-shelf tools [Su et al., 2025d] or image\ngeneration models [Xu et al., 2025e], achieving more interpretable reasoning processes. Other\nmethods [Chu et al., 2025b, Chung et al., 2025] implement implicit multimodal-interleaved COT\nby copying and routing visual tokens during the reasoning stage, which mitigates hallucinations\nin long text-based CoT. Despite the remarkable success, several challenges remain to be addressed:\n1) Inconsistent reasoning and answering: The thinking generated by the model fails to map to\nthe final answer. 2) Long-chain exploration collapse: As the response length increases, the model\nbecomes fragile and prone to generating hallucinations. 3) Sensitivity to data quality: RL sample\nselection is crucial, as low-quality training data may lead to suboptimal performance or even\nnegative optimization.\n• RL in Video Understanding: Extending video understanding capacity to interpret and reason\nover dynamic visual content is essential for multimodal understanding. To achieve this goal,\nVideo-R1 [Feng et al., 2025b] introduces a systematic RL framework for video Multimodal Large\nLanguage Models (MLLMs), using a temporal-aware GPRO algorithm (T-GRPO) to improve\nspatial-temporal reasoning. Focused Thinking [Dang et al., 2025] employs a token-weighted\nreward scheme that trims verbose, generic chains-of-thought and uses graded (partial-credit)\nrewards to enhance video reasoning. VQ-Insight [Zhang et al., 2025n] designs hierarchical\nrewards with general task-specific temporal learning tailored QA process over long videos. To\nunderstand human daily lives from a first-person perspective, Ego-R1 [Tian et al., 2025] trains\na chain-of-tool-thought agent via RL to tackle ultra-long egocentric videos (days or weeks in\nlength) by dynamically invoking retrieval and vision tools for stepwise reasoning. Likewise,\nLongVILA [Chen et al., 2025t]’s Long-RL framework builds a large LongVideo-Reason dataset\nand a specialized two-stage CoT-SFT and RL pipeline with sequence parallelism, enabling MLLMs\nto process ultra-long videos. To automate more video CoT data creation, VideoRFT [Wang\net al., 2025k] uses an LLM to generate initial rationales from rich video descriptors with a VLM\nrefinement and introduces a semantic consistency reward to align textual reasoning with visual\nevidence. Meanwhile, VideoChat-R1 [Li et al., 2025n] demonstrates that targeted multi-task\nRL fine-tuning can markedly enhance specific spatio-temporal skills without degrading general\nchat performance. Collectively, these studies pave the way for the development of robust and\ngeneralizable video reasoning through RL.\n52\nA Survey of Reinforcement Learning for Large Reasoning Models\n• RL in 3D Understanding: While MLLMs have made significant progress in 2D visual understand-\ning through RL, extending their ability to visual-spatial understanding in 3D space remains a\nchallenging frontier [Wu et al., 2025b, Yang et al., 2025c]. MetaSpatial [Pan and Liu, 2025] em-\nploys a multi-turn RL-based optimization mechanism that integrates physics-aware constraints to\nenhance spatial reasoning in MLLMs. Building upon GRPO [Shao et al., 2024], Spatial-MLLM [Wu\net al., 2025b] and SpaceR [Ouyang et al., 2025] demonstrate that even small-scale models can\nclose the performance gap with much larger counterparts through R1-Zero-like training [Liao\net al., 2025c]. Further, RoboRefer [Zhou et al., 2025a] expand RL-based spatial reasoning to\nembodied settings to ground reasoning in real-world dynamics.\nMultimodal Generation. The exploration of RL in LLMs has also been extended to multimodal\ngeneration. Pioneering researches on test-time scaling [Liu et al., 2025b, Ma et al., 2025b, Singhal\net al., 2025] and DPO [Black et al., 2024b, Liang et al., 2025d, Liu et al., 2025k, Tong et al., 2025a,\nWallace et al., 2024] have driven significant progress in aesthetic and text fidelity in image and video\ngeneration. Recently, increasing attention has been devoted to enhance reasoning capabilities in\nimage and video generation [Guo et al., 2025f, Jiang et al., 2025b].\n• RL in Image Generation: Diffusion models have substantially advanced visual generation [Esser\net al., 2024, Liu et al., 2023b, Rombach et al., 2022], and a growing body of research incorporates\nRL to implicitly perform reasoning by treating the denoising steps as the CoT trajectory [Liu et al.,\n2025d, Pan et al., 2025b, Xue et al., 2025]. However, GRPO exhibits an inherent conflict between\nordinary differential equation (ODE) sampling in diffusion models. Specifically, GRPO relies\non stochastic sampling to estimate advantage, whereas ODE sampling follows a deterministic\ndenoising trajectory, which limits the diversity of rollout samples. To address this issue, an ODE-to-\nSDE conversion is employed [Liu et al., 2025d, Wu et al., 2025a, Xue et al., 2025] to encourage the\nstochastic term in the sampling process. Considering the inefficiency of SDE, MixGRPO [Li et al.,\n2025d] designs mixed sampling strategies through the integration of SDE and ODE. In addition,\nTempFlow-GRPO [He et al., 2025g] explicitly exploits the temporal structure in the flow-based\nmodel, enabling more precise credit assignment and policy optimization. Recently, GPT-4o has\ndemonstrated powerful text fidelity and editing consistency [OpenAI, 2024], sparking interest in\nthe controllability of autoregressive models. Building on large-scale image–text training data,\nSimpleAR [Wang et al., 2025i] directly applies GRPO for post-training and achieves remarkable\nperformance in high-resolution image generation. To strengthen adherence to fine-grained\nattributes such as spatial relations and numerical consistency, FocusDiff [Pan et al., 2025e]\nconstructs paired datasets that differ only in subtle attribute variations and uses them to train\nthe generation model. Furthermore, RePrompt [Wu et al., 2025f] incorporates an additional\nmultimodal understanding model into the image generation framework and trains it with GRPO\nto refine prompts. Meanwhile, T2I-R1 [Jiang et al., 2025b], GoT-R1 [Duan et al., 2025], and\nReasonGen-R1 [Zhang et al., 2025s] unify prompt refinement and image generation within a\nsingle model, leveraging GRPO for joint optimization.\n• RL in Video Generation: Compared to image generation, extending RL to video generation poses\ngreater challenges in terms of temporal coherence and physical realism. DanceGRPO [Xue et al.,\n2025] conducts post-training on HunyuanVideo [Kong et al., 2024], and uses VideoAlign [Liu\net al., 2025e] to provide rewards based on video aesthetics, motion quality, and text-video\nconsistency. Furthermore, InfLVG [Fang et al., 2025b] employs GRPO to guide token selection\naccording to contextual relevance, thereby enabling semantically consistent and temporally\ncoherent long video generation. In addition, Phys-AR [Lin et al., 2025b] introduces velocity and\nmass as verifiable rewards for ball motion scenario, substantially enhancing the physical realism\nof video generation.\n53\nA Survey of Reinforcement Learning for Large Reasoning Models\nCurrently, several ULM models employ a unified framework to optimize multimodal understanding\nand generation simultaneously. To this end, bidirectional [Jiang et al., 2025c] and dual [Hong et al.,\n2025c] rewards from text to image and from image to text are proposed to enhance both the generation\nand understanding capabilities. For multimodal understanding, Deepeyes and CoF have attempted to\nemploy generative models or external tools to realize multimodal CoT. For multimodal generation,\nusing refined text as the CoT also relies on the multimodal understanding capability. Therefore,\nexploring unified post-training methods for multimodal understanding and generation is an urgent\ntask for future research. From the perspective of specific-domain, code generation can serve as a\nbridge between text and image generation. The application of RL to facilitate models to reason\nover complex charts and produce structured code for domain-specific image generation [Chen et al.,\n2025e,f, Tan et al., 2025b] is a promising application.\n6.4. Multi-Agent Systems\nTakeaways\n• It is important to improve collaboration, reasoning, and credit assignment in Multi-Agent\nSystems (MAS), enabling more stable and effective teamwork on complex tasks.\n• Key challenges remain in developing efficient collaboration and interaction mechanisms\nto fully unlock collective capabilities and further raise agent performance.\nCurrently, most of the research on RL for LLM-based reasoning predominantly centers on single\nmodels, whereas applying RL to MAS has emerged as a prominent and frontier research direction.\nThis section begins with an overview of the fundamental concepts of traditional RL and Multi-Agent\nRL (MARL), highlighting their primary challenges. Furthermore, the section discusses innovative\napplications of LLMs in MARL, emphasizing their advantages in information sharing and credit\nassignment. Finally, recent advances in MAS integrating RL with LLMs are examined, with a focus on\nhow RL can be exploited to enhance collaboration and policy optimization among agents, thereby\npromoting the development of multi-agent reasoning capabilities.\nTraditional MARL. In recent years, as a complex distributed intelligent system, MAS have attracted\nwidespread attention in the field of RL [Dorri et al., 2018]. Traditional MARL [Busoniu et al., 2008]\nprimarily focuses on the interactions and joint learning of multiple agents within a shared environment\nto achieve global objectives. The main challenges in conventional MARL include the complexity of\ncredit assignment, the nonstationarity of the environment, and the efficiency of communication and\ncooperation among agents [Canese et al., 2021]. To address these issues, researchers propose a\ncentralized training with decentralized execution (CTDE) paradigm [Lowe et al., 2017], in which\nagents share global information for policy optimization during the training phase, while decision-\nmaking during execution relies solely on local observations. Based on the CTDE paradigm, researchers\nintroduce value-based methods (such as VDN [Sunehag et al., 2017] and QMIX [Rashid et al., 2020]),\npolicy gradient-based methods (such as MADDPG [Lowe et al., 2017]), and actor-critic methods\n(such as COMA [Foerster et al., 2018]). Moreover, as PPO is considered to be SOTA in traditional RL,\nMAPPO has also been shown to have surprising effects in some simple collaborative tasks [Yu et al.,\n2022]. However, as the number of agents increases and the task complexity rises, traditional MARL\nmethods face significant challenges in terms of sample efficiency and scalability. To address this issue,\nscholars have considered replacing current agent with neighboring agents in the interaction with all\nagents (such as MF-MARL [Yang et al., 2018]), which effectively alleviates the dimensionality curse\ncaused by the increase in the number of agents in MARL. However, it still cannot be efficiently applied\nto complex task scenarios that require multiple agents to collaborate simultaneously.\n54\nA Survey of Reinforcement Learning for Large Reasoning Models\nLLM for MARL. The rapid development of LLMs has demonstrated tremendous potential in addressing\nchallenges within MARL. Leveraging their powerful natural language understanding and generation\ncapabilities, LLMs can provide effective information-sharing mechanisms in MAS. For instance, in\ncredit assignment problems of MARL, researchers utilize LLMs to design intuitive reward allocation\nmechanisms, thereby enhancing the accuracy and interpretability of credit assignment. Zhang et al.\n[2023b] significantly improve multi-agent collaboration efficiency in sparse reward scenarios by\nenabling the LLMs to infer each agent’s intention in real time and generate the next cooperative plan.\nDing et al. [2023] leverage LLMs to parse natural language task descriptions into executable entity-\nlevel sub-goals, thereby achieving reward shaping and policy sharing, which effectively alleviates the\ncredit assignment problem in MARL. Li et al. [2023a] utilize the LLM’s “theory of mind” capability,\nallowing agents to generate linguistic beliefs about teammates’ potential strategies, thus enabling\nmore accurate decision-making in multi-agent coordination.\nRL for LLM-based MAS. In the context of integrating RL with LLMs, research on MAS based on\nLLMs has gradually become a hotspot. Related studies primarily focus on how to fully leverage the\nlanguage understanding and generation capabilities of LLMs, while utilizing RL to achieve efficient\ncollaboration and policy optimization among multiple agents. Frameworks such as LLaMAC and\nCTRL integrate LLMs with the actor-critic architecture. LLaMAC [Zhang et al., 2023a] employs a\ncentralized LLM-Critic to provide natural language-based value feedback to multiple LLM-Actors,\nthereby facilitating collaborative learning among multiple agents. CTRL [Xie et al., 2025e] trains\nLLMs to “self-criticize” by using synthetic data, and iteratively refines model outputs through RL\n(such as GRPO), which can improve test-time performance without the need for human annotation.\nIn large-scale multi-agent collaboration scenarios, MAPoRL [Park et al., 2025] promotes efficient\nand transferable collaboration in multi-turn tasks by jointly training multiple LLMs and introducing\nreasoning-aware rewards. MAGRPO [Liu et al., 2025n] models LLM collaboration as a cooperative\nmulti-agent RL problem, which proposes a group-level relative policy optimization mechanism that\nsignificantly enhances the quality of multi-turn joint outputs in tasks such as writing and code\ngeneration. ReMA [Wan et al., 2025] introduces dual LLM structure of high-level agent and low-level\nagent, which achieves synergistic enhancement of meta-thinking and reasoning abilities through\nalternating freezing and updating of policies. JoyAgents-R1 [Han et al., 2025] designs a joint\nevolutionary training process, facilitating both diversity and consistency within heterogeneous LLM\nteams in open-domain question answering tasks through alternating global experience replay and\nindividual PPO updates. AlphaEvolve [Novikov et al., 2025] designs an evolutionary optimization\nmechanism to coordinate multi-LLM collaboration. By directly modifying code and continuously\nreceiving evaluation feedback, the MAS enhances the capability to handle complex coding tasks.\nAutoAgents [Chen et al., 2023a] significantly enhance the adaptability and problem-solving capabilities\nof MAS in complex tasks by dynamically generating specialized agents tailored to task requirements\nand incorporating an observer role for reflection and improvement.\n6.5. Robotics Tasks\nTakeaways\n• RL addresses data scarcity and generalization challenges in robotics by adapting LLM-style\napproaches to Vision-Language-Action (VLA) models.\n• Allowing VLAs to learn from environment interaction and simple rewards, recent RL\nmethods (e.g., GRPO, RLOO, PPO) achieve superior performance and novel behaviors with\nminimal supervision.\n55\nA Survey of Reinforcement Learning for Large Reasoning Models\nRL in Robotics Tasks. RL has been extensively applied in robotics, primarily focusing on three domains:\nrobot control, Vision-and-Language Navigation (VLN), and robotic manipulation tasks. Traditional RL\nresearch in robot control has reached maturity with widespread applications, like action generation\nwith human-like robots [Peng et al., 2018], robust quadruped locomotion execution [Hwangbo\net al., 2019] and dexterous hand manipulation [Chen et al., 2023b]. Similarly, VLN tasks have seen\nsignificant progress [Anderson et al., 2018, Wang et al., 2018, 2019]. However, these domains differ\nsubstantially from LLM-based RL in terms of model architecture, scale, task types, reward function\ndesign, optimization objectives, and algorithmic approaches, and thus fall outside the scope of this\nsurvey.\nRobotic manipulation tasks, enabling robots to solve diverse manipulation problems in real-\nworld environments, represent the most challenging and fundamental aspect of embodied intelli-\ngence [Firoozi et al., 2025]. These tasks demand not only a comprehensive understanding of visual\nand textual information and fine-grained motor control, but also physical reasoning, long-horizon\nplanning, and logical inference capabilities. Leveraging the remarkable text and vision processing\ncapabilities of LLMs and VLMs, several studies have explored using these models as core components\ncombined with action modules for manipulation tasks, such as RobotBrain [Ji et al., 2025b] and\nRT-2 [Zitkovich et al., 2023].\nVision-Language-Action Models. Recently, Vision-Language-Action (VLA) models, which integrate\nVLM backbones with action modules through unified end-to-end training, have emerged as the most\npromising solution and become the mainstream approach for robotic manipulation [Zhong et al.,\n2025]. Current VLA models follow a two-stage paradigm [Sapkota et al., 2025]: pretraining on\nmultimodal data (e.g., Open X-Embodiment [O’Neill et al., 2024]) followed by supervised fine-tuning\non teleoperated robot trajectories. However, this imitation learning paradigm suffers from critical\nlimitations: its performance heavily depends on high-quality trajectory data that is expensive and\ninefficient to collect, and the resulting models exhibit poor generalization to unseen scenarios. Given\nthe architectural, scale, and methodological similarities between VLAs and LLMs [Zhong et al., 2025],\nadapting LLM-style RL approaches to VLA training presents a promising direction for addressing data\nscarcity and generalization challenges.\nApplying DeepSeek-R1’s RL methodology to VLAs requires addressing several challenges: 1)\nUnlike LLMs that complete tasks in a single round, VLAs require multi-round environment interactions\nto generate complete trajectories; 2) VLAs operate in continuous action spaces; 3) Traditional RL\nmethods rely on hand-crafted process rewards, limiting scalability. Recent works including SimpleVLA-\nRL [SimpleVLA-RL Team, 2025], VLA-RL [Lu et al., 2025c], VLA RL Generalization [Liu et al., 2025f],\nRIPT-VLA [Tan et al., 2025a], and ConRFT [Chen et al., 2025s] have pioneered the application of\nDeepSeek-R1’s methodology to VLA training.\nSimpleVLA-RL [SimpleVLA-RL Team, 2025] enables VLA models to interact with environments to\nrollout diverse complete trajectories, employing binary success/failure rewards as supervision signals\nand training OpenVLA-OFT [Kim et al., 2025] using the GRPO algorithm. With just a single demon-\nstration trajectory, this RL approach surpasses state-of-the-art VLA models like 𝜋0 [Black et al., 2024a]\non LIBERO and RobotWin2.0 benchmarks, achieving SOTA performance and outperforming advanced\nRDT models in real-robot experiments. In addition, as an upgraded version of 𝜋0, 𝜋0.5 [Intelligence\net al., 2025] uses multimodal robot data from different scenarios and sources for heterogeneous\ntraining, allowing VLA to provide a new milestone in generalizable real-world robot operation tasks.\nSimilar to DeepSeek-R1’s “aha moments”, RL-trained VLAs also discover novel behavioral patterns.\nVLA RL Generalization [Liu et al., 2025f] investigates RL’s impact on VLA generalization capabilities,\ndemonstrating significant improvements over SFT in unseen environments, objects, and textures,\nwhile comparing GRPO and PPO effectiveness. RIPT-VLA [Tan et al., 2025a] employs RLOO [Ah-\n56\nA Survey of Reinforcement Learning for Large Reasoning Models\nmadian et al., 2024] for VLA RL training. RLinf [Team, 2025h] designed a flexible, scalable RL\nframework for VLA RL that unifies rendering, inference, and training, improving both VLA training\nefficiency and performance. ConRFT [Chen et al., 2025s] iteratively trains VLAs through alternating\nRL and SFT rounds, progressively enhancing performance through multiple iterations.\nThe data efficiency, improved generalization, and minimal supervision requirements of RL effec-\ntively address VLA’s current challenges of data scarcity and poor generalization. By allowing VLAs to\nautonomously explore and learn from trial-and-error with only outcome supervision, this approach\ndramatically reduces implementation costs compared to complex and expensive teleoperation data\ncollection. Moreover, RL’s data efficiency eliminates the need for large-scale expensive trajectory\ndatasets, enabling scalable VLA post-training capabilities.\nHowever, current VLA RL research remains primarily simulation-based.\nWhile SimpleVLA-\nRL [SimpleVLA-RL Team, 2025] achieved real-world deployment through Sim2Real transfer [Chen\net al., 2025n], few works have yet deployed physical robots to collect real-world trajectories for\nRL. In addition, research on VLA RL is also limited by the current development of RL in robotics,\nincluding but not limited to sample efficiency, reward sparsity, and sim2real. Key challenges include\nautonomous sampling on physical robots requiring multiple devices for efficiency, continuous manual\nresetting and annotation.\n6.6. Medical Tasks\nTakeaways\n• RL for medical LLMs faces distinct challenges: verifiable tasks allow stable reward design,\nwhile non-verifiable tasks make reward definition difficult.\n• Verifiable tasks use SFT+RL with rule-based rewards; non-verifiable tasks leverage DPO,\nrubrics, curriculum RL, or offline RL, though scalability and stability remain open issues.\nRL optimizations in medical LLMs typically aim to enhance reasoning and generalization ability,\noften adopting a two-stage pipeline of SFT followed by RL. Existing works can be broadly categorized\ninto verifiable problems with rule-based rewards, and non-verifiable problems with generative\nor rubric-based rewards.\nMedical Understanding. These tasks, such as multiple-choice QA, structured prediction, clinical\ncoding, or visual grounding, allow the use of deterministic rewards, making them the most mature\nfield for RL in medical LLMs. The typical paradigm is a two-stage pipeline of SFT followed by RL, where\nalgorithms such as GRPO optimize models directly against correctness-based signals. For example,\nHuatuoGPT-o1 [Chen et al., 2024a] enhances reasoning ability by synthesizing reliable reasoning\ntrajectory data with a medical verifier and training the model with SFT and RL. Med-U1 [Zhang et al.,\n2025k] employs mixed binary correctness rewards with length penalties to ensure both accuracy\nand format compliance, while MED-RLVR [Zhang et al., 2025i] applies verifiable rewards to MCQA,\nimproving OOD generalization. Open-Medical-R1 [Qiu et al., 2025] demonstrates that careful data\nfiltering improves the efficiency of RL. Gazal-R1 [Arora et al., 2025] designs a multi-component\nreward system that refines accuracy, format adherence, and reasoning quality through GRPO for\nenhanced medical reasoning. ProMed [Ding et al., 2025] shifts medical LLMs from reactive to\nproactive paradigms, where LLMs can ask clinically valuable questions before decision-making, using\nShapley Information Gain rewards during MCTS-guided trajectory exploration and RL.\nBeyond textual QA, recent models extend rule-based rewards to vision and multi-modal tasks.\nMedVLM-R1 [Pan et al., 2025d] employs an RL framework that incentivizes the model to discover\n57\nA Survey of Reinforcement Learning for Large Reasoning Models\nhuman-interpretable reasoning paths without using any reasoning references through format and\naccuracy rewards. MedGround-R1 [Xu and Nie, 2025] introduces spatial-semantic rewards, which\ncombine spatial accuracy reward and semantic consistency reward, for the medical imaging grounding\ntask. ARMed [Liu and Wei, 2025] addresses reward collapse in open-ended medical VQA through\nadaptive semantic rewards that dynamically adjust the semantic reward during training based on\nhistorical reward distributions. Liu and Li [2025] leverage rule-based format and matching rewards to\nguide structured JSON generation for medical visual information extraction with only 100 annotated\nsamples. MMedAgent-RL [Xia et al., 2025b] is an RL-based multi-agent framework that enables\ndynamic and optimized collaboration among medical agents. MedGemma [Sellergren et al., 2025]\nwas post-trained with RL and is further evaluated on MedXpertQA [Zuo et al., 2025a], which is an\nexpert-level medical multi-choice benchmark and includes a subset for assessing reasoning models.\nFor other clinical applications, DRG-Sapphire [Wang, 2025] applies GRPO with rule-based rewards\nto diagnosis-related grouping. EHRMIND [Lin and Wu, 2025] combines SFT warmup and RL VR\nfor complex clinical reasoning tasks using electronic health records (EHR) data, including medical\ncalculations, patient trial matching, and disease diagnosis. ChestX-Reasoner [Fan et al., 2025e]\nincorporates process rewards from clinical reports to train the model to emulate radiologists’ step-\nby-step reasoning. CX-Mind [Li et al., 2025j] employs SFT and RL with format, result, and process\nrewards to train interleaved reasoning for chest X-ray diagnostics. To enable benchmarking of code-\nbased medical reasoning, MedAgentGym [Xu et al., 2025b] presents a benchmark for code generation\nof medical agents, and demonstrates that RL can improve this reasoning ability.\nMedical Generation. These tasks include multi-turn clinical dialogue [Bani-Harouni, 2025], treatment\nplanning [Nusrat, 2025], and diagnostic narratives [Yooseok Lim, 2025], which lack unique ground-\ntruth answers. As such, rule-based rewards are not directly applicable. While DPO has been applied\nto improve medical LLMs on preference-aligned generation tasks [Yang et al., 2025h, Yu et al.,\n2025c], large-scale RL on non-verifiable tasks is emerging but remains relatively underexplored. For\nexample, DOLA [Nusrat, 2025] integrates LLM agents with a commercial treatment planning system,\nincorporating a reward function that guides the trade-offs between target coverage and organ at\nrisk sparing for optimized treatment plan generation. LA-CDM [Bani-Harouni, 2025] proposes a\ntwo-agent structure trained via a hybrid training paradigm which combined supervised fine-tuning\nwith RL to balance diagnostic accuracy, uncertainty calibration, and decision efficiency. In diagnostic\ndialogue, PPME [Sun et al., 2025i] develops a plug-and-play framework using large-scale EMRs and\nhybrid training to enhance LLM interactive diagnostic capabilities through specialized inquiry and\ndiagnosis models. In clinical decision support, MORE-CLEAR [Yooseok Lim, 2025] applies multi-modal\noffline RL to sepsis treatment policies, improving survival-predictive decision-making in MIMIC-III/IV.\nBaichuan-M1 [Inc., 2025] employs a three-stage RL approach: ELO (Exploratory Log-likelihood\nOptimization) to enhance chain-of-thought reasoning diversity, TDPO (Token-Level Direct Preference\nOptimization) to address length-dependent constraints, and finally PPO with reward model feedback\nfor policy refinement.\nOverall, RL in medical LLMs is well established for verifiable problems, where deterministic\ncorrectness allows for rule-based rewards and stable GRPO training. In contrast, generation-oriented\ntasks remain challenging: current solutions adopt rubric-based rewards, curriculum transfer, or offline\nRL to approximate quality signals. The scarcity of scalable RL on non-verifiable tasks highlights a\ncritical future direction for building trustworthy, reasoning-capable medical foundation models.\n58\nA Survey of Reinforcement Learning for Large Reasoning Models\n7. Future Directions\nWhile RL for LLMs has made remarkable strides, many fundamental challenges and opportunities\nlie ahead. This section outlines several promising directions that are poised to shape the next wave\nof advances in the field. We highlight the importance of continual RL for adapting to evolving data\nand tasks (§ 7.1), memory-based and model-based RL for enhancing reasoning capabilities (§ 7.2\nand § 7.3), and emerging approaches for teaching LLMs both efficient and latent-space reasoning\n(§ 7.4 and § 7.5). We also discuss frontiers in leveraging RL during pre-training (§ 7.6), applying RL\nto diffusion-based architectures (§ 7.7), and driving scientific discovery (§ 7.8). Finally, we consider\nthe challenges and prospects of architecture-algorithms co-design to meet the demands of ever-larger\nand high-efficiency intelligent models (§ 7.9). By surveying these directions, we aim to provide both\na roadmap and inspiration for future research in RL for LLMs.\n7.1. Continual RL for LLMs\nTo enhance the multi-domain performance of LLMs during RL-based post-training, the mainstream\napproach is to mix data from different tasks and train in a unified manner [Guo et al., 2025a, Yang\net al., 2025a]. On synthetic data [Chen et al., 2025d, Liu et al., 2025g], Multi-stage RL has been\nshown to perform worse than training with mixed data, and even curriculum learning with increasing\ndifficulty may not be necessary in RL [Xie et al., 2025c]. However, Chen et al. [2025d] suggest that\nmulti-stage RL across different tasks has advantages in generalizing to difficult or unseen problems.\nDespite these ongoing debates of multi-stage RL’s effectiveness, as the field advances toward building\nAI systems that must adapt to evolving data and tasks in dynamic environments, it becomes necessary\nto explore Continual Reinforcement Learning (CRL) for LLMs.\nSimilar to traditional CRL, LLMs face the fundamental challenge of balancing stability and\nplasticity during multi-stage RL training [Pan et al., 2025a]. Plasticity may be particularly concerning\nfor LLMs, as widely used deep learning techniques can cause large models to perform no better than\nshallow networks in continual learning settings [Dohare et al., 2024]. Another challenge of CRL for\nLLMs lies in the entangled nature of knowledge and reasoning in LLMs, which distinguishes from\ntraditional RL settings where tasks can be discretely defined and policies can be modularly organized,\nsuch as in game-like environments [Chevalier-Boisvert et al., 2023, Towers et al., 2024] or embodied\nscenarios [Todorov et al., 2012, Wołczyk et al., 2021].\nExisting methodological frameworks from traditional CRL research provide a promising foundation\nfor addressing LLM-specific requirements. Core methodological insights from traditional CRL research,\nincluding Experience Replay [Berseth et al., 2021, Li et al., 2021, Rolnick et al., 2019], Policy Reuse\n[Garcia and Thomas, 2019, Gaya et al., 2022], and Reward Shaping [Jiang et al., 2021, Zheng\net al., 2022]. It remains a valuable research direction for developing CRL frameworks tailored to\nLRMs. The development of specialized CRL techniques for LLMs or LRMs will be crucial for creating\nmore adaptive and efficient AI systems capable of lifelong learning and operating in dynamic and\never-changing environments.\n7.2. Memory-based RL for LLMs\nAlthough many works in agentic RL have explored memory mechanisms, ranging from external\nlong-term storage and insertion [Chhikara et al., 2025, Xu et al., 2025d, Zhong et al., 2024] to\ninternal memory processing and working-memory control [Yu et al., 2025b, Zhou et al., 2025i], most\ndesigns remain tailored to the current task with limited generalization beyond it. As Silver and Sutton\n[2025] emphasize, the next generation of intelligent agents will learn primarily from experience,\n59\nA Survey of Reinforcement Learning for Large Reasoning Models\nacquiring skills through continual interaction. In this spirit, a key direction is to transform agent\nmemory from task-specific buffers into experience repositories that are structured, reusable, and\ntransferable across diverse tasks, allowing memory to evolve into a foundation for broader adaptability\nand lifelong learning. Such an experience-centric view also aligns naturally with RL, since the data\ngenerated from the interactions between an agent and its environment provides rich experiential\ntraces that can be utilized effectively. Moreover, although recent works have explored maintaining a\nshared pool of experiences to retrieve relevant strategies from past histories and adapt other agents’\nexperiences to new task scenarios [Tang et al., 2025], this direction remains underexplored. A core\nchallenge here is enabling agents, through RL, to automatically learn how to operate and manage\nmemory, composing and generalizing experiential knowledge across tasks. Addressing this challenge\nis essential for moving toward an “experience era” where collective interaction traces become a\nfoundation for broader agent intelligence.\n7.3. Model-based RL for LLMs\nA core challenge in RL lies in obtaining scalable and robust reward signals as well as meaningful\nstate representations from the environment. Prior work has investigated the construction of world\nmodels [Luo et al., 2024, Moerland et al., 2023] to supply informative states for RL agents, and more\nrecently, LLMs have been adopted as world models in various RL contexts [Benechehab et al., 2024,\nGu et al., 2024, Hu and Shu, 2023]. In the case of RL with LLMs, especially for language agents, the\nability to construct world models that accurately capture environmental states and generate reliable\nrewards is critical. Recent advances show that generative world models, including those enhanced by\nvideo pre-training [Assran et al., 2025, Ball et al., 2025, Bruce et al., 2024], are both practical and\neffective. Nevertheless, seamlessly integrating world models with RL for LLM-based agents remains an\nopen research problem. As such, model-based RL with LLMs is emerging as a particularly promising\nand scalable direction for future research.\n7.4. Teaching LRMs Efficient Reasoning\nInference-time scaling has improved the accuracy of LRMs on difficult tasks, but it also introduces\nsystematic over-thinking (needlessly long reasoning chains for easy instances) [Chen et al., 2024b, Qu\net al., 2025a, Sui et al., 2025, Yan et al., 2025b] and, under aggressive truncation, under-thinking\n(premature halting and reliance on brittle shortcuts) [Su et al., 2025b, Wang et al., 2025s]. A central\nchallenge for RL-for-LLMs is to develop compute-allocation policies that adapt the depth and halting of\nreasoning to instance difficulty and epistemic uncertainty. Current research has explored hard-coded\nreasoning levels in prompts [Agarwal et al., 2025a, Wen et al., 2025a, Zhu et al., 2025g], adaptive\nlength-based reward shaping [Liu et al., 2025o, Yuan et al., 2025a], and the use of length penalties\nin the loss function [Aggarwal and Welleck, 2025, Xiang et al., 2025].\nHowever, generalizing these approaches into a principled cost-performance trade-off remains an\nopen question [Gan et al., 2025]. Teaching LRMs to be resource-rational, to reason longer only when\nthe marginal utility justifies it, remains a central, unsolved problem for RL in language reasoning.\n7.5. Teaching LLMs Latent Space Reasoning\nCoT [Wei et al., 2022] encourages step-by-step reasoning by prompting models to articulate interme-\ndiate steps, improving both interpretability and accuracy. Recent research has combined CoT and RL\nto further improve reasoning quality, which samples long-form thought before answering for modeling\ntraining [Guo et al., 2025a]. However, current implementations often rely on token-level sampling\n[Cui et al., 2025a, Ouyang et al., 2022, Rafailov et al., 2023] in a discrete scalar space, which can act\n60\nA Survey of Reinforcement Learning for Large Reasoning Models\nas a bottleneck as the lost of meaningful semantic information in continuous space [Hua et al., 2024].\nA recently proposed method, named Latent Space Reasoning (LSR) [Arriola et al., 2025, Geiping\net al., 2025, Hao et al., 2024], may be more friendly for RL optimization. LSR operates reasoning in\nthe continuous latent space of LLMs, facilitating more nuanced and fluid semantic reasoning. This\ncharacteristic contributes to smoother learning dynamics and a better integration with RL techniques.\nThe combination of RL and LSR holds significant potential for the development of more powerful\nand adaptable reasoning models in the future. However, assessing the quality of continuous latent\nthought is more challenging than evaluating token-based thought. This will complicate the provision\nof accurate supervisory signals, such as rewards and advantages, which will become an open challenge\nagainst the combination of LSR and RL.\n7.6. RL for LLMs Pre-training\nTraditional pre-training relies on large text corpora and next-token prediction, and scaling this\nparadigm has already been shown to be central to the development of foundation models [Brown\net al., 2020, Kaplan et al., 2020]. Emerging research now explores shifting RL earlier in the pipeline,\napplying it not only in post-training but also during pre-training itself. For instance, Reinforcement\nPre-Training [Dong et al., 2025c] reconceptualizes next-token prediction as an RL problem with\nverifiable rewards derived from the corpus, reporting consistent gains that increase with available\ncompute, thereby positioning RL as a promising scaling strategy for pre-training.\nIn parallel, open initiatives such as avataRL [tokenbender, 2025] demonstrate training language\nmodels from random initialization purely with RL, bootstrapping token-level rewards and employing\niterative “referee” scoring, thus illustrating a concrete path toward RL-from-scratch training. It is\nconsistent with the reincarnated RL paradigm [Agarwal et al., 2022], in which previously acquired\ncomputational knowledge (the pre-trained critic) is leveraged rather than training from the ground up.\nThese developments sharpen a practical question: how can RL-style pre-training be made cost-effective\nat scale? Addressing this challenge will likely require reducing both the verifier burden and the costs\nassociated with reward engineering, which appear to be critical for scaling RL-based pre-training.\nMoreover, this line of research is closely related to unsupervised reward design introduced in § 3.1.4,\nraising important questions about how to obtain rewards that are both scalable and reliable.\n7.7. RL for Diffusion-based LLMs\nDiffusion Large Language Models (DLLMs) [Cheng et al., 2025c, Labs et al., 2025, Nie et al., 2025,\nTae et al., 2025, Xie et al., 2025f, Ye et al., 2025c] represent an emerging paradigm in language\ngeneration. Compared to autoregressive (AR) models, DLLMs offer advantages including superior\ndecoding efficiency and a greater potential for self-correction through multiple rounds of diffusion.\nInitial efforts have begun to explore RL for DLLMs [Borso et al., 2025, Gong et al., 2025, Yang et al.,\n2025d], yet several key issues remain unresolved.\nA central challenge in applying RL to DLLMs lies in accurately and efficiently estimating log\nprobabilities of sampled responses. This is due to a fundamental difference in how autoregressive\nmodels and diffusion language models inherently model the likelihood of samples. AR models\ngenerate sequences through next-token prediction and factorize joint probabilities via the chain rule,\nenabling straightforward left-to-right sampling. However, DLLMs approximate likelihood optimization\nby maximizing the Evidence Lower Bound (ELBO). ELBO involves a double expectation over diffusion\ntimesteps and masked data, and typically demands extensive sampling to achieve accurate estimates;\notherwise, it introduces high variance during preference optimization. Although methods like the\none-step estimator in [Zhao et al., 2025c] and the sampling allocation strategy in [Zhu et al., 2025b]\n61\nA Survey of Reinforcement Learning for Large Reasoning Models\nhave been proposed to mitigate variance, efficient and accurate ELBO estimation remains an open\nproblem for on-policy learning.\nFurthermore, the existence of multiple feasible decoding trajectories in DLLMs introduces an\nadditional research dimension: leveraging RL to guide the model toward optimal sampling traces.\nThis requires designing effective reward functions for intermediate denoising steps. For example,\nHe et al. [2025c] formulate denoising as a multi-step decision problem and applies reward models\nto intermediate states, [Wang et al., 2025o] proposed a diffusion-based value model that computes\nprefix-conditioned, token-wise advantages to enable trajectory-level rewards, while Song et al. [2025c]\nutilize edit-distance-based rewards to maximize decoding efficiency. Future work may also draw\ninspiration from RL techniques developed for continuous diffusion models in computer vision [Black\net al., 2024b, Xue et al., 2025, Yang et al., 2024b], potentially paving the way toward a unified\nmultimodal framework.\n7.8. RL for LLMs in Scientific Discovery\nRecent research has shown that involving RL can improve the performance of LLMs on reasoning-\nheavy scientific tasks, in some cases even allowing them to surpass specialized methods [Fallahpour\net al., 2025, Fang et al., 2025c, Narayanan et al., 2025, Rizvi et al., 2025]. In domains such as\nbiology and chemistry, a core challenge for RL is performing result verification at scale, a process\nconventionally dependent on wet lab experimentation. Several existing methods have focused on\nreplacing or supplementing experimental verification: Pro-1 [Hla, 2025] uses a Rosetta energy\nfunction as the reward function for optimizing protein stability, and rbio1 [Istrate et al., 2025] verifies\ngene perturbation result predictions using biological models and external knowledge sources.\nMuch room for exploration remains for both reward formulation and improving the oracle models\nthemselves. Related to this is the broader problem of constructing suitable RL environments that\nsupport rapid experimentation-feedback loops. Agentic systems such as Coscientist [Boiko et al.,\n2023] and Robin [Ghareeb et al., 2025] have gained success through lab-in-the-loop verification, but\nsuch sparse, delayed, and costly feedback signals are impractical for directly training the underlying\nLLM. In silico simulations of experimental environments, for instance perturbation response prediction\nat the cellular level [Bunne et al., 2024, Noutahi et al., 2025], represent a potential path forward.\nHowever, many of these systems are far from sufficient for replacing realistic lab environments due\nto their limited scope and critical lack of accuracy and generalizability [Ahlmann-Eltze et al., 2025,\nKedzierska et al., 2023]. Other lines of research have explored incorporating domain-specific models\ninto LLM training to handle scientific data [Fallahpour et al., 2025] and developing generalist models\ncapable of a suite of well-defined tasks [Bigaud et al., 2025, Narayanan et al., 2025]. These directions,\ncoupled with advances in general RL methodology, will continue expanding the use cases of LLMs\nfrom narrowly defined tasks to complex interactions with open-ended objectives, enabling them to\nmore substantially contribute to novel discoveries.\n7.9. RL for Architecture-Algorithm Co-Design\nMost current RL pipelines for LLMs assume a dense Transformer [Vaswani et al., 2017] or Mixture-\nof-Experts (MoE) [Dai et al., 2024, Jiang et al., 2024, Shazeer et al., 2017] backbone, optimizing\nrewards that are almost exclusively tied to task accuracy. As a result, architectural degrees of\nfreedom, and their hardware implications are left outside the learning loop. In parallel, a new\nwave of hardware, architecture co-design has emerged (e.g., hardware-aligned sparse attention as in\nDeepSeek’s NSA [Yuan et al., 2025b] and model–system co-design in Step-3 [Wang et al., 2025a]),\nindicating that greater efficiency and capability can be achieved by aligning model structure with\n62\nA Survey of Reinforcement Learning for Large Reasoning Models\ncomputational substrates.\nWe argue that making architecture a first-class action space in RL represents an open and high-\nimpact challenge for next-generation LLMs. For instance, reinforced MoE approaches could enable\nmodels to learn routing policies, expert activation, capacity allocation, or sparsity patterns during RL,\noptimizing not only for task reward, but also for hardware-aware objectives such as latency, memory\ntraffic, energy consumption, and activation budgets. In this framing, RL is tasked with learning to\n“reason” not only over tokens [Guo et al., 2025a], but also across parameters and modules, dynamically\nadapting the model’s topology to each prompt’s difficulty and to real-time compute constraints. This\nperspective goes beyond classic RL-based neural architecture search (NAS) [Zoph and Le, 2016],\nwhich typically finds a fixed architecture for a given task or dataset. In contrast, reinforced MoE\nfocuses on optimizing routing and modular adaptation per input during inference [Han et al., 2021],\npotentially yielding both greater efficiency and flexibility. Key open questions include designing robust\nmulti-objective reward functions that avoid trivial solutions (e.g., all-expert sparsity), achieving stable\ncredit assignment when architectural actions modify network topology, and amortizing architecture\npolicy learning across prompts, tasks, and deployment scales. Addressing these challenges will be\ncrucial for enabling truly integrated architecture–algorithm co-optimization in future LLMs.\n8. Conclusion\nWe survey recent advances in RL for LRMs with a particular emphasis on reasoning, effectively\ntransforming LLMs into LRMs. In contrast to prior approaches such as RLHF or DPO, which are\nprimarily designed for human alignment, our focus is on RLVR for LLMs. RLVR enhances the reasoning\nabilities of LLMs by providing direct outcome-level rewards. Firstly, we present the core components\nof RLVR, including reward design, policy optimization, and sampling strategies. We summarize\nmultiple research directions and existing work for each section. And then we discuss several of the\nmost hotly debated issues in RL training for LLMs. In addition, we introduce training resources for RL\nof LLMs, covering static datasets, dynamic environments, and RL infrastructure. Finally, we review\ndownstream applications of RL in LLMs across various scenarios and highlight several promising\nresearch directions aimed at achieving super-intelligence through RL-based LLMs.\n63\nA Survey of Reinforcement Learning for Large Reasoning Models\nAuthor Contributions\nWe present below the contributions of all participating authors, specifying the primary responsible\nindividual and the contributing participants for each section.\n• Corresponding Author: Biqing Qi, Ning Ding, Bowen Zhou\n• Project Lead: Kaiyan Zhang, Yuxin Zuo\n• Introduction: Kaiyan Zhang\n• Preliminaries:\n– Background: Kaiyan Zhang\n– Frontier Models: Shang Qu, Yuru Wang (Survey)\n– Related Surveys: Yuxin Zuo\n• Foundational Components:\n– Verifiable Rewards: Yuxin Zuo\n– Generative Rewards: Bingxiang He, Sihang Zeng (Draft)\n– Dense Rewards: Runze Liu, Yu Fu (Turn-level)\n– Unsupervised Rewards: Bingxiang He, Yuxin Zuo (Survey)\n– Reward Shaping: Kaiyan Zhang\n– Policy Gradient Objective: Youbang Sun, Kaiyan Zhang (Formula)\n– Critic-based Algorithms: Youbang Sun, Kaiyan Zhang (Formula)\n– Critic-Free Algorithms: Youbang Sun, Kaiyan Zhang (Formula)\n– Off-policy Optimization: Xingtai Lv, Yu Fu (Replay Buffer)\n– Regularization Objectives: Yuchen Zhang, Bingxiang He (Entropy), Yuxin Zuo (Survey)\n– Dynamic and Structured Sampling: Yuchen Fan, Xuekai Zhu (Efficiency-oriented Sampling)\n– Sampling Hyper-parameters: Yuxin Zuo, Bingxiang He (Survey)\n• Foundational Problems: Kaiyan Zhang, Yuxin Zuo\n• Training Resource:\n– Static Corpus: Kai Tian, Zhenzhao Yuan (Survey)\n– Dynamic Environment: Che Jiang\n– RL Infrastructure: Kaiyan Zhang\n• Applications:\n– Coding Tasks: Pengfei Li, Xiang Xu (Tool-Integrated Reasoning)\n– Agentic Tasks: Yuchen Fan, Xinwei Long (GUI/Computer-use)\n– Multimodal Tasks: Guoli Jia, Fangfu Liu (Video/3D), Xinwei Long (Image)\n– Multi-Agent Systems: Shijie Wang\n– Robotics Tasks: Haozhan Li\n– Medical Tasks: Sihang Zeng, Jiaze Ma (Survey)\n64\nA Survey of Reinforcement Learning for Large Reasoning Models\n• Future Directions:\n– Che Jiang (CRL), Yu Fu (MemRL), Ermo Hua (Latent), Yuxin Zuo (Pre-Training), Yihao Liu\n(Diffusion), Shang Qu (Scientific), Kaiyan Zhang (Rest of All)\n• Other Contributions:\n– Figures: Yuru Wang, Kaiyan Zhang, Yuxin Zuo\n– Review and Editing: Zhiyuan Ma, Ganqu Cui, Huayu Chen, Weize Chen, Yafu Li, Xiaoye Qu,\nJunqi Gao, Dong Li, Zonglin Li, and all above authors\n65\nA Survey of Reinforcement Learning for Large Reasoning Models\nReferences\na-m team. Am-deepseek-r1-0528-distilled, June 2025. URL https://github.com/a-m-team/\na-m-models.\nMarah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao\nChen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4-reasoning\ntechnical report. arXiv preprint arXiv:2504.21318, 2025.\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.\nReincarnating reinforcement learning: Reusing prior computation to accelerate progress. Advances\nin neural information processing systems, 35:28955–28971, 2022.\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K.\nArora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arxiv\npreprint arXiv: 2508.10925, 2025a.\nShivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness\nof entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025b.\nPranjal Aggarwal and Sean Welleck. L1: Controlling how long a reasoning model thinks with\nreinforcement learning. arXiv preprint arXiv:2503.04697, 2025.\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang,\nStephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative\nmixed-modal language models. In International Conference on Machine Learning, pages 265–279.\nPMLR, 2023.\nConstantin Ahlmann-Eltze, Wolfgang Huber, and Simon Anders. Deep-learning-based gene pertur-\nbation effect prediction does not yet outperform simple linear baselines. Nature Methods, pages\n1657–1661, 2025.\nWasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain,\nJocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation\nfor competitive coding. arXiv preprint arXiv:2504.01943, 2025.\nArash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin,\nAhmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning\nfrom human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.\nMoonshot AI. Kimi-researcher: End-to-end rl training for emerging agentic capabilities. https:\n//moonshotai.github.io/Kimi-Researcher/, 2025. Accessed: 2025-08-13.\nAlon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait\nSingh, Chase Blagden, Violet Xiang, Dakota Mahan, et al. Big-math: A large-scale, high-quality\nmath dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387,\n2025.\nChenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu,\nXipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: A post-training recipe for scaling\nreinforcement learning on advanced reasoning models, 2025. URL https://hkunlp.github.\nio/blog/2025/Polaris.\n66\nA Survey of Reinforcement Learning for Large Reasoning Models\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen\nGould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded\nnavigation instructions in real environments. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 3674–3683, 2018.\nZachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D Chang, and Prithviraj Ammanabrolu.\nCritique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024.\nAnthropic. Claude 3.7 sonnet and claude code, 2025a. URL https://www.anthropic.com/news\n/claude-3-7-sonnet.\nAnthropic. Claude opus 4.1, 2025b. URL https://www.anthropic.com/claude/opus.\nIván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and\nArthur Conmy. Chain-of-thought reasoning in the wild is not always faithful. arXiv preprint\narXiv:2503.08679, 2025.\nDaman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint\narXiv:2502.04463, 2025.\nPranav Arora, Rohan Gupta, and Kavya Patel. Gazal-r1: Scaling medical reasoning with grpo and\nmulti-component reward design, 2025. URL https://arxiv.org/abs/2506.21594.\nMarianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar\nSahoo, and Volodymyr Kuleshov.\nBlock diffusion: Interpolating between autoregressive and\ndiffusion language models. arXiv preprint arXiv:2503.09573, 2025.\nMido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar\nRizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models\nenable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025.\nIbragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik,\nAndrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. Swe-rebench: An\nautomated pipeline for task collection and decontaminated evaluation of software engineering\nagents. arXiv preprint arXiv:2505.20411, 2025.\nLei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng\nChen, Ying Chen, Yongkang Chen, et al. Intern-s1: A scientific multimodal foundation model. arXiv\npreprint arXiv:2508.15763, 2025.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness\nfrom ai feedback. arXiv preprint arXiv:2212.08073, 2022b.\nBaidu-ERNIE-Team. Ernie 4.5 technical report. https://ernie.baidu.com/blog/publicati\non/ERNIE_Technical_Report.pdf, 2025.\nBowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y Guan, Aleksander Madry, Wojciech\nZaremba, Jakub Pachocki, and David Farhi. Monitoring reasoning models for misbehavior and the\nrisks of promoting obfuscation. arXiv preprint arXiv:2503.11926, 2025.\n67\nA Survey of Reinforcement Learning for Large Reasoning Models\nPhilip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, and et al. Genie 3: A\nnew frontier for world models. https://deepmind.google/discover/blog/genie-3-a-n\new-frontier-for-world-models/, 2025.\nDavid Bani-Harouni. Language agents for hypothesis-driven clinical decision making with reinforce-\nment learning, 2025. URL https://arxiv.org/abs/2506.13474.\nAbdelhakim Benechehab, Youssef Attia El Hili, Ambroise Odonnat, Oussama Zekri, Albert Thomas,\nGiuseppe Paolo, Maurizio Filippone, Ievgen Redko, and Balázs Kégl. Zero-shot model-based\nreinforcement learning using large language models. arXiv preprint arXiv:2410.11711, 2024.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil,\nZach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models.\narxiv preprint arXiv: 2505.00949, 2025.\nGlen Berseth, Zhiwei Zhang, Grace Zhang, Chelsea Finn, and Sergey Levine. Comps: Continual meta\npolicy search. arXiv preprint arXiv:2112.04467, 2021.\nNathan Bigaud, Vincent Cabeli, Meltem Gürel, Arthur Pignet, John Klein, Gilles Wainrib, and Eric\nDurand. OwkinZero: Accelerating biological discovery with AI. arXiv preprint arXiv: 2508.16315,\n2025.\nKevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai,\nLachy Groom, Karol Hausman, Brian Ichter, et al. pi0: A vision-language-action flow model for\ngeneral robot control. arXiv preprint arXiv:2410.24164, 2024a.\nKevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models\nwith reinforcement learning. In The Twelfth International Conference on Learning Representations,\n2024b. URL https://openreview.net/forum?id=YCWjhGrJFD.\nDaniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with\nlarge language models. Nature, 624(7992):570–578, 2023.\nUmberto Borso, Davide Paglieri, Jude Wells, and Tim Rocktäschel. Preference-based alignment of\ndiscrete diffusion models. arXiv preprint arXiv:2503.08295, 2025.\nMichael Bowling, John D Martin, David Abel, and Will Dabney. Settling the reward hypothesis. In\nInternational Conference on Machine Learning, pages 3003–3020. PMLR, 2023.\nBradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia\nMirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv\npreprint arXiv:2407.21787, 2024.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot\nlearners. Advances in neural information processing systems, 33:1877–1901, 2020.\nJake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes,\nMatthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive\nenvironments. In Forty-first International Conference on Machine Learning, 2024.\nCharlotte Bunne, Yusuf Roohani, Yanay Rosen, Ankit Gupta, Xikun Zhang, Marcel Roed, Theo\nAlexandrov, Mohammed AlQuraishi, Patricia Brennan, Daniel B Burkhardt, et al. How to build the\nvirtual cell with artificial intelligence: Priorities and opportunities. Cell, 187(25):7045–7063, 2024.\n68\nA Survey of Reinforcement Learning for Large Reasoning Models\nCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner,\nYining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization:\nEliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023.\nLucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent\nreinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications\nand Reviews), 38(2):156–172, 2008.\nLorenzo Canese, Gian Carlo Cardarilli, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Marco Re,\nand Sergio Spanò. Multi-agent reinforcement learning: A review of challenges and applications.\nApplied Sciences, 11(11):4948, 2021.\nMeng Cao, Haoze Zhao, Can Zhang, Xiaojun Chang, Ian Reid, and Xiaodan Liang. Ground-r1: Incen-\ntivizing grounded visual reasoning via reinforcement learning. arXiv preprint arXiv:2505.20272,\n2025.\nJun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio\nStarace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning\nagents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024.\nOlivier Chapelle and Alexander Zien. Semi-supervised classification by low density separation. In\nInternational workshop on artificial intelligence and statistics, pages 57–64. PMLR, 2005.\nAili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu,\nChao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning\nattention. arXiv preprint arXiv:2506.13585, 2025a.\nDing Chen, Qingchen Yu, Pengyuan Wang, Wentao Zhang, Bo Tang, Feiyu Xiong, Xinchi Li, Minchuan\nYang, and Zhiyu Li. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint\narXiv:2504.10481, 2025b.\nGuangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson, Jie Fu, and Yemin\nShi. Autoagents: A framework for automatic agent generation. arXiv preprint arXiv:2309.17288,\n2023a.\nHuayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu\nLiu, Jun Zhu, and Haoxiang Wang. Bridging supervised learning and reinforcement learning in\nmath reasoning. arXiv preprint arXiv:2505.18116, 2025c.\nJiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu,\nXuefeng Li, Jiaze Chen, et al. Enigmata: Scaling logical reasoning in large language models with\nsynthetic verifiable puzzles. arXiv preprint arXiv:2505.19914, 2025d.\nJunying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou,\nand Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms, 2024a. URL\nhttps://arxiv.org/abs/2412.18925.\nLei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Liming Zheng, Yufeng Zhong, and Lin Ma.\nBreaking the sft plateau: Multimodal structured reinforcement learning for chart-to-code generation.\narXiv preprint arXiv:2508.13587, 2025e.\nLei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Yufeng Zhong, and Lin Ma.\nChart-r1:\nChain-of-thought supervision and reinforcement for advanced chart reasoner.\narXiv preprint\narXiv:2507.15509, 2025f.\n69\nA Survey of Reinforcement Learning for Large Reasoning Models\nLiang Chen, Hongcheng Gao, Tianyu Liu, Zhiqi Huang, Flood Sung, Xinyu Zhou, Yuxin Wu, and\nBaobao Chang. G1: Bootstrapping perception and reasoning abilities of vision-language model via\nreinforcement learning. arXiv preprint arXiv:2505.13426, 2025g.\nLiang Chen, Xueting Han, Li Shen, Jing Bai, and Kam-Fai Wong. Beyond two-stage training: Coopera-\ntive sft and rl for llm reasoning, 2025h. URL https://arxiv.org/abs/2509.06948.\nLili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Self-questioning\nlanguage models. arXiv preprint arXiv:2508.03682, 2025i.\nMinghan Chen, Guikun Chen, Wenguan Wang, and Yi Yang. Seed-grpo: Semantic entropy enhanced\ngrpo for uncertainty-aware policy optimization. arXiv preprint arXiv:2505.12346, 2025j.\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen\nZhang, Huajun Chen, Fan Yang, et al. Learning to reason with search for llms via reinforcement\nlearning. arXiv preprint arXiv:2503.19470, 2025k.\nNuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He.\nJudgelrm: Large reasoning models as a judge. arXiv preprint arXiv:2504.00050, 2025l.\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu,\nYuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: A survey of long chain-of-thought\nfor reasoning large language models. arXiv preprint arXiv:2503.09567, 2025m.\nTianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Qiwei Liang, Zixuan Li, Xianliang Lin,\nYiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: A scalable data generator and benchmark with strong\ndomain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088,\n2025n.\nXiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier,\nYoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for llm reasoning. arXiv preprint\narXiv:2505.14970, 2025o.\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu,\nMengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of\no1-like llms. arXiv preprint arXiv:2412.21187, 2024b.\nXiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang,\nDenghui Zhang, Tong Zhang, et al.\nRm-r1: Reward modeling as reasoning.\narXiv preprint\narXiv:2505.02387, 2025p.\nYang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro,\nand Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement\nlearning. arXiv preprint arXiv:2505.16400, 2025q.\nYongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, and Chuchu Fan.\nR1-code-interpreter: Training llms to reason with code via supervised and reinforcement learning.\narXiv preprint arXiv:2505.21668, 2025r.\nYuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang Jiang, Zongqing Lu, Hao Dong,\nand Yaodong Yang. Bi-dexhands: Towards human-level bimanual dexterous manipulation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 46(5):2804–2818, 2023b.\n70\nA Survey of Reinforcement Learning for Large Reasoning Models\nYuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. Conrft: A\nreinforced fine-tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450,\n2025s.\nYukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo\nMolchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966,\n2025t.\nYuyang Chen, Kaiyan Zhao, Yiming Wang, Ming Yang, Jian Zhang, and Xiaoguang Niu. Enhancing\nllm agents for code generation with possibility and pass-rate prioritized experience replay. arXiv\npreprint arXiv:2410.12236, 2024c.\nZhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin\nZhao, Zheng Liu, Xu Miao, Yang Lu, et al. An empirical study on eliciting and improving r1-like\nreasoning models. arXiv preprint arXiv:2503.04548, 2025u.\nZhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi.\nPass@k training for adaptively balancing exploration and exploitation of large reasoning models.\narXiv preprint arXiv:2508.10751, 2025v.\nDaixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu\nWei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025a.\nJie Cheng, Ruixi Qiao, Lijun Li, Chao Guo, Junle Wang, Gang Xiong, Yisheng Lv, and Fei-Yue Wang.\nStop summation: Min-form credit assignment is all process reward model needs for reasoning.\narXiv preprint arXiv:2504.15275, 2025b.\nShuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenghai Wang, Qipeng\nGuo, Kai Chen, Biqing Qi*, and Bowen Zhou. Sdar: A synergistic diffusion–autoregression paradigm\nfor scalable sequence generation, 2025c. URL https://github.com/JetAstra/SDAR.\nZhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao\nZhuang, Nilabjo Dey, Yuheng Zha, et al. Revisiting reinforcement learning for llm reasoning from a\ncross-domain perspective. arXiv preprint arXiv:2506.14965, 2025d.\nMaxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo Perez-Vicente, Lucas Willems, Salem\nLahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & miniworld: Modular &\ncustomizable reinforcement learning environments for goal-oriented tasks. Advances in Neural\nInformation Processing Systems, 36:73383–73394, 2023.\nPrateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building\nproduction-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413,\n2025.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing systems,\n30, 2017.\nTianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V\nLe, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation\nmodel post-training. arXiv preprint arXiv:2501.17161, 2025a.\nXu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, and Weiping Li.\nQwen look again: Guiding vision-language reasoning models to re-attention visual information.\narXiv preprint arXiv:2505.23558, 2025b.\n71\nA Survey of Reinforcement Learning for Large Reasoning Models\nJiwan Chung, Junhyeok Kim, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, and Youngjae Yu. Don’t\nlook only once: Towards multimodal interactive reasoning with selective visual revisitation. arXiv\npreprint arXiv:2505.18842, 2025.\nTaco Cohen, David W Zhang, Kunhao Zheng, Yunhao Tang, Remi Munos, and Gabriel Synnaeve. Soft\npolicy optimization: Online off-policy rl for sequence models. arXiv preprint arXiv:2503.05453,\n2025.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon,\nMarcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with\nadvanced reasoning, multimodality, long context, and next generation agentic capabilities. arxiv\npreprint arXiv: 2507.06261, 2025.\nGanqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu\nYu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint\narXiv:2502.01456, 2025a.\nGanqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan,\nHuayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning\nlanguage models. arXiv preprint arXiv:2505.22617, 2025b.\nDamai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-\nof-experts language models. arXiv preprint arXiv:2401.06066, 2024.\nMuzhi Dai, Chenxu Yang, and Qingyi Si. S-grpo: Early exit via reinforcement learning in reasoning\nmodels. arXiv preprint arXiv:2505.07686, 2025a.\nYaxun Dai, Wenxuan Xie, Xialie Zhuang, Tianyu Yang, Yiying Yang, Haiqin Yang, Yuhang Zhao, Pingfu\nChao, and Wenhao Jiang. Reex-sql: Reasoning with execution-aware reinforcement learning for\ntext-to-sql. arXiv preprint arXiv:2505.12768, 2025b.\nJisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng,\nMeng Wang, and Tat-Seng Chua. Reinforcing video reasoning with focused thinking. arXiv preprint\narXiv:2505.24718, 2025.\nAlan Dao and Thinh Le. Rezero: Enhancing llm search ability by trying one-more-time. arXiv preprint\narXiv:2504.11001, 2025.\nAlan Dao and Dinh Bach Vu. Jan-nano technical report. arXiv preprint arXiv:2506.22760, 2025.\nAntoine Dedieu, Joseph Ortiz, Xinghua Lou, Carter Wendelken, Wolfgang Lehrach, J Swaroop\nGuntupalli, Miguel Lazaro-Gredilla, and Kevin Patrick Murphy. Improving transformer world\nmodels for data-efficient rl. In ICLR 2025 Workshop on World Models: Understanding, Modelling and\nScaling, 2025.\nJia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng\nGao, Wayne Xin Zhao, and Ji-Rong Wen. From trial-and-error to improvement: A systematic analysis\nof llm exploration mechanisms in rlvr. arXiv preprint arXiv:2508.07534, 2025a.\nYong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai,\nShuo Yang, Zhanwei Zhang, Qiwen Wang, et al. Atom-searcher: Enhancing agentic deep research\nvia fine-grained atomic thought reward. arXiv preprint arXiv:2508.12800, 2025b.\n72\nA Survey of Reinforcement Learning for Large Reasoning Models\nHongxin Ding, Baixiang Huang, and Yue Fang. Promed: Shapley information gain guided reinforce-\nment learning for proactive medical llms, 2025. URL https://arxiv.org/abs/2508.13514.\nZiluo Ding, Wanpeng Zhang, Junpeng Yue, Xiangjun Wang, Tiejun Huang, and Zongqing Lu. Entity\ndivider with language grounding in multi-agent reinforcement learning. In International Conference\non Machine Learning, pages 8103–8119. PMLR, 2023.\nDai Do, Manh Nguyen, Svetha Venkatesh, and Hung Le. Sparft: Self-paced reinforcement fine-tuning\nfor large language models. arXiv preprint arXiv:2508.05015, 2025.\nShibhansh Dohare, J Fernando Hernandez-Garcia, Qingfeng Lan, Parash Rahman, A Rupam Mahmood,\nand Richard S Sutton. Loss of plasticity in deep continual learning. Nature, 632(8026):768–774,\n2024.\nGuanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui\nZhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via\nreinforcement learning. arXiv preprint arXiv:2505.16410, 2025a.\nGuanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen,\nJiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv\npreprint arXiv:2507.19849, 2025b.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng\nZhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation\nmodel alignment. arXiv preprint arXiv:2304.06767, 2023.\nQingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, and Furu Wei. Reinforcement\npre-training. arXiv preprint arXiv:2506.08007, 2025c.\nAli Dorri, Salil S Kanhere, and Raja Jurdak. Multi-agent systems: A survey. Ieee Access, 6:28573–28593,\n2018.\nShihan Dou, Muling Wu, Jingwen Xu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Improving\nrl exploration for llm reasoning through retrospective replay. arXiv preprint arXiv:2504.14363,\n2025.\nMingzhe Du, Luu Anh Tuan, Yue Liu, Yuhao Qing, Dong Huang, Xinyi He, Qian Liu, Zejun Ma,\nand See-kiong Ng. Afterburner: Reinforcement learning facilitates self-improving code efficiency\noptimization. arXiv preprint arXiv:2505.23387, 2025a.\nYong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, and Yongliang\nShen. Test-time reinforcement learning for gui grounding via region consistency. arXiv preprint\narXiv:2508.05615, 2025b.\nChengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng\nLi, and Xihui Liu. Got-r1: Unleashing reasoning capability of mllm for visual generation with\nreinforcement learning. arXiv preprint arXiv:2505.17022, 2025.\nSam Earle, Graham Todd, Yuchen Li, Ahmed Khalifa, Muhammad Umair Nasir, Zehua Jiang, Andrzej\nBanburski-Fahey, and Julian Togelius. Puzzlejax: A benchmark for reasoning and learning, 2025.\nURL https://arxiv.org/abs/2508.16821.\nAhmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis\nSong, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with\nlarge reasoning models. arXiv preprint arXiv:2502.06807, 2025.\n73\nA Survey of Reinforcement Learning for Large Reasoning Models\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In Forty-first international conference on machine learning, 2024.\nBenjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl\nproblems. arXiv preprint arXiv:2103.06257, 2021.\nHugging Face. Open r1: A fully open reproduction of deepseek-r1, january 2025. URL https://github.\ncom/huggingface/open-r1, page 9, 2025.\nAdibvafa Fallahpour, Andrew Magnuson, Purav Gupta, Shihao Ma, Jack Naimer, Arnav Shah, Haonan\nDuan, Omar Ibrahim, Hani Goodarzi, Chris J. Maddison, and Bo Wang. BioReason: Incentivizing\nmultimodal biological reasoning within a DNA-LLM model. arXiv preprint arXiv: 2505.23579, 2025.\nRun-Ze Fan, Zengzhi Wang, and Pengfei Liu. Megascience: Pushing the frontiers of post-training\ndatasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025a.\nTiantian Fan, Lingjun Liu, Yu Yue, Jiaze Chen, Chengyi Wang, Qiying Yu, Chi Zhang, Zhiqi Lin, Ruofei\nZhu, Yufeng Yuan, et al. Truncated proximal policy optimization. arXiv preprint arXiv:2506.15050,\n2025b.\nYuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai\nZhu, Che Jiang, Yuchen Zhang, et al. Ssrl: Self-search reinforcement learning. arXiv preprint\narXiv:2508.10874, 2025c.\nYue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Naraya-\nnaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint\narXiv:2505.15879, 2025d.\nZiqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie. Chestx-reasoner:\nAdvancing radiology foundation models with reasoning through step-by-step verification. arXiv\npreprint arXiv:2504.20930, 2025e.\nWenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song,\nand Dacheng Tao. Serl: Self-play reinforcement learning for large language models with limited\ndata. arXiv preprint arXiv:2505.20347, 2025a.\nXueji Fang, Liyuan Ma, Zhiyang Chen, Mingyuan Zhou, and Guo-jun Qi. Inflvg: Reinforce inference-\ntime consistent long video generation with grpo. arXiv preprint arXiv:2505.17574, 2025b.\nYin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Ji-\nawei Han, and Zhiyong Lu. Cell-o1: Training LLMs to solve single-cell reasoning puzzles with\nreinforcement learning, 2025c.\nWu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, and Xiansheng Hua. Self-\nguided process reward optimization with redefined step-wise advantage for process reinforcement\nlearning. arXiv preprint arXiv:2507.01551, 2025a.\nXiang Fei, Siqi Wang, Shu Wei, Yuxiang Nie, Wei Shi, Hao Feng, and Can Huang. Post-completion\nlearning for language models. arXiv preprint arXiv:2507.20252, 2025b.\nJiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang,\nJinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv\npreprint arXiv:2504.11536, 2025a.\n74\nA Survey of Reinforcement Learning for Large Reasoning Models\nKaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu,\nXiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms.\narXiv preprint arXiv:2503.21776, 2025b.\nSicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. Efficient reasoning models: A survey.\narXiv preprint arXiv:2504.10903, 2025c.\nRoya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu,\nShuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications,\nchallenges, and the future. The International Journal of Robotics Research, 44(5):701–739, 2025.\nJakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.\nCounterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial\nintelligence, volume 32, 2018.\nTingchen Fu, Jiawei Gu, Yafu Li, Xiaoye Qu, and Yu Cheng. Scaling reasoning, losing control:\nEvaluating instruction following in large reasoning models. arXiv preprint arXiv:2505.14810,\n2025a.\nWei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei,\nJiashu Wang, et al. Areal: A large-scale asynchronous reinforcement learning system for language\nreasoning. arXiv preprint arXiv:2505.24298, 2025b.\nYuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao\nZhang, Yuanheng Zhu, and Dongbin Zhao. Srft: A single-stage method with supervised and\nreinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025c.\nYasuhiro Fujita. Experience replay with random reshuffling. arXiv preprint arXiv:2503.02269, 2025.\nMarcos Fuster-Pena, David de Fitero-Dominguez, Antonio Garcia-Cabot, and Eva Garcia-Lopez.\nRepaca: Leveraging reasoning large language models for static automated patch correctness\nassessment. arXiv preprint arXiv:2507.22580, 2025.\nKushal Gajjar, Harshit Sikchi, Arpit Singh Gautam, Marc Hammons, and Saurabh Jha. Cognisql-r1-\nzero: Lightweight reinforced reasoning for efficient sql generation. arXiv preprint arXiv:2507.06013,\n2025.\nZeyu Gan, Hao Yi, and Yong Liu. Cot-space: A theoretical framework for internal slow-thinking via\nreinforcement learning, 2025. URL https://arxiv.org/abs/2509.04027.\nKanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D Goodman. Cognitive\nbehaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint\narXiv:2503.01307, 2025.\nJiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu.\nBeyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv\npreprint arXiv:2508.07976, 2025a.\nLongxi Gao, Li Zhang, and Mengwei Xu. Uishift: Enhancing vlm-based gui agents through self-\nsupervised reinforcement learning. arXiv preprint arXiv:2505.12493, 2025b.\nFrancisco Garcia and Philip S Thomas. A meta-mdp approach to exploration for lifelong reinforcement\nlearning. Advances in Neural Information Processing Systems, 32, 2019.\n75\nA Survey of Reinforcement Learning for Large Reasoning Models\nJean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer, and Roberta Raileanu.\nBuilding a subspace of policies for scalable continual learning. arXiv preprint arXiv:2211.10445,\n2022.\nJonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R Bartoldson,\nBhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent\nreasoning: A recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025.\nXinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong\nWu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontiers of vision-language deep\nresearch agent. arXiv preprint arXiv:2508.05748, 2025.\nAli Essam Ghareeb, Benjamin Chang, Ludovico Mitchener, Angela Yiu, Caralyn J. Szostkiewicz, Jon M.\nLaurent, Muhammed T. Razzak, Andrew D. White, Michaela M. Hinks, and Samuel G. Rodriques.\nRobin: A multi-agent system for automating scientific discovery, 2025.\nMajid Ghasemi, Amir Hossein Moosavi, and Dariush Ebrahimi. A comprehensive survey of rein-\nforcement learning: From algorithms to practical challenges. arXiv preprint arXiv:2411.18892,\n2024.\nElliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falk-\nman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. Frontiermath: A\nbenchmark for evaluating advanced mathematical reasoning in ai. arXiv preprint arXiv:2411.04872,\n2024.\nShansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe\nZhang. Diffucoder: Understanding and improving masked diffusion models for code generation.\narXiv preprint arXiv:2506.20639, 2025.\nPrasoon Goyal, Scott Niekum, and Raymond J Mooney. Using natural language for reward shaping in\nreinforcement learning. arXiv preprint arXiv:1903.02020, 2019.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of\nmodels. arXiv preprint arXiv:2407.21783, 2024.\nJihao Gu, Qihang Ai, Yingyao Wang, Pi Bu, Jingxuan Xing, Zekun Zhu, Wei Jiang, Ziming Wang,\nYingxiu Zhao, Ming-Liang Zhang, et al. Mobile-r1: Towards interactive reinforcement learning for\nvlm-based mobile agent via task-level rewards. arXiv preprint arXiv:2506.20332, 2025.\nYu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava,\nYanan Xie, Peng Qi, et al. Is your llm secretly a world model of the internet? model-based planning\nfor web agents. arXiv preprint arXiv:2411.06559, 2024.\nZhong Guan, Likang Wu, Hongke Zhao, Jiahui Wang, and Le Wu. Recall-extend dynamics: Enhancing\nsmall language models through controlled exploration and refined offline integration. arXiv preprint\narXiv:2508.16677, 2025.\nLeon Guertler, Bobby Cheng, Simon Yu, Bo Liu, Leshem Choshen, and Cheston Tan. Textarena, 2025.\nURL https://arxiv.org/abs/2504.11442.\nAnisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics as\nrewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746,\n2025.\n76\nA Survey of Reinforcement Learning for Large Reasoning Models\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via\nreinforcement learning. arXiv preprint arXiv:2501.12948, 2025a.\nJiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, and Furu Wei. Reward\nreasoning model. arXiv preprint arXiv:2505.14674, 2025b.\nYiran Guo, Lijie Xu, Jie Liu, Dan Ye, and Shuang Qiu. Segment policy optimization: Effective\nsegment-level credit assignment in rl for large language models. arXiv preprint arXiv:2505.23564,\n2025c.\nYongxin Guo, Wenbo Deng, Zhenglin Cheng, and Xiaoying Tang. G2 rpo-a: Guided group relative\npolicy optimization with adaptive guidance. arXiv preprint arXiv:2508.13023, 2025d.\nZhihan Guo, Jiele Wu, Wenqian Cui, Yifei Zhang, Minda Hu, Yufei Wang, and Irwin King. From\ngeneral to targeted rewards: Surpassing gpt-4 in open-ended long-context generation. arXiv preprint\narXiv:2506.16024, 2025e.\nZiyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann\nHeng. Can we generate images with cot? let’s verify and reinforce image generation step by step.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025f.\nAbhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham Kakade, and Sergey Levine. Unpacking reward\nshaping: Understanding the benefits of reward engineering on sample complexity. Advances in\nNeural Information Processing Systems, 35:15281–15295, 2022.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International conference\non machine learning, pages 1861–1870. Pmlr, 2018.\nDylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse\nreward design. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran\nAssociates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/201\n7/file/32fdab6559cdfa4f167f8c31b9199643-Paper.pdf.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nAi Han, Junxing Hu, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, and Zicheng Zhang. Joyagents-\nr1: Joint evolution dynamics for versatile multi-llm agents with reinforcement learning. arXiv\npreprint arXiv:2506.19846, 2025.\nYizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural\nnetworks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(11):7436–\n7456, 2021.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong\nTian. Training large language models to reason in a continuous latent space. arXiv preprint\narXiv:2412.06769, 2024.\nBingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen\nHong, Longtao Huang, Hui Xue, et al. Air: A systematic analysis of annotations, instructions, and\nresponse pairs in preference dataset. arXiv preprint arXiv:2504.03612, 2025a.\n77\nA Survey of Reinforcement Learning for Large Reasoning Models\nFeng He, Zijun Chen, Xinnian Liang, Tingting Ma, Yunqi Qiu, Shuangzhi Wu, and Junchi Yan.\nProtoreasoning: Prototypes as the foundation for generalizable reasoning in llms. arXiv preprint\narXiv:2506.15211, 2025b.\nHaoyu He, Katrin Renz, Yong Cao, and Andreas Geiger. Mdpo: Overcoming the training-inference\ndivide of masked diffusion language models. arXiv preprint arXiv:2508.13148, 2025c.\nJujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang\nZhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint\narXiv:2505.22312, 2025d.\nQianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, and Jiangjie Chen. Thinkdial: An open recipe\nfor controlling reasoning effort in large language models. arXiv preprint arXiv:2508.18773, 2025e.\nTao He, Rongchuan Mu, Lizi Liao, Yixin Cao, Ming Liu, and Bing Qin. Good learners think their\nthinking: Generative prm makes large reasoning model more efficient math learner. arXiv preprint\narXiv:2507.23317, 2025f.\nXiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang.\nTempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324,\n2025g.\nZhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu,\nZhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: A large-scale, challenging, decontaminated,\nand verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456,\n2025h.\nMichael Hla. Pro-1, 2025. URL https://michaelhla.com/blog/pro1.html.\nHaitao Hong, Yuchen Yan, Xingyu Wu, Guiyang Hou, Wenqi Zhang, Weiming Lu, Yongliang Shen,\nand Jun Xiao. Cooper: Co-optimizing policy and reward models in reinforcement learning for large\nlanguage models. arXiv preprint arXiv:2508.05613, 2025a.\nIlgee Hong, Changlong Yu, Liang Qiu, Weixiang Yan, Zhenghao Xu, Haoming Jiang, Qingru Zhang,\nQin Lu, Xin Liu, Chao Zhang, et al. Think-rm: Enabling long-horizon reasoning in generative\nreward models. arXiv preprint arXiv:2505.16265, 2025b.\nJixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, and Rui Yan. Reinforcing\nmultimodal understanding and generation with dual self-rewards. arXiv preprint arXiv:2506.07963,\n2025c.\nZhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. Treerl: Llm reinforcement\nlearning with on-policy tree search. arXiv preprint arXiv:2506.11902, 2025.\nHaichuan Hu, Xiaochen Xie, and Quanjun Zhang. Repair-r1: Better test before repair. arXiv preprint\narXiv:2507.22853, 2025a.\nJian Hu. Reinforce++: A simple and efficient approach for aligning large language models. arXiv\npreprint arXiv:2501.03262, 2025.\nJian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An easy-to-use,\nscalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024a.\nJingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-\nreasoner-zero: An open source approach to scaling up reinforcement learning on the base model.\narXiv preprint arXiv:2503.24290, 2025b.\n78\nA Survey of Reinforcement Learning for Large Reasoning Models\nLanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P Xing, Ion Stoica, Tajana Rosing,\nHaojian Jin, and Hao Zhang. lmgame-bench: How good are llms at playing games? arXiv preprint\narXiv:2505.15146, 2025c.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,\nYuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models\nwith scalable training strategies. arXiv preprint arXiv:2404.06395, 2024b.\nYujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, and\nChangjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping. Advances\nin Neural Information Processing Systems, 33:15931–15941, 2020.\nZhiting Hu and Tianmin Shu. Language models, agent models, and world models: The law for\nmachine reasoning and planning. arXiv preprint arXiv:2312.05230, 2023.\nErmo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, Ning Ding, Xingtai Lv, Kai Tian, and Bowen Zhou. Intuitive\nfine-tuning: Towards simplifying alignment into a single process. arXiv preprint arXiv:2405.11870,\n2024.\nMaggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Gra-\nham Neubig, and Xiang Yue. Does math reasoning improve general llm capabilities? understanding\ntransferability of llm reasoning. arXiv preprint arXiv:2507.00432, 2025.\nChengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin\nHuang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint\narXiv:2508.05004, 2025a.\nShengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun Wang.\nThe 37 implementation details of proximal policy optimization. The ICLR Blog Track 2023, 2022.\nTing Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene\nunderstanding. arXiv preprint arXiv:2507.23478, 2025b.\nWenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and\nShaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models.\narXiv preprint arXiv:2503.06749, 2025c.\nYanxing Huang, Xinling Jin, Sijie Liang, Peng Li, and Yang Liu. Formarl: Enhancing autoformalization\nwith no labeled data. arXiv preprint arXiv:2508.18914, 2025d.\nYuzhen Huang, Weihao Zeng, Xingshan Zeng, Qi Zhu, and Junxian He. Pitfalls of rule-and model-based\nverifiers–a case study on mathematical reasoning. arXiv preprint arXiv:2505.22203, 2025e.\nZenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Peng, Jiaqi Hu,\nZhanming Shen, Xiaomeng Hu, et al. Reinforcement learning with rubric anchors. arXiv preprint\narXiv:2508.12790, 2025f.\nZeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M Ponti, and Ivan Titov. Blend-\ning supervised and reinforcement fine-tuning with prefix sampling. arXiv preprint arXiv:2507.01679,\n2025g.\nHugging Face. Open r1: A fully open reproduction of deepseek-r1, January 2025. URL https:\n//github.com/huggingface/open-r1.\n79\nA Survey of Reinforcement Learning for Large Reasoning Models\nDom Huh and Prasant Mohapatra. Multi-agent reinforcement learning: A comprehensive survey.\narXiv preprint arXiv:2312.10256, 2023.\nJemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun,\nand Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 4\n(26):eaau5872, 2019.\nBaichuan Inc. Baichuan-m1: Pushing the medical capability of large language models. arXiv preprint\narXiv::2502.12671, 2025.\nPhysical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess,\nAdnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. pi 0.5: a vision-language-action\nmodel with open-world generalization. arXiv preprint arXiv:2504.16054, 2025.\nEdward L Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics,\n17(2):295–311, 2008.\nAna-Maria Istrate, Fausto Milletari, Fabrizio Castrotorres, Jakub M Tomczak, Michaela Torkar, Donghui\nLi, and Theofanis Karaletsos. rbio1-training scientific reasoning LLMs with biological world models\nas soft verifiers. bioRxiv 2025.08.18.670981, 2025.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024.\nNaman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2e-gym:\nProcedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint\narXiv:2504.07164, 2025.\nKunal Jha, Wilka Carvalho, Yancheng Liang, Simon Shaolei Du, Max Kleiman-Weiner, and Natasha\nJaques. Cross-environment cooperation enables zero-shot multi-agent coordination. In Forty-second\nInternational Conference on Machine Learning, 2025.\nHaozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, and Minlie Huang. Towards\nefficient exact optimization of language model alignment. arXiv preprint arXiv:2402.00856, 2024.\nXingguang Ji, Yahui Liu, Qi Wang, Jingyuan Zhang, Yang Yue, Rui Shi, Chenxi Sun, Fuzheng Zhang,\nGuorui Zhou, and Kun Gai. Leanabell-prover-v2: Verifier-integrated reasoning for formal theorem\nproving via reinforcement learning. arXiv preprint arXiv:2507.08649, 2025a.\nYuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang,\nMengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: A unified brain model for robotic manipulation\nfrom abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference,\npages 1724–1734, 2025b.\nRuipeng Jia, Yunyi Yang, Yongbo Gai, Kai Luo, Shihao Huang, Jianhe Lin, Xiaoxi Jiang, and Guanjun\nJiang. Writing-zero: Bridge the gap between non-verifiable tasks and verifiable rewards. arXiv\ne-prints, pages arXiv–2506, 2025.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of\nexperts. arXiv preprint arXiv:2401.04088, 2024.\nDaniel R Jiang, Alex Nikulkov, Yu-Chia Chen, Yang Bai, and Zheqing Zhu. Improving generative ad\ntext on facebook using reinforcement learning. arXiv preprint arXiv:2507.21983, 2025a.\n80\nA Survey of Reinforcement Learning for Large Reasoning Models\nDongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann\nHeng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level\nand token-level cot. arXiv preprint arXiv:2505.00703, 2025b.\nJingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, and Chao Ma. Co-reinforcement learning for\nunified multimodal understanding and generation. arXiv preprint arXiv:2505.17534, 2025c.\nPengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, and Jiawei Han.\ns3: You don’t need that much data to train a search agent via rl. arXiv preprint arXiv:2505.14146,\n2025d.\nYuhua Jiang, Yuwen Xiong, Yufeng Yuan, Chao Xin, Wenyuan Xu, Yu Yue, Qianchuan Zhao, and Lin\nYan. Pag: Multi-turn reinforced llm self-correction with policy as generative verifier. arXiv preprint\narXiv:2506.10406, 2025e.\nYuqian Jiang, Suda Bharadwaj, Bo Wu, Rishi Shah, Ufuk Topcu, and Peter Stone. Temporal-logic-based\nreward shaping for continuing reinforcement learning tasks. In Proceedings of the AAAI Conference\non artificial Intelligence, volume 35, pages 7995–8003, 2021.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik\nNarasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint\narXiv:2310.06770, 2023.\nBowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O Arik, and Jiawei Han. An empirical study on\nreinforcement learning for reasoning-search interleaved llm agents. arXiv preprint arXiv:2505.15117,\n2025a.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and\nJiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement\nlearning. arXiv preprint arXiv:2503.09516, 2025b.\nCan Jin, Yang Zhou, Qixin Zhang, Hongwu Peng, Di Zhang, Marco Pavone, Ligong Han, Zhang-Wei\nHong, Tong Che, and Dimitris N Metaxas. Your reward function for rl is your best prm for search:\nUnifying rl and search-based tts. arXiv preprint arXiv:2508.14313, 2025c.\nHangzhan Jin, Sicheng Lv, Sifan Wu, and Mohammad Hamdaqa. Rl is neither a panacea nor a\nmirage: Understanding supervised vs. reinforcement learning fine-tuning for llms. arXiv preprint\narXiv:2508.16546, 2025d.\nZhehan Kan, Yanlin Liu, Kun Yin, Xinghua Jiang, Xin Li, Haoyu Cao, Yinsong Liu, Deqiang Jiang, Xing\nSun, Qingmin Liao, et al. Taco: Think-answer consistency for optimized long-chain reasoning and\nefficient data learning via reinforcement learning in lvlms. arXiv preprint arXiv:2505.20777, 2025.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv\npreprint arXiv:2001.08361, 2020.\nAmirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron\nCourville, and Nicolas Le Roux. VinePPO: Refining credit assignment in RL training of LLMs. In\nForty-second International Conference on Machine Learning, 2025. URL https://openreview.n\net/forum?id=Myx2kJFzAn.\nKasia Z. Kedzierska, Lorin Crawford, Ava P. Amini, and Alex X. Lu. Assessing the limits of zero-shot\nfoundation models in single-cell biology, 2023.\n81\nA Survey of Reinforcement Learning for Large Reasoning Models\nMoo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing\nspeed and success. arXiv preprint arXiv:2502.19645, 2025.\nRobert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward\nGrefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and\ndiversity. arXiv preprint arXiv:2310.06452, 2023.\nAndrew Kiruluta, Andreas Lemos, and Priscilla Burity. A self-supervised reinforcement learning\napproach for fine-tuning large language models using cross-attention signals.\narXiv preprint\narXiv:2502.10482, 2025.\nWeijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu,\nJianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models.\narXiv preprint arXiv:2412.03603, 2024.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating\nSystems Principles, 2023.\nInception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer\nBirnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, et al. Mercury: Ultra-fast language models\nbased on diffusion. arXiv preprint arXiv:2506.17298, 2025.\nHanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao,\nYuxiao Dong, and Jie Tang. Computerrl: Scaling end-to-end online reinforcement learning for\ncomputer use agents, 2025. URL https://arxiv.org/abs/2508.14040.\nNathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,\nLester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open\nlanguage model post-training. arXiv preprint arXiv:2411.15124, 2024.\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in representation learning, ICML, volume 3, page\n896. Atlanta, 2013.\nDong Won Lee, Hae Won Park, Yoon Kim, Cynthia Breazeal, and Louis-Philippe Morency. Improving\ndialogue agents by decomposing one global explicit annotation with local implicit multimodal\nfeedback. arXiv preprint arXiv:2403.11330, 2024a.\nDong Won Lee, Hae Won Park, Cynthia Breazeal, and Louis-Philippe Morency. Aligning dialogue\nagents with global feedback via large language model reward decomposition. arXiv preprint\narXiv:2505.15922, 2025.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren Lu, Colton\nBishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF vs. RLHF:\nScaling reinforcement learning from human feedback with AI feedback. In Ruslan Salakhutdinov,\nZico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp,\neditors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of\nProceedings of Machine Learning Research, pages 26874–26901. PMLR, 21–27 Jul 2024b. URL\nhttps://proceedings.mlr.press/v235/lee24t.html.\nChengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou\nWang, Xiang Wang, Junyang Lin, et al. Cort: Code-integrated reasoning within thinking. arXiv\npreprint arXiv:2506.09820, 2025a.\n82\nA Survey of Reinforcement Learning for Large Reasoning Models\nChunmao Li, Yang Li, Yinliang Zhao, Peng Peng, and Xupeng Geng. Sler: Self-generated long-term\nexperience replay for continual reinforcement learning. Applied Intelligence, 51(1):185–201, 2021.\nDerek Li, Jiaming Zhou, Amirreza Kazemi, Qianyi Sun, Abbas Ghaddar, Mohammad Ali Alomrani,\nLiheng Ma, Yu Luo, Dong Li, Feng Wen, et al. Omni-think: Scaling cross-domain generalization in\nllms via multi-task rl with hybrid rewards. arXiv preprint arXiv:2507.14783, 2025b.\nHao Li, He Cao, Bin Feng, Yanjun Shao, Xiangru Tang, Zhiyuan Yan, Li Yuan, Yonghong Tian, and\nYu Li. Beyond chemical qa: Evaluating llm’s chemical reasoning with modular chemical operations.\narXiv preprint arXiv:2505.21318, 2025c.\nHu Li, Xuezhong Qian, and Wei Song. Prioritized experience replay based on dynamics priority.\nScientific Reports, 14(1):6014, 2024a.\nHuao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, and Katia\nSycara. Theory of mind for multi-agent collaboration via large language models. arXiv preprint\narXiv:2310.10701, 2023a.\nJia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul,\nLonghui Yu, Albert Q Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths\nwith 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024b.\nJunlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for\nevaluating alignment. arXiv preprint arXiv:2310.05470, 2023b.\nJunzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo:\nUnlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025d.\nKuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan\nLi, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent.\narXiv preprint arXiv:2507.02592, 2025e.\nPeiji Li, Jiasheng Ye, Yongkang Chen, Yichuan Ma, Zijie Yu, Kedi Chen, Ganqu Cui, Haozhan Li,\nJiacheng Chen, Chengqi Lyu, et al. Internbootcamp technical report: Boosting llm reasoning with\nverifiable task scaling. arXiv preprint arXiv:2508.08636, 2025f.\nPengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence is\nall you need: Few-shot rl fine-tuning of language models. arXiv preprint arXiv:2506.06395, 2025g.\nTianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lan-\nchantin, and Tianlu Wang. Jointly reinforcing diversity and quality in language model generations,\n2025h. URL https://arxiv.org/abs/2509.02534.\nWeizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang,\nQianben Chen, Weichen Sun, Qiexiang Wang, et al. Chain-of-agents: End-to-end agent foundation\nmodels via multi-agent distillation and agentic rl. arXiv preprint arXiv:2508.13167, 2025i.\nWenjie Li, Yujie Zhang, and Haoran Sun. Cx-mind: A pioneering multimodal large language model\nfor interleaved reasoning in chest x-ray via curriculum-guided reinforcement learning, 2025j. URL\nhttps://arxiv.org/abs/2508.03733.\nXiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan\nSadagopan, and Anurag Beniwal. When thinking fails: The pitfalls of reasoning for instruction-\nfollowing in llms. arXiv preprint arXiv:2505.11423, 2025k.\n83\nA Survey of Reinforcement Learning for Large Reasoning Models\nXiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and\nZhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability.\narXiv preprint arXiv:2504.21776, 2025l.\nXingxuan Li, Yao Xiao, Dianwen Ng, Hai Ye, Yue Deng, Xiang Lin, Bin Wang, Zhanfeng Mo, Chong\nZhang, Yueyi Zhang, et al. Miromind-m1: An open-source advancement in mathematical reasoning\nvia context-aware multi-stage policy optimization. arXiv preprint arXiv:2507.14683, 2025m.\nXinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang,\nand Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning.\narXiv preprint arXiv:2504.06958, 2025n.\nXuefeng Li, Haoyang Zou, and Pengfei Liu.\nLimr: Less is more for rl scaling.\narXiv preprint\narXiv:2502.11886, 2025o.\nXuefeng Li, Haoyang Zou, and Pengfei Liu.\nTorl: Scaling tool-integrated rl.\narXiv preprint\narXiv:2503.23383, 2025p.\nYizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou,\nXingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang,\nGe Zhang, and Wenhao Huang. Treepo: Bridging the gap of policy optimization and efficacy and\ninference efficiency with heuristic tree-based modeling. arXiv preprint arXiv:2508.17445, 2025q.\nYue Li, Meng Tian, Dechang Zhu, Jiangtong Zhu, Zhenyu Lin, Zhiwei Xiong, and Xinhai Zhao. Drive-\nr1: Bridging reasoning and planning in vlms for autonomous driving with reinforcement learning.\narXiv preprint arXiv:2506.18234, 2025r.\nYuetai Li, Zhangchen Xu, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin,\nXiang Yue, and Radha Poovendran. Temporal sampling for forgotten reasoning in llms. arXiv\npreprint arXiv:2505.20196, 2025s.\nZaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Weili Guan, Dongmei Jiang, and Liqiang Nie.\nOptimus-3: Towards generalist multimodal minecraft agents with scalable task experts. arXiv\npreprint arXiv:2506.10357, 2025t.\nZhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu,\nJunhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: A survey of reasoning\nlarge language models. arXiv preprint arXiv:2502.17419, 2025u.\nZiniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: A\nsimple, effective, and efficient reinforcement learning method for aligning large language models.\narXiv preprint arXiv:2310.10505, 2023c.\nJing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, and Jianye Hao. Squeeze\nthe soaked sponge: Efficient off-policy reinforcement finetuning for large language model. arXiv\npreprint arXiv:2507.06892, 2025a.\nXiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu,\nand Weizhu Chen. Sws: Self-aware weakness-driven problem synthesis in reinforcement learning\nfor llm reasoning. arXiv preprint arXiv:2506.08989, 2025b.\nXiao Liang, Zhongzhi Li, Yeyun Gong, Yelong Shen, Ying Nian Wu, Zhijiang Guo, and Weizhu\nChen. Beyond pass@ 1: Self-play with variational problem synthesis sustains rlvr. arXiv preprint\narXiv:2508.14029, 2025c.\n84\nA Survey of Reinforcement Learning for Large Reasoning Models\nZhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and\nLiang Zheng. Aesthetic post-training diffusion models from generic preferences with step-by-step\npreference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference,\npages 13199–13208, 2025d.\nJianxing Liao, Tian Zhang, Xiao Feng, Yusong Zhang, Rui Yang, Haorui Wang, Bosi Wen, Ziying Wang,\nand Runzhi Shi. Rlmr: Reinforcement learning with mixed rewards for creative writing. arXiv\npreprint arXiv:2508.18642, 2025a.\nMengqi Liao, Xiangyu Xi, Ruinian Chen, Jia Leng, Yangen Hu, Ke Zeng, Shuai Liu, and Huaiyu\nWan. Enhancing efficiency and exploration in reinforcement learning for llms. arXiv preprint\narXiv:2505.18573, 2025b.\nZhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, and Zhijie Deng.\nImproved visual-spatial reasoning via r1-zero-like training. arXiv preprint arXiv:2504.00883, 2025c.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/f\norum?id=v8L0pN6EOi.\nDarryl Lin, Sachin Talathi, and Sreekanth Annapureddy. Fixed point quantization of deep convolutional\nnetworks. In International conference on machine learning, pages 2849–2858. PMLR, 2016.\nHongyu Lin, Yuchen Li, Haoran Luo, Kaichun Yao, Libo Zhang, Mingjie Xing, and Yanjun Wu.\nOs-r1: Agentic operating system kernel tuning with reinforcement learning.\narXiv preprint\narXiv:2508.12551, 2025a.\nJiacheng Lin and Zhenbang Wu. Training llms for ehr-based reasoning tasks via reinforcement\nlearning, 2025. URL https://arxiv.org/abs/2505.24105.\nWang Lin, Liyu Jia, Wentao Hu, Kaihang Pan, Zhongqi Yue, Wei Zhao, Jingyuan Chen, Fei Wu,\nand Hanwang Zhang. Reasoning physical video generation with diffusion timestep tokens via\nreinforcement learning. arXiv preprint arXiv:2504.15932, 2025b.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437, 2024.\nBo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston\nTan, Weiyan Shi, Min Lin, et al. Spiral: Self-play on zero-sum games incentivizes reasoning via\nmulti-agent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025a.\nFangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. Video-t1:\nTest-time scaling for video generation. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, 2025b.\nJia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, ShaoGuo Liu, and TingTing Gao.\nEttrl: Balancing exploration and exploitation in llm test-time reinforcement learning via entropy\nmechanism. arXiv preprint arXiv:2508.11356, 2025c.\nJiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. https:\n//github.com/ganler/code-r1, 2025.\n85\nA Survey of Reinforcement Learning for Large Reasoning Models\nJie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang,\nand Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint\narXiv:2505.05470, 2025d.\nJie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang,\nWenyu Qin, Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint\narXiv:2501.13918, 2025e.\nJijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What\ncan rl bring to vla generalization? an empirical study. arXiv preprint arXiv:2505.19789, 2025f.\nJunteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng,\nAili Chen, Shiqi Chen, et al. Synlogic: Synthesizing verifiable reasoning data at scale for learning\nlogical reasoning and beyond. arXiv preprint arXiv:2505.19641, 2025g.\nLijun Liu and Ruiyang Li. Efficient medical vie via reinforcement learning, 2025. URL https:\n//arxiv.org/abs/2506.13363.\nLiyuan Liu, Feng Yao, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Flashrl:\n8bit rollouts, full power rl, August 2025h. URL https://fengyao.notion.site/flash-rl.\nMingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl:\nProlonged reinforcement learning expands reasoning boundaries in large language models. arXiv\npreprint arXiv:2505.24864, 2025i.\nMingyang Liu, Gabriele Farina, and Asuman Ozdaglar. Uft: Unifying supervised and reinforcement\nfine-tuning. arXiv preprint arXiv:2505.16984, 2025j.\nRuntao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo:\nOmni-preference alignment for video diffusion generation. In Proceedings of the Computer Vision\nand Pattern Recognition Conference, pages 8009–8019, 2025k.\nRunze Liu, Fengshuo Bai, Yali Du, and Yaodong Yang.\nMeta-reward-net: Implicitly differen-\ntiable reward learning for preference-based reinforcement learning. In S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Process-\ning Systems, volume 35, pages 22270–22284. Curran Associates, Inc., 2022.\nURL https:\n//proceedings.neurips.cc/paper_files/paper/2022/file/8be9c134bb193d8\nbd3827d4df8488228-Paper-Conference.pdf.\nRunze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen\nZhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint\narXiv:2502.06703, 2025l.\nShudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei\nZhang, Derek F Wong, Songyang Zhang, et al. Compassverifier: A unified and robust verifier for\nllms evaluation and outcome reward. arXiv preprint arXiv:2508.03686, 2025m.\nShuo Liu, Zeyu Liang, Xueguang Lyu, and Christopher Amato. Llm collaboration with multi-agent\nreinforcement learning. arXiv preprint arXiv:2508.04652, 2025n.\nTianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu.\nStatistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657,\n2023a.\n86\nA Survey of Reinforcement Learning for Large Reasoning Models\nWei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and\nJunxian He. Learn to reason efficiently with adaptive length-based reward shaping. arXiv preprint\narXiv:2505.15612, 2025o.\nXingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data\nwith rectified flow. In The Eleventh International Conference on Learning Representations, 2023b.\nYifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, and Mao\nYang. rstar-coder: Scaling competitive code reasoning with a large-scale verified dataset. arXiv\npreprint arXiv:2505.21297, 2025p.\nYizhou Liu and Jingwei Wei. Breaking reward collapse: Adaptive reinforcement for open-ended\nmedical reasoning with enhanced semantic discrimination, 2025. URL https://arxiv.org/ab\ns/2508.12957.\nYuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and\nFei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners.\narXiv preprint arXiv:2504.14239, 2025q.\nZexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, and Siheng Chen. Ml-\nagent: Reinforcing llm agents for autonomous machine learning engineering.\narXiv preprint\narXiv:2505.23723, 2025r.\nZichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha\nmoment in r1-zero-like training—a pilot study, 2025s.\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin.\nUnderstanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783, 2025t.\nZihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, and Wei\nPing. Acereason-nemotron 1.1: Advancing math and code reasoning through sft and rl synergy.\narXiv preprint arXiv:2506.13284, 2025u.\nZihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong,\nJu Huang, Jian Hu, et al. Part i: Tricks or traps? a deep dive into rl for llm reasoning. arXiv preprint\narXiv:2508.08221, 2025v.\nZijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-\ntime scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025w.\nZiyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi\nWang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025x.\nZongkai Liu, Fanqing Meng, Lingxiao Du, Zhixiang Zhou, Chao Yu, Wenqi Shao, and Qiaosheng\nZhang. Cpgd: Toward stable rule-based reinforcement learning for language models. arXiv preprint\narXiv:2505.12504, 2025y.\nRyan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-\nagent actor-critic for mixed cooperative-competitive environments. Advances in neural information\nprocessing systems, 30, 2017.\nDakuan Lu, Xiaoyu Tan, Rui Xu, Tianchu Yao, Chao Qu, Wei Chu, Yinghui Xu, and Yuan Qi. Scp-116k:\nA high-quality problem-solution dataset and a generalized pipeline for automated extraction in the\nhigher education science domain. arXiv preprint arXiv:2501.15587, 2025a.\n87\nA Survey of Reinforcement Learning for Large Reasoning Models\nFanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Arpo: End-to-end policy optimization\nfor gui agents with experience replay. arXiv preprint arXiv:2505.16282, 2025b.\nGuanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong\nTang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable\nreinforcement learning. arXiv preprint arXiv:2505.18719, 2025c.\nJianqiao Lu, Zhiyang Dou, Hongru Wang, Zeyu Cao, Jianbo Dai, Yunlong Feng, and Zhijiang Guo.\nAutopsv: Automated process-supervised verifier. Advances in Neural Information Processing Systems,\n37:79935–79962, 2024.\nQuanfeng Lu, Zhantao Ma, Shuai Zhong, Jin Wang, Dahai Yu, Michael K Ng, and Ping Luo. Swirl:\nA staged workflow for interleaved reinforcement learning in mobile gui control. arXiv preprint\narXiv:2508.20018, 2025d.\nSongshuo Lu, Hua Wang, Zhi Chen, and Yaohua Tang. Urpo: A unified reward & policy optimization\nframework for large language models. arXiv preprint arXiv:2507.17515, 2025e.\nZhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing\nXiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement\nlearning. arXiv preprint arXiv:2503.21620, 2025f.\nFan-Ming Luo, Tian Xu, Hang Lai, Xiong-Hui Chen, Weinan Zhang, and Yang Yu. A survey on\nmodel-based reinforcement learning. Science China Information Sciences, 67(2):121101, 2024.\nHaotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and\nDacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv\npreprint arXiv:2501.12570, 2025a.\nMichael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel\nXin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder:\nA fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site\n/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c1468\n0b3bee5eb349a512a51, 2025b. Notion Blog.\nMichael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y Tang, Manan Roongta, Colin Cai,\nJeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with a 1.5 b\nmodel by scaling rl. Notion Blog, 2025c.\nRun Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: A generalist r1-style vision-language action\nmodel for gui agents. arXiv preprint arXiv:2504.10458, 2025d.\nXufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K Qiu, and\nYuqing Yang. Agent lightning: Train any ai agents with reinforcement learning. arXiv preprint\narXiv:2508.03680, 2025e.\nXingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Lixuan He, Xuekai Zhu,\nKaiyan Zhang, Bingning Wang, et al. Towards a unified view of large language model post-training.\narXiv preprint arXiv:2509.04419, 2025.\nChengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin\nLi, Qian Zhao, Haian Huang, et al. Exploring the limit of outcome reward for learning mathematical\nreasoning. arXiv preprint arXiv:2502.06781, 2025.\n88\nA Survey of Reinforcement Learning for Large Reasoning Models\nLu Ma, Hao Liang, Meiyi Qiang, Lexiang Tang, Xiaochen Ma, Zhen Hao Wong, Junbo Niu, Chengyu\nShen, Runming He, Bin Cui, et al. Learning what reinforcement learning can’t: Interleaved online\nfine-tuning for hardest questions. arXiv preprint arXiv:2506.07527, 2025a.\nNanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang,\nYandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond\nscaling denoising steps. arXiv preprint arXiv:2501.09732, 2025b.\nXueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner:\nAdvancing llm reasoning across all domains. arXiv preprint arXiv:2505.14652, 2025c.\nDakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-\nPhilipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint\narXiv:2410.12832, 2024.\nJustus Mattern, Sami Jaghouar, Manveer Basra, Jannik Straube, Matthew Di Ferrante, Felix Gabriel,\nJack Min Ong, Vincent Weisser, and Johannes Hagemann. Synthetic-1: Two million collaboratively\ngenerated reasoning traces from deepseek-r1, 2025. URL https://www.primeintellect.ai/\nblog/synthetic-1-release.\nJianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xinyu Cai,\nXing Gao, Yu Yang, et al. O2-searcher: A searching-based agent model for open-domain open-ended\nquestion answering. arXiv preprint arXiv:2505.16582, 2025.\nGrégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a bench-\nmark for general ai assistants. In The Twelfth International Conference on Learning Representations,\n2023.\nThomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based reinforcement\nlearning: A survey. Foundations and Trends® in Machine Learning, 16(1):1–118, 2023.\nIvan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schif-\nferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical\nreasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025.\nSagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, and Hao Peng. Reinforcement learning finetunes\nsmall subnetworks in large language models. arXiv preprint arXiv:2505.11711, 2025.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nJaehyun Nam, Jinsung Yoon, Jiefeng Chen, Jinwoo Shin, Sercan Ö Arık, and Tomas Pfister. Mle-\nstar: Machine learning engineering agent via search and targeted refinement. arXiv preprint\narXiv:2506.15692, 2025.\nSiddharth M. Narayanan, James D. Braza, Ryan-Rhys Griffiths, Albert Bou, Geemi Wellawatte,\nMayk Caldas Ramos, Ludovico Mitchener, Samuel G. Rodriques, and Andrew D. White. Training a\nscientific reasoning model for chemistry. arXiv preprint arXiv: 2506.17238, 2025.\nDeepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens,\nAmar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, et al. Mlgym: A new\nframework and benchmark for advancing ai research agents. arXiv preprint arXiv:2502.14499,\n2025.\n89\nA Survey of Reinforcement Learning for Large Reasoning Models\nShen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong\nWen, and Chongxuan Li. Large language diffusion models, 2025. URL https://arxiv.org/ab\ns/2502.09992.\nDatta Nimmaturi, Vaishnavi Bhargava, Rajat Ghosh, Johnu George, and Debojyoti Dutta. Predictive\nscaling laws for efficient grpo training of large reasoning models. arXiv preprint arXiv:2507.18014,\n2025.\nEmmanuel Noutahi, Jason Hartford, Prudencio Tossou, Shawn Whitfield, Alisandra K. Denton, Cas\nWognum, Kristina Ulicna, Michael Craig, Jonathan Hsu, Michael Cuccarese, et al. Virtual cells:\nPredict, explain, discover, 2025.\nAlexander Novikov, Ngân V˜u, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner,\nSergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: A\ncoding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025.\nHumza Nusrat. Autonomous radiotherapy treatment planning using dola: A privacy-preserving,\nllm-based optimization agent, 2025. URL https://arxiv.org/abs/2503.17553.\nNVIDIA-NeMo. Nemo rl: A scalable and efficient post-training library. https://github.com/NVI\nDIA-NeMo/RL, 2025. GitHub repository.\nHayeon Oh.\nLaviplan:\nLanguage-guided visual path planning with rlvr.\narXiv preprint\narXiv:2507.12911, 2025.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia,\nYuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656,\n2024.\nOpenAI. Introducing gpt-4o image generation. https://openai.com/index/introducing-4\no-image-generation/, 2024. Accessed: 2025-08-25.\nOpenAI. Gpt-5 system card. Blog, 2025a.\nOpenAI. Openai o3 and o4-mini system card. Blog, 2025b.\nKun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun.\nSpacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural information processing systems, 35:27730–\n27744, 2022.\nAbby O’Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham\nLee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic\nlearning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International\nConference on Robotics and Automation (ICRA), pages 6892–6903. IEEE, 2024.\nChaofan Pan, Xin Yang, Yanhua Li, Wei Wei, Tianrui Li, Bo An, and Jiye Liang. A survey of continual\nreinforcement learning. arXiv preprint arXiv:2506.21872, 2025a.\nJiadong Pan, Zhiyuan Ma, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Self-reflective reinforcement\nlearning for diffusion-based image reasoning generation. arXiv preprint arXiv:2505.22407, 2025b.\n90\nA Survey of Reinforcement Learning for Large Reasoning Models\nJiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang.\nTraining software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139,\n2024.\nJiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr.\nTinyzero.\nhttps://github.com/Jiayi-Pan/TinyZero, 2025c. Accessed: 2025-01-24.\nJiazhen Pan, Che Liu, and Junde Wu. Medvlm-r1: Incentivizing medical reasoning capability of\nvision-language models (vlms) via reinforcement learning, 2025d. URL https://arxiv.org/ab\ns/2502.19634.\nKaihang Pan, Wendong Bu, Yuruo Wu, Yang Wu, Kai Shen, Yunfei Li, Hang Zhao, Juncheng Li,\nSiliang Tang, and Yueting Zhuang. Focusdiff: Advancing fine-grained text-image alignment for\nautoregressive visual generation through rl. arXiv preprint arXiv:2506.05501, 2025e.\nZhenyu Pan and Han Liu. Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse.\narXiv preprint arXiv:2503.18470, 2025.\nShubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang,\nJames Caverlee, Dileep Kalathil, et al. Curriculum reinforcement learning from easy to hard tasks\nimproves llm reasoning. arXiv preprint arXiv:2506.06632, 2025.\nChanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim.\nMaporl: Multi-agent post-co-training for collaborative large language models with reinforcement\nlearning. arXiv preprint arXiv:2502.18439, 2025.\nJupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro.\nReuse, don’t retrain: A recipe for continued pretraining of language models.\narXiv preprint\narXiv:2407.07263, 2024.\nGuilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Pi-\nqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra.\nCodeforces cots. https://huggingface.co/datasets/open-r1/codeforces-cots, 2025.\nXue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Example-guided\ndeep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG),\n37(4):1–14, 2018.\nJulien Perolat, Bart De Vylder, Daniel Hennes, Eugene Tarassov, Florian Strub, Vincent de Boer, Paul\nMuller, Jerome T Connor, Neil Burch, Thomas Anthony, et al. Mastering the game of stratego with\nmodel-free multiagent reinforcement learning. Science, 378(6623):990–996, 2022.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, and et al. Humanity’s\nlast exam, 2025. URL https://arxiv.org/abs/2501.14249.\nGabriel Poesia, David Broman, Nick Haber, and Noah Goodman. Learning formal mathematics from\nintrinsic motivation. Advances in Neural Information Processing Systems, 37:43032–43057, 2024.\nMohammadreza Pourreza, Shayan Talaei, Ruoxi Sun, Xingchen Wan, Hailong Li, Azalia Mirhoseini,\nAmin Saberi, Sercan Arik, et al. Reasoning-sql: Reinforcement learning with sql tailored partial\nrewards for reasoning-enhanced text-to-sql. arXiv preprint arXiv:2503.23157, 2025.\nMihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak.\nMaximizing confidence alone improves reasoning. arXiv preprint arXiv:2505.22660, 2025.\n91\nA Survey of Reinforcement Learning for Large Reasoning Models\nOri Press, Ravid Shwartz-Ziv, Yann LeCun, and Matthias Bethge. The entropy enigma: Success and\nfailure of entropy minimization. arXiv preprint arXiv:2405.05012, 2024.\nPrimeIntellect. Synthetic-2 release: Four million collaboratively generated reasoning traces. https:\n//www.primeintellect.ai/blog/synthetic-2-release#synthetic-2-dataset,\n2025. Technical Report.\nZehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang,\nJiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum\nreinforcement learning. arXiv preprint arXiv:2411.02337, 2024.\nCheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur,\nand Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025.\nRushi Qiang, Yuchen Zhuang, Yinghao Li, Rongzhi Zhang, Changhao Li, Ian Shu-Hei Wong, Sherry\nYang, Percy Liang, Chao Zhang, Bo Dai, et al. Mle-dojo: Interactive environments for empowering\nllm agents in machine learning engineering. arXiv preprint arXiv:2505.07782, 2025.\nChongli Qin and Jost Tobias Springenberg. Supervised fine tuning on curated data is reinforcement\nlearning (and can be improved). arXiv preprint arXiv:2507.12856, 2025.\nYujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao\nLi, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents.\narXiv preprint arXiv:2501.12326, 2025.\nZhongxi Qiu, Zhang Zhang, Yan Hu, Heng Li, and Jiang Liu. Open-medical-r1: How to choose data\nfor rlvr training at medicine domain, 2025. URL https://arxiv.org/abs/2504.13950.\nZipeng Qiu. Opentable-r1: A reinforcement learning augmented tool agent for open-domain table\nquestion answering. arXiv preprint arXiv:2507.03018, 2025.\nXiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu,\nShuxian Liang, Junxian He, et al. A survey of efficient reasoning for large reasoning models:\nLanguage, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025a.\nYuxiao Qu, Matthew Y. R. Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan\nSalakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement finetuning.\nIn Forty-second International Conference on Machine Learning, 2025b. URL https://openreview\n.net/forum?id=TqODUDsU4u.\nQwen Team. Qvq: To see the world with wisdom, 2025. URL https://qwenlm.github.io/blo\ng/qvq-72b-preview.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. Advances in\nneural information processing systems, 36:53728–53741, 2023.\nRafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q star: Your language model is\nsecretly a q-function. In First Conference on Language Modeling, 2024. URL https://openrevi\new.net/forum?id=kEVcNxtqXk.\nTabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,\nand Shimon Whiteson. Monotonic value function factorisation for deep multi-agent reinforcement\nlearning. Journal of Machine Learning Research, 21(178):1–51, 2020.\n92\nA Survey of Reinforcement Learning for Large Reasoning Models\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations\nenable training deep learning models with over 100 billion parameters. In Proceedings of the 26th\nACM SIGKDD international conference on knowledge discovery & data mining, pages 3505–3506,\n2020.\nAbhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep\nBarmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arxiv\npreprint arXiv: 2506.10910, 2025.\nZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe\nFu, Qihao Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning\nvia reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025.\nSyed Asad Rizvi, Daniel Levine, Aakash Patel, Shiyang Zhang, Eric Wang, Sizhuang He, David\nZhang, Cerise Tang, Zhuoyang Lyu, Rayyan Darji, Chang Li, Emily Sun, David Jeong, Lawrence\nZhao, Jennifer Kwan, David Braun, Brian Hafler, Jeffrey Ishizuka, Rahul M Dhodapkar, Hattie\nChung, Shekoofeh Azizi, Bryan Perozzi, and David van Dijk. Scaling large language models for\nnext-generation single-cell analysis. bioRxiv: 2025.04.14.648850, 2025.\nDavid Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience\nreplay for continual learning. Advances in neural information processing systems, 32, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10684–10695, 2022.\nNicolas Le Roux, Marc G Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves, Alex\nFréchette, Carolyne Pelletier, Eric Thibodeau-Laufer, Sándor Toth, and Sam Work. Tapered off-policy\nreinforce: Stable and efficient reinforcement learning for llms. arXiv preprint arXiv:2503.14286,\n2025.\nLloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and\nGianluca Corrado. Gaia-2: A controllable multi-view generative world model for autonomous\ndriving. arXiv preprint arXiv:2503.20523, 2025.\nRanjan Sapkota, Yang Cao, Konstantinos I Roumeliotis, and Manoj Karkee. Vision-language-action\nmodels: Concepts, progress, applications and challenges. arXiv preprint arXiv:2505.04769, 2025.\nAli Satvaty, Suzan Verberne, and Fatih Turkmen. Undesirable memorization in large language models:\nA survey. arXiv preprint arXiv:2410.02650, 2024.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. Advances in Neural Information Processing Systems, 36:68539–68551, 2023.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon\nSchmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go,\nchess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy\noptimization. In International conference on machine learning, pages 1889–1897. PMLR, 2015a.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional\ncontinuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438,\n2015b.\n93\nA Survey of Reinforcement Learning for Large Reasoning Models\nJohn Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning.\narXiv preprint arXiv:1704.06440, 2017a.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.\nByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi\nWang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models\nwith reinforcement learning. arXiv preprint arXiv:2504.13914, 2025a.\nByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi\nWang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1.5-thinking: Advancing superb reasoning models\nwith reinforcement learning, 2025b.\nAndrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo\nKohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, et al. Medgemma technical report.\narXiv preprint arXiv:2507.05201, 2025.\nAmrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh\nAgarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process\nverifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024.\nAmrith Setlur, Matthew YR Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz,\nand Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms.\narXiv preprint arXiv:2506.09026, 2025.\nZeyang Sha, Shiwen Cui, and Weiqiang Wang. Sem: Reinforcement learning for search-efficient large\nlanguage models. arXiv preprint arXiv:2505.07903, 2025.\nSheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can large\nreasoning models self-train? arXiv preprint arXiv:2505.21444, 2025.\nShijie Shang, Ruosi Wan, Yue Peng, Yutong Wu, Xiong-hui Chen, Jie Yan, and Xiangyu Zhang.\nStepfun-prover preview: Let’s think and verify step by step. arXiv preprint arXiv:2507.20199, 2025.\nRulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du,\nNathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals\nin rlvr. arXiv preprint arXiv:2506.10947, 2025.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in\nopen language models. arXiv preprint arXiv:2402.03300, 2024.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and\nJeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538, 2017.\nHaozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun\nZhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: A stable and generalizable r1-style large\nvision-language model. arXiv preprint arXiv:2504.07615, 2025a.\nWei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao,\nPeiyu Wang, Jianhao Zhang, et al. Skywork-r1v3 technical report. arXiv preprint arXiv:2507.06167,\n2025b.\n94\nA Survey of Reinforcement Learning for Large Reasoning Models\nIdan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rl’s razor: Why online reinforcement learning\nforgets less. arXiv preprint arXiv:2509.04259, 2025.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng,\nHaibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. In Proceedings of\nthe Twentieth European Conference on Computer Systems, pages 1279–1297, 2025.\nJiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu\nWen, Bingli Wang, Yancheng He, et al. Korgym: A dynamic game platform for llm reasoning\nevaluation. arXiv preprint arXiv:2505.14552, 2025a.\nTaiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning\nvia adaptive curriculum learning. arXiv preprint arXiv:2504.05520, 2025b.\nYucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, and\nDong Yu. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online\nenvironment. arXiv preprint arXiv:2507.05720, 2025c.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In\nInternational Conference on Learning Representations, 2020.\nVaishnavi Shrivastava, Ahmed Awadallah, Vidhisha Balachandran, Shivam Garg, Harkirat Behl, and\nDimitris Papailiopoulos. Sample more to think less: Group filtered policy optimization for concise\nreasoning. arXiv preprint arXiv:2508.09726, 2025.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Ander-\nson. The curse of recursion: Training on generated data makes models forget. arXiv preprint\narXiv:2305.17493, 2023.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai\nmodels collapse when trained on recursively generated data. Nature, 631(8022):755–759, 2024.\nDavid Silver and Richard S Sutton. Welcome to the era of experience. Google AI, 1, 2025.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the\ngame of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi\nby self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,\n2017.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement\nlearning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–\n1144, 2018.\nDavid Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artificial\nintelligence, 299:103535, 2021.\n95\nA Survey of Reinforcement Learning for Large Reasoning Models\nSimpleVLA-RL Team. Simplevla-rl: Online rl with simple reward enables training vla models with only\none trajectory. https://github.com/PRIME-RL/SimpleVLA-RL, 2025. GitHub repository.\nRaghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and\nRajesh Ranganath. A general framework for inference-time scaling and steering of diffusion models.\nIn Forty-second International Conference on Machine Learning, 2025.\nCharlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024.\nHuatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and\nJi-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\narXiv preprint arXiv:2503.05592, 2025a.\nHuatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian\nMin, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher++: Incentivizing the dynamic\nknowledge acquisition of llms via reinforcement learning. arXiv preprint arXiv:2505.17005, 2025b.\nYuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang\nYang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: A large-scale diffusion language model with\nhigh-speed inference. arXiv preprint arXiv:2508.02193, 2025c.\nSaksham Sahai Srivastava and Vaneet Aggarwal.\nA technical survey of reinforcement learning\ntechniques for large language models. arXiv preprint arXiv:2507.04136, 2025.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nneural information processing systems, 33:3008–3021, 2020.\nZafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour,\nand Andreas Köpf. Reasoning gym: Reasoning environments for reinforcement learning with\nverifiable rewards. arXiv preprint arXiv:2505.24760, 2025.\nAlex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing\npixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966,\n2025a.\nJinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthinking:\nAn empirical study of reasoning length and correctness in llms. arXiv preprint arXiv:2505.00127,\n2025b.\nYi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing\nthe reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint\narXiv:2503.23829, 2025c.\nZhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen,\nJiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual\ntool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025d.\nZhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide\nZeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations,\nmethods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025e.\n96\nA Survey of Reinforcement Learning for Large Reasoning Models\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu,\nAndrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: A survey on efficient\nreasoning for large language models. arXiv preprint arXiv:2503.16419, 2025.\nHao Sun. Supervised fine-tuning as inverse reinforcement learning. arXiv preprint arXiv:2403.12017,\n2024.\nHao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei\nHuang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching.\narXiv preprint arXiv:2505.04588, 2025a.\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu\nDing, Hongyang Li, Mengzhe Geng, et al. A survey of reasoning with foundation models: Concepts,\nmethodologies, and outlook. ACM Computing Surveys, 57(11):1–43, 2025b.\nLin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, and Ning Wu. Freeprm: Training process\nreward models without ground truth process labels. arXiv preprint arXiv:2506.03570, 2025c.\nShengjie Sun, Runze Liu, Jiafei Lyu, Jing-Wen Yang, Liangpeng Zhang, and Xiu Li. A large language\nmodel-driven reward design framework via dynamic feedback for reinforcement learning. Knowledge-\nBased Systems, 326:114065, 2025d. ISSN 0950-7051. doi: https://doi.org/10.1016/j.knosys.2025.\n114065. URL https://www.sciencedirect.com/science/article/pii/S09507051250\n11104.\nYifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, and Huan\nZhang. Improving data efficiency for llm reinforcement fine-tuning through difficulty-targeted\nonline data selection and rollout replay. arXiv preprint arXiv:2506.05316, 2025e.\nYu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Chenghao Xiao, Long Li, Yu Rong, Wenbing Huang,\nQifeng Bai, and Tingyang Xu. Reasonmed: A 370k multi-agent generated dataset for advancing\nmedical reasoning. arXiv preprint arXiv:2506.09513, 2025f.\nZetian Sun, Dongfang Li, Zhuoen Chen, Yuhuai Qin, and Baotian Hu. Stabilizing long-term multi-turn\nreinforcement learning with gated rewards. arXiv preprint arXiv:2508.10548, 2025g.\nZhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, and Jun Xu. Detection and mitigation of\nhallucination in large reasoning models: A mechanistic perspective. arXiv preprint arXiv:2505.12886,\n2025h.\nZhoujian Sun, Ziyi Liu, Cheng Luo, Jiebin Chu, and Zhengxing Huang.\nImproving interactive\ndiagnostic ability of a large language model agent through clinical experience learning, 2025i. URL\nhttps://arxiv.org/abs/2503.16463.\nPeter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max\nJaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition\nnetworks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.\nRichard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT\npress Cambridge, 1998.\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for\nreinforcement learning with function approximation. Advances in neural information processing\nsystems, 12, 1999.\n97\nA Survey of Reinforcement Learning for Large Reasoning Models\nGokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and J Andrew Bagnell. All roads lead\nto likelihood: The value of reinforcement learning in fine-tuning. arXiv preprint arXiv:2503.01067,\n2025.\nJaesung Tae, Hamish Ivison, Sachin Kumar, and Arman Cohan. Tess 2: A large-scale generalist\ndiffusion language model. arXiv preprint arXiv:2502.13917, 2025.\nHongze Tan and Jianfei Pan. Gtpo and grpo-s: Token and sequence-level reward shaping with policy\nentropy. arXiv preprint arXiv:2508.04349, 2025.\nShuhan Tan, Kairan Dou, Yue Zhao, and Philipp Krähenbühl. Interactive post-training for vision-\nlanguage-action models. arXiv preprint arXiv:2505.17016, 2025a.\nWentao Tan, Qiong Cao, Chao Xue, Yibing Zhan, Changxing Ding, and Xiaodong He. Chartmaster:\nAdvancing chart-to-code generation with real-world charts and chart similarity reinforcement\nlearning. arXiv preprint arXiv:2508.17608, 2025b.\nXiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng\nXia, Fang Wu, He Zhu, et al. Agent kb: Leveraging cross-domain experience for agentic problem\nsolving. arXiv preprint arXiv:2507.06229, 2025.\nZhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen\nZhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-\nseeking formalization. arXiv preprint arXiv:2507.15061, 2025.\nByteDance Seed Team. Seed-oss open-source models, 2025a. URL https://github.com/ByteD\nance-Seed/seed-oss.\nDolphin Team. Dolphin r1 dataset. https://huggingface.co/datasets/QuixiAI/dolphi\nn-r1, 2025b. URL https://huggingface.co/datasets/QuixiAI/dolphin-r1. Dataset,\nApache-2.0 license.\nGLM-V. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale\nCheng, Ji Qi, Junhui Ji, et al. GLM-4.5v and GLM-4.1v-thinking: Towards versatile multimodal\nreasoning with scalable reinforcement learning, 2025a.\nKimi Team. Kimi k2: Open agentic intelligence, 2025c. URL https://arxiv.org/abs/2507.205\n34.\nKimi Team. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599,\n2025d.\nMiroMind AI Team. Mirothinker: An open-source agentic model series trained for deep research and\ncomplex, long-horizon problem solving. https://github.com/MiroMindAI/MiroThinker,\n2025e.\nMiroMind Data Team. Miroverse v0.1: A reproducible, full-trajectory, ever-growing deep research\ndataset, 2025f. URL https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.\n1.\nPrime Intellect Team, Sami Jaghouar, Justus Mattern, Jack Min Ong, Jannik Straube, Manveer\nBasra, Aaron Pazdera, Kushal Thaman, Matthew Di Ferrante, Felix Gabriel, et al. Intellect-2: A\nreasoning model trained through globally decentralized reinforcement learning. arXiv preprint\narXiv:2505.07291, 2025b.\n98\nA Survey of Reinforcement Learning for Large Reasoning Models\nQwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025g. URL https:\n//qwenlm.github.io/blog/qwq-32b/.\nRLinf Team. Rlinf: Reinforcement learning infrastructure for agentic ai. https://github.com/R\nLinf/RLinf, 2025h. GitHub repository.\nTencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng\nXu, Chenhao Wang, Decheng Wu, Dengpeng Wu, et al. Hunyuan-turbos: Advancing large lan-\nguage models through mamba-transformer synergy and adaptive chain-of-thought. arXiv preprint\narXiv:2505.15431, 2025c.\nTHUDM. slime: An sglang-native post-training framework for rl scaling. https://github.com/T\nHUDM/slime, 2025. GitHub repository.\nShulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang,\nHao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-tool-thought for ultra-long egocentric\nvideo reasoning. arXiv preprint arXiv:2506.13654, 2025.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In\n2012 IEEE/RSJ international conference on intelligent robots and systems, pages 5026–5033. IEEE,\n2012.\ntokenbender. avatarl: training language models from scratch with pure reinforcement learning, 2025.\nURL https://github.com/tokenbender/avatarl.\nChengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li,\nand Pheng-Ann Heng. Delving into rl for image generation with cot: A study on dpo vs. grpo. arXiv\npreprint arXiv:2505.17017, 2025a.\nJingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song,\nJiahao Zhan, Yuyang Lu, et al. Code2logic: Game-code-driven data synthesis for enhancing vlms\ngeneral reasoning. arXiv preprint arXiv:2505.13886, 2025b.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nMark Towers, Ariel Kwiatkowski, Jordan Terry, John U Balis, Gianluca De Cola, Tristan Deleu, Manuel\nGoulão, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. Gymnasium: A standard interface\nfor reinforcement learning environments. arXiv preprint arXiv:2407.17032, 2024.\nHarsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank\nGupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: A controllable world of apps\nand people for benchmarking interactive coding agents. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 16022–16076, 2024.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and\noutcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.\nCarel van Niekerk, Renato Vukovic, Benjamin Matthias Ruppik, Hsien-chin Lin, and Milica Gašić.\nPost-training large language models via reinforcement learning from self-feedback. arXiv preprint\narXiv:2507.21931, 2025.\n99\nA Survey of Reinforcement Learning for Large Reasoning Models\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017.\nDheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Megh Thakkar,\nThibault Le Sellier de Chezelles, Nicolas Gontier, Miguel Muñoz-Mármol, Sahar Omidi Shayegan,\nStefania Raimondo, et al. How to train your llm web agent: A statistical diagnosis. arXiv preprint\narXiv:2507.04103, 2025.\nPablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn.\nWill we run out of data? limits of llm scaling based on human-generated data. arXiv preprint\narXiv:2211.04325, 2022.\nVijay Viswanathan, Yanchao Sun, Shuang Ma, Xiang Kong, Meng Cao, Graham Neubig, and Tong-\nshuang Wu. Checklists are better than reward models for aligning language models. arXiv preprint\narXiv:2507.18624, 2025.\nLeandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan\nLambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement\nlearning. https://github.com/huggingface/trl, 2020.\nChristian Walder and Deep Karkhanis. Pass@k policy optimization: Solving harder reinforcement\nlearning problems. arXiv preprint arXiv:2505.15201, 2025.\nBram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano\nErmon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct\npreference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8228–8238, 2024.\nZiyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun\nWang, Weinan Zhang, Shuyue Hu, et al. Rema: Learning to meta-think for llms with multi-agent\nreinforcement learning. arXiv preprint arXiv:2503.09501, 2025.\nBin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang\nLi, Nuo Chen, Siyu Chen, et al. Step-3 is large yet affordable: Model-system co-design for cost-\neffective decoding. arXiv preprint arXiv:2507.19427, 2025a.\nChen Wang, Lai Wei, Yanzhi Zhang, Chenyang Shao, Zedong Dan, Weiran Huang, Yue Wang, and\nYuzhi Zhang. Eframe: Deeper reasoning via exploration-filtering-replay reinforcement learning\nframework. arXiv preprint arXiv:2506.22200, 2025b.\nChenglong Wang, Yang Gan, Yifu Huo, Yongyu Mu, Qiaozhi He, Murun Yang, Bei Li, Tong Xiao,\nChunliang Zhang, Tongran Liu, et al. Gram: A generative foundation reward model for reward\ngeneralization. arXiv preprint arXiv:2506.14175, 2025c.\nHaiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, MD Santos, Flood Sung, Marina\nVinyes, Zhenzhe Ying, Zekai Zhu, et al. Kimina-prover preview: Towards large formal reasoning\nmodels with reinforcement learning, 2025. URL https://arxiv. org/abs/2504.11354, 2025d.\nHanlin Wang, Chak Tou Leong, Jiashuo Wang, Jian Wang, and Wenjie Li. Spa-rl: Reinforcing llm\nagents via stepwise progress attribution. arXiv preprint arXiv:2505.20732, 2025e.\nHanyin Wang. Reinforcement learning for out-of-distribution reasoning in llms: An empirical study\non diagnosis-related group coding, 2025. URL https://arxiv.org/abs/2505.21908.\n100\nA Survey of Reinforcement Learning for Large Reasoning Models\nHaoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu,\nQinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with\nmulti-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025f.\nHaozhe Wang, Qixin Xu, Che Liu, Junhong Wu, Fangzhen Lin, and Wenhu Chen. Emergent hierarchical\nreasoning in llms through reinforcement learning. arXiv preprint arXiv:2509.03646, 2025g.\nJiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, and Guorui Zhou. Stabilizing knowledge, promoting\nreasoning: Dual-token constraints for rlvr. arXiv preprint arXiv:2507.15778, 2025h.\nJicheng Wang, Yifeng He, and Hao Chen. Repogenreflex: Enhancing repository-level code completion\nwith verbal reinforcement and retrieval-augmented generation. arXiv preprint arXiv:2409.13122,\n2024a.\nJunke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang.\nSimplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl.\narXiv preprint arXiv:2504.11455, 2025i.\nPeiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\nMath-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku,\nAndre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 9426–9439, Bangkok, Thailand, August\n2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.510. URL\nhttps://aclanthology.org/2024.acl-long.510/.\nPeiyu Wang, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei,\nJianhao Zhang, Yunzhuo Hao, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for\nreasoning, 2025j.\nQi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning\ncapability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025k.\nRuoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is\nyour agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 11279–11298, 2022.\nShenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen,\nJianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive\neffective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025l.\nWeixun Wang, Shaopan Xiong, Gengru Chen, Wei Gao, Sheng Guo, Yancheng He, Ju Huang, Jiaheng\nLiu, Zhendong Li, Xiaoyang Li, et al. Reinforcement learning optimization for large-scale learning:\nAn efficient and user-friendly scaling library. arXiv preprint arXiv:2506.06122, 2025m.\nWeiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin\nJing, Shenglong Ye, Jie Shao, et al. InternVL3.5: Advancing open-source multimodal models in\nversatility, reasoning, and efficiency, 2025n.\nXin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before you leap: Bridg-\ning model-free and model-based reinforcement learning for planned-ahead vision-and-language\nnavigation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 37–53,\n2018.\n101\nA Survey of Reinforcement Learning for Large Reasoning Models\nXin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang,\nWilliam Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation\nlearning for vision-language navigation. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 6629–6638, 2019.\nYanlin Wang, Yanli Wang, Daya Guo, Jiachi Chen, Ruikai Zhang, Yuchi Ma, and Zibin Zheng. Rlcoder:\nReinforcement learning for repository-level code completion. arXiv preprint arXiv:2407.19487,\n2024c.\nYinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement\nlearning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025o.\nYinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. Co-evolving llm coder and unit tester\nvia reinforcement learning. arXiv preprint arXiv:2506.03136, 2025p.\nYiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He,\nKuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models\nwith one training example. arXiv preprint arXiv:2504.20571, 2025q.\nYiting Wang, Guoheng Sun, Wanghao Ye, Gang Qu, and Ang Li.\nVerireason: Reinforcement\nlearning with testbench feedback for reasoning-enhanced verilog generation.\narXiv preprint\narXiv:2505.11849, 2025r.\nYue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao\nLi, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms.\narXiv preprint arXiv:2501.18585, 2025s.\nZengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes\nreinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025t.\nZhilin Wang, Zhe Yang, Yun Luo, Yafu Li, Haoran Zhang, Runzhe Zhan, Derek F Wong, Jizhe Zhou,\nand Yu Cheng. Synthesizing sheet music problems for evaluation and reinforcement learning. arXiv\npreprint arXiv:2509.04059, 2025u.\nZiliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu.\nStepsearch: Igniting llms search ability via step-wise proximal policy optimization. arXiv preprint\narXiv:2505.15107, 2025v.\nYuyang Wanyan, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Jiabo Ye, Yutong Kou, Ming\nYan, Fei Huang, Xiaoshan Yang, et al. Look before you leap: A gui-critic-r1 model for pre-operative\nerror diagnosis in gui automation. arXiv preprint arXiv:2506.04614, 2025.\nJason Wei. The asymmetry of verification and verifier’s law. https://www.jasonwei.net/blog/\nasymmetry-of-verification-and-verifiers-law, 2025. Accessed: 2025-07-15.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural\ninformation processing systems, 35:24824–24837, 2022.\nJason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung,\nAlex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: A simple yet challenging\nbenchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025a.\nYifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, and Li Du. Autotir: Autonomous\ntools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.21836, 2025b.\n102\nA Survey of Reinforcement Learning for Large Reasoning Models\nYuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried,\nGabriel Synnaeve, Rishabh Singh, and Sida I Wang. Swe-rl: Advancing llm reasoning via reinforce-\nment learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025c.\nZhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao\nZhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement\nlearning. arXiv preprint arXiv:2505.16421, 2025d.\nHao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Ya-Qin Zhang, and\nYuanchun Li. Budgetthinker: Empowering budget-aware llm reasoning with control tokens. arXiv\npreprint arXiv:2508.17196, 2025a.\nLiang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang,\nXiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv\npreprint arXiv:2503.10460, 2025b.\nXumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang,\nJunjie Li, Ziming Miao, et al. Reinforcement learning with verifiable rewards implicitly incentivizes\ncorrect reasoning in base llms. arXiv preprint arXiv:2506.14245, 2025c.\nChenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep\nSaha. J1: Incentivizing thinking in llm-as-a-judge via reinforcement learning. arXiv preprint\narXiv:2505.10320, 2025.\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8(3):229–256, 1992.\nRonald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning\nalgorithms. Connection Science, 3(3):241–268, 1991.\nMaciej Wołczyk, Michał Zając, Razvan Pascanu, Łukasz Kuciński, and Piotr Miłoś. Continual world: A\nrobotic benchmark for continual reinforcement learning. Advances in Neural Information Processing\nSystems, 34:28496–28510, 2021.\nChenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai,\nXiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a.\nDiankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in\nvisual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025b.\nHaoyuan Wu, Xueyi Chen, Rui Ming, Jilong Gao, Shoubo Hu, Zhuolun He, and Bei Yu. Totrl: Unlock\nllm tree-of-thoughts reasoning potential through puzzles solving. arXiv preprint arXiv:2505.12717,\n2025c.\nJialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang,\nZekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency.\narXiv preprint arXiv:2505.22648, 2025d.\nLixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, and Yitao Duan. Confucius3-math: A lightweight high-\nperformance reasoning llm for chinese k-12 mathematics learning. arXiv preprint arXiv:2506.18330,\n2025e.\nMingrui Wu, Lu Wang, Pu Zhao, Fangkai Yang, Jianjin Zhang, Jianfeng Liu, Yuefeng Zhan, Weihao\nHan, Hao Sun, Jiayi Ji, et al. Reprompt: Reasoning-augmented reprompting for text-to-image\ngeneration via reinforcement learning. arXiv preprint arXiv:2505.17540, 2025f.\n103\nA Survey of Reinforcement Learning for Large Reasoning Models\nPenghao Wu, Shengnan Ma, Bo Wang, Jiaheng Yu, Lewei Lu, and Ziwei Liu. Gui-reflection: Em-\npowering multimodal gui models with self-reflection behavior. arXiv preprint arXiv:2506.08012,\n2025g.\nTianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston,\nand Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with\nllm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024.\nWeijia Wu, Chen Gao, Joya Chen, Kevin Qinghong Lin, Qingwei Meng, Yiming Zhang, Yuke Qiu,\nHong Zhou, and Mike Zheng Shou. Reinforcement learning in vision: A survey. arXiv preprint\narXiv:2508.08189, 2025h.\nXiaobao Wu. Sailing by the stars: A survey on reward models and learning strategies for learning\nfrom rewards. arXiv preprint arXiv:2505.02686, 2025.\nYongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi,\nMing-Hsuan Yang, and Xu Yang. On the generalization of sft: A reinforcement learning perspective\nwith reward rectification. arXiv preprint arXiv:2508.05629, 2025i.\nJiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts\nin visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677, 2025a.\nPeng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Shujie\nLiu, Yan Lu, et al. Mmedagent-rl: Optimizing multi-agent collaboration for multimodal medical\nreasoning. arXiv preprint arXiv:2506.00555, 2025b.\nYu Xia, Rui Wang, Xu Liu, Mingyan Li, Tong Yu, Xiang Chen, Julian McAuley, and Shuai Li. Beyond\nchain-of-thought: A survey of chain-of-x paradigms for llms. arXiv preprint arXiv:2404.15676,\n2024.\nYunhui Xia, Wei Shen, Yan Wang, Jason Klein Liu, Huifeng Sun, Siyue Wu, Jian Hu, and Xiaolong Xu.\nLeetcodedataset: A temporal dataset for robust evaluation and efficient training of code llms. arXiv\npreprint arXiv:2504.14655, 2025c.\nViolet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, and Nick Haber.\nJust enough thinking: Efficient reasoning with adaptive length penalties reinforcement learning.\narXiv preprint arXiv:2506.05256, 2025.\nChangyi Xiao, Mengdi Zhang, and Yixin Cao. Bnpo: Beta normalization policy optimization. arXiv\npreprint arXiv:2506.02864, 2025a.\nTeng Xiao, Yige Yuan, Mingxiao Li, Zhengyu Chen, and Vasant G Honavar. On a connection between\nimitation learning and rlhf. arXiv preprint arXiv:2503.05079, 2025b.\nLLM Xiaomi, Bingquan Xia, Bowen Shen, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu,\nJiebao Xiao, Jinhao Dong, et al. Mimo: Unlocking the reasoning potential of language model–from\npretraining to posttraining. arXiv preprint arXiv:2505.07608, 2025.\nChengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, and Kai Chen. Swe-fixer: Training\nopen-source llms for effective and efficient github issue resolution. arXiv preprint arXiv:2501.05040,\n2025a.\nGuofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, and Xiao Zhang. Capo: Towards enhancing llm\nreasoning through verifiable generative credit assignment. arXiv preprint arXiv:2508.02298, 2025b.\n104\nA Survey of Reinforcement Learning for Large Reasoning Models\nTian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu,\nZhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement\nlearning. arXiv preprint arXiv:2502.14768, 2025c.\nTianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and\nTao Yu. Text2reward: Reward shaping with language models for reinforcement learning. arXiv\npreprint arXiv:2309.11489, 2023.\nYunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, and Chen Wei. Play to generalize: Learning\nto reason through game play. arXiv preprint arXiv:2506.08011, 2025d.\nZhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. Teaching language models\nto critique via reinforcement learning. arXiv preprint arXiv:2502.03492, 2025e.\nZhihui Xie, Jiacheng Ye, Lin Zheng, Jiahui Gao, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan\nGong, Xin Jiang, Zhenguo Li, et al. Dream-coder 7b: An open diffusion language model for code.\narXiv preprint arXiv:2509.01142, 2025f.\nRihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, and Bingning Wang.\nSurrogate signals from format and length: Reinforcement learning for solving mathematical\nproblems without ground truth answers. arXiv preprint arXiv:2505.19439, 2025.\nWei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang,\nCaiming Xiong, et al. A minimalist approach to llm reasoning: from rejection sampling to reinforce.\narXiv preprint arXiv:2504.11343, 2025a.\nWei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Self-rewarding\ncorrection for mathematical reasoning. arXiv preprint arXiv:2502.19613, 2025b.\nFengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan,\nJiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: A survey of\nreinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025a.\nHuihui Xu and Yuanpeng Nie. Medground-r1: Advancing medical image grounding via spatial-\nsemantic rewarded group relative policy optimization, 2025. URL https://arxiv.org/abs/25\n07.02994.\nRan Xu, Yuchen Zhuang, Yishan Zhong, Yue Yu, Xiangru Tang, Hang Wu, May D Wang, Peifeng\nRuan, Donghan Yang, Tao Wang, et al. Medagentgym: Training llm agents for code-based medical\nreasoning at scale. arXiv preprint arXiv:2506.04405, 2025b.\nWenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, and Yonghui Wu. A unified pairwise\nframework for rlhf: Bridging generative reward modeling and policy optimization. arXiv preprint\narXiv:2504.04950, 2025c.\nWujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic\nmemory for llm agents. arXiv preprint arXiv:2502.12110, 2025d.\nYi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulić. Visual\nplanning: Let’s think only with images. arXiv preprint arXiv:2505.11409, 2025e.\nYuyang Xu, Yi Cheng, Haochao Ying, Zhuoyun Du, Renjun Hu, Xing Shi, Wei Lin, and Jian Wu. Sspo:\nSelf-traced step-wise preference optimization for process supervision and reasoning compression.\narXiv preprint arXiv:2508.12604, 2025f.\n105\nA Survey of Reinforcement Learning for Large Reasoning Models\nZhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin,\nand Radha Poovendran. Tinyv: Reducing false negatives in verification improves rl for llm reasoning.\narXiv preprint arXiv:2505.14625, 2025g.\nZhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: A diverse,\nchallenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025h.\nZeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu,\nQiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint\narXiv:2505.07818, 2025.\nJianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning\nto reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025a.\nKaiwen Yan, Xuanqing Shi, Hongcheng Guo, Wenxuan Wang, Zhuosheng Zhang, and Chengwei Qin.\nDrqa: Dynamic reasoning quota allocation for controlling overthinking in reasoning large language\nmodels. arXiv preprint arXiv:2508.17803, 2025b.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong\nTu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical\nexpert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024a.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, et al. Qwen3 technical report. arxiv preprint arXiv: 2505.09388,\n2025a.\nChenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Weijie Su, Xuehui Wang, Zhaoyang Liu, Jinguo\nZhu, Hao Li, et al. Zerogui: Automating online gui learning at zero human cost. arXiv preprint\narXiv:2505.23762, 2025b.\nJihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in\nspace: How multimodal large language models see, remember, and recall spaces. In Proceedings of\nthe Computer Vision and Pattern Recognition Conference, pages 10632–10643, 2025c.\nKai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li.\nUsing human feedback to fine-tune diffusion models without any reward model. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8941–8951, 2024b.\nLing Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada:\nMultimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025d.\nWenjie Yang, Mao Zheng, Mingyang Song, Zheng Li, and Sitong Wang. Ssr-zero: Simple self-rewarding\nreinforcement learning for machine translation. arXiv preprint arXiv:2505.16637, 2025e.\nYaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-agent\nreinforcement learning. In International conference on machine learning, pages 5571–5580. PMLR,\n2018.\nZhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. Treerpo:\nTree relative policy optimization. arXiv preprint arXiv:2506.05183, 2025f.\nZhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan\nLiang, and Jing Tang. Depth-breadth synergy in rlvr: Unlocking llm reasoning gains with adaptive\nexploration. arXiv preprint arXiv:2508.13755, 2025g.\n106\nA Survey of Reinforcement Learning for Large Reasoning Models\nZongxian Yang, Jiayu Qian, Zegao Peng, Haoyu Zhang, and Zhi-An Huang. Med-refl: Medical\nreasoning enhancement via self-corrected fine-grained reflection, 2025h. URL https://arxiv.\norg/abs/2506.13793.\nFeng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your\nefficient rl framework secretly brings you off-policy rl training, August 2025a.\nURL https:\n//fengyao.notion.site/off-policy-rl.\nFeng Yao, Zilong Wang, Liyuan Liu, Junxia Cui, Li Zhong, Xiaohan Fu, Haohui Mai, Vish Krishnan,\nJianfeng Gao, and Jingbo Shang. Training language models to generate quality code with program\nanalysis feedback. arXiv preprint arXiv:2505.22704, 2025b.\nChenlu Ye, Zhou Yu, Ziji Zhang, Hao Chen, Narayanan Sadagopan, Jing Huang, Tong Zhang, and\nAnurag Beniwal. Beyond correctness: Harmonizing process and outcome rewards through rl\ntraining. arXiv preprint arXiv:2509.03403, 2025a.\nJiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao,\nJunjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv\npreprint arXiv:2508.15144, 2025b.\nJiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong.\nDream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025c.\nYixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for\nreasoning. arXiv preprint arXiv:2502.03387, 2025d.\nZhangyue Yin, Qiushi Sun, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu, and Xuanjing Huang. Dynamic\nand generalizable process reward modeling. arXiv preprint arXiv:2507.17849, 2025.\nByoungJun Jeon Yooseok Lim. More-clear: Multimodal offline reinforcement learning for clinical notes\nleveraged enhanced state representation, 2025. URL https://arxiv.org/abs/2508.07681.\nAiling Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li,\nYun Yue, et al. Medreseacher-r1: Expert-level medical deep researcher via a knowledge-informed\ntrajectory synthesis framework. arXiv preprint arXiv:2508.14880, 2025a.\nChao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The\nsurprising effectiveness of ppo in cooperative multi-agent games. Advances in neural information\nprocessing systems, 35:24611–24624, 2022.\nHongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang,\nWei-Ying Ma, Jingjing Liu, Mingxuan Wang, et al. Memagent: Reshaping long-context llm with\nmulti-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025b.\nHongzhou Yu, Tianhao Cheng, Yingwen Wang, Wen He, Qing Wang, Ying Cheng, Yuejie Zhang, Rui\nFeng, and Xiaobo Zhang. Finemedlm-o1: Enhancing medical knowledge reasoning ability of llm\nfrom supervised fine-tuning to test-time training, 2025c. URL https://arxiv.org/abs/2501\n.09213.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan,\nGaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale.\narXiv preprint arXiv:2503.14476, 2025d.\n107\nA Survey of Reinforcement Learning for Large Reasoning Models\nTianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao,\nZhiyuan Liu, et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint\narXiv:2506.18254, 2025e.\nZhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. Z1: Efficient test-time\nscaling with code. arXiv preprint arXiv:2504.00810, 2025f.\nDanlong Yuan, Tian Xie, Shaohan Huang, Zhuocheng Gong, Huishuai Zhang, Chong Luo, Furu Wei,\nand Dongyan Zhao. Efficient rl training for reasoning models via length-aware optimization. arXiv\npreprint arXiv:2505.12284, 2025a.\nJingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie,\nYX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively\ntrainable sparse attention. arXiv preprint arXiv:2502.11089, 2025b.\nLifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan\nLiu, Maosong Sun, and Hao Peng. From f(x) and g(x) to f(g(x)): LLMs learn new skills in RL by\ncomposing old ones. https://husky-morocco-f72.notion.site/From-f-x-and-g-x-t\no-f-g-x-LLMs-Learn-New-Skills-in-RL-by-Composing-Old-Ones-2499aba4486f8\n02c8108e76a12af3020, 2025c. Notion blog post, available online.\nLifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu,\nand Hao Peng. Free process rewards without process labels. In Forty-second International Conference\non Machine Learning, 2025d. URL https://openreview.net/forum?id=8ThnPFhGm8.\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason\nWeston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 3, 2024.\nWeizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Ilia Kulikov, Kyunghyun Cho, Dong\nWang, Yuandong Tian, Jason E Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8 m\nchallenging questions. arXiv preprint arXiv:2502.13124, 2025e.\nYufeng Yuan, Yu Yue, Ruofei Zhu, Tiantian Fan, and Lin Yan. What’s behind ppo’s collapse in long-cot?\nvalue optimization holds the secret. arXiv preprint arXiv:2503.01491, 2025f.\nChuhuai Yue, Chengqi Dong, Yinan Gao, Hang He, Jiajun Chai, Guojun Yin, and Wei Lin. Promoting\nefficient reasoning with verifiable stepwise reward. arXiv preprint arXiv:2508.10293, 2025a.\nYang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does\nreinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv\npreprint arXiv:2504.13837, 2025b.\nYu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang,\nTianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced\nreasoning tasks. arXiv preprint arXiv:2504.05118, 2025c.\nAohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin,\nHao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models.\narXiv preprint arXiv:2508.06471, 2025a.\nGuangtao Zeng, Maohao Shen, Delin Chen, Zhenting Qi, Subhro Das, Dan Gutfreund, David Cox,\nGregory Wornell, Wei Lu, Zhang-Wei Hong, et al. Satori-swe: Evolutionary test-time scaling for\nsample-efficient software engineering. arXiv preprint arXiv:2505.23604, 2025b.\n108\nA Survey of Reinforcement Learning for Large Reasoning Models\nSihang Zeng, Kai Tian, Kaiyan Zhang, Junqi Gao, Runze Liu, Sa Yang, Jingxuan Li, Xinwei Long,\nJiaheng Ma, Biqing Qi, et al. Reviewrl: Towards automated scientific review with rl. arXiv preprint\narXiv:2508.10308, 2025c.\nSiliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong. Re-\ninforcing multi-turn reasoning in llm agents via turn-level credit assignment.\narXiv preprint\narXiv:2505.11821, 2025d.\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo:\nInvestigating and taming zero reinforcement learning for open base models in the wild. arXiv\npreprint arXiv:2503.18892, 2025e.\nKaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane S Boning, and Dina Katabi.\nRl tango: Reinforcing generator and verifier together for language reasoning. arXiv preprint\narXiv:2505.15034, 2025.\nBin Zhang, Hangyu Mao, Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang, Zhiwei Xu, Dapeng Li, Ziyue\nLi, Rui Zhao, et al. Controlling large language model-based agents for large-scale decision-making:\nAn actor-critic approach. arXiv preprint arXiv:2311.13884, 2023a.\nCeyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei\nZhang, Anji Liu, Song-Chun Zhu, et al. Proagent: Building proactive cooperative ai with large\nlanguage models. CoRR, 2023b.\nChong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng\nMo, Qi Zhang, et al. 100 days after deepseek-r1: A survey on replication studies and more directions\nfor reasoning language models. arXiv preprint arXiv:2505.00551, 2025a.\nGuibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou, Zhongzhi\nLi, Xiangyuan Xue, Yijiang Li, Yifan Zhou, Yang Chen, Chen Zhang, Yutao Fan, Zihu Wang, Songtao\nHuang, Yue Liao, Hongru Wang, Mengyue Yang, Heng Ji, Michael Littman, Jun Wang, Shuicheng\nYan, Philip Torr, and Lei Bai. The landscape of agentic reinforcement learning for llms: A survey,\n2025b. URL https://arxiv.org/abs/2509.02547.\nHongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, and Guorui Zhou. Rlep:\nReinforcement learning with experience replay for llm reasoning. arXiv preprint arXiv:2507.07451,\n2025c.\nKaiyan Zhang, Runze Liu, Xuekai Zhu, Kai Tian, Sihang Zeng, Guoli Jia, Yuchen Fan, Xingtai Lv,\nYuxin Zuo, Che Jiang, Ziyang Liu, Jianyu Wang, Yuru Wang, Ruotong Zhao, Ermo Hua, Yibo Wang,\nShijie Wang, Junqi Gao, Xinwei Long, Youbang Sun, Zhiyuan Ma, Ganqu Cui, Lei Bai, Ning Ding,\nBiqing Qi, and Bowen Zhou. Marti: A framework for multi-agent llm systems reinforced training\nand inference, 2025d. URL https://github.com/TsinghuaC3I/MARTI.\nKaiyan Zhang, Jiayuan Zhang, Haoxin Li, Xuekai Zhu, Ermo Hua, Xingtai Lv, Ning Ding, Biqing Qi,\nand Bowen Zhou. OpenPRM: Building open-domain process-based reward models with preference\ntrees. In The Thirteenth International Conference on Learning Representations, 2025e. URL https:\n//openreview.net/forum?id=fGIqGfmgkW.\nKaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, and Rui Yan. Stephint: Multi-\nlevel stepwise hints enhance reinforcement learning to reason. arXiv preprint arXiv:2507.02841,\n2025f.\n109\nA Survey of Reinforcement Learning for Large Reasoning Models\nKongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, and\nDacheng Tao. Consistent paths lead to truth: Self-rewarding reinforcement learning for llm\nreasoning. arXiv preprint arXiv:2506.08745, 2025g.\nLunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.\nGenerative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240,\n2024a.\nQingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already\nhalf the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812,\n2025h.\nRuize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Wenhao Tang, Shiyu Huang, Deheng\nYe, Wenbo Ding, Yaodong Yang, et al. A survey on self-play methods in reinforcement learning.\narXiv preprint arXiv:2408.01072, 2024b.\nSheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon.\nMed-rlvr:\nEmerging medical reasoning from a 3b base model via reinforcement learning. arXiv preprint\narXiv:2502.19655, 2025i.\nWenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, and\nJingren Zhou. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and\nreinforcement learning via dynamic weighting. arXiv preprint arXiv:2508.11408, 2025j.\nXiaotian Zhang, Yuan Wang, Zhaopeng Feng, Ruizhe Chen, Zhijie Zhou, Yan Zhang, Hongxia Xu,\nJian Wu, and Zuozhu Liu. Med-u1: Incentivizing unified medical reasoning in llms via large-scale\nreinforcement learning. arXiv preprint arXiv:2506.12307, 2025k.\nXiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng.\nCritique-grpo: Advancing llm reasoning with natural language and numerical feedback. arXiv\npreprint arXiv:2506.03106, 2025l.\nXintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei\nWu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-focus: Adaptive visual search and zooming for\nmultimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025m.\nXuanyu Zhang, Weiqi Li, Shijie Zhao, Junlin Li, Li Zhang, and Jian Zhang. Vq-insight: Teaching vlms\nfor ai-generated video quality understanding via progressive visual reinforcement learning. arXiv\npreprint arXiv:2506.18564, 2025n.\nXuechen Zhang, Zijian Huang, Yingcong Li, Chenshun Ni, Jiasi Chen, and Samet Oymak.\nBread: Branched rollouts from expert anchors bridge sft and rl for reasoning. arXiv preprint\narXiv:2506.17211, 2025o.\nYanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang,\nShuxin Zheng, and Jiyan He. No free lunch: Rethinking internal feedback for llm reasoning. arXiv\npreprint arXiv:2506.17219, 2025p.\nYifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, and Andrew C Yao. On the design\nof kl-regularized policy gradient algorithms for llm reasoning. arXiv preprint arXiv:2505.17508,\n2025q.\nYimeng Zhang, Tian Wang, Jiri Gesi, Ziyi Wang, Yuxuan Lu, Jiacheng Lin, Sinong Zhan, Vianne Gao,\nRuochen Jiao, Junze Liu, et al. Shop-r1: Rewarding llms to simulate human behavior in online\nshopping via reinforcement learning. arXiv preprint arXiv:2507.17842, 2025r.\n110\nA Survey of Reinforcement Learning for Large Reasoning Models\nYu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong\nLuo, and Lili Qiu. Reasongen-r1: Cot for autoregressive image generation models through sft and\nrl. arXiv preprint arXiv:2505.24875, 2025s.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning. arXiv preprint arXiv:2501.07301, 2025t.\nZhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian\nChen, Yankai Lin, et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning.\narXiv preprint arXiv:2506.01391, 2025u.\nZizhuo Zhang, Jianing Zhu, Xinmu Ge, Zihua Zhao, Zhanke Zhou, Xuan Li, Xiao Feng, Jiangchao\nYao, and Bo Han. Co-reward: Self-supervised reinforcement learning for large language model\nreasoning via contrastive agreement. arXiv preprint arXiv:2508.00410, 2025v.\nAndrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Matthieu Lin, Shenzhi Wang, Qingyun Wu,\nZilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. arXiv\npreprint arXiv:2505.03335, 2025a.\nJian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian,\nBiqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative\nreasoning. arXiv preprint arXiv:2504.00891, 2025b.\nSiyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion\nlarge language models via reinforcement learning, 2025c. URL https://arxiv.org/abs/2504\n.12216.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint\narXiv:2303.18223, 1(2), 2023a.\nWeikang Zhao, Xili Wang, Chengdi Ma, Lingbin Kong, Zhaohua Yang, Mingxiang Tuo, Xiaowei Shi,\nYitao Zhai, and Xunliang Cai. Mua-rl: Multi-turn user-interacting agent reinforcement learning for\nagentic tool use. arXiv preprint arXiv:2508.18669, 2025d.\nXuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason\nwithout external rewards. arXiv preprint arXiv:2505.19590, 2025e.\nYanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid\nShojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data\nparallel. arXiv preprint arXiv:2304.11277, 2023b.\nYuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang,\nLei Cui, Qixiang Ye, et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673,\n2025f.\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong\nLiu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071,\n2025a.\nHaizhong Zheng, Yang Zhou, Brian R Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi\nChen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts.\narXiv preprint arXiv:2506.02177, 2025b.\n111\nA Survey of Reinforcement Learning for Large Reasoning Models\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena.\nAdvances in neural information processing systems, 36:46595–46623, 2023.\nTianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu\nWen, Chenghua Lin, Wenhao Huang, et al. First return, entropy-eliciting explore. arXiv preprint\narXiv:2507.07017, 2025c.\nXuejing Zheng, Chao Yu, and Minjie Zhang. Lifelong reinforcement learning with temporal logic\nformulas and reward machines. Knowledge-Based Systems, 257:109650, 2022.\nYaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong.\nEasyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiy\nouga/EasyR1, 2025d.\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu.\nDeepresearcher: Scaling deep research via reinforcement learning in real-world environments.\narXiv preprint arXiv:2504.03160, 2025e.\nZiwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing\nYu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint\narXiv:2505.14362, 2025f.\nWanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large\nlanguage models with long-term memory. In 38th AAAI Conference on Artificial Intelligence, AAAI\n2024, Feb 20-27 2024 Vancouver, Canada, volume 38, pages 19724–19731. Association for the\nAdvancement of Artificial Intelligence (AAAI), 2024.\nYifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang,\nShaoyang Guo, Tianrui Guan, Ka Nam Lui, et al. A survey on vision-language-action models: An\naction tokenization perspective. arXiv preprint arXiv:2507.01925, 2025.\nEnshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan\nWang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in\nvision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025a.\nJunting Zhou, Wang Li, Yiyan Liao, Nengyuan Zhang, Tingjia Miaoand Zhihui Qi, Yuhan Wu, and\nTong Yang. Academicbrowse: Benchmarking academic browse ability of llms. arXiv preprint\narXiv:2506.13784, 2025b.\nMeng Zhou, Bei Li, Jiahao Liu, Xiaowen Shi, Yang Bai, Rongxiang Weng, Jingang Wang, and Xun-\nliang Cai. Libra: Assessing and improving reward model by learning to think. arXiv preprint\narXiv:2507.21645, 2025c.\nRuochen Zhou, Minrui Xu, Shiqi Chen, Junteng Liu, Yunqi Li, Xinxin Lin, Zhengyu Chen, and Junxian\nHe. Does learning mathematical problem-solving generalize to broader reasoning? arXiv preprint\narXiv:2507.04391, 2025d.\nXiangxin Zhou, Zichen Liu, Anya Sims, Haonan Wang, Tianyu Pang, Chongxuan Li, Liang Wang, Min\nLin, and Chao Du. Reinforcing general reasoning without verifiers. arXiv preprint arXiv:2505.21493,\n2025e.\nYang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv, Kongcheng\nZhang, Yihe Zhou, Hengtong Lu, et al. Breaking the exploration bottleneck: Rubric-scaffolded\nreinforcement learning for general llm reasoning. arXiv preprint arXiv:2508.16949, 2025f.\n112\nA Survey of Reinforcement Learning for Large Reasoning Models\nYifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and\nXian Li. Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks. arXiv preprint\narXiv:2503.15478, 2025g.\nYuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding\nr1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025h.\nZijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan\nKian Hsiang Low, and Paul Pu Liang. Mem1: Learning to synergize memory and reasoning for\nefficient long-horizon agents. arXiv preprint arXiv:2506.15841, 2025i.\nDingwei Zhu, Shihan Dou, Zhiheng Xi, Senjie Jin, Guoqiang Zhang, Jiazheng Zhang, Junjie Ye,\nMingxu Chai, Enyu Zhou, Ming Zhang, et al. Vrpo: Rethinking value modeling for robust rl training\nunder noisy supervision. arXiv preprint arXiv:2508.03058, 2025a.\nFengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen,\nYankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large\nlanguage diffusion models. arXiv preprint arXiv:2505.19223, 2025b.\nHui Zhu, Xv Wang, Zhenyu Wang, and Kai Xv. An emotion-sensitive dialogue policy for task-oriented\ndialogue system. Scientific Reports, 14(1):19759, 2024.\nJinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen\nDuan, Weijie Su, Jie Shao, et al. InternVL3: Exploring advanced training and test-time recipes for\nopen-source multimodal models, 2025c.\nQin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing\nHuang, and Junyang Lin. Autologi: Automated generation of logic puzzles for evaluating reasoning\nabilities of large language models. arXiv preprint arXiv:2502.16906, 2025d.\nWenhong Zhu, Ruobing Xie, Rui Wang, Xingwu Sun, Di Wang, and Pengfei Liu. Proximal supervised\nfine-tuning. arXiv preprint arXiv:2508.17784, 2025e.\nYaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan\nMu, Jinghua Wang, Yang Zhao, et al. Codev-r1: Reasoning-enhanced verilog generation. arXiv\npreprint arXiv:2505.24183, 2025f.\nYekun Zhu, Guang Chen, and Chengjun Mao. Think in blocks: Adaptive reasoning from direct\nresponse to deep reasoning. arXiv preprint arXiv:2508.15507, 2025g.\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse\nreinforcement learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.\nBrianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan\nWelker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to\nrobotic control. In Conference on Robot Learning, pages 2165–2183. PMLR, 2023.\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\narXiv:1611.01578, 2016.\nJiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, and Mengdi Wang. Reasonflux-\nprm:\nTrajectory-aware prms for long chain-of-thought reasoning in llms.\narXiv preprint\narXiv:2506.18896, 2025.\n113\nA Survey of Reinforcement Learning for Large Reasoning Models\nYuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding,\nand Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding.\narXiv preprint arXiv:2501.18362, 2025a.\nYuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xin-\nwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084,\n2025b.\nAdam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting\nlanguage models. arXiv preprint arXiv:2506.10943, 2025.\n114\n",
    "content": "# Interpretation and Analysis of \"A Survey of Reinforcement Learning for Large Reasoning Models\"\n\n## I. Core Content and Major Contributions of the Paper\n\nThis paper systematically reviews the **applications and recent advances of Reinforcement Learning (RL) in Large Reasoning Models (LRMs)**. It delves into the role of RL in Large Language Models (LLMs) from five perspectives: **basic components**, **core issues**, **training resources**, **application areas**, and **future directions**. It also introduces the concept of the \"**Verifier’s Law**\", emphasizing that the verifiability of tasks is key to efficient RL optimization.\n\n### Major Contributions:\n1. **Systematic Review of Key RL Components in LRMs**:\n   - Includes **reward design** (Verifiable Rewards, Generative Rewards, Dense Rewards, Unsupervised Rewards, Reward Shaping), **policy optimization** (Policy Gradient, Critic-based Algorithms, Critic-free Algorithms, Off-policy Optimization), and **sampling strategies** (Dynamic Sampling, Hyper-parameter Adjustment).\n\n2. **Identification of Foundational Problems**:\n   - Explores whether RL in reasoning tasks serves to \"**strengthen existing capabilities**\" or \"**discover new ones**\".\n   - Compares RL with Supervised Fine-Tuning (SFT) in terms of generalization and memorization.\n   - Investigates the impact of model priors on RL performance.\n   - Analyzes challenges in training strategies (Tricks vs. Traps) and reward types (Process vs. Outcome) for model training.\n\n3. **Summary of Training Resources**:\n   - Includes **static corpora**, **dynamic environments**, and **infrastructure frameworks** (e.g., OpenRLHF, TRL, veRL, slime, AReaL).\n\n4. **Analysis of Application Areas**:\n   - Explores applications of RL in **coding tasks**, **agent tasks**, **multimodal tasks**, **multi-agent systems**, **healthcare**, **robotics**, and more.\n\n5. **Future Directions**:\n   - Envisions research directions such as **Continual RL**, **memory-based RL**, **model-based RL**, **teaching-efficient reasoning**, **latent space reasoning**, **RL in pre-training**, **RL in diffusion models**, **RL for scientific discovery**, and **co-design of architectures and algorithms**.\n\n---\n\n## II. Breakthroughs and Innovations in the Paper\n\n### 1. **Verifier’s Law**\nThe paper proposes the \"**Verifier’s Law**\": *the higher the verifiability of a task, the more efficient the RL optimization*. This theory offers a new perspective on RL applicability in reasoning tasks. For instance, tasks like math and code reasoning can be efficiently trained using automated verification mechanisms (e.g., unit tests, math solvers), while open-domain tasks (e.g., open-ended Q&A, free writing) face verification challenges, limiting the effectiveness of RL.\n\n### 2. **From Outcome Rewards to Process Rewards**\nTraditional RL focuses on **Outcome Rewards**, while this paper emphasizes the importance of **Process Rewards** in complex reasoning tasks. By incorporating **step-level feedback** and **turn-level rewards**, models can more accurately learn reasoning paths, enhancing interpretability and stability in reasoning.\n\n### 3. **Unsupervised Reward Mechanisms**\nThe paper introduces the concept of **Unsupervised Rewards**, where reward signals are generated internally by the model rather than relying on human annotations. Methods such as **output consistency**, **internal confidence**, and **self-generated knowledge** are explored to build reward functions, offering new directions for large-scale training.\n\n### 4. **Integration of RL and SFT**\nThe paper argues that **SFT and RL are complementary rather than opposing**. SFT can serve as a pre-training stage for RL, accelerating convergence, while RL further enhances generalization. Approaches like **ProRL**, **SRFT**, and **UFT** combine SFT and RL for improved reasoning performance.\n\n### 5. **Dynamic Sampling and Multimodal Reasoning**\nThe paper discusses how **Dynamic Sampling strategies** help models better explore and optimize in multimodal tasks. Techniques such as **TreeRL** and **MCTS** (Monte Carlo Tree Search) offer more granular exploration paths in image and video reasoning tasks.\n\n### 6. **Model-Based RL and World Models**\nThe paper highlights the potential of **Model-Based RL** and **World Models** in future reasoning models. By building internal environment simulators, models can train efficiently without relying on real-world environments, opening new paths for **long-term reasoning tasks** (e.g., scientific discovery).\n\n### 7. **Multi-Agent and Collaborative Learning**\nThe paper emphasizes the advantages of **Multi-Agent Systems** in RL training for collaborative reasoning and task distribution. Approaches like **LLaMAC**, **CTRL**, and **MAGRPO** enhance reasoning capabilities through cooperative mechanisms among agents.\n\n---\n\n## III. Entrepreneurial Project Suggestions Based on the Paper\n\n### 1. **Automated Math and Programming Training Platform**\n**Project Name: MathAI Coach**\n\n- **Background**: While LLMs excel in math and programming, efficiently training them remains a challenge.\n- **Entrepreneurial Direction**: Build an **RL-based math and programming training platform** that provides personalized training and feedback via automated verification (e.g., SymPy, unit tests).\n- **Technical Highlights**:\n  - Use **Verifiable Rewards** for automatic scoring.\n  - Dynamically adjust training difficulty (Curriculum Learning).\n  - Incorporate **Process Rewards** to encourage intermediate reasoning.\n- **Use Cases**:\n  - EdTech platforms.\n  - Competitive programming training (e.g., LeetCode, Codeforces).\n  - Code review and optimization in enterprises.\n\n### 2. **AI-Powered Research Assistant**\n**Project Name: SciAgent**\n\n- **Background**: AI is increasingly used in scientific reasoning, but current models struggle with complex and long-term tasks.\n- **Entrepreneurial Direction**: Develop an **RL-based research agent system** to assist in **automated experiment design, data generation, and paper writing**.\n- **Technical Highlights**:\n  - Use **World Models** for virtual experiment simulation.\n  - Apply **Tree-based RL** (e.g., TreeRPO) for optimizing complex reasoning paths.\n  - Implement **Multi-Agent RL** for multi-task collaboration.\n- **Use Cases**:\n  - Scientific research in chemistry, biology, and medicine.\n  - Automated experimental data generation.\n  - Paper editing and structural optimization.\n\n### 3. **Medical Reasoning and Diagnostic Enhancement System**\n**Project Name: MediThinker**\n\n- **Background**: AI in healthcare needs advanced reasoning capabilities for **diagnosis, case analysis, and treatment recommendations**.\n- **Entrepreneurial Direction**: Build an **RL-based medical reasoning system** to enhance AI performance in **medical Q&A, diagnostic workflows, and treatment recommendations**.\n- **Technical Highlights**:\n  - Use **Model-based Verifiers** for medical answer validation.\n  - Introduce **Unsupervised Reward Design** to reduce reliance on human labeling.\n  - Implement **Multi-Agent RL** for collaborative expert diagnosis.\n- **Use Cases**:\n  - Medical AI assistants.\n  - Electronic Health Record (EHR) analysis.\n  - Clinical Decision Support Systems (CDSS).\n\n### 4. **Multimodal Vision-Language Reasoning Engine**\n**Project Name: VisualThinker**\n\n- **Background**: Multimodal models are advancing, with growing interest in **image, video, and 3D spatial reasoning**.\n- **Entrepreneurial Direction**: Develop a **multimodal reasoning engine** for **image understanding, video reasoning, and 3D navigation**.\n- **Technical Highlights**:\n  - Use **Flow-based GRPO** for image generation and reasoning.\n  - Combine **Dense Rewards** and **Step-level Feedback** to improve reasoning explainability.\n  - Explore **Vision-Language-Action (VLA) models** for real-world decision-making.\n- **Use Cases**:\n  - Autonomous driving vision reasoning.\n  - Medical image analysis.\n  - Video content understanding and automatic summarization.\n\n### 5. **RL-Based Code Generation and Optimization Tool**\n**Project Name: CodeAgent**\n\n- **Background**: LLMs generate code well, but improving code quality and executability with RL remains a challenge.\n- **Entrepreneurial Direction**: Build an **RL-driven code generation and optimization platform** to help developers produce high-quality, executable code.\n- **Technical Highlights**:\n  - Use **Verifiable Rewards** (e.g., unit tests, compilation feedback).\n  - Apply **Dense Token-level Rewards** to improve intermediate code quality.\n  - Use **Multi-Agent RL** for collaborative module optimization.\n- **Use Cases**:\n  - Automated software engineering.\n  - Code review and bug fixing.\n  - Low-code/no-code development platforms.\n\n### 6. **AI-Powered Robotics Reasoning and Operation Training System**\n**Project Name: RoboThinker**\n\n- **Background**: Robots need strong reasoning for **long-term task planning** and **multimodal perception**.\n- **Entrepreneurial Direction**: Build an **RL-based robotics reasoning training system** to enhance decision-making and execution in complex environments.\n- **Technical Highlights**:\n  - Use **Vision-Language-Action (VLA) models** for multimodal reasoning.\n  - Introduce **Self-play RL** for adaptability in unknown environments.\n  - Use **Model-based RL** to simulate real-world environments, reducing training costs.\n- **Use Cases**:\n  - Automated manufacturing and assembly.\n  - Medical robotic assistance.\n  - Autonomous driving and robot navigation.\n\n### 7. **RL-Driven AI Content Creation Platform**\n**Project Name: CreativeAI Studio**\n\n- **Background**: Content creation (e.g., ads, scripts, music, images) increasingly relies on AI, but enhancing creative reasoning with RL is still a challenge.\n- **Entrepreneurial Direction**: Build an **RL-based content generation platform** that combines text, image, and audio reasoning to boost AI creativity and logic.\n- **Technical Highlights**:\n  - Use **Dense Rewards** for process monitoring.\n  - Explore integration of **Diffusion-based LLMs** with RL.\n  - Use **Self-Play** mechanisms for creative evolution.\n- **Use Cases**:\n  - Ad copy generation.\n  - Video script writing.\n  - AI-assisted music and image creation.\n\n---\n\n## Conclusion\n\nThis paper provides a comprehensive overview of the **applications and challenges of reinforcement learning in large reasoning models**, offering multiple practical and innovative directions. Based on these insights, entrepreneurs can develop RL-powered AI products in areas such as **EdTech**, **research assistance**, **medical reasoning**, **multimodal reasoning**, **code generation**, **robotics**, and **content creation**, helping models evolve from \"**memorizing knowledge**\" to \"**autonomous reasoning**\" and ultimately advancing toward **Artificial Superintelligence (ASI)**.",
    "github": "https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
    "hf": ""
}