{
    "id": "2506.14511",
    "title": "MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution",
    "summary": "This paper proposes an end-to-end framework for micro-expression recognition that integrates the ideas of transformer, graph convolution, and convolution. It introduces a new F5C module that directly extracts local-global features from raw frame sequences. Additionally, it addresses the issue of insufficient training data by jointly training optical flow estimation and facial landmark detection tasks.",
    "abstract": "Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available atthis https URL.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Multi-Agent",
    "authors": "Zhiwen Shao,Yifan Cheng,Feiran Li,Yong Zhou,Xuequan Lu,Yuan Xie,Lizhuang Ma",
    "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "comments": "Comments:This paper has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence",
    "keypoint": "- The proposed MOL framework outperforms state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks.\n- MOL achieves competitive performance for both optical flow estimation and facial landmark detection tasks.\n- The framework can capture facial subtle muscle actions in local regions associated with micro-expressions.\n- Experiments show that joint learning of optical flow estimation and facial landmark detection improves MER performance.\n- The proposed F5C block integrating fully-connected convolution and channel correspondence convolution is crucial for extracting effective features.\n- Using pairs of consecutive frames as input performs better than using individual frames or simple arithmetic operations between features.\n- MOL shows robust performance across different numbers of input frames, even when only onset and apex frames are used.\n- Ablation studies demonstrate that all main components of the framework contribute positively to its performance.\n- MOL achieves lower average endpoint error (EPE) for optical flow estimation compared to specialized methods like RAFT.\n- For facial landmark detection, MOL outperforms dedicated methods like HRNetV2 in terms of mean error and failure rate.\n- Visualization results confirm that MOL can accurately capture subtle movements related to micro-expressions.",
    "date": "2025-06-20",
    "paper": "arXiv:2506.14511v1  [cs.CV]  17 Jun 2025\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n1\nMOL: Joint Estimation of Micro-Expression,\nOptical Flow, and Landmark via\nTransformer-Graph-Style Convolution\nZhiwen Shao, Yifan Cheng, Feiran Li, Yong Zhou, Xuequan Lu, Yuan Xie, and Lizhuang Ma\nAbstract—Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME)\nactions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks\nlimited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework\nwith advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed\nof fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw\nframes, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features\nwhile maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations\namong feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-\nglobal features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of\ninsufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on\nCASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture\nfacial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.\nIndex Terms—Micro-action-aware, transformer-style fully-connected convolution, graph-style channel correspondence convolution,\nmicro-expression recognition, optical flow estimation, facial landmark detection\n✦\n1\nINTRODUCTION\nFacial micro-expression recognition (MER) is a popular task\nin the fields of computer vision and affective comput-\ning [1]. It has applications in wide areas such as medicine,\neducation, and criminal investigation. Micro-expressions\n(MEs) are subtle and involuntary that convey genuine\nemotions [2], and contribute to the recognition of mental\ncondition or deception of humans. Different from macro-\nexpressions [3], [4], MEs are fine-grained and last only\nfor a very short interval of time, i.e. not more than 500\nmilliseconds [5]. In literature, MER remains a challenging\n•\nZ. Shao, Y. Cheng, F. Li, and Y. Zhou are with the School of Computer\nScience and Technology, China University of Mining and Technology,\nXuzhou 221116, China, and also with the Mine Digitization Engineering\nResearch Center of the Ministry of Education, Xuzhou 221116, China. Z.\nShao is also with the Department of Computer Science and Engineering,\nThe Hong Kong University of Science and Technology, Clear Water Bay,\nKowloon 999077, Hong Kong, and also with the School of Computer\nScience, Shanghai Jiao Tong University, Shanghai 200240, China. E-mail:\n{zhiwen shao; yifan cheng; feiran li; yzhou}@cumt.edu.cn.\n•\nX. Lu is with the Department of Computer Science and Software Engineer-\ning, The University of Western Australia, Crawley WA 6009, Australia.\nE-mail: bruce.lu@uwa.edu.au.\n•\nY. Xie is with the School of Computer Science and Technology,\nEast China Normal University, Shanghai 200062, China. E-mail:\nyxie@cs.ecnu.edu.cn.\n•\nL. Ma is with the School of Computer Science, Shanghai Jiao Tong\nUniversity, Shanghai 200240, China, also with the MoE Key Lab of\nArtificial Intelligence, Shanghai Jiao Tong University, Shanghai 200240,\nChina, and also with the School of Computer Science and Technology,\nEast China Normal University, Shanghai 200062, China. E-mail: ma-\nlz@cs.sjtu.edu.cn.\nManuscript received April, 2023. (Corresponding authors: Yifan Cheng,\nYong Zhou, and Lizhuang Ma.)\nOptical  Flow\nFacial landmarks\nColor Coding\nLandmarks on ۷௞\nLandmarks on ۷௞ାଵ\n۷௞\n۷௞ାଵ\nFig. 1. Illustration of optical flow and facial landmark differences between\ntwo consecutive frames Ik and Ik+1. We use a color coding to visualize\nthe optical flow, in which the color of each point in the color coding\ndenotes its displacement including orientation and magnitude to the\ncentral point. Although facial subtle muscle actions from Ik to Ik+1 are\nhard to perceive by human eyes, they are reflected in optical flow and\nfacial landmark differences.\nproblem due to the short duration, subtlety, and small-scale\nand low-diversity datasets of MEs.\nOne typical way is to extract hand-crafted features con-\ntaining correlated ME information. Typical hand-crafted\nfeatures include optical flow and histogram of oriented\noptical flow (HOOF) [6] with motion pattern, local binary\npatterns from three orthogonal planes (LBP-TOP) [7] with\nspatio-temporal information, and histogram of oriented gra-\ndients (HOG) [8] and histogram of image gradient orienta-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n2\ntion (HIGO) [9] with local contrast information. However,\nthese features have limited robustness on challenging MEs\nwith short-duration and inconspicuous motions. Besides,\nkey frames like onset, apex, and offset frames of MEs are\nsometimes required in feature extraction [10].\nAnother popular solution involves the use of prevailing\ndeep neural networks. Khor et al. [11] first combined the\noptical flow, the derivatives of the optical flow, and the raw\nimages as input, then used a convolutional neural network\n(CNN) to extract the feature of each frame and used long\nshort-term memory (LSTM) modules to learn the temporal\ndynamics. However, this method relies on the pre-extracted\noptical flow. Reddy et al. [12] adopted a 3D CNN to extract\nfeatures from both spatial and temporal domains, in which\nthe performance is limited by insufficient training samples.\nXia et al. [13] employed macro-expression recognition as\nan auxiliary task, in which macro-expression recognition\nnetwork is used to guide the fine-tuning of MER network\nfrom both label and feature space. However, fine-grained\ninformation is not explicitly emphasized in this method.\nThe above methods suffer from limited capacity of hand-\ncrafted features, requirement of key frames, or fail to thor-\noughly exploit the feature learning ability of deep networks due\nto insufficient training data. To tackle these limitations, we\npropose to integrate automatic feature learning from raw\nframe sequence, capturing of facial motion information, and\nlocalization of facial fine-grained characteristics into an end-\nto-end framework. Considering the prevailing multi-task\nlearning technique is convenient to guide and assist the\ntraining of main task, we design a novel micro-action-aware\ndeep learning framework called MOL that jointly models\nMER, optical flow estimation, and facial landmark detection\nvia transformer-graph-style convolution. As illustrated in\nFig. 1, the two latter tasks are beneficial for capturing facial\nsubtle muscle actions associated with MEs, which relaxes\nthe requirement of large-scale training data. Moreover, we\npropose a novel F5C block to directly extract local-global\nfeatures from raw images, which is combined by our pro-\nposed fully-connected convolution and channel correspon-\ndence convolution. The transformer-style fully-connected\nconvolution can extract local features while maintaining\nglobal receptive fields, and the graph-style channel corre-\nspondence convolution can model the correlations among\nfeature map channels. Finally, we feed a sequence of pair\nfeatures composed of the local-global features of consecu-\ntive two frames into a 3D CNN to achieve MER. The use\nof pair features rather than frame features contributes to\npreserving each sub-action clip, which can also be regarded\nas the sliding windows. The entire framework is end-to-end\nwithout any post-processing operation, and all the modules\nare optimized jointly.\nThe contributions of this paper are threefold:\n•\nWe propose a micro-action-aware joint learning\nframework of MER, optical flow estimation, and\nfacial landmark detection, in which pre-extracted\nfeatures as well as prior knowledge of key frames\nare not required. To our knowledge, joint modeling\nof automatic ME feature learning from raw frame\nsequence, facial motion information capturing, and\nfacial fine-grained characteristic localization via deep\nneural networks has not been done before.\n•\nWe propose a new local-global feature extractor\nnamed F5C composed by fully-connected convolu-\ntion and channel correspondence convolution, which\nintegrates the advantages of transformer, graph con-\nvolution, and vanilla convolution.\n•\nExtensive experiments on benchmark datasets show\nthat our method outperforms the state-of-the-art\nMER approaches, achieves competitive performance\nfor both optical flow estimation and facial landmark\ndetection, and can capture facial subtle muscle ac-\ntions in local regions related to MEs.\n2\nRELATED WORK\nWe review the previous works those are closely related to\nour method, including hand-crafted feature based MER,\ndeep learning based MER, and MER with combination of\nhand-crafted feature and deep learning.\n2.1\nHand-Crafted Feature Based MER\nEarlier works propose hand-crafted features to try to cap-\nture fine-scale ME details. LBP-TOP [7] is a typical hand-\ncrafted feature, which combines temporal information with\nspatial information from three orthogonal planes. Later,\nBen et al. [14] employed hot wheel patterns from three\northogonal planes (HWP-TOP) to make the most of the\ndirectional information. Besides, Wang et al. [15] proposed\nlocal binary patterns with six intersection points (LBP-SIP)\nto avoid repeated coding in LBP-TOP. Another widely used\nfeature is histogram of oriented gradients (HOG) [8], which\ncomputes gradients of image pixels. A histogram of image\ngradient orientation (HIGO) [9] feature is further proposed,\nwhich can maintain the invariance of geometric and optical\ntransformation of images.\nOptical flow describes the action pattern of each pixel\nfrom one frame to another frame, which is highly related\nto MEs. Happy et al. [16] improved histogram of oriented\noptical flow (HOOF) [6] as FHOOF by collecting the action\ndirections into angular bins based on the fuzzy membership\nfunction, and also extended FHOOF to be fuzzy histogram\nof optical flow orientations (FHOFO) by ignoring the action\nmagnitude in computation. Liong et al. [10] introduced bi-\nweighted oriented optical flow (Bi-WOOF) to encode essen-\ntial expressiveness of the apex frame in ME videos.\nHowever, the extraction process of hand-crafted features\noften discards important information, in which the charac-\nteristics of subtle and diverse MEs are hard to be modeled.\nBesides, key frames of MEs are often required, which limits\nthe applicability.\n2.2\nDeep Learning Based MER\nRecently, the prevailing deep learning technique has been\napplied to MER. Reddy et al. [12] employed a 3D CNN\nto achieve MER, which extracts spatial and temporal in-\nformation from raw image sequences. Lei et al. [17] ex-\ntracted shape representations based on facial landmarks,\nand then adopted a graph-temporal convolutional network\n(Graph-TCN) to capture local muscle actions of MEs. Wei et\nal. [18] proposed an attention-based magnification-adaptive\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n3\nConv. Stack\nConv.\nMER \nClassifier\nF5C Block\nFacial Landmark Detection\nFC\nFC\n3D Conv.\n3D Max-Pooling\nMicro-Expression Recognition\nConcat.\nF5C Block\nFully-Connected \nConvolution\nChannel Correspondence Convolution\n…\n1x1 Conv.\nOptical Flow Estimation\n𝐈𝑘+𝟏\n𝐈𝑘\n𝐅𝑘\n𝑟\n𝐅𝑘+1\n𝑟\n𝐅𝑘+1\n𝑔\n𝐅𝑘\n𝑐\n𝐅𝑘\n𝑔\nFlowNet\n…\nk-NN\nmax\nmax\n෡𝐎𝑘\nመ𝐥𝑘+1\nFig. 2. The architecture of our MOL framework. Given a sequence of t frames {I0, I1, · · · , It−1}, MOL first extracts rich feature F(r)\nk\nof each frame\nIk by a stack of vanilla convolutional layers. For each pair of consecutive frames {Ik, Ik+1}, F(r)\nk\nand F(r)\nk+1 are then fed into the same F5C block to\nextract local-global features F(g)\nk\nand F(g)\nk+1, respectively. Afterwards, F(g)\nk+1 is fed into a facial landmark detection module to predict facial landmark\nlocations ˆlk+1 of the frame Ik+1, while F(g)\nk , F(g)\nk+1, Ik, and Ik+1 are simultaneously fed into an optical flow estimation module to predict optical\nflow ˆOk including horizontal component and vertical component. F(g)\nk\nand F(g)\nk+1 are further concatenated to be F(c)\nk\nas the feature of the k-th pair.\nFinally, the sequence of t −1 pair features {F(c)\n0 , F(c)\n1 , · · · , F(c)\nt−2} is fed into a MER module to predict the ME category.\nnetwork (AMAN), in which a magnification attention mod-\nule is used to focus on appropriate magnification levels of\ndifferent MEs, and a frame attention module is used to focus\non discriminative frames in a ME video.\nBesides single MER task based methods, some works\nincorporate auxiliary tasks correlated with MER into a deep\nmulti-task learning framework. Since action units (AUs)\ndescribe facial local muscle actions [19], [20], Xie et al. [21]\nproposed an AU-assisted graph attention convolutional net-\nwork (AU-GACN), which uses graph convolutions to model\nthe correlations among AUs so as to facilitate MER. Xia et\nal. [13] used macro-expression recognition as an auxiliary\ntask, in which macro-expression recognition network can\nguide the fine-tuning of MER network from both label and\nfeature space.\nDifferent from the above methods, we employ an end-to-\nend deep framework for joint learning of MER, optical flow\nestimation, and facial landmark detection.\n2.3\nMER with Combination of Hand-Crafted Feature\nand Deep Learning\nConsidering deep networks are limited by small-scale and\nlow-diversity ME datasets, some approaches combine hand-\ncrafted features with deep learning framework. Verma et\nal. [22] proposed a dynamic image which preserves facial\naction information of a video, and input the dynamic image\nto a lateral accretive hybrid network (LEARNet). Nie et\nal. [23] also generated the dynamic image of the input video,\nand input it to a dual-stream network with two tasks of MER\nand gender recognition.\nAnother commonly used hand-crafted feature is optical\nflow. Zhou et al. [24] calculated the optical flow between\nonset and apex frames of the input ME video, in which\nits horizontal and vertical components are fed into a dual-\ninception network to achieve MER. With the same input\nsetting, Shao et al. [25] achieved AU recognition and MER\nsimultaneously, in which AU features are aggregated into\nME features. Besides, Hu et al. [26] fused local Gabor binary\npattern from three orthogonal panels (LGBP-TOP) feature\nand CNN feature, and then formulated MER as a multi-task\nclassification problem, in which each category classification\ncan be regard as a one-against-all pairwise classification\nproblem.\nAll these methods require pre-extracted hand-crafted\nfeatures, in which the representation power of deep net-\nworks is not thoroughly exploited. In contrast, our network\ndirectly processes raw images, and contains a novel local-\nglobal feature extractor. Besides, instead of treating optical\nflow estimation as a preprocessing, we put it into a joint\nframework to guide the capturing of facial subtle motions.\n3\nMOL\nFOR\nJOINT\nESTIMATION\nOF\nMICRO-\nEXPRESSION, OPTICAL FLOW AND LANDMARK\n3.1\nOverview\nGiven a video clip with t frames {I0, I1, · · · , It−1}, our\nmain goal is to design a micro-action-aware deep learning\nframework to predict ME category of the overall clip, facial\nlandmark locations {ˆl1,ˆl2, · · · ,ˆlt−1} of the last t−1 frames,\nand optical flow { ˆO0, ˆO1, · · · , ˆOt−2} of the t−1 consecutive\nframe pairs {(I0, I1), (I1, I2), · · · , (It−2, It−1)}. We choose\nto directly process raw video clips without the dependence\non hand-crafted features, and discard additional limitations\nlike the prior knowledge of onset and apex frames. Fig. 2\nillustrates the overall structure of our MOL framework.\nA stack of vanilla convolutional layers are first used to\nextract rich feature F(r)\nk\nof the k-th frame Ik in the input\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n4\nTABLE 1\nThe structure of the stack of vanilla convolutional layers for extracting\nrich feature. Cin and Cout denote the number of input channels and\noutput channels, respectively.\nLayer Name\nCin\nCout\nKernel\nStride\nPadding\nConvolution 1\n1\n8\n4 × 4\n(2,2)\n(0,0)\nConvolution 2\n8\n32\n3 × 3\n(2,2)\n(0,0)\nConvolution 3\n32\n64\n2 × 2\n(2,2)\n(1,1)\nConvolution 4\n64\n128\n1 × 1\n(1,1)\n(0,0)\nvideo, respectively. TABLE 1 shows the detailed architecture\nof this module. Then, for each pair of consecutive frames\n{Ik, Ik+1}, an F5C block is used to learn local-global features\nF(g)\nk\nand F(g)\nk+1, respectively. The local-global features are\nshared by three tasks for joint learning, in which optical flow\nestimation and facial landmark detection as auxiliary tasks\nare devised for promoting the main task MER in temporal\nand spatial domains, respectively.\nTo estimate the optical flow ˆOk between Ik and Ik+1,\nwe simultaneously feed Ik, Ik+1, F(g)\nk , and F(g)\nk+1 into an\noptical flow estimation module. To predict the landmark\nlocations ˆlk+1 of Ik+1, we input F(g)\nk+1 to a landmark de-\ntection module. Finally, we feed a sequence of t −1 pair\nfeatures {F(c)\n0 , F(c)\n1 , · · · , F(c)\nt−2} into a 3D CNN to predict\nthe ME category of the whole video clip, in which F(c)\nk\nis\nthe concatenation of F(g)\nk\nand F(g)\nk+1. This use of pair features\nrather than frame features is beneficial for preserving each\nsub-action clip.\n3.2\nF5C Block\nThe architecture of our proposed F5C block is shown in\nthe upper part of Fig. 2. We name this block as F5C be-\ncause it consists of two main operations, fully-connected\nconvolution (FCC) and channel correspondence convolution\n(CCC). FCC is developed from the conventional circular\nconvolution [27] by integrating the style of the prevail-\ning transformer [28], which can gather local information\nfrom local receptive fields like convolutions and extract\nglobal information from the entire spatial locations like\nself-attention [28]. CCC is designed to model the correla-\ntions among feature map channels in a manner of graph\nconvolution [29]. Two residual structures [30] along with\nFCC and CCC are beneficial for mitigating the vanishing\ngradient problem. The design of F5C integrates the merits\nof transformer, graph convolution, and vanilla convolution.\n3.2.1\nFully-Connected Convolution\nIt is known that vanilla convolution works well in extracting\nlocal features. We propose to enhance its ability of extracting\nglobal features from three aspects. First, similar to trans-\nformers [28], [31], we treat each column (in vertical direc-\ntion) or each row (in horizontal direction) of the input as a\npatch, and apply positional embedding to patches to per-\nceive contextual information. Second, we conduct circular\nconvolution on each patch via fully-connected operation to\nenlarge the receptive field. Third, we perform operations in\nboth vertical and horizontal directions to more completely\ncover regions. Such structure is named as transformer-style\nfully-connected convolution.\n𝐗\n𝐏(𝑣)\nFCC-V\nFCC-H\nFCC-H\nFCC-V\nY\n𝐗(𝑣)\n𝐗(ℎ)\n𝐘(ℎ)\n𝐘(𝑣)\n𝐶× 𝐻× 𝑊\n𝐶× 𝐻\n𝐶× 𝑊\n𝐶× 𝐻× 𝑊\n𝐶× 𝐻× 𝑊\n𝐶× 𝐻× 𝑊\n𝐶× 𝐻× 𝑊\n𝐶× 𝐻× 𝑊\n𝐏(ℎ)\n1x1 Conv.\nConcat.\n𝐔(𝑣)\n𝐔(ℎ)\n…\n…\n1x1 Conv.\n⊙𝑣\n⊕𝑣\n⊕ℎ\n⊙ℎ\nFig. 3. The structure of our proposed transformer-style fully-connected\nconvolution. An input feature map X with a size of C × H × W is\nfirst processed by vanilla 1 × 1 convolution, and further goes through\ntwo branches, respectively, in which the first branch consists of FCC-\nV and FCC-H in order while the second branch uses the reverse order.\nThen, the outputs of the two branches are concatenated along with 1×1\nconvolution to obtain the final output Y with the same size as X.\nAs shown in Fig. 3, an FCC is composed of two main\ncomponents, FCC-V in vertical direction and FCC-H in\nhorizontal direction. It uses two branches of FCC-H after\nFCC-V and FCC-V after FCC-H, and then fuses two outputs\nby concatenation and vanilla 1 × 1 convolution. In this way,\nthe receptive field of FCC can cover positions in both vertical\nand horizontal directions so as to extract complete local-\nglobal features.\nSpecifically, given an input X ∈RC×H×W , we conduct\nthe 1 × 1 convolution as a preprocessing. In FCC-V, we first\nemploy a positional embedding [28] to make it aware of the\nposition information:\nX(v) = X ⊕v P(v),\n(1)\nwhere P(v) ∈RC×H denotes the positional embedding, and\n⊕v denotes element-wise sum operation, in which P(v) is\nexpanded with W times along horizontal direction so as to\nmatch the size of X. Then, the output Y(v) ∈RC×H×W at\nelement (c, i, j) is defined as\nY (v)\nc,i,j =\nH−1\nX\ns=0\nU (v)\nc,s X(v)\nc,(i+s)%H,j,\n(2)\nwhere % denotes the remainder operation, and U(v) ∈\nRC×H is a learnable parameter. The elements of X in vertical\ndirection are fully-connected in a circular manner, so we\nname this process as fully-connected convolution-vertical\n(FCC-V). We represent Eq. (2) as Y(v) = U(v) ⊙v X(v) for\nsimplicity.\nSimilarly, the process of FCC-H can be formulated as\nX(h) = X ⊕h P(h),\n(3a)\nY (h)\nc,i,j =\nW −1\nX\ns=0\nU (h)\nc,s X(h)\nc,i,(j+s)%W ,\n(3b)\nwhere P(h) ∈RC×W is the positional embedding, ⊕h\ndenotes the element-wise sum operation by expanding P(h)\nwith H times along vertical direction, U(h) ∈RC×W is\na learnable parameter, and Eq. (3b) can be represented as\nY(h) = U(h) ⊙h X(h) for simplicity.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n5\n3.2.2\nChannel Correspondence Convolution\nSince each feature map channel encodes a type of visual\npattern [32], we propose the CCC to reason the relationships\namong feature map channels so as to further refine the\nextracted local-global features by FCC. The process of CCC\nis illustrated in the upper side of Fig. 2.\nInspired by the structure of dynamic graph convo-\nlution [33], we first construct a k-nearest neighbors (k-\nNN) [34] graph to find similar patterns. In particular, this\ndirected graph is defined as G = (V, E), where the vertex\nset V = {0, 1, · · · , C −1} contains all the C feature map\nchannels, and the edge set E ⊆V × V. The size of the i-th\nfeature map channel is given by H × W, and we reshape\nit to be an HW-dimensional vector for the convenience\nof measuring similarity, denoted as fi. The neighbors of a\nvertex are chosen as the feature map channels with the top-\nk cosine similarities.\nGiven a directed edge fi ←fj, fj is treated as a neighbor\nof fi. To obtain this edge feature ei,j ∈RHW , we incor-\nporate the global information encoded by fi and the local\nneighborhood characteristics captured by fj −fi:\nei,j,s = R(v(1)\ns\n⊤fi + v(2)\ns\n⊤(fj −fi)),\n(4)\nwhere R(·) denotes the rectified linear unit (ReLU) [35]\nfunction, v(1)\ns\n∈RHW and v(2)\ns\n∈RHW are learnable\nparameters, ⊤is used as the transpose of a vector, and ei,j,s\nis the s-th element of ei,j. Eq. (4) can be implemented by the\nconvolution operation.\nFinally, we adopt a maximum aggregation function to\ncapture the most salient features:\nf (o)\ni,s =\nmax\n{j|(i,j)∈E} ei,j,s,\n(5)\nwhere f (o)\ni\n∈RHW is the output of the i-th feature map\nchannel, and is further reshaped to the size of H × W and\nthen is processed by a 1 × 1 convolution. With learnable\nparameters, our proposed CCC can adaptively model the\ncorrelations across feature map channels. As shown in Fig. 2\nand Fig. 3, the input and output sizes of FCC and CCC, as\nwell as their composed F5C are all C × H × W. In this way,\nour proposed FCC, CCC, and F5C can all be used as plug-\nand-play modules.\n3.3\nJoint Learning of Tasks\n3.3.1\nMicro-Expression Recognition\nSince MEs are subtle and short-duration, our method needs\nto check potential sub-action clips between each two con-\nsecutive frames so as to avoid the loss of ME clues. In this\ncase, we concatenate local-global features F(g)\nk\nand F(g)\nk+1\nof each pair of consecutive frames {Ik, Ik+1} to be F(c)\nk ,\nand input the sequence of {F(c)\n0 , F(c)\n1 , · · · , F(c)\nt−2} to a 3D\nCNN. This feature fusion strategy can also be regarded as\nan application of the sliding window mechanism.\nThe detailed structure is shown in the lower right\ncorner of Fig. 2. It consists of a 3D convolutional layer\nand a 3D max-pooling layer, and is followed by a MER\nclassifier with two fully-connected layers. In contrast to a\n2D CNN operated in spatial domain, a 3D CNN uses 3D\nconvolutional kernels to extract features in both spatial and\nFig. 4. The structure of the optical flow estimation module, which con-\nsists of (a) an encoder and (b) a decoder.\ntemporal directions. The use of 3D max-pooling layer is to\nreduce the feature dimension while maintaining important\ninformation.\nConsidering MER is a classification task, we employ the\ncross entropy loss:\nLe = −\nn−1\nX\ns=0\nps log(ˆps),\n(6)\nwhere n is the number of ME classes, and ˆps denotes the\npredicted probability that the sample is in the s-th class.\nps denotes the ground-truth probability, which is 1 if the\nsample is in the s-th class and is 0 otherwise.\n3.3.2\nOptical Flow Estimation\nSince MEs are subtle and low-intensity, it is difficult to\nextract related features from raw frames. Considering the\noptical flow contains motion information of facial muscles,\nwhich is strongly correlated to MEs, we use optical flow\nestimation as an auxiliary task to facilitate the learning of\nME features.\nThe architecture of the optical flow estimation module is\ndetailed in Fig. 4, which is based on FlowNet [36] with an\nencoder and a decoder. The inputs are two raw consecutive\nframes Ik and Ik+1, as well as their local-global features\nF(g)\nk\nand F(g)\nk+1 output by the F5C block. The encoder models\nthe correlations between two frames and extracts multi-level\nfeatures, in which the feature at each level is fed into the\ndecoder for the final estimation of optical flow ˆOk. The\noptical flow estimation loss is defined as\nLf =\n1\nt −1\nt−2\nX\nk=0\nMSE(Ok, ˆOk),\n(7)\nwhere Ok denotes the ground-truth optical flow between Ik\nand Ik+1, and MSE(·) denotes mean squared error (MSE)\nloss.\n3.3.3\nFacial Landmark Detection\nConsidering facial important regions like eyes and lips are\nclosely related to MEs, we introduce another auxiliary task\nof facial landmark detection. The architecture of this task\nmodule is illustrated in the bottom part of Fig. 2, which\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n6\ncontains one convolutional layer and two fully-connected\nlayers. The facial landmark detection loss is defined as\nLm =\n1\nm(t −1)\nt−2\nX\nk=0\nm−1\nX\ns=0\n(|lk+1,2s −ˆlk+1,2s|+\n|lk+1,2s+1 −ˆlk+1,2s+1|)/d(o)\nk+1,\n(8)\nwhere lk+1 = (lk+1,0, lk+1,1, · · · , lk+1,2m−2, lk+1,2m−1) de-\nnotes the ground-truth locations of m landmarks in the\nframe Ik+1, and lk+1,2s and lk+1,2s+1 are the ground-truth\nx-coordinate and y-coordinate of the s-th landmark. Due\nto the differences of face sizes across samples, we use\nthe ground-truth inter-ocular distance d(o)\nk+1 for normaliza-\ntion [37], [38].\n3.3.4\nFull Loss\nIn our micro-action-aware joint learning framework, the full\nloss is composed of Le, Lf, and Lm:\nL = Le + λfLf + λmLm,\n(9)\nwhere λf and λm are parameters to control the importance\nof optical flow estimation and facial landmark detection\ntasks, respectively. Besides the contributions to MER, the\ntwo auxiliary tasks can alleviate negative impact of insuffi-\ncient training data.\n4\nEXPERIMENTS\n4.1\nDatasets and Settings\n4.1.1\nDatasets\nThere are three widely used ME datasets: CASME II [39],\nSAMM [40], and SMIC [41].\n•\nCASME II contains 255 ME videos captured from 26\nsubjects, in which each video has a 280 × 340 frame\nsize at 200 frames per second (FPS). These videos\nare selected from nearly 3, 000 elicited facial move-\nments. Similar to the previous methods [17], [21], we\nuse ME categories of happiness, disgust, repression,\nsurprise, and others for five-classes evaluation, and\nuse ME categories of positive, negative, and surprise\nfor three-classes evaluation.\n•\nSAMM consists of 159 ME videos from 29 subjects,\nwhich are collected by a gray-scale camera at 200 FPS\nin controlled lighting conditions without flickering.\nFollowing the previous works [17], [21], we select ME\ncategories of happiness, anger, contempt, surprise,\nand others for five-classes evaluation, and select\nME categories of positive, negative, and surprise for\nthree-classes evaluation.\n•\nSMIC includes 164 ME videos from 16 subjects. Each\nvideo is recorded at the speed of 100 FPS and is\nlabeled with three ME classes (positive, negative,\nand surprise). It is only adopted for three-classes\nevaluation.\nSince facial landmarks and optical flow are not annotated\nin these datasets, we use a powerful landmark detection\nlibrary Dlib [42], [43] to detect 68 landmarks of each frame,\nand use a popular optical flow algorithm TV-L1 [44] to\ncompute optical flow between frames, both as the ground-\ntruth annotations.\nTABLE 2\nThe number of videos for each ME class in CASME II [39] and\nSAMM [40] datasets, in which “-” denotes the dataset does not contain\nthis class, and the classes used in five-classes evaluation are\nhighlighted with its number in bold.\nClass\nDataset\nCASME II\nSAMM\nHappiness\n32\n26\nAnger\n-\n57\nContempt\n-\n12\nDisgust\n63\n9\nFear\n2\n8\nRepression\n27\n-\nSurprise\n28\n15\nSadness\n4\n6\nOthers\n99\n26\nTABLE 3\nThe number of videos for each of three ME classes used in the\ncomposite dataset evaluation task. “Composite” denotes the\ncombination of SMIC [41], CASME II [39], and SAMM [40] datasets.\nLabel\nDataset\nCASME II\nSAMM\nSMIC\nComposite\nPositive\n32\n26\n51\n109\nNegative\n88\n92\n70\n250\nSurprise\n25\n15\n43\n83\nTotal\n145\n133\n164\n442\n4.1.2\nEvaluation Metrics\nFor single dataset evaluation, we conduct experiments on\nCASME II, SAMM, and SMIC, respectively, in which the\nnumber of videos for each ME category in CASME II and\nSAMM are summarized in TABLE 2. To achieve compre-\nhensive evaluations, we also conduct a composite dataset\nevaluation task [55], in which 24 subjects from CASME II,\n28 subjects from SAMM, and 16 subjects from SMIC are\ncombined into a single composite dataset with three cate-\ngories used. The data distributions of the composite dataset\nevaluation task are given in TABLE 3. Similar to most of the\nprevious works [13], [17], [21], leave-one-subject-out (LOSO)\ncross-validation is employed in the single dataset evaluation\nand the composite dataset evaluation, in which each subject\nis used as the test set in turn while the remaining subjects\nare used as the training set. Besides, following the setting\nin [21], we conduct a cross-dataset evaluation with three\nME classes, in which CASME II and SAMM are used as the\ntraining set, respectively, and SMIC is used as the test set.\nFollowing the previous works [13], [56], we report ac-\ncuracy (Acc) and weighted F1 score (WF1) for the single\ndataset evaluation and the cross-dataset evaluation, and\nreport unweighted F1 score (UF1) and unweighted average\nrecall (UAR) for the composite dataset evaluation. WF1,\nUF1, and UAR are defined as\nWF1 =\nn−1\nX\nj=0\nNj\nN\n2TPj\n2TPj + FPj + FNj\n,\n(10a)\nUF1 = 1\nn\nn−1\nX\nj=0\n2TPj\n2TPj + FPj + FNj\n,\n(10b)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n7\nTABLE 4\nComparison with state-of-the-art methods on CASME II [39] and SAMM [40]. “DL” denotes deep learning based methods, and “NDL” denotes\nnon-deep learning based methods. “PF” denotes the use of pre-extracted hand-crafted features, “RI” denotes the use of raw images, and “KF”\ndenotes the requirement on key frames such as onset, apex, and offset frames of MEs. “Cate.” denotes the number of ME categories. “-” denotes\nthe result is not reported in its paper. The best results are highlighted in bold, and the second best results are highlighted by an underline.\nMethod\nPaper\nType\nCASME II\nSAMM\nCate.\nAcc\nWF1\nCate.\nAcc\nWF1\nOFF-ApexNet [45]\nSPIC’19\nDL+PF+KF\n3\n88.28\n86.97\n3\n68.18\n54.23\nAU-GACN [21]\nMM’20\nDL+RI\n3\n71.20\n35.50\n3\n70.20\n43.30\nGACNN [46]\nCVPRW’21\nDL+PF\n3\n89.66\n86.95\n3\n88.72\n81.18\nMER-Supcon [47]\nPRL’22\nDL+PF+KF\n3\n89.65\n88.06\n3\n81.20\n71.25\nMOL\nOurs\nDF+RI\n3\n91.26\n88.91\n3\n88.36\n82.72\nSparseSampling [48]\nTAFFC’17\nNDL\n5\n49.00\n51.00\n-\n-\n-\nBi-WOOF [10]\nSPIC’18\nNDL+KF\n5\n58.85\n61.00\n-\n-\n-\nHIGO+Mag [9]\nTAFFC’18\nNDL\n5\n67.21\n-\n-\n-\n-\nFHOFO [16]\nTAFFC’19\nNDL\n5\n56.64\n52.48\n-\n-\n-\nDSSN [49]\nICIP’19\nDL+PF+KF\n5\n70.78\n72.97\n5\n57.35\n46.44\nGraph-TCN [17]\nMM’20\nDL+RI+KF\n5\n73.98\n72.46\n5\n75.00\n69.85\nMicroNet [13]\nMM’20\nDL+RI+KF\n5\n75.60\n70.10\n5\n74.10\n73.60\nLGCcon [50]\nTIP’21\nDL+PF+KF\n5\n62.14\n60.00\n5\n35.29\n23.00\nAU-GCN [51]\nCVPRW’21\nDL+PF+KF\n5\n74.27\n70.47\n5\n74.26\n70.45\nGEME [23]\nNeurocomputing’21\nDL+PF\n5\n75.20\n73.54\n5\n55.88\n45.38\nMERSiamC3D [52]\nNeurocomputing’21\nDL+PF+KF\n5\n81.89\n83.00\n5\n68.75\n64.00\nMER-Supcon [47]\nPRL’22\nDL+PF+KF\n5\n73.58\n72.86\n5\n67.65\n62.51\nAMAN [18]\nICASSP’22\nDL+RI\n5\n75.40\n71.25\n5\n68.85\n66.82\nSLSTT [53]\nTAFFC’22\nDL+PF\n5\n75.81\n75.30\n5\n72.39\n64.00\nDynamic [54]\nTAFFC’22\nDL+RI+KF\n5\n72.61\n67.00\n-\n-\n-\nI2Transformer [25]\nAPIN’23\nDL+PF+KF\n5\n74.26\n77.11\n5\n68.91\n73.01\nMOL\nOurs\nDL+RI\n5\n79.23\n75.85\n5\n76.68\n71.90\nUAR = 1\nn\nn−1\nX\nj=0\nTPj\nNj\n,\n(10c)\nwhere Nj denotes the number of samples of the j-th ME\nclass, N denotes the total number of samples, and TPj,\nFPj, and FNj denote the number of true positives, false\npositives, and false negatives for the j-th class, respectively.\nIn the following sections, all the metric results are reported\nin percentages, in which % is omitted for simplicity.\n4.1.3\nImplementation Details\nIn our experiments, we uniformly sample t frames from\na video to obtain a clip as the input of our MOL. We\napply similarity transformation to each frame image based\non facial landmarks, in which facial shape is preserved\nwithout changing the expression. Particularly, each image\nis aligned to 3 × 144 × 144, and is randomly cropped into\n3 × 128 × 128 and further horizontally flipped to enhance\nthe diversity of training data. During testing, each image is\ncentrally cropped into 3 × 128 × 128 to adapt to the input\nsize. The number of frames in the input video clip is set as\nt = 8, the number of facial landmarks is set as m = 68,\nand the dimensions C, H, and W of feature maps in the\nCCC are set as 128, 16, and 16, respectively. The trade-off\nparameters λf and λm are set to 0.1 and 68, respectively.\nTo set an appropriate value for the number k of the nearest\nneighbors in the graph construction of CCC, we conduct\nLOSO cross-validation on the CAMSE II dataset with five\nclasses. In each validation experiment, we select a small set\nfrom the training set as the validation set. k is set as 4 for\nthe overall best performance on the validation sets, and is\nfixed for experiments on other datasets.\nOur MOL is implemented via PyTorch [57], with the\nAdam optimizer [58], an initial learning rate of 5×10−5, and\nTABLE 5\nComparison with state-of-the-art methods on SMIC [41] with three ME\ncategories.\nMethod\nPaper\nType\nSMIC\nAcc\nWF1\nSparseSampling [48]\nTAFFC’17\nNDL\n58.00 60.00\nBi-WOOF [10]\nSPIC’18\nNDL+KF\n62.20 62.00\nHIGO+Mag [9]\nTAFFC’18\nNDL\n68.29\n-\nFHOFO [16]\nTAFFC’19\nNDL\n51.83 52.43\nOFF-ApexNet [45]\nSPIC’19\nDL+PF+KF 67.68 67.09\nMicroNet [13]\nMM’20\nDL+RI+KF 76.80 74.40\nLGCcon [50]\nTIP’21\nDL+PF+KF 63.41 62.00\nGEME [23]\nNeurocomputing’21\nDL+PF\n64.63 61.58\nSLSTT [53]\nTAFFC’22\nDL+PF\n73.17 72.40\nDynamic [54]\nTAFFC’22\nDL+RI+KF 76.06 71.00\nMOL\nOurs\nDL+RI\n80.71 78.81\na mini-batch size of 32. Before training on ME datatsets, we\npre-train MOL on a popular in-the-wild macro-expression\ndataset Aff-Wild2 [59], [60]. It contains 323 videos annotated\nby seven expression categories (neutral, anger, disgust, fear,\nhappiness, sadness, and surprise). We also annotate the\nfacial landmarks of each frame and the optical flow between\nframes by Dlib [42], [43] and TV-L1 [44], respectively. Since\nmacro-expressions are long-duration, we divide each video\ninto multiple clips, and use each clip as the input of MOL.\nAll the experiments are conducted on a single NVIDIA\nGeForce RTX 3090 GPU.\n4.2\nComparison with State-of-the-Art Methods\nWe compare our MOL against state-of-the-art methods\nunder the same evaluation setting. These methods can\nbe divided into non-deep learning (NDL) based methods\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n8\nTABLE 6\nComparison with state-of-the-art methods in terms of composite dataset evaluation [55] with three ME classes.\nMethod\nPaper\nType\nComposite\nCASME II\nSAMM\nSMIC\nUF1\nUAR\nUF1\nUAR\nUF1\nUAR\nUF1\nUAR\nLBP-TOP [7]\nTPAMI’07\nNDL\n58.82\n57.85\n70.26\n74.29\n39.54\n41.02\n20.00\n52.80\nBi-WOOF [10]\nSPIC’18\nNDL+KF\n62.96\n62.27\n78.05\n80.26\n52.11\n51.39\n57.27\n58.29\nOFF-ApexNet [45]\nSPIC’19\nDL+PF+KF\n71.96\n70.96\n87.64\n86.81\n54.09\n53.92\n68.17\n66.95\nCapsuleNet [61]\nFG’19\nDL+RI+KF\n65.20\n65.06\n70.68\n70.18\n62.09\n59.89\n58.20\n58.77\nDual-Inception [24]\nFG’19\nDL+PF\n73.22\n72.78\n86.21\n85.60\n58.68\n56.63\n66.45\n67.26\nSTSTNet [56]\nFG’19\nDL+PF+KF\n73.53\n76.05\n83.82\n86.86\n65.88\n68.10\n68.01\n70.13\nPart+Adversarial+EMR [62]\nFG’19\nDL+PF+KF\n78.85\n78.24\n82.93\n82.09\n77.54\n71.52\n74.61\n75.30\nMicroNet [13]\nMM’20\nDL+RI+KF\n86.40\n85.70\n87.00\n87.20\n82.50\n81.90\n86.40\n81.00\nFRL-DGT [63]\nCVPR’23\nDL+RI+KF\n81.20\n81.10\n91.90\n90.30\n77.20\n75.80\n74.30\n74.90\nSelfME [64]\nCVPR’23\nDL+RI+KF\n-\n-\n90.78\n92.90\n-\n-\n80.25\n81.51\nMOL\nOurs\nDL+RI\n87.79\n85.42\n90.08\n89.92\n89.72\n89.00\n81.00\n72.34\nTABLE 7\nComparison with state-of-the-art methods in terms of cross-dataset\nevaluation [21] with three ME classes. CASME II →SMIC denotes\ntraining on CASME II and testing on SMIC. Each method is presented\nwith its paper in a bracket, and its results are reported by [21].\nMethod\nType\nCASME II\n→SMIC\nSAMM\n→SMIC\nAcc\nWF1\nAcc\nWF1\nSTCNN [12]\n(IJCNN’19)\nDL+RI\n31.40\n19.00\n32.50\n19.00\nCapsuleNet [61]\n(FG’19 )\nDL+RI+KF\n32.20\n15.20\n32.40\n17.90\nMER-GCN [65]\n(MIPR’20)\nDL+RI\n36.70\n27.20\n36.10\n17.80\nAU-GACN [21]\n(MM’20)\nDL+RI\n34.40\n31.90\n45.10\n30.90\nMOL\nDL+RI\n47.13\n43.91\n44.58\n32.32\nand deep learning (DL) based methods. The latter can\nbe further classified into pre-extracted feature (PF) based\nmethods and raw image (RI) based methods according\nto the type of network input. Specifically, NDL methods\ninclude LBP-TOP [7], SparseSampling [48], Bi-WOOF [10],\nHIGO+Mag [9], and FHOFO [16]. DL+PF methods include\nOFF-ApexNet [45], DSSN [49], Dual-Inception [24], STST-\nNet [56], Part+Adversarial+EMR [62] GACNN [46], LGC-\ncon [50], AU-GCN [51], GEME [23], MERSiamC3D [52],\nMER-Supcon [47], SLSTT [53], and I2Transformer [25].\nDL+RI methods include STCNN [12], CapsuleNet [61],\nAU-GACN [21], Graph-TCN [17], MER-GCN [65], Mi-\ncroNet [13], AMAN [18], Dynamic [54], FRL-DGT [63], and\nSelfME [64]. Besides, some of these methods rely on key\nframes (KF) including onset, apex, and offset frames of MEs.\n4.2.1\nSingle Dataset Evaluation\nTABLE 4 and TABLE 5 show the comparison results on\nsingle dataset of CAMSE II, SAMM, and SMIC, respectively.\nIt can be observed that DL based methods are often superior\nto NDL based methods, which demonstrates the strength\nof deep neural networks. Besides, our MOL outperforms\nmost of the previous methods, especially for three-classes\nMER tasks. Note that MicroNet [13], GACNN [46], MER-\nSiamC3D [52], and I2Transformer [25] outperform MOL in\na few cases. However, GACNN uses hand-crafted features,\nMERSiamC3D and I2Transformer rely on hand-crafted fea-\nTABLE 8\nAcc and WF1 results of MOL variants without auxiliary task modules of\noptical flow estimation (OFE) or facial landmark detection (FLD). These\nresults are obtained on CASME II [39] with five classes. The best\nresults are highlighted in bold.\nMethod\nAcc\nWF1\nMOL w/o OFE&FLD\n66.55\n58.93\nMOL w/o FLD\n76.60\n72.74\nMOL w/o OFE\n75.48\n72.31\nMOL\n79.23\n75.85\ntures and key frames, and MicroNet requires key frames,\nin which their applicabilities are limited. In contrast, MOL\ndirectly processes raw frame images without requiring the\nprior knowledge of key frames, which is a more universal\nsolution to MER.\n4.2.2\nComposite Dataset Evaluation\nThe results of composite dataset evaluation are presented in\nTABLE 6. It can be seen that our MOL achieves competitive\nperformance compared to state-of-the-art methods. Besides,\nwe find that our method is the only one DL based method\nwith raw frame images as input. In contrast, most previous\nworks suffer from small-scale and low-diversity training\ndata when using deep neural networks, in which pre-\nextracted hand-crafted features or key frames are required.\nIn our method, this data scarcity issue is alleviated, due\nto the correlated knowledge and information provided by\ntwo auxiliary tasks of optical flow estimation and facial\nlandmark detection.\n4.2.3\nCross-Dataset Evaluation\nWe take CASME II and SAMM as the training set, respec-\ntively, in which SMIC is used as the test set. The comparison\nresults are shown in TABLE 7. It can be seen that our MOL\nachieves the highest WF1 results, which demonstrates the\nstrong generalization ability of MOL. The joint learning with\noptical flow estimation and facial landmark detection facili-\ntates the extraction of ME related features, which improves\nthe robustness and the micro-action-aware ability of our\nmethod for unseen samples.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n9\nTABLE 9\nAcc and WF1 results of MOL variants without partial or complete F5C\nblock. The F5C block includes two main operations of fully-connected\nconvolution (FCC) and channel correspondence convolution (CCC).\nMethod\nAcc\nWF1\nMOL w/o F5C\n62.90\n62.52\nMOL w/o CCC\n76.86\n74.44\nMOL w/o FCC\n65.45\n63.19\nMOL\n79.23\n75.85\nTABLE 10\nAcc and WF1 results of MOL variants with different number of F5C\nblocks in each branch of frame pair.\nNumber of F5C\nAcc\nWF1\n1\n79.23\n75.85\n2\n78.68\n73.46\n3\n78.27\n73.22\n4.3\nAblation Study\nIn this section, we design ablation experiments to investi-\ngate the effectiveness of auxiliary tasks, F5C block, as well as\nfeature fusion strategy for MER input. We conduct ablation\nstudies on the CASME II dataset in terms of five classes.\n4.3.1\nAuxiliary Tasks\nTo investigate the effects of optical flow estimation and fa-\ncial landmark detection tasks on MER, we implement MOL\nw/o OFE and MOL w/o FLD by removing the optical flow\nestimation module and the facial landmark detection mod-\nule of MOL, respectively. Besides, we further implement\nMOL w/o OFE&FLD by removing the both task modules.\nTABLE 8 shows the results of these variants of MOL. We\ncan see that MOL w/o OFE and MOL w/o FLD both per-\nform worse than MOL, and the performance of MOL w/o\nOFE&FLD is further significantly decreased after removing\nboth auxiliary tasks. This is because the removal of optical\nflow estimation or landmark detection weakens the ability\nof learning facial subtle motions. We also notice that MOL\nw/o OFE is slightly worse than MOL w/o FLD, which\nindicates that optical flow estimation is more correlated\nwith MER. In our end-to-end joint learning framework, both\noptical flow estimation and facial landmark detection are\nbeneficial for MER.\n4.3.2\nF5C Block\nWe verify the impact of F5C block as well as its main\ncomponents on MOL in TABLE 9. When removing the\nwhole F5C block, MOL w/o F5C only achieves the Acc of\n62.90 and the WF1 of 62.52. This indicates the importance\nof F5C block. Furthermore, when removing FCC or CCC in\nthe F5C block, MOL w/o FCC and MOL w/o CCC both\nshow poor performances. It is inferred that the removal of\ntransformer-style FCC decreases the capacity of maintaining\nglobal receptive field, and the removal of graph-style CCC\nmay cause the failure of modeling the correlations among\nfeature patterns. Moreover, we implement variants of MOL\nusing multiple stacked F5C blocks in each branch of frame\npair, as presented in TABLE 10. It can be observed that using\nTABLE 11\nAcc and WF1 results of MOL variants with different structures of FCC.\nMethod\nAcc\nWF1\nVanilla Transformer [28]\n76.30\n73.61\nFCC-V\n78.04\n74.58\nFCC-H\n77.64\n74.16\nFCC\n79.23\n75.85\nTABLE 12\nAcc and WF1 results of MOL variants with different feature fusion\nstrategies for MER input. F(c)\nk\nis the concatenation of F(g)\nk\nand F(g)\nk+1,\nF(a)\nk\nis the element-wise addition of F(g)\nk\nand F(g)\nk+1 , and F(s)\nk\nis the\nelement-wise subtraction of F(g)\nk\nand F(g)\nk+1.\nInput\nAcc\nWF1\n{F(g)\n0\n, F(g)\n1\n, · · · , F(g)\nt−2}\n75.04\n72.41\n{F(g)\n1\n, F(g)\n2\n, · · · , F(g)\nt−1}\n73.80\n72.11\n{F(g)\n0\n, F(g)\n1\n, · · · , F(g)\nt−1}\n74.65\n71.94\n{F(a)\n0\n, F(a)\n1\n, · · · , F(a)\nt−2}\n72.51\n69.90\n{F(s)\n0 , F(s)\n1 , · · · , F(s)\nt−2}\n72.13\n69.34\n{F(c)\n0 , F(c)\n1 , · · · , F(c)\nt−2}\n79.23\n75.85\na single FC5 block achieves the best performance. Since the\ntraining sets of ME datasets like CASME II are small-scale\nand low-diversity, one FC5 block is already sufficient to\nextract correlated ME features.\n4.3.3\nFCC vs. Transformer\nTo verify the effect of transformer-style FCC, we implement\nvariants of MOL by replacing the whole FCC block with\nvanilla transformer, FCC-V, and FCC-H, respectively. The\nresults are shown in TABLE 11. It can be seen that the com-\nplete FCC structure outperforms the vanilla transformer.\nBesides, FCC-V or FCC-H with one-directional perception\nstill performs better. This is due to the insufficiency of\nME training data, in which the power of transformer is\nlimited, while our proposed FCC has a stronger learning\nability of both local and global features. The fully-connected\nconvolution in both vertical and horizontal directions works\nthe best in terms of perceiving micro-actions related to MEs.\n4.3.4\nFeature Fusion Strategy for MER Input\nAs shown in Fig. 2, local-global features F(g)\nk\nand F(g)\nk+1\nof consecutive frames Ik and Ik+1 are concatenated to\nbe F(c)\nk\nas the feature of the k-th frame pair, then the\nsequence of t −1 pair features {F(c)\n0 , F(c)\n1 , · · · , F(c)\nt−2} is\nfed into the MER module. Here we investigate the effects\nof different feature fusion strategies for MER input, as\nshown in TABLE 12. If we do not fuse the local-global\nfeatures of each two consecutive frames, the performances\nare all degraded for three types of inputting the first t −1\nframe features {F(g)\n0 , F(g)\n1 , · · · , F(g)\nt−2}, inputting the last\nt −1 frame features {F(g)\n1 , F(g)\n2 , · · · , F(g)\nt−1}, and inputting\nall the t frame features {F(g)\n0 , F(g)\n1 , · · · , F(g)\nt−1}. This is due\nto the sub-action clips between each two consecutive frames,\nwhich are highly related to MEs. We also implement another\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n10\nTABLE 13\nAcc and WF1 results of our MOL with different numbers of input frames\non CASME II [39].\nNumber of Input Frames\nAcc\nWF1\n2 (Onset and Apex)\n79.88\n76.03\n4\n75.17\n72.81\n8\n79.23\n75.85\n16\n78.98\n75.25\nTABLE 14\nAverage EPE results of different optical flow estimation methods on\nCASME II [39]. The best results are highlighted in bold, and the second\nbest results are highlighted by an underline.\nMethod\nAverage EPE\nUnsupFlownet [66]\n1.048\nRAFT [67]\n0.465\nMOL w/o MER&FLD\n1.145\nMOL w/o FLD\n0.972\nMOL w/o MER\n0.839\nMOL\n0.650\ntwo feature fusion strategies, element-wise addition and\nelement-wise subtraction of frame features. However, both\nperformances become much worse, which indicates that\nconcatenation is a better way to preserve sub-action clips.\n4.3.5\nNumber of Input Frames\nHere we investigate the impacts of different numbers of\ninput frames on our MOL. Due to the characteristic of pro-\ncessing pairs of consecutive frames in the input video clip,\nwe can directly feed a video clip composed of only the onset\nand apex frames into MOL without changing the network\nstructure. TABLE 13 shows the results of different inputs\nto MOL, including key frames only and video clips with\ndifferent frame amounts, in which the latter are sampled at\nequal intervals from the raw videos. Compared to the results\nof inputting 8 frames, the performance of inputting onset\nand apex frames shows a slight improvement, which can be\nattributed to the fact that the prior key frames contain the\nmost prominent ME motion characteristics. When inputting\n4 frames, the performance is significantly lower than the\ncases of 8 or 16 frames. This is because when sampling\nat equal intervals, if the number of sampled frames is too\nsmall, the obtained video clips are likely to miss some\nframes with high ME intensities. When inputting 8 or 16\nframes, the results are relatively close. This is because the\nsampled clips already contain enough ME frames with high\nintensities. With the strong feature capture ability of F5C\nblock and the joint framework, our MOL is competitive to\nthose methods relying on key frames.\n4.4\nMOL for Optical Flow Estimation and Facial Land-\nmark Detection\nWe have validated the contributions of optical flow estima-\ntion and facial landmark detection to MER in Sec. 4.3.1. In\nthis section, we also investigate the effectiveness of MER\nfor these two tasks in our micro-action-aware joint learning\nframework.\nTABLE 15\nMean error and failure rate results of different facial landmark detection\nmethods on CASME II [39].\nMethod\nMean Error\nFailure Rate\nTCDCN [68]\n6.75\n3.84\nHRNetV2 [69]\n4.68\n2.99\nMOL w/o MER&OFE\n5.46\n7.87\nMOL w/o OFE\n5.22\n6.60\nMOL w/o MER\n4.89\n2.42\nMOL\n2.35\n2.13\n4.4.1\nMOL for Optical Flow Estimation\nWe implement a baseline method MOL w/o MER&FLD\nwhich only achieves the optical flow estimation task by\nremoving the MER and facial landmark detection modules.\nBesides, we implement MOL w/o MER and MOL w/o\nFLD by discarding MER and facial landmark detection,\nrespectively. We also compare with two recent deep learning\nbased optical flow estimation methods UnsupFlownet [66]\nand RAFT [67] with code released. Average end-point error\n(EPE) is reported as the evaluation metric.\nTABLE 14 shows the average EPE results on the CASME\nII benchmark. With the help of MER and facial landmark\ndetection, MOL outperforms MOL w/o MER&FLD by a\nlarge margin of 0.495. When only removing one module, the\nresults of MOL w/o MER and MOL w/o FLD are also both\nbetter than MOL w/o MER&FLD. It is demonstrated that\nMEs and facial landmarks are closely related to the motion\npatterns captured by optical flow. Furthermore, despite be-\ning designed for MER, our MOL shows competitive results\ncompared with the state-of-the-art optical flow estimation\nmethods.\n4.4.2\nMOL for Facial Landmark Detection\nWe implement MOL w/o MER&OFE as a baseline method\nwhich only achieves the facial landmark detection task\nwithout the MER and optical flow estimation modules.\nBesides, we implement MOL w/o MER and MOL w/o\nOFE by removing MER and optical flow estimation, respec-\ntively. We also compare with two popular facial landmark\ndetection methods TCDCN [68] and HRNetV2 [69] with\ncode released. We report two metrics, inter-ocular distance\nnormalized mean error, and failure rate, in which the mean\nerror larger than 10% is treated as a failure. For simplicity,\n% is omitted in the following mean error and failure rate\nresults.\nTABLE 15 shows the landmark detection results on\nCASME II. We can see that MOL w/o OFE and MOL\nw/o MER both perform better than the baseline MOL\nw/o MER&OFE, which proves that MER and optical flow\nestimation both contribute to facial landmark detection.\nMoreover, MOL outperforms all the above three variants,\nwhich demonstrates that our joint framework is beneficial\nfor improving the performance of facial landmark detection.\nBesides, the comparison with TCDCN and HRNetV2 indi-\ncates the superiority of our MOL for landmark detection.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n11\nColor Coding\n𝐈𝐈𝑘𝑘\n𝐈𝐈𝑘𝑘+𝟏𝟏\ñ𝐈𝐈𝑘𝑘+𝟏𝟏\n෡𝐎𝐎𝑘𝑘\n𝐈𝐈𝑘𝑘\n𝐈𝐈𝑘𝑘+𝟏𝟏\ñ𝐈𝐈𝑘𝑘+𝟏𝟏\n෡𝐎𝐎𝑘𝑘\n𝐈𝐈𝑘𝑘\n𝐈𝐈𝑘𝑘+𝟏𝟏\ñ𝐈𝐈𝑘𝑘+𝟏𝟏\n෡𝐎𝐎𝑘𝑘\nFig. 5. Visualization of optical flow estimation results for three example frame pairs Ik and Ik+1 from CASME II [39], SAMM [40], and SMIC [41],\nrespectively. ˆOk is estimated optical flow, and ˜Ik+1 is warped from Ik+1 by ˆOk. The color coding with its central point as the original point is used\nto visualize the optical flow, in which the color of each point denotes its displacement including orientation and magnitude to the original point. “GT”\ndenotes the ground-truth optical flow.\n4.5\nVisual Results\nTo prove that our proposed method can pay attention to\nthe subtle movements related to MEs, we visualize the esti-\nmated optical flow of different methods on several example\nframe pairs in Fig. 5. For a better view, we use ˆOk with\nhorizontal component ˆAk and vertical component ˆBk to\nwarp Ik+1, in which the warped image ˜Ik+1 at each pixel\nposition (a, b) is formulated as\n˜Ik+1,a,b = Ik+1,a+ ˆ\nAk,a,b,b+ ˆ\nBk,a,b,\n(11)\nwhere bilinear sampling is employed, and ˜Ik+1 is expected\nto be similar to Ik. We can see that our MOL achieves the\nmost accurate optical flow estimations, in which the slightly\nclosed eyes in the first example, the slightly shaking eyes,\nnose and mouth in the second example, and the slightly\nopen eyes in the third example are all captured. When the\nmodules of MER or facial landmark detection are removed,\nmany nonexistent motion patterns are estimated. Therefore,\nour MOL can capture facial subtle muscle movements as-\nsociated with MEs due to the introduction of optical flow\nestimation.\nWe also show facial landmark detection results on sev-\neral example images in Fig. 6. We can observe that our\nMOL more accurately localize facial landmarks than other\nvariants especially for the landmarks in regions of eyes and\nmouth. With the help of landmark detection, our MOL can\ncapture important facial local regions where ME actions\noften occur.\n5\nCONCLUSION\nIn this paper, we have developed a micro-action-aware deep\nlearning framework for joint MER, optical flow estimation,\nand facial landmark detection, in which the three tasks\ncontribute to each other by sharing features. In addition,\nwe have proposed the F5C block including fully-connected\n(a) CASME II\n(b) SAMM\n(c) SMIC\nFig. 6. Facial landmark detection results for example images from\nCASME II [39], SAMM [40], and SMIC [41]. “GT” denotes the ground-\ntruth locations of facial landmarks.\nconvolution and channel correspondence convolution to ex-\ntract local-global features and model the correlations among\nfeature channels. Our framework is end-to-end, and does\nnot depend on pre-extracted features and key frames, which\nis a new solution to MER with good applicability.\nWe have compared our approach with state-of-the-art\nmethods on the challenging SMIC, CASME II, and SAMM\nbenchmarks. It is demonstrated that our approach out-\nperforms previous MER works in terms of single dataset\nevaluation, composite dataset evaluation, and cross-dataset\nevaluation. In addition, we have conducted an ablation\nstudy which indicates that main components in our frame-\nwork are all beneficial for MER. Besides, the experiments on\noptical flow estimation and facial landmark detection show\ncompetitive performance of our method, and the visual\nresults demonstrate that our approach can capture facial\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n12\nsubtle muscle motions in local regions associated with MEs.\nConsidering the MER task requires only a single ME in\nan input video, in the future work we will explore end-\nto-end ME spotting and recognition. In this case, multiple\nMEs are simultaneously localized and recognized. This is a\nchallenging task, and is also promising for the development\nof the ME field.\nACKNOWLEDGMENTS\nThis work was supported by the National Natural Science\nFoundation of China (No. 62472424), the China Postdoctoral\nScience Foundation (No. 2023M732223), and the Hong Kong\nScholars Program (No. XJ2023037/HKSP23EG01). It was\nalso partially supported by the National Natural Science\nFoundation of China (No. 62272461, No. 72192821, No.\n62472282, No. 62222602, and No. 62176092).\nREFERENCES\n[1]\nX. Ben, Y. Ren, J. Zhang, S.-J. Wang, K. Kpalma, W. Meng, and\nY.-J. Liu, “Video-based facial micro-expression analysis: A survey\nof datasets, features and algorithms,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 44, no. 9, pp. 5826–5846, 2022.\n[2]\nP. Ekman, Telling lies: Clues to deceit in the marketplace, politics, and\nmarriage (revised edition).\nWW Norton & Company, 2009.\n[3]\nT. Chen, T. Pu, H. Wu, Y. Xie, L. Liu, and L. Lin, “Cross-domain\nfacial expression recognition: A unified evaluation benchmark and\nadversarial graph learning,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 44, no. 12, pp. 9887–9903, 2022.\n[4]\nZ. Shao, H. Zhu, Y. Zhou, X. Xiang, B. Liu, R. Yao, and L. Ma, “Fa-\ncial action unit detection by adaptively constraining self-attention\nand causally deconfounding sample,” International Journal of Com-\nputer Vision, vol. 133, no. 4, pp. 1711–1726, 2025.\n[5]\nW.-J. Yan, Q. Wu, J. Liang, Y.-H. Chen, and X. Fu, “How fast are\nthe leaked facial expressions: The duration of micro-expressions,”\nJournal of Nonverbal Behavior, vol. 37, no. 4, pp. 217–230, 2013.\n[6]\nR. Chaudhry, A. Ravichandran, G. Hager, and R. Vidal, “His-\ntograms of oriented optical flow and binet-cauchy kernels on non-\nlinear dynamical systems for the recognition of human actions,” in\nIEEE Conference on Computer Vision and Pattern Recognition.\nIEEE,\n2009, pp. 1932–1939.\n[7]\nG. Zhao and M. Pietikainen, “Dynamic texture recognition using\nlocal binary patterns with an application to facial expressions,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 29, no. 6, pp. 915–928, 2007.\n[8]\nA. K. Davison, M. H. Yap, and C. Lansley, “Micro-facial movement\ndetection using individualised baselines and histogram-based de-\nscriptors,” in IEEE International Conference on Systems, Man, and\nCybernetics.\nIEEE, 2015, pp. 1864–1869.\n[9]\nX. Li, X. Hong, A. Moilanen, X. Huang, T. Pfister, G. Zhao, and\nM. Pietik¨ainen, “Towards reading hidden emotions: A compara-\ntive study of spontaneous micro-expression spotting and recog-\nnition methods,” IEEE Transactions on Affective Computing, vol. 9,\nno. 4, pp. 563–577, 2018.\n[10] S.-T. Liong, J. See, K. Wong, and R. C.-W. Phan, “Less is more:\nMicro-expression recognition from video using apex frame,” Sig-\nnal Processing: Image Communication, vol. 62, pp. 82–92, 2018.\n[11] H.-Q. Khor, J. See, R. C. W. Phan, and W. Lin, “Enriched long-\nterm recurrent convolutional network for facial micro-expression\nrecognition,” in IEEE International Conference on Automatic Face &\nGesture Recognition.\nIEEE, 2018, pp. 667–674.\n[12] S. P. T. Reddy, S. T. Karri, S. R. Dubey, and S. Mukherjee, “Sponta-\nneous facial micro-expression recognition using 3d spatiotemporal\nconvolutional neural networks,” in International Joint Conference on\nNeural Networks.\nIEEE, 2019, pp. 1–8.\n[13] B. Xia, W. Wang, S. Wang, and E. Chen, “Learning from macro-\nexpression: a micro-expression recognition framework,” in ACM\nInternational Conference on Multimedia, 2020, pp. 2936–2944.\n[14] X. Ben, X. Jia, R. Yan, X. Zhang, and W. Meng, “Learning effective\nbinary descriptors for micro-expression recognition transferred by\nmacro-information,” Pattern Recognition Letters, vol. 107, pp. 50–58,\n2018.\n[15] Y. Wang, J. See, R. C.-W. Phan, and Y.-H. Oh, “Efficient spatio-\ntemporal local binary patterns for spontaneous facial micro-\nexpression recognition,” PLOS ONE, vol. 10, no. 5, p. e0124674,\n2015.\n[16] S. Happy and A. Routray, “Fuzzy histogram of optical flow\norientations for micro-expression recognition,” IEEE Transactions\non Affective Computing, vol. 10, no. 3, pp. 394–406, 2019.\n[17] L. Lei, J. Li, T. Chen, and S. Li, “A novel graph-tcn with a graph\nstructured representation for micro-expression recognition,” in\nACM International Conference on Multimedia, 2020, pp. 2237–2245.\n[18] M. Wei, W. Zheng, Y. Zong, X. Jiang, C. Lu, and J. Liu, “A\nnovel micro-expression recognition approach using attention-\nbased magnification-adaptive networks,” in IEEE International\nConference on Acoustics, Speech and Signal Processing.\nIEEE, 2022,\npp. 2420–2424.\n[19] X. Liu, K. Yuan, X. Niu, J. Shi, Z. Yu, H. Yue, and J. Yang, “Multi-\nscale promoted self-adjusting correlation learning for facial action\nunit detection,” IEEE Transactions on Affective Computing, 2024.\n[20] K. Yuan, Z. Yu, X. Liu, W. Xie, H. Yue, and J. Yang, “Auformer:\nVision transformers are parameter-efficient facial action unit de-\ntectors,” in European Conference on Computer Vision. Springer, 2024.\n[21] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “Au-assisted\ngraph attention convolutional network for micro-expression\nrecognition,” in ACM International Conference on Multimedia.\nACM, 2020, pp. 2871–2880.\n[22] M. Verma, S. K. Vipparthi, G. Singh, and S. Murala, “Learnet:\nDynamic imaging network for micro expression recognition,”\nIEEE Transactions on Image Processing, vol. 29, pp. 1618–1627, 2020.\n[23] X. Nie, M. A. Takalkar, M. Duan, H. Zhang, and M. Xu, “Geme:\nDual-stream multi-task gender-based micro-expression recogni-\ntion,” Neurocomputing, vol. 427, pp. 13–28, 2021.\n[24] L. Zhou, Q. Mao, and L. Xue, “Dual-inception network for cross-\ndatabase micro-expression recognition,” in IEEE International Con-\nference on Automatic Face & Gesture Recognition.\nIEEE, 2019, pp.\n1–5.\n[25] Z. Shao, F. Li, Y. Zhou, H. Chen, H. Zhu, and R. Yao, “Identity-\ninvariant representation and transformer-style relation for micro-\nexpression recognition,” Applied Intelligence, vol. 53, pp. 19 860–\n19 871, 2023.\n[26] C. Hu, D. Jiang, H. Zou, X. Zuo, and Y. Shu, “Multi-task micro-\nexpression recognition combining deep and handcrafted features,”\nin International Conference on Pattern Recognition.\nIEEE, 2018, pp.\n946–951.\n[27] D. F. Elliott, “Handbook of digital signal processing: Engineering\napplications,” 1987.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems.\nCurran\nAssociates, Inc., 2017, pp. 5998–6008.\n[29] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks\nand deep locally connected networks on graphs,” in International\nConference on Learning Representations, 2014.\n[30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” in IEEE Conference on Computer Vision and\nPattern Recognition.\nIEEE, 2016, pp. 770–778.\n[31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in International Con-\nference on Learning Representations, 2021.\n[32] Q. Zhang, Y. N. Wu, and S.-C. Zhu, “Interpretable convolutional\nneural networks,” in IEEE Conference on Computer Vision and Pat-\ntern Recognition.\nIEEE, 2018, pp. 8827–8836.\n[33] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, “Dynamic graph cnn for learning on point clouds,” Acm\nTransactions On Graphics, vol. 38, no. 5, pp. 1–12, 2019.\n[34] T. Cover and P. Hart, “Nearest neighbor pattern classification,”\nIEEE Transactions on Information Theory, vol. 13, no. 1, pp. 21–27,\n1967.\n[35] V. Nair and G. E. Hinton, “Rectified linear units improve restricted\nboltzmann machines,” in International Conference on Machine Learn-\ning, 2010, pp. 807–814.\n[36] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov,\nP. Van Der Smagt, D. Cremers, and T. Brox, “Flownet: Learning\noptical flow with convolutional networks,” in IEEE International\nConference on Computer Vision, 2015, pp. 2758–2766.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n13\n[37] Z. Shao, H. Zhu, X. Tan, Y. Hao, and L. Ma, “Deep multi-center\nlearning for face alignment,” Neurocomputing, vol. 396, pp. 477–\n486, 2020.\n[38] Z. Shao, Z. Liu, J. Cai, and L. Ma, “Jˆaa-net: Joint facial action unit\ndetection and face alignment via adaptive attention,” International\nJournal of Computer Vision, vol. 129, no. 2, pp. 321–340, 2021.\n[39] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y.-J. Liu, Y.-H. Chen, and\nX. Fu, “Casme ii: An improved spontaneous micro-expression\ndatabase and the baseline evaluation,” PLOS ONE, vol. 9, no. 1,\np. e86041, 2014.\n[40] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap,\n“Samm: A spontaneous micro-facial movement dataset,” IEEE\nTransactions on Affective Computing, vol. 9, no. 1, pp. 116–129, 2016.\n[41] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietik¨ainen, “A\nspontaneous micro-expression database: Inducement, collection\nand baseline,” in IEEE International Conference and Workshops on\nAutomatic Face and Gesture Recognition.\nIEEE, 2013, pp. 1–6.\n[42] D. E. King, “Dlib-ml: A machine learning toolkit,” Journal of\nMachine Learning Research, vol. 10, pp. 1755–1758, 2009.\n[43] V. Kazemi and J. Sullivan, “One millisecond face alignment with\nan ensemble of regression trees,” in IEEE Conference on Computer\nVision and Pattern Recognition.\nIEEE, 2014, pp. 1867–1874.\n[44] C. Zach, T. Pock, and H. Bischof, “A duality based approach for\nrealtime tv-l 1 optical flow,” in Joint Pattern Recognition Symposium.\nSpringer, 2007, pp. 214–223.\n[45] Y. S. Gan, S.-T. Liong, W.-C. Yau, Y.-C. Huang, and L.-K. Tan, “Off-\napexnet on micro-expression recognition system,” Signal Process-\ning: Image Communication, vol. 74, pp. 129–139, 2019.\n[46] A. J. R. Kumar and B. Bhanu, “Micro-expression classification\nbased on landmark relations with graph attention convolutional\nnetwork,” in IEEE Conference on Computer Vision and Pattern Recog-\nnition Workshops.\nIEEE, 2021, pp. 1511–1520.\n[47] R. Zhi, J. Hu, and F. Wan, “Micro-expression recognition with\nsupervised contrastive learning,” Pattern Recognition Letters, vol.\n163, pp. 25–31, 2022.\n[48] A. C. Le Ngo, J. See, and R. C.-W. Phan, “Sparsity in dynamics\nof spontaneous subtle emotions: analysis and application,” IEEE\nTransactions on Affective Computing, vol. 8, no. 3, pp. 396–411, 2017.\n[49] H.-Q. Khor, J. See, S.-T. Liong, R. C. Phan, and W. Lin, “Dual-\nstream shallow networks for facial micro-expression recognition,”\nin IEEE International Conference on Image Processing.\nIEEE, 2019,\npp. 36–40.\n[50] Y. Li, X. Huang, and G. Zhao, “Joint local and global information\nlearning with single apex frame detection for micro-expression\nrecognition,” IEEE Transactions on Image Processing, vol. 30, pp.\n249–263, 2021.\n[51] L. Lei, T. Chen, S. Li, and J. Li, “Micro-expression recognition\nbased on facial graph representation learning and facial action\nunit fusion,” in IEEE Conference on Computer Vision and Pattern\nRecognition Workshops.\nIEEE, 2021, pp. 1571–1580.\n[52] S. Zhao, H. Tao, Y. Zhang, T. Xu, K. Zhang, Z. Hao, and E. Chen, “A\ntwo-stage 3d cnn based learning method for spontaneous micro-\nexpression recognition,” Neurocomputing, vol. 448, pp. 276–289,\n2021.\n[53] L. Zhang, X. Hong, O. Arandjelovi´c, and G. Zhao, “Short and\nlong range relation based spatio-temporal transformer for micro-\nexpression recognition,” IEEE Transactions on Affective Computing,\nvol. 13, no. 4, pp. 1973–1985, 2022.\n[54] B. Sun, S. Cao, D. Li, J. He, and L. Yu, “Dynamic micro-expression\nrecognition using knowledge distillation,” IEEE Transactions on\nAffective Computing, vol. 13, no. 2, pp. 1037–1043, 2022.\n[55] J. See, M. H. Yap, J. Li, X. Hong, and S.-J. Wang, “Megc 2019–\nthe second facial micro-expressions grand challenge,” in IEEE\nInternational Conference on Automatic Face & Gesture Recognition.\nIEEE, 2019, pp. 1–5.\n[56] S.-T. Liong, Y. S. Gan, J. See, H.-Q. Khor, and Y.-C. Huang,\n“Shallow triple stream three-dimensional cnn (ststnet) for micro-\nexpression recognition,” in IEEE International Conference on Auto-\nmatic Face & Gesture Recognition.\nIEEE, 2019, pp. 1–5.\n[57] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,\nA. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,\nB. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative\nstyle, high-performance deep learning library,” in Advances in\nNeural Information Processing Systems.\nCurran Associates, Inc.,\n2019, pp. 8024–8035.\n[58] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” in International Conference on Learning Representations, 2015.\n[59] D. Kollias and S. Zafeiriou, “Expression, affect, action unit recogni-\ntion: Aff-wild2, multi-task learning and arcface,” in British Machine\nVision Conference.\nBMVA Press, 2019, p. 297.\n[60] ——, “Analysing affective behavior in the second abaw2 compe-\ntition,” in IEEE International Conference on Computer Vision Work-\nshops.\nIEEE, 2021, pp. 3652–3660.\n[61] N. Van Quang, J. Chun, and T. Tokuyama, “Capsulenet for micro-\nexpression recognition,” in IEEE International Conference on Auto-\nmatic Face & Gesture Recognition.\nIEEE, 2019, pp. 1–7.\n[62] Y. Liu, H. Du, L. Zheng, and T. Gedeon, “A neural micro-\nexpression recognizer,” in IEEE International Conference on Auto-\nmatic Face & Gesture Recognition.\nIEEE, 2019, pp. 1–4.\n[63] Z. Zhai, J. Zhao, C. Long, W. Xu, S. He, and H. Zhao, “Feature\nrepresentation learning with adaptive displacement generation\nand transformer fusion for micro-expression recognition,” in IEEE\nConference on Computer Vision and Pattern Recognition.\nIEEE, 2023,\npp. 22 086–22 095.\n[64] X. Fan, X. Chen, M. Jiang, A. R. Shahid, and H. Yan, “Selfme: Self-\nsupervised motion learning for micro-expression recognition,” in\nIEEE Conference on Computer Vision and Pattern Recognition.\nIEEE,\n2023, pp. 13 834–13 843.\n[65] L. Lo, H.-X. Xie, H.-H. Shuai, and W.-H. Cheng, “Mer-gcn: Micro-\nexpression recognition based on relation modeling with graph\nconvolutional networks,” in IEEE Conference on Multimedia Infor-\nmation Processing and Retrieval.\nIEEE, 2020, pp. 79–84.\n[66] J. J. Yu, A. W. Harley, and K. G. Derpanis, “Back to basics: Un-\nsupervised learning of optical flow via brightness constancy and\nmotion smoothness,” in European Conference on Computer Vision\nWorkshops, 2016.\n[67] Z. Teed and J. Deng, “Raft: Recurrent all-pairs field transforms for\noptical flow,” in European Conference on Computer Vision. Springer,\n2020, pp. 402–419.\n[68] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Learning deep rep-\nresentation for face alignment with auxiliary attributes,” IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 38,\nno. 5, pp. 918–930, 2016.\n[69] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu,\nY. Mu, M. Tan, X. Wang et al., “Deep high-resolution representa-\ntion learning for visual recognition,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 43, no. 10, pp. 3349–3364,\n2021.\nZhiwen Shao is currently an Associate Profes-\nsor with the China University of Mining and Tech-\nnology, China. He received the B.Eng. degree\nand the Ph.D. degree in Computer Science and\nTechnology from the Northwestern Polytechnical\nUniversity, China and the Shanghai Jiao Tong\nUniversity, China in 2015 and 2020, respectively.\nHis research interests lie in computer vision and\naffective computing. He has served as an Area\nChair for ACM MM, an Associate Editor for TVC,\nand a Publication Chair for CGI.\nYifan Cheng received the B.Eng. and M.S. de-\ngrees from the School of Computer Science\nand Technology, China University of Mining and\nTechnology, China in 2022 and 2025, respec-\ntively. He will purse the Ph.D. degree at the\nSchool of Computer Science and Technology,\nEast China Normal University, China. His re-\nsearch interest lies in facial expression recogni-\ntion.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. X, NO. X, X\n14\nFeiran Li received the B.Eng. degree from\nthe School of Computer Science and Technol-\nogy, China University of Mining and Technology,\nChina in 2023. He is currently pursuing the Ph.D.\ndegree at the University of Chinese Academy\nof Sciences, China. His research interest lies in\nfacial expression recognition.\nYong Zhou is currently a Full Professor with\nthe School of Computer Science and Technol-\nogy, China University of Mining and Technology,\nChina. He received the M.S. and Ph.D. degrees\nin Control Theory and Control Engineering from\nthe China University of Mining and Technology,\nChina in 2003 and 2006, respectively. His re-\nsearch interests include machine learning, intel-\nligence optimization, and data mining. He has\nbeen serving as an Associate Editor for ACM\nTOMM.\nXuequan Lu is currently a Senior Lecturer with\nthe Department of Computer Science and Soft-\nware Engineering, The University of Western\nAustralia, Australia. He spent more than two\nyears as a Research Fellow in Singapore. His\nresearch interests mainly fall into visual comput-\ning. He has served as an Associate Editor for\nNeurocomputing and TVC, a Program Co-Chair\nfor ICVR 2023, as well as a Session/Area Chair\nfor ICME 2023, ICONIP 2022, and ICONIP 2020.\nYuan Xie is currently a Full Professor with the\nSchool of Computer Science and Technology,\nEast China Normal University, China. He is the\nrecipient of the National Science Fund for Excel-\nlent Young Scholars. He received the Ph.D. de-\ngree in Pattern Recognition and Intelligent Sys-\ntems from the Institute of Automation, Chinese\nAcademy of Sciences in 2013. His research in-\nterests include computer vision, machine learn-\ning, and pattern recognition. He has been serv-\ning as a SPC member for IJCAI and CIKM.\nLizhuang Ma is currently a Distinguished Pro-\nfessor with the Department of Computer Science\nand Engineering, Shanghai Jiao Tong Univer-\nsity, China. He is the recipient of the National\nScience Fund for Distinguished Young Schol-\nars. He received the B.S. and Ph.D. degrees\nfrom the Zhejiang University, China in 1985 and\n1991, respectively. His research interests in-\nclude computer graphics, computer animation,\nand theory and applications for computer graph-\nics, CAD/CAM.\n",
    "content": "### 1. What are the core contents and main contributions of this paper?\n\nThis paper proposes an end-to-end deep learning framework called MOL (Micro-action-aware Optimal Learning) for jointly estimating micro-expressions (Micro-Expression Recognition, MER), optical flow (Optical Flow Estimation), and facial landmark detection (Facial Landmark Detection). The main contributions of this framework include:\n\n- **No manual features or keyframes required**: MOL can directly process raw video frames without requiring pre-extracted manual features or keyframes (such as start, vertex, and end frames), making the model more generalizable.\n- **A novel F5C module**: The paper introduces a new local-global feature extractor called F5C (Fully-Connected Convolution and Channel Correspondence Convolution), which combines fully connected convolutions with channel correspondence convolutions. This allows simultaneous extraction of local and global features while modeling correlations between feature maps.\n- **Multi-task joint learning**: By jointly training MER, optical flow estimation, and facial landmark detection, auxiliary tasks (optical flow estimation and facial landmark detection) capture subtle facial motion information, mitigating the problem of insufficient training data.\n\n---\n\n### 2. What breakthroughs or innovations does this paper achieve?\n\nThis paper achieves several breakthroughs and innovations in the following areas:\n\n- **A new micro-action-aware framework**: For the first time, it jointly models automatic micro-expression feature learning, facial motion information capture, and fine-grained facial feature localization through a deep neural network, introducing a novel approach to micro-action perception.\n- **An efficient feature extraction module F5C**: The F5C module combines Transformer-style fully connected convolutions with graph convolution-style channel correspondence convolutions, enabling the extraction of local features while maintaining a global receptive field and modeling correlations between feature channels.\n- **Multi-task collaborative optimization**: By jointly training MER, optical flow estimation, and facial landmark detection, it effectively leverages complementary information between different tasks, significantly improving MER performance while also demonstrating competitive results in optical flow estimation and facial landmark detection.\n- **Extensive experimental validation**: Extensive experiments were conducted on multiple benchmark datasets such as CASME II, SAMM, and SMIC, proving MOL's superior performance in single-dataset evaluation, composite dataset evaluation, and cross-dataset evaluation.\n\n---\n\n### 3. Based on the content of this paper, what are some good ideas for startup projects?\n\nBased on the research content of this paper, here are some potential startup project ideas:\n\n#### (1) **Micro-expression Analysis Tool**\n- **Application scenarios**: Psychological testing, recruitment interviews, criminal investigations, etc.\n- **Function description**: Develop a micro-expression analysis tool based on the MOL framework that captures and analyzes real-time micro-expressions from users' faces to perform emotion analysis. For example, during recruitment interviews, it could help HR identify hidden emotions of candidates; in psychological therapy, it could assist doctors in assessing patients' inner states.\n- **Technical implementation**: Use the MOL framework to process video streams in real-time and output micro-expression categories along with their confidence levels.\n\n#### (2) **Virtual Character Expression Generation**\n- **Application scenarios**: Virtual streamers, game characters, metaverse social platforms, etc.\n- **Function description**: Based on the MOL framework, capture users' micro-expressions and map them onto virtual characters' expressions, making the virtual characters' expressions more nuanced and realistic.\n- **Technical implementation**: Combine the MOL framework's facial landmark detection and micro-expression recognition capabilities to design a real-time expression-driven system.\n\n#### (3) **Emotion Monitoring System in Education**\n- **Application scenarios**: Online education, classroom teaching, etc.\n- **Function description**: Develop a system that captures students' learning states (e.g., attention, confusion, fatigue) via cameras and provides real-time feedback to teachers to optimize teaching effectiveness.\n- **Technical implementation**: Use the MOL framework to analyze students' micro-expressions and combine other behavioral data (e.g., mouse click frequency, keyboard input speed) to generate comprehensive emotional reports.\n\n#### (4) **Mental Health Monitoring in Healthcare**\n- **Application scenarios**: Mental illness diagnosis, mental health assessments, etc.\n- **Function description**: Build a mental health monitoring platform that analyzes changes in patients' micro-expressions to assist doctors in early screening for depression, anxiety, and other diseases.\n- **Technical implementation**: Use the MOL framework to track and analyze patients' micro-expressions over time in specific contexts.\n\n#### (5) **Advertising Effectiveness Evaluation Tool**\n- **Application scenarios**: Marketing, advertising effectiveness assessment, etc.\n- **Function description**: Develop a tool that analyzes viewers' micro-expression reactions while watching advertisements to evaluate their emotional appeal and memorability.\n- **Technical implementation**: Combine the MOL framework with big data analytics technology to generate advertising effectiveness evaluation reports.\n\nThese projects not only leverage the technical advantages of the MOL framework but also allow for customized development tailored to specific scenarios, offering broad market prospects and commercial value.",
    "github": "https://github.com/CYF-cuber/MOL",
    "hf": ""
}