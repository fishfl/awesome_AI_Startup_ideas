{
    "id": "2507.00037",
    "title": "Model Fusion via Neuron Interpolation",
    "summary": "This paper proposes a neuron-based model fusion algorithm that can effectively combine multiple pre-trained neural networks into one network under different training data distributions.",
    "abstract": "Model fusion aims to combine the knowledge of multiple models by creating one representative model that captures the strengths of all of its parents. However, this process is non-trivial due to differences in internal representations, which can stem from permutation invariance, random initialization, or differently distributed training data. We present a novel, neuron-centric family of model fusion algorithms designed to integrate multiple trained neural networks into a single network effectively regardless of training data distribution. Our algorithms group intermediate neurons of parent models to create target representations that the fused model approximates with its corresponding sub-network. Unlike prior approaches, our approach incorporates neuron attribution scores into the fusion process. Furthermore, our algorithms can generalize to arbitrary layer types. Experimental results on various benchmark datasets demonstrate that our algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID fusion scenarios. The code is available atthis https URL.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh",
    "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:5 figures, 15 tables, 23 pages",
    "keypoint": "- The paper introduces a novel neuron-centric family of model fusion algorithms that effectively integrate multiple neural networks into a single model, regardless of training data distribution.\n- These algorithms group intermediate neurons of parent models to create target representations approximated by the fused model's sub-network.\n- Neuron attribution scores are incorporated into the fusion process, which is a key innovation distinguishing this approach from prior methods.\n- The proposed method generalizes to arbitrary layer types, enhancing its applicability across different neural network architectures.\n- Experimental results demonstrate that these algorithms outperform previous fusion techniques, especially in zero-shot and non-IID fusion scenarios.\n- The approach formulates fusion as a representation matching problem decomposed into grouping and approximation components.\n- A two-step procedure is proposed: first clustering neuron outputs while considering their importance scores, then fitting the fused model to match the cluster centers.\n- Theoretical guarantees are provided for the algorithm under specific conditions, including optimality for linear layers with one-to-one neuron matching and approximation bounds for K-means Fusion.\n- The method shows robust performance improvements over existing approaches across various setups, including non-IID, sharded, and full-dataset regimes.",
    "date": "2025-07-03",
    "paper": "arXiv:2507.00037v1  [cs.LG]  18 Jun 2025\nModel Fusion via Neuron Interpolation\nPhoomraphee Luenam∗,1\nAndreas Spanopoulos∗,1\nAmit Sant∗,1\nThomas Hofmann1\nSotiris Anagnostidis1\nSidak Pal Singh1\n1ETH Zürich, Switzerland\nAbstract\nModel fusion aims to combine the knowledge of multiple models by creating\none representative model that captures the strengths of all of its parents. How-\never, this process is non-trivial due to differences in internal representations,\nwhich can stem from permutation invariance, random initialization, or differ-\nently distributed training data. We present a novel, neuron-centric family of\nmodel fusion algorithms designed to integrate multiple trained neural networks\ninto a single network effectively regardless of training data distribution. Our\nalgorithms group intermediate neurons of parent models to create target repre-\nsentations that the fused model approximates with its corresponding sub-network.\nUnlike prior approaches, our approach incorporates neuron attribution scores\ninto the fusion process. Furthermore, our algorithms can generalize to arbitrary\nlayer types. Experimental results on various benchmark datasets demonstrate\nthat our algorithms consistently outperform previous fusion techniques, particu-\nlarly in zero-shot and non-IID fusion scenarios. The code is available at: https:\n//github.com/AndrewSpano/neuron-interpolation-model-fusion.\n1\nIntroduction\nAs Deep Neural Networks (DNNs) grow larger and larger in scale, the field faces a host of new\nchallenges to deliver good outcomes. Models now are more complex than ever before, often having a\nunique profile of strengths and weaknesses and requiring increasingly larger datasets and parameters\nto meet ever increasing performance benchmarks [Sevilla et al., 2022]. This makes the prospect of\nretraining models on new data from scratch almost certainly expensive and possibly even impossible,\nsuch as in cases where data security is paramount. Fusing model parameters addresses both of these\nissues by not requiring expensive retraining or transmission of sensitive data.\nWhile ensembling still provides unparalleled performance, the requirement to maintain many models\nis becoming increasingly impractical with today’s models. In response to this, recent methods like\nOTFusion [Singh and Jaggi, 2020], Git Re-Basin [Ainsworth et al., 2022], and a host of federated\nlearning algorithms [McMahan et al., 2017, Wang et al., 2020] have made strides in merging many\nmodels into one.\nHowever, with increasingly data-hungry models, training data is often scraped together from a diverse\nset of sources [Xu et al., 2024, Gao et al., 2020]. Thus, in the fusion context, if avoiding expensive\nretraining is desired, there is a need for an algorithm that can effectively combine the strengths of\nmodels trained with differently distributed training data.\nContributions. In this work, we introduce a new family of neuron-centric model fusion algorithms\nwith the following key innovations:\n∗Equal contribution. Correspondence to {luenamp, aspanopoulos, amsant}@ethz.ch\nPreprint. Under review.\n• Principled formulation of fusion as a representation matching problem. We provide\na formal perspective on neuron-level fusion by decomposing the fusion objective into two\ninterpretable components: grouping and approximation. This formulation leads to a natural\nand efficient two-stage procedure that enables consistent fusion of models with differing\ninternal representations, including those trained on Non-IID data.\n• Incorporation of neuron importance into the fusion process. Unlike prior approaches that\ntreat all neurons equally, our methods leverage neuron attribution scores to guide the fusion\nprocess toward preserving salient features. We demonstrate that incorporating these scores\nsignificantly improves performance – not only in our own methods, but also when retrofitted\ninto existing techniques such as Git Re-Basin. Comparatively, our proposed framework does\nallows for more flexible alignments and incorporates neuron saliency metrics.\n2\nRelated Work\n2.1\nFusion Algorithms\nOTFusion [Singh and Jaggi, 2020] formulates neuron alignment as a discrete optimal transport\n(OT) problem. Given multiple models, OTFusion selects an initial reference model, aligns each of\nthe others’ layers to its layers via optimal transport on neuron activations, and averages the aligned\nparameters to produce a fused model. One downside of OTFusion is that it was initially designed to\nhandle only linear layers, and hence it could not be applied to arbitrary architectures. Despite this,\nlater work adapted OTFusion to the transformer [Vaswani et al., 2017] architecture [Imfeld et al.,\n2023]. Our proposed algorithm is more general, as it works on any differentiable DNN architecture,\nwithout requiring adaptation as OTFusion does for transformers.\nGit Re-Basin [Ainsworth et al., 2022] proposes three strategies to perform neuron alignment. In\nthis work, we focus exclusively on the “Matching Activations” approach, which is most directly\ncomparable to our methods that make use of activations. Activation-based Git Re-Basin finds a\npermutation matrix that minimizes the L2 distance between neuron activations across models. This\nmakes it equivalent to OTFusion, when the latter is made to use an activations-based ground metric.\nWhile effective, activations-based Git Re-Basin is limited to pairwise fusion, restricts itself to a\none-to-one matching paradigm, and does not account for neuron saliency. We discuss our extension\nto incorporate scores in Appendix C.\nFederated Learning algorithms. In federated learning (FL), decentralized clients train local models\non private data and periodically send updates to a central server, which aggregates them into a global\nmodel. Thus, model fusion on non-IID data distributions is central to FL.\nSome of the most well-known methods in this domain include Federated Averaging (FedAvg)\n[McMahan et al., 2017] and Federated Matching Averaging (FedMA) [Wang et al., 2020]. The\nformer averages model weights across clients proportionally to the number of local updates or data\nsamples. While simple and popular, FedAvg performs poorly under strong non-IID conditions, as\nmodel weights can diverge significantly in the presence of heterogeneous data, especially for deeper\nNeural Network architectures. FedMA aims to address this challenge by aligning neurons before\naveraging. However, FedMA requires retraining the fused model after the alignment of every layer to\nensure performance recovery, which makes it a non-zero-shot fusion algorithm.\nLastly, we briefly mention the traditional methods of aggregating knowledge from different models.\nEnsembles, which average the predictions of base models, typically, represent an upper bound on the\nperformance we can achieve by zero-shot fusion. Vanilla Averaging blindly averages the weights\nof two identical models without alignment. In Knowledge Distillation (KD) [Hinton et al., 2015],\na model is trained to predict soft-targets originating from other model(s). KD can be applied for\nany arbitrary group of models, and Li et al. [2024] recently showed it to be quite efficient for vision\ntransformers [Dosovitskiy et al., 2020].\n2.2\nNeuron Attribution\nA novel feature of our work is incorporating the optional use of neuron attribution scores, commonly\nreferred to as neuron importance scores, into the fusion process to bias the preservation of salient\nfeatures. Uniform importance distributes an equal weight of 1/n on each neuron in a layer of\n2\nn neurons. Conductance [Dhamdhere et al., 2018] extends Integrated Gradients [Sundararajan\net al., 2017], which attributes feature importance by integrating gradients along a straight-line path\nfrom a baseline input to the actual input. Conductance applies the chain rule to propagate these\nimportance scores to hidden neurons, enabling internal saliency estimation. DeepLIFT [Shrikumar\net al., 2017] provides another method for attributing importance scores to neurons. It computes the\ncontribution of each neuron by comparing its activation to a reference activation and propagating\nthese differences through a modified chain rule. Unlike gradient-based methods, DeepLIFT can\nassign non-zero importance scores even when gradients are zero or poorly behaved and requires only\na single backward pass, making it computationally efficient.\n3\nMotivation\nIn a neural network, each group of neurons within a hidden state can be interpreted as encoding a\nspecific amount of information which is used by future layers to produce an output. Consequently,\nminimizing information loss during fusion is crucial to model performance. We can approximate the\ninformation held by a neuron by looking at its distribution across a variety of inputs. Empirically, in\nadjacent tasks like model pruning, it has been shown that removing neurons with high correlation to\nother neurons to minimize information loss is effective [Ebid et al., 2025]. We employ the squared\nEuclidean distance as a proxy for information loss due to its greater computational efficiency and\nbecause it has been shown that minimizing the Euclidean distance between normalized vectors is\nequivalent to maximizing the Pearson Correlation [Berthold and Höppner, 2016]. By weighing this\nloss with importance scores, we especially penalize distorting more essential neurons. This motivates\na method that explicitly minimizes this neuron distance between the fused neuron and its predecessors\nfrom the base models on average. Alternatively, one can conceptualize the objective as placing the\nactivations of the fused model at any layer “close” to all the activations of the base models at that\nsame layer. We now introduce the necessary notation to formalize this approach.\nA (Deep) Neural Network (DNN) is a function fw : Rd 7→Ro parameterized by weights w\nwhere d is the number of input features and o is the number of output features. DNNs can be\nconceptualized as a “composition” of L sequential functions: fw = f L\nwL ◦· · · ◦f 1\nw1. Additionally,\nwe can arbitrarily group those functions into so-called composition-levels. For example, we can\ndecompose fw = f 3\nw3 ◦f 2\nw2 ◦f 1\nw1 into fw = c\nf 2 bw2 ◦c\nf 1 bw1, where c\nf 2 bw2\n=\nf 3\nw3 and\nc\nf 1 bw1 = f 2\nw2 ◦f 1\nw1. Functionally, levels are simply a paradigm through which one can flexibly\npartition a DNN into a favorable composition of functions. An observation worth mentioning is that\nlayers with branching (e.g. skip connections) can be contained in one level to satisfy the sequential\ncompositionality. Given that function composition is associative, we gain the flexibility of partitioning\nDNNs of different width/depths into networks with the same number of levels.\nWe now introduce notation regarding fusion. Let {M1, M2, . . . , Mn} be a collection of pretrained\nbase models. Each model is strategically partitioned to have L levels. For a given input x ∈Rd,\nlet zMk,i(x) ∈RdMk,i be the output vector at level i of model Mk. Note that the output z is a\nfunction of the weights wMk,i the input x and previous levels – but we omit them to simplify notation.\nFor convolutional and transformer outputs, we define as dMk,i the output channels and hidden size,\nrespectively, and the remaining dimensions can be flattened out with the batch dimension.\nWe denote by zi = concat(zM1,i, . . . , zMn,i) ∈Rdi the concatenated outputs at level i, of total size\ndi. For a fused model F, we write zF,i ∈RlF,i for its outputs at level i. We also use si\nj for the\nimportance score of neuron j at level i (of the concatenated outputs zi). Now, we can introduce the\nrepresentation cost of using weights wF,i at level i of the fused model F (for a given input x):\nJwF,i(x) =\ndi\nX\nj=1\nmin\nk∈{1,...,lF,i}\n\u001a\u0010\nzF,i\nk\n(x) −zi\nj (x)\n\u00112\u001b\n= min\nP∈P\n\r\rPzF,i (x) −zi (x)\n\r\r2\n2\n(1)\nwhere lF,i is the width of F at level i and P is the set of binary right stochastic matrices of dimension\ndi ×lF,i. Intuitively, for each neuron in the concatenated base outputs zi(x), we compute the squared\nL2 distance to its closest neuron in the fused model output zF,i(x), and sum these distances.\n3\nTo solve for the desired weights wF,i of the fused model F at level i, we can minimize this cost:\nwF,i∗= arg min\nwF,i\nE\nx∼D [JwF,i (x)]\n(2)\nPopular layer-wise fusion algorithms [Singh and Jaggi, 2020, Ainsworth et al., 2022] attempt to solve\nEq. (2) by solving for soft/hard permutation matrices, and then proceed to average the aligned weights\n(of the base models), layer-by-layer. In Section 5 we empirically demonstrate that these algorithms:\na) fail to perform on par with base models in zero-shot fusion, i.e. they require a fine-tuning phase;\nand b) fail to achieve meaningful transfer knowledge in the non-IID regime.\nWe hypothesize that these shortcomings arise because: a) current algorithms ignore how the fused\nmodel evolves across levels, treating each level in isolation by only tracking past permutation or\nalignment matrices, without accounting for potential non-linear changes in the semantic content of\nprevious level outputs; and b) not all neurons contribute equally to a model’s prediction on average,\nbut are getting averaged with equal importance. This can especially be an issue in the non-IID setting,\nwhere the activations on unseen data may be noisy or irrelevant.\n4\nProposed Method\n4.1\nTwo-Step Objective\nTo overcome these limitations we propose two novel ideas. First, we incorporate neuron importance\nscores in the main objective. This creates the neuron-aware representation cost objective:\nJIMP\nwF,i(x) =\ndi\nX\nj=1\nsi\nj\nmin\nk∈{1...lF,i}\n\u001a\u0010\nzF,i\nk\n(x) −zi\nj (x)\n\u00112\u001b\n= min\nP∈P\n\r\rSi \u0000PzF,i (x) −zi (x)\n\u0001\r\r2\n2\n(3)\nwhere Si = diag\n\u0000si\n1, . . . , si\ndi\n\u0001 1\n2 is a diagonal matrix containing the square roots of neuron importance\nscores. The key difference from Eq. (1) is the introduction of weights si\nj, to account for possibly less\nimportant outputs. For notational clarity, we will now omit the input x.\nSecond, we decouple the objective by introducing an auxiliary vector T of size lF,i, which we refer\nto as the target vector and it enables a more tractable decomposition of the cost function. This yields\nthe following Theorem.\nTheorem 1. Let T ∈RlF,i be a vector whose components are the importance-weighted means of\nclustered level outputs. Then the cost in Eq. (3) can be decomposed as follows:\nJIMP\nwF,i =\nlF,i\nX\nk=1\nX\nj:kj=k\nsi\nj\n\u0010\nzF,i\nk\n−Tk\n\u00112\n|\n{z\n}\napproximation error\n+\nlF,i\nX\nk=1\nX\nj:kj=k\nsi\nj\n\u0000Tk −zi\nj\n\u00012\n|\n{z\n}\ngrouping error\n(4)\nwhere kj are the assignment indices that achieve the minimum in Eq. (3).\nProof. See Appendix A.\nWe observe that the sum of JIMP\nwF,i(x) over a batch is subdifferentiable with respect to wF,i and\nthat this objective could potentially be optimized with subgradient descent in the same spirit as the\nweighted K-means objective [Bottou and Bengio, 1994]. However, we leave this for future work.\nThe resulting objective naturally decomposes into two interpretable components. The grouping error\nmeasures how well the original neurons cluster together – specifically, how far each output zi\nj is from\nthe importance-weighted cluster center Tk it was assigned to. The approximation error, on the other\nhand, quantifies how closely the fused model can reproduce these cluster centers through its own\noutput neurons zF,i.\nFor a batch of B values of x, minimizing the total grouping error by constructing an optimal T through\nan effective clustering of the layer outputs zi\nj is a critical challenge. This problem corresponds to the\nK-means problem in RB which is known to be NP-hard in general [Aloise et al., 2009]. Nonetheless,\n4\nModel 1\nModel 2\nCompute level outputs\nand group neurons\ncluster centers\nFused model F\nFit weights to minimize\ndistance between fused level\noutputs and cluster centers\nFigure 1: Overview of our method. Given two MLPs with two hidden layers of size 4 with each layer defined as\nits own level, we compute level outputs for the base models, cluster their neurons, and train the first level of the\nfused model to match the cluster centers using MSE loss. The process is repeated in subsequent levels.\npractical approximation algorithms such as Lloyd’s algorithm [Lloyd, 1982] or local search-based\nmethods [Kanungo et al., 2002] can be employed to obtain effective solutions in practice.\nAfter having determined T (and hence, the clusters kj), we can solve for the weights of a level\nby minimizing the approximation error, which is equivalent to a weighted mean squared error loss\nfunction. We can choose to either keep the weights of previous level frozen (and just optimize the\ncurrent level), or optimize the whole subnetwork. Without loss of generality, we choose the former:\nwF,i∗= arg min\nwF,i\nE\nx∼D\n\n\nlF,i\nX\nk=1\nX\nj:kj=k\nsi\nj\n\u0010\nzF,i\nk\n(x) −Tk(x)\n\u00112\n\n\n(5)\nThis decomposition offers a more interpretable and stable optimization target by isolating the chal-\nlenges of clustering and function fitting, instead of trying to solve them jointly.\n4.2\nProposed Algorithm\nFollowing the derivation in Section 4.1, we propose a two-step algorithm to find weights wF,i that\nminimize Eq. (4) in expectation. The algorithm constructs the fused model in a bottom-up way,\niterating through the levels of base models, and producing the corresponding level of the fused model.\nAt each level, the algorithm computes a matching/clustering to minimize the grouping error. It then\nuses this clustering to compute cluster centers (weighted by importance score). Finally, it uses the\ncluster centers as a target for the current level’s outputs. We then fit weights of the fused model to\nminimize the approximation error. In practice, we use a finite batch to approximate the expected\nrepresentation cost. An intuitive illustration of our algorithm can be found in Fig. 1. High-level\npseudo-code is provided in Algorithm 1. For more implementation details, refer to Appendix F.\n4.3\nMinimizing the Grouping and Approximation Errors\n4.3.1\nGrouping Error\nFor the grouping error, we distinguish between two cases based on the architecture of the base models\nand the constraints imposed on the assignment. More details can be found in Appendix B.1.\n(a) Equal-size models with level-wise one-to-one matching. This case induces a cost that can be\nminimized using the Hungarian Matching algorithm [Kuhn, 1955] as discussed in Section 3. We refer\nto this special case of our algorithm as Hungarian Fusion (HF).\n(b) General case with arbitrary model sizes. In this setup, the grouping problem becomes a general\nclustering task. As highlighted in Section 4.1, a solution can be approximated using heuristic K-means\nalgorithms. We refer to this general case as K-means Fusion (KF).\n4.3.2\nApproximation Error\nFor the approximation error, we distinguish two cases. More details can be found in Appendix B.2.\n5\nAlgorithm 1 Neuron Interpolation Model Fusion\nRequire: Trained models {Mk}n\nk=1, neuron importance scores {s(i)\nj }L\ni=1, input dataset X ∈RB×d\nEnsure: Fused model F with weights WF\n1: for each level i = 1, 2, . . . , L do\n2:\nGather layer outputs: zi = concat\n\u0000zM1,i, . . . , zMK,i\u0001\n3:\nGather scores: si = concat\n\u0000sM1,i, . . . , sMK,i\u0001\n4:\nObtain an assignment kj for every level output zi\nj\n{Hungarian Matching or K-means}\n5:\nfor each centroid k = 1, . . . , lF,i do\n6:\nT i\nk ←\nP\nj:kj =k si\njzi\nj\nP\nj:kj =k si\nj\n{Compute importance-weighted mean}\n7:\nend for\n8:\nOptimize the weights wF,i of the current level using:\nwF,i ←arg min\nw\nB\nX\nm=1\n\n\nlF,i\nX\nk=1\nX\nj:kj=k\nsi\nj\n\u0010\nzF,i\nk\n(xm) −T i\nk(xm)\n\u00112\n\n\n{Fit Fused Model level output to target means}\n9: end for\n10: return Fused Model F\n(a) Linear levels. When all trainable levels are affine transformations such as fully connected or\nconvolutional layers, the outputs zi\nj are linear functions of the level weights wF,i, and the problem\nbecomes weighted least squares, which has a closed-form solution. In this case, we project the base\nmodels’ level outputs onto the image of the previous fused level’s outputs, before running HF or KF\nfor the level. We call these algorithms the Linear version of HF and KF respectively.\n(b) General case. For arbitrary differentiable (and possibly non-linear) levels, we optimize Eq. (5)\nusing stochastic gradient descent (SGD). In this setting, initialization plays a critical role. A simple\nstrategy is to initialize with the weights of the corresponding level from one of the base models,\nperturbed by noise ϵ. Note that for linear levels, the objective is convex, reducing to case (a). For this\ncase, we will only consider KF and refer to this algorithm as the Gradient version of KF.\n4.3.3\nGuarantees\nWe now present theoretical guarantees for Algorithm 1 under specific conditions.\nTheorem 2. Let the parametrized levels of the base models and fused model be affine functions.\nThen:\n(a) For two models with equal-sized levels and a one-to-one matching constraint, the Hungarian\nFusion algorithm returns an optimal solution to the decoupled objective in Eq. (4).\n(b) For an arbitrary number of models with possibly different numbers of neurons per level, the\nK-means Fusion algorithm produces a solution whose representation cost is at most (9 + ϵ)\ntimes the optimal, when using the local-search algorithm from Kanungo et al. [2002].\nProof. See Appendix A.\n5\nExperiments\nWe evaluate our fusion algorithms across three distinct training regimes, each characterized by a\ndifferent data distribution used to train the base models. This setup is designed to test the robustness\nand generality of our method. We benchmark our algorithms against previous baselines, ensembles,\nvanilla averaging, and last-layer knowledge distillation (KD) for non-IID setups. We restrict KD to\nthe last layer because most of the fusion algorithms require only a small amount of data, and applying\nfull KD led to diminished performance due to overfitting in our experiments.\n6\n5.1\nOn the Performance of Base Models in Non-IID Setups\nBefore presenting our results, we emphasize an important consideration in evaluating base model\nperformance under non-IID conditions. In these settings, each model has access to only a small and\noften imbalanced portion of the dataset, which naturally limits its accuracy. For example, in 6-way or\n8-way splits, each model sees only 10-20% of the full data, leading to lower performance compared\nto centralized training.\nDespite these constraints, gains in this setup are meaningful. Improving over weak, heterogeneous\nbase models in a zero-shot setting is a challenging task, and our method demonstrates robustness\nwhere baseline methods fail.\n5.2\nNon-IID Setup\nThe primary goal of our method is to effectively combine the knowledge of similar models trained\non unbalanced datasets. To test this, we split the data into unbalanced, non-IID, and disjoint\nsubsets to train each model on. This induces class-specific overfitting and leads to diverse semantic\nrepresentations, increasing the potential gains from fusion. This setup is also common across\nFederated Learning applications, where various edge devices have access to a small, skewed subset\nof the available data. More details about this data partitioning process can be found in Appendix D.\nWe evaluate algorithms in a zero-shot fusion setting, where models are not allowed to retrain after\nfusing. We train and fuse VGG11s on CIFAR-10 and ViTs on CIFAR-100. In these experiments, all\nresults are averaged over five random seeds used to split the initial datasets and to initialize and train\nthe base models. For each seed, base models are first sorted by performance before being averaged to\nget the individual performance. Each fusion algorithm is applied once per neuron importance score,\nand we report the result with the best-performing importance score. Results for VGG11 are shown in\nTable 1, and for ViT in Table 8.\nTable 1: Test accuracy comparison when fusing VGG11 networks on CIFAR-10 for Non-IID splits. Fusion\nwas performed using 400 data points sampled from the dataset seen by the first model. The same fusion data\nwas used for all algorithms. For the full table, refer to Table 11.\nMethod\n2-WAY SPLIT\n4-WAY SPLIT\n8-WAY SPLIT\nBase Models\n81.5,\n75.1\n77.2, 75.3,\n73.6, 66.8\n69.2, 68.0, 66.0, 63.0\n61.8, 59.6, 56.1, 52.6\nVanilla Averaging\n11.2 ±2.2\n10.0 ±0.0\n10.0 ±0.0\nLast-layer KD\n82.7 ±0.9\n60.1 ±6.8\n43.7 ±9.8\nEnsemble\n87.5 ±0.4\n83.7 ±0.9\n76.6 ±0.7\nOTFusion1\n43.4 ±6.9\n20.4 ±7.6\n12.9 ±2.6\nGit Re-Basin1\n71.8 ±3.4\nN/A\nN/A\nHungarian Linear Fusion (Ours)\n84.6 ±0.8\nN/A\nN/A\nK-means Linear Fusion (Ours)\n84.5 ±0.8\n78.9 ±1.3\n69.6 ±1.2\nK-means Gradient Fusion (Ours)\n83.3 ±1.1\n78.6 ±2.1\n68.6 ±4.5\n5.3\nSharded Setup\nThe next setup, which we refer to as “sharded”, represents an extreme non-IID case where each model\nsees all the samples from specific classes, but for a small subset of all classes (per model). Although\nless common in practice, it serves as a stress test and highlights our method’s robustness.\nSimilarly to the Non-IID case, we again evaluate our algorithms on zero-shot fusion, and for VGGs\nand ViTs on CIFAR-10 and CIFAR-100 respectively. The experiments setup is the same as with\nNon-IID. The results for VGGs can be found in Table 9, and the results for ViTs in Table 2.\n1Git Re-Basin reduces to OTFusion when solving the OT problem exactly with uniform importance scores. In\npractice, OTFusion uses preactivations [Singh and Jaggi, 2020], while Git Re-Basin uses activations [Ainsworth\net al., 2022]; we follow these defaults. Empirically, both yield the same fused model when using preactivations.\n7\nTable 2: Test accuracy comparison when fusing ViT networks on CIFAR-100 for Sharded splits. Fusion was\nperformed using 7000 data points sampled from the dataset seen by the first model. The same fusion data was\nused for last-layer KD as well. For the full table, please refer to Table 13.\nMETHOD\n2-WAY SPLIT\n4-WAY SPLIT\n6-WAY SPLIT\nBase Models\n37.9,\n36.6\n19.4, 18.9,\n18.7, 18.3\n13.7, 13.3, 13.1\n12.8, 12.4, 11.5\nVanilla Averaging\n1.8 ±0.4\n1.2 ±0.2\n1.0 ±0.0\nLast-layer KD\n36.0 ±1.7\n18.8 ±2.3\n12.8 ±0.5\nEnsemble\n61.9 ±0.6\n50.2 ±0.6\n44.3 ±0.5\nK-means Gradient Fusion (Ours)\n49.4 ±0.8\n38.5 ±1.1\n33.1 ±1.1\n5.4\nFull Dataset Setup\nIn their recent work, Solans et al. [2024] highlight that most fusion and aggregation algorithms are\nevaluated under a single training configuration, limiting insight into broader applicability. To this\nend, we also evaluate the performance of our algorithms on the setup where all models are trained on\nthe full dataset but with different random seeds. Despite similar performance, these models occupy\ndifferent regions in weight space, which presents an opportunity for a performance boost by fusion.\nTable 3: Test accuracy comparison for ViTs trained and fine-tuned on CIFAR-100 in the Full-Dataset setup.\nFusion used 6000 samples from the shared training dataset. We note that base models can slightly benefit from\nthe fine-tuning phase as well; In 2-way fusion, the best accuracy ramped up to an average of 73.6, while for\n4-way no model surpassed 74.2. Vanilla Averaging failed to retrain. For the full table, please refer to Table 14.\nZero-shot\nFine-tuning\nBase Models\nVanilla\nAveraging\nK-means Gradient\nFusion (Ours)\nEnsemble\nK-means Gradient\nFusion (Ours)\nEnsemble\n2-way fusion:\n73.4, 73.1\n2.0 ±0.1 -71.4\n72.7 ±0.1 -0.7\n75.3 ±0.1 +1.9\n74.0 ±0.3 +0.6\n75.3 ±0.0 +1.9\nInference Cost:\n×1\n×1\n×1\n×2\n×1\n×2\n4-way fusion:\n74.2, 73.6, 73.3, 73.7\n1.3 ±0.1 -72.9\n71.8 ±1.6 -2.5\n76.7 ±0.0 +2.5\n74.5 ±0.2 +0.3\n76.9 ±0.2 +2.7\nInference Cost:\n×1\n×1\n×1\n×4\n×1\n×4\nWhile our algorithms were not specifically tailored to address this setup, we demonstrate in our\nexperiments that we outperform baselines. Following prior work [Singh and Jaggi, 2020, Ainsworth\net al., 2022], we evaluate our fusion algorithms by applying a shared fine-tuning phase to recover\npost-fusion performance. As in previous setups, we train VGGs on CIFAR-10 and ViTs on CIFAR-\n100, with three random seeds. Base models are sorted by pre-fusion accuracy and averaged. VGG11\nresults are in Table 10, and ViT in Table 3.\nFine-tuning is identical across all models: 200 epochs on the full dataset using a cosine learning rate\nschedule starting at 10−5. This differs from Imfeld et al. [2023], who perform a grid search over\nlearning rates and epochs to select the best checkpoint. Although we intended to benchmark against\nTransformer OTFusion [Imfeld et al., 2023], we found the publicly available implementation difficult\nto adapt within our experimental timeline. Nevertheless, their fused models exhibit low zero-shot\naccuracy (< 12%), with the fine-tuning yielding modest gains ( +0.8), despite significantly weaker\nbase model performance (< 65% accuracy).\n5.5\nFused Model Analysis and Insights\nIn this subsection we dive deeper into the clustering component of our algorithms, and we briefly\nexplore the quality of fused models, in terms of robustness.\nAs shown in Appendix G, neuron importance scores consistently improve performance across fused\nmodels. Their main effect is during clustering, where they transform standard K-means into weighted\nK-means. Approximation is less sensitive to these scores (Appendix B.2). To illustrate this, we ablate\non three MLPs (MNIST, non-IID, width 10) and compare clustering with uniform vs. Conductance-\n8\nbased importance scores. Fig. 2 shows that weighting changes both cluster assignments and center\npositions.\nFigure 2: Clustering behavior with 3 MLPs on non-IID\nMNIST is shown under uniform (top) and conductance-\nbased (bottom) importance scores. Marker sizes reflect\nneuron importance.\nIn Fig. 3, we visualize the loss and accu-\nracy landscapes of ViT base models trained on\nCIFAR-10, along with one of our fused mod-\nels, using linear interpolation between model\nweights, as in Hochreiter and Schmidhuber\n[1997]. The contour plot reveals flatter basins\naround the fused model, which, as noted by\nHochreiter and Schmidhuber [1997], is often\nindicative of improved generalization.\n6\nLimitations\nDespite the strong performance of our proposed\nalgorithms – often surpassing baselines and ap-\nproaching that of ensembles – there remain areas\nfor improvement.\nFirst, the gradient-based variant of our approach\nis sensitive to hyperparameters and requires non-\ntrivial tuning. While we were experimentally\nable to verify a set of hyperparameters that gen-\neralize well across our setups, this is not suffi-\ncient to claim universality.\nSecond, the effectiveness of our gradient-based\nfusion algorithm appears to scale with the size\nof the fusion dataset. While this dependency\nis encouraging in that more data yields better performance, it also highlights a shortcoming: the\nalgorithm underperforms its linear counterpart when both are trained on the same limited data. This\nneed can slow down the algorithm as demonstrated in Table 7.\n7\nFuture Work\nFigure 3: ViT Landscapes for CIFAR10,\nshowing fused model from K-means Gradi-\nent Fusion using DeepLIFT scores\nWhile our approach demonstrates strong empirical perfor-\nmance across a variety of fusion scenarios, several avenues\nremain open for further exploration and refinement.\nAutomating Fusion Hyperparameter Selection and\nLevel Partitioning. Future work could explore principled\nmethods for automatically tuning fusion hyperparameters,\nincluding the choice of level granularity and whether to\nend levels at before or after activation functions. In par-\nticular, task-specific heuristics could help adapt the level\ndefinitions for complex architectures like transformers.\nBeyond K-means for Grouping Optimization. While\nthis work uses Hungarian Matching and K-means Clus-\ntering to minimize grouping error, future research could\nevaluate more expressive or adaptive clustering methods to\nbetter capture inter-neuron similarity and improve fusion\nperformance.\nUnderstanding the Contribution of Error Components.\nA deeper investigation into the relative impact of grouping\nerror versus approximation error on fusion quality could\nyield valuable insights.\n9\nFusion as a Model Compression Technique. Future work could explore using fusion to compress a\nlarge model into a smaller, more cost-efficient network. By leveraging the clustering process to distill\nrepresentations, this paradigm may offer a new approach to model compression without retraining.\n8\nConclusion\nIn this work, we introduced a novel neuron-aware approach to model fusion that supports fusing\ngeneric model architectures. Our algorithms, to our knowledge, are the first to successfully incorporate\nneuron importance scores in model fusion. Furthermore, our empirical results across diverse setups-\nincluding non-IID, sharded, and full-dataset regimes-consistently show that our fusion algorithms are\ncompetitive with or outperform existing baselines, especially in the zero-shot scenario, and in some\ncases approach ensemble-level performance.\nAcknowledgments\nAndreas Spanopoulos gratefully acknowledges financial support from the John S. Latsis Public\nBenefit Foundation, the Bodossaki Foundation, and the Union of Greek Shipowners.\nReferences\nSamuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo\npermutation symmetries. arXiv preprint arXiv:2209.04836, 2022.\nDaniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of euclidean sum-of-squares\nclustering. Machine learning, 75:245–248, 2009.\nMichael R. Berthold and Frank Höppner. On clustering time series using euclidean distance and pearson\ncorrelation. CoRR, abs/1601.02213, 2016. URL http://arxiv.org/abs/1601.02213.\nLeon Bottou and Yoshua Bengio. Convergence properties of the k-means algorithms. Advances in neural\ninformation processing systems, 7, 1994.\nEkin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning\naugmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.\nKedar Dhamdhere, Mukund Sundararajan, and Qiqi Yan.\nHow important is a neuron?\narXiv preprint\narXiv:1805.12233, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\nShaimaa EK Ebid, Samah El-Tantawy, Doaa Shawky, and Hany L Abdel-Malek. Correlation-based pruning\nalgorithm with weight compensation for feedforward neural networks. Neural Computing and Applications,\npages 1–17, 2025.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027, 2020.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nSepp Hochreiter and Jürgen Schmidhuber. Flat minima. Neural computation, 9(1):1–42, 1997.\nMoritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and Sidak Pal Singh.\nTransformer fusion with optimal transport. arXiv preprint arXiv:2310.05719, 2023.\nTapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu.\nA local search approximation algorithm for k-means clustering. In Proceedings of the eighteenth annual\nsymposium on Computational geometry, pages 10–18, 2002.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\n10\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander\nMelnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. Captum: A unified and\ngeneric model interpretability library for pytorch, 2020. URL https://arxiv.org/abs/2009.07896.\nHarold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):\n83–97, 1955.\nAlex Li, Yuandong Tian, Beidi Chen, Deepak Pathak, and Xinlei Chen. On the surprising effectiveness of\nattention transfer for vision transformers. Advances in Neural Information Processing Systems, 37:113963–\n113990, 2024.\nStuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129–137,\n1982.\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-\nefficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages\n1273–1282. PMLR, 2017.\nomihub777. Vit-cifar. https://github.com/omihub777/ViT-CIFAR. Accessed: 2025-05-16.\nJaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. Compute\ntrends across three eras of machine learning. In 2022 International Joint Conference on Neural Networks\n(IJCNN), pages 1–8. IEEE, 2022.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating\nactivation differences. In International conference on machine learning, pages 3145–3153. PMlR, 2017.\nSidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural Information\nProcessing Systems, 33:22045–22055, 2020.\nDavid Solans, Mikko Heikkila, Andrea Vitaletti, Nicolas Kourtellis, Aris Anagnostopoulos, Ioannis Chatzigian-\nnakis, et al. Non-iid data in federated learning: A survey with taxonomy, metrics, methods, frameworks and\nfuture directions. arXiv preprint arXiv:2411.12377, 2024.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International\nconference on machine learning, pages 3319–3328. PMLR, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated\nlearning with matched averaging. arXiv preprint arXiv:2002.06440, 2020.\nYipei Xu, Dakuan Lu, Jiaqing Liang, Jin Zhao, Xintao Wang, Hengkui Wu, Ken Chen, Liujiang Liu, Yingsi Xin,\nXuepeng Liu, et al. Source prompt: Coordinated pre-training of language models on diverse corpora from\nmultiple sources. In Proceedings of the 33rd ACM International Conference on Information and Knowledge\nManagement, pages 2732–2741, 2024.\nA\nProofs\nWe first present the proof for Theorem 1.\nProof. We proceed to decompose the cost of Eq. (3) as follows, omitting the input x for notational\nclarity:\nJIMP\nwF,i =\ndi\nX\nj=1\nsi\nj min\nk\n\u001a\u0010\nzF,i\nk\n−zi\nj\n\u00112\u001b\n=\ndi\nX\nj=1\nsi\nj\n\u0010\nzF,i\nkj −zi\nj\n\u00112\n\u0012\nkj ∈arg min\nk\n\u0010\nzF,i\nk\n−zi\nj\n\u00112\u0013\n=\ndi\nX\nj=1\nsi\nj\n\u0014\u0010\nzF,i\nkj −Tkj\n\u00112\n+ 2\n\u0010\nzF,i\nkj −Tkj\n\u0011 \u0000Tkj −zi\nj\n\u0001\n+\n\u0000Tkj −zi\nj\n\u00012\u0015\n\u0000±Tkj\n\u0001\n(6)\n11\nSince no constraints are imposed on the target vector T, we retain the flexibility to define it in a\nmanner that simplifies the optimization. Specifically, if we rearrange the summation in Eq. (6) into\ntwo nested summations – first over neurons in the fused model (i.e., k = 1, . . . , lF,i), and then over\noriginal neurons j assigned to each k (i.e., j : kj = k)) – and define Tk as the importance-weighted\nmean of the assigned level outputs, i.e., Tk =\nP\nj:kj =k si\njzi\nj\nP\nj:kj =k si\nj , then the cross-term in Eq. (6) vanishes:\ndi\nX\nj=1\n2si\nj\n\u0010\nzF,i\nkj −Tkj\n\u0011 \u0000Tkj −zi\nj\n\u0001\n= 2\nlF,i\nX\nk=1\nX\nj:kj=k\nsi\nj\n\u0010\nzF,i\nk\n−Tk\n\u0011 \u0000Tk −zi\nj\n\u0001\n= 2\nlF,i\nX\nk=1\n\u0010\nzF,i\nk\n−Tk\n\u0011 X\nj:kj=k\nsi\nj\n\u0000Tk −zi\nj\n\u0001\n= 2\nlF,i\nX\nk=1\n\u0010\nzF,i\nk\n−Tk\n\u0011 X\nj:kj=k\nsi\nj\n P\nl:kl=k si\nlzi\nl\nP\nl:kl=k si\nl\n−zi\nj\n!\n= 2\nlF,i\nX\nk=1\n\u0010\nzF,i\nk\n−Tk\n\u0011\n\nX\nl:kl=k\nsi\nlzi\nl −\nX\nj:kj=k\nsi\njzi\nj\n\n\n= 0\nTherefore, Eq. (6) becomes:\nJIMP\nwF,i =\ndi\nX\nj=1\nsi\nj\n\u0014\u0010\nzF,i\nkj −Tkj\n\u00112\n+\n\u0000Tkj −zi\nj\n\u00012\u0015\n=\nlF,i\nX\nk=1\nX\nj:kj=k\nsi\nj\n\u0014\u0010\nzF,i\nk\n−Tk\n\u00112\n+ si\nj\n\u0000Tk −zi\nj\n\u00012\u0015\n(re-express sum over output neurons)\nWe now present the proof for Theorem 2.\nProof. We analyze Hungarian Fusion and K-means Fusion separately.\n(a) Optimality of Hungarian Fusion: As established in Section 4.1, the decoupled objective Eq. (4)\nseparates into two terms: the grouping error and the approximation error. For linear levels, the layer\noutputs zF,i\nk\nare affine functions of the weights wF,i, and thus the approximation error reduces to a\nweighted least squares problem, which admits a closed-form solution.\nConsequently, minimizing the total cost reduces to minimizing the grouping error. In the special case\nof two models with equal-sized layers and one-to-one neuron matching, this corresponds to a Linear\nSum Assignment Problem (LSAP) with importance-weighted squared error as the cost matrix. The\nHungarian algorithm solves this problem exactly in polynomial time [Kuhn, 1955], hence the HF\nalgorithm returns the optimal solution.\n(b) Approximation Bound for K-means Fusion: We consider a fixed assignment of neurons, where\nwe assign the jth base model neuron to the fused neuron kj. Consider the total representation cost\nassociated with all the base model neurons assigned to the kth fused neuron for the layer i (in this\nproof we will omit the index i to simplify the notation). That is, the total representation cost of all\nbase neurons j that have kj = k. For a single sample, this is P\nj:kj=k sj(zF\nk −zj)2. If we stack this\nover the samples, we get P\nj:kj=k sj||zF\nk −zj||2, where zF\nk , zk ∈Rn are column vectors with each\nentry corresponding to the preactivation for one input sample. We let the previous layer’s activations\nbe X ∈Rn×lF,i−1. Now, since it is a linear function of the previous layer’s activations, we have\nzF\nk = Xwk, with wk being the weights associated with the kth fused neuron (here we append a\ncolumn of 1s to X if we also have a bias term). Consider the projection matrix P = X(XT X)+XT\n12\nthat projects vectors to the column space of X. Then I −P projects to the orthogonal complement of\nthe image of X. Recall that PX = X and (I −P)X = 0. We then have\nX\nj:kj=k\nsj||zF\nk −zj||2 =\nX\nj:kj=k\nsj||Xwk −zj||2\n=\nX\nj:kj=k\nsj||P(Xwk −zj) + (I −P)(Xwk −zj)||2\n=\nX\nj:kj=k\nsj(||P(Xwk −zj)||2 + ||(I −P)(Xwk −zj)||2)\n(due to orthogonality)\n=\nX\nj:kj=k\nsj||Xwk −Pzj||2 +\nX\nj:kj=k\nsj||(I −P)zj||2\nWe consider the term P\nj:kj=k sj||Xwk −Pzj||2.\nLetting ¯zk\n=\nP\nj:kj =k sjzj\nP\nj:kj =k sj\n(such that\nP\nj:kj=k sj(P¯zk −Pzj) = 0), we have\nX\nj:kj=k\nsj||Xwk −Pzj||2 =\nX\nj:kj=k\nsj||(Xwk −P¯zk) + (P¯zk −Pzj)||2\n=\nX\nj:kj=k\nsj||Xwk −P¯zk||2 + 2\nX\nj:kj=k\nsj(Xwk −P¯zk)T (P¯zk −Pzj)\n+\nX\nj:kj=k\nsj||P¯zk −Pzj||2\n=\nX\nj:kj=k\nsj||Xwk −P¯zk||2 + 2(Xwk −P¯zk)T\n\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\u0018\n:0\nX\nj:kj=k\nsj(P¯zk −Pzj)\n+\nX\nj:kj=k\nsj||P¯zk −Pzj||2\n=\nX\nj:kj=k\nsj||Xwk −P¯zk||2 +\nX\nj:kj=k\nsj||P¯zk −Pzj||2\nThus, substituting this back, and summing over k to get the whole layer’s representation cost, we get\nlF\nX\nk=1\nX\nj:kj=k\nsj||zF\nk −zj||2 =\nlF\nX\nk=1\nX\nj:kj=k\nsj||Xwk −P¯zk||2\n+\nlF\nX\nk=1\nX\nj:kj=k\nsj||P¯zk −Pzj||2 +\nd\nX\nj=1\nsj||(I −P)zj||2\nNote first that the last term in the sum on the right is always incurred independently of the assignment\nkj or the chosen weights wk.\nAssume we have a solution that obtains the optimal representation cost OPT. Then, since the first\nterm in the sum is nonnegative, the representative cost of the optimal solution is at least the optimal\nvalue of PlF\nk=1\nP\nj:kj=k sj||P¯zk −Pzj||2 + Pd\nj=1 sj||(I −P)zj||2. If we let OPTgrouping be the\nminimum possible value of PlF\nk=1\nP\nj:kj=k sj||P¯zk −Pzj||2, then we get OPT ≥OPTgrouping +\nPd\nj=1 sj||(I −P)zj||2\n13\nWe now consider the sum on the right for KF. Recall that KF first projects the activations to the image\nof X and then finds the K-means clusters. That is, it finds the K-means clustering of (Pzj)d\nj=1 to\nfind the assignments kj, aiming to minimize PlF\nk=1\nP\nj:kj=k sj||P¯zk −Pzj||2. Then, it fits wk such\nthat ||Xwk −P¯zk||2 is minimized by solving a weighted least squares problem as elaborated in\nAppendix B.2. Notice that this achieves ||Xwk −P¯zk||2 = 0, since, as P¯zk is in the image of X\nby virtue of being a projection to that image, there is a wk satisfying Xwk = P¯zk. Thus, the total\nrepresentation cost for KF for any layer will be PlF\nk=1\nP\nj:kj=k sj||P¯zk −Pzj||2 + Pd\nj=1 sj||(I −\nP)zj||2, where the first term is the the weighted K-means loss with respect to clustering the projected\npreactivations, and the second term is always incurred regardless of assignment or chosen weights.\nThe K-means problem is NP-hard in general [Aloise et al., 2009]. However, the local-search algorithm\nintroduced by Kanungo et al. [2002], provides a (9 + ϵ)-approximation guarantee for the weighted\nK-means cost under squared Euclidean distance. By using this algorithm to construct the cluster\nassignments in KF, we obtain a solution where the term PlF\nk=1\nP\nj:kj=k sj||P¯zk −Pzj||2 is within\na constant factor of the optimal cost OPTgrouping.\nThus, the cost for this layer attained by KF is at most (9+ϵ)OPTgrouping +Pd\nj=1 sj||(I−P)zj||2 ≤\n(9 + ϵ)(OPTgrouping + Pd\nj=1 sj||(I −P)zj||2) ≤(9 + ϵ)OPT, showing that it is a (9 + ϵ)-\napproximation for the layer representation cost.\nB\nEfficiently Minimizing the Fusion Errors\nB.1\nMinimizing the Grouping Error\nFor the special case (a), we can re-express Eq. (3) as a sum over the two base models:\nJIMP\nwF,i =\ndM1,i\nX\nj=1\nsM1,i\nj\nmin\nk\n\u001a\u0010\nzF,i\nk\n−zM1,i\nj\n\u00112\u001b\n+\ndM2,i\nX\nj=1\nsM2,i\nj\nmin\nk\n\u001a\u0010\nzF,i\nk\n−zM2,i\nj\n\u00112\u001b\n(7)\nThis cost can also be decomposed analogously to Eq. (4). We can now define the cost of matching\nneuron j1 of M1 to neuron j2 of M2, as the cost of trying to approximate the resulting cluster center\nTj1,j2, from any neuron of the fused model F. A simpler alternative/heuristic is to just compute the\ndistances between the level outputs. After defining this cost matrix, we can then run the Hungarian\nMatching algorithm [Kuhn, 1955] to find a one-to-one matching that minimizes Eq. (7).\nFor the general case (b), we can run Lloyd’s [Lloyd, 1982] algorithm for K-means, since it usually\noffers a good tradeoff between simplicity and effectiveness. By making use of the K-means++\ninitialization, we usually get better clusterings. Note that for this task, we treat neurons “as data\npoints”, in the sense that we want to cluster neurons together. Therefore, the features of a neuron\nare the values (e.g. activations) it takes for different samples x in the dataset. Clustering is then\nperformed over these vectors using importance-weighted K-means, where the number of clusters “k”\nis set to the desired number of neurons in the fused layer. Once clusters are formed, we compute the\ncorresponding importance-weighted centroids, giving us the target matrix T ∈RB×lF,i, where B is\nthe batch dimension.\nB.2\nMinimizing the Approximation Error\nFor the special case where a level is a linear function of its weights wF,i, i.e. zF,i = XwF,i for\nsome X, then the approximation error in Eq. (5) admits to a closed-form weighted-MSE solution:\nwF,i∗=\n\u0000X⊤SX\n\u0001+ X⊤STi\nwhere S = diag(si), and ()+ denotes the Moore-Penrose pseudoinverse.\nFor the general case, where a level is a non-linear differentiable function of its weights, we can obtain\na local minima by optimizing with SGD. In practice we often use Adam [Kingma and Ba, 2014].\n14\n(a) Conductance\n(b) DeepLIFT\nFigure 4: Histogram of Conductance and DeepLIFT Importance Scores\nFurthermore, in practice, we do not minimize the weighted MSE, but rather the plain MSE. This is\ndue to the fact that neurons with low importance scores will barely change if we use the weighted\nMSE. While this could only minimally affect the representation loss for the current level, it could\nlead to noisy inputs in later levels, or just poor intermediate representations. In our experiments we\nnoticed that, especially in non-IID cases, many neurons tend to be attributed scores that are virtually\nzero as seen in Fig. 4.\nC\nComparison of Hungarian Fusion with Existing Algorithms\nWe note that Hungarian Fusion is in spirit very similar to both OTFusion [Singh and Jaggi, 2020]\nand the activations-based version of Git Re-Basin [Ainsworth et al., 2022]. In the case of Equal-size\nmodels with level-wise one-to-one matching that we restrict HF to, they all construct a matching\nbetween neurons (or equivalent a transport map or permutation matrix to align the second model to\nthe first) by solving a minimum cost matching problem.\nHowever, a key difference is that HF accounts for the effect of refitting the previous layers and\ncorrespondingly refits the weights of the current layer being considered to minimize the effect of the\naccumulated error. Empirically, this significantly improves the zero-shot performance significantly as\nshown in our experimental results.\nWith regards to the incorporation of neuron importance scores, for OTFusion, Singh and Jaggi [2020]\nproposed using the neuron importance as the probability measure assigned to a neuron in the optimal\ntransport problem setup, while Ainsworth et al. [2022] did not discuss applying importance scores\nin Git Re-Basin. In our experiments, we follow the recommendation of Singh and Jaggi [2020] for\nOTFusion and for Git Re-Basin we weigh the neuron weights according to the neuron’s score when\naveraging, in the same manner as we do for HF.\nD\nData Partitioning Regimes\nD.1\nNon-IID Splits\nTo simulate non-IID splits, we utilize a Dirichlet distribution to create unbalanced class distributions\nacross models. Let Nc represent the number of data points in class c, and αk denote the concentration\nparameter for model k. The data for each class c is distributed across models as:\nsplitk ∼Dir(α1, . . . , αk)\nwhere Dir(·) represents the Dirichlet distribution. The concentration parameters αk are arranged in\nan ordered sequence, determined by the parameter min_max_ratio:\nalpha_min = 1.0\nmin_max_ratio = 0.2\nalpha_max = alpha_min / min_max_ratio\n15\nalphas = linspace(alpha_min , alpha_max , min_max_ratio )\nA smaller ratio results in a wider disparity between splits, amplifying the heterogeneity. The Dirichlet-\ndistributed probabilities dictate the number of samples assigned to each model for class c, ensuring\nthat splits exhibit diverse and non-uniform class distributions. Random shuffling of class indices and\nconcentration parameters for each class c introduces additional randomness in the resulting splits.\nD.2\nSharded Splits\nIn the sharded partitioning regime, the dataset is split such that each model receives examples from a\ndisjoint subset of classes. That is, no two models share any classes in their local datasets, simulating\na strongly divergently distributed base model training datasets scenario based on class exclusivity\nrather than distributional imbalance.\nLet C denote the set of all classes in the dataset. The class set is first randomly permuted and then\nevenly partitioned into K disjoint subsets, where K is the number of models. Each subset Ck is\nassigned to model k, and all examples belonging to classes in Ck are included in that model’s local\ndataset:\nK\n[\nk=1\nCk = C,\nCi ∩Cj = ∅\n∀i ̸= j\nE\nModel Training Details\nWe trained VGGs on CIFAR-10 and ViTs on CIFAR100. All models were trained on NVIDIA RTX\nA5000 GPUs.\nThe VGGs followed the VGG11 architecture and the implementation is based on the open source\nimplementation provided by 2Singh and Jaggi [2020].\nThe ViTs implementation is based on 3omihub777, and we used the following model hyperparameters:\nModel Hyperparameter\nValue\nPatch Size\n8\nAttention Heads\n12\nEncoder Blocks\n7\nFeed Forward Network Hidden Size\n384\nEncoder Hidden Size\n384\nTo train the models, we use the non-exhaustive list of hyperparameters listed in Table 4\nSplits\nVGG Epochs\nViT Epochs\nFull Dataset\n300\n350\nSplit By 2\n200\n250\nSplit By 4\n150\n225\nSplit By 6\n–\n200\nSplit By 8\n125\n–\n(a) Number of training epochs\nTraining Hyperparameter\nValue\nWarmup Epochs\n5\nMinimum Learning Rate\n10−5\nLearning Rate\n10−3\nLabel Smoothing\n0.1\nBatch Size\n128\n(b) Training hyperparameters\nTable 4: Training configurations and schedules for VGG and ViT models.\nThe following torch augmentations were used for training: RandomCrop, RandomHorizonalFlip,\nNormalize, and other augmentations as in omihub777 based on Cubuk et al. [2018].\nThe model was trained using torch’s Gradual Warmup Scheduler with torch’s CosineAnnealingLR\nscheduler.\n2https://github.com/sidak/otfusion\n3https://github.com/omihub777/ViT-CIFAR\n16\nWe believe that these are sufficient to reproduce the main claims of our work. Additional information\nabout hyperparameters can be found in our open-source code repository.\nF\nFusion Implementation Details\nF.1\nFusion Hyperparameters\nOur proposed fusion algorithms include both linear and gradient-based variants, each with distinct\nhyperparameter considerations.\nLinear Variants. The linear fusion algorithms (e.g., plain Hungarian Fusion and K-means Fusion)\nrequire minimal hyperparameter tuning. The primary decisions involve whether to normalize (i)\nneuron outputs and/or (ii) neuron importance scores. In our experiments, we found that omitting\nnormalization typically yielded better results across both all data partitioning settings. This is likely\nbecause normalization can distort relative differences in neuron output magnitude that are informative\nfor matching or clustering.\nGradient-based Variants. In contrast, the gradient-based fusion variant introduces a broader set of\nhyperparameters. These include:\n1. Optimization Parameters: learning rate, weight decay, number of gradient steps per level,\nand batch size, validation split, validation patience.\n2. Initialization Scheme: initialization of the fused model weights at each level (e.g., weights\nfrom a randomly selected base model with added noise ϵ).\n3. Clustering Settings: number of clusters (typically matched to the fused model’s layer\nwidth), use of K-means++ initialization, early stopping criteria and whether to normalize\nneuron outputs just for the clustering stage.\n4. Importance Weighting: whether and how to incorporate neuron importance scores into\nboth clustering and loss weighting.\nWhile this added complexity increases flexibility and modeling capacity, it also requires careful\ntuning for stable and effective optimization. To mitigate this, we conducted extensive experiments and\nidentified two sets of hyperparameter configurations that generalized well across datasets (CIFAR-\n10, CIFAR-100), model architectures (VGG11, ViT), and fusion regimes (Full Dataset, Non-IID,\nSharded). Specifically, we found the hyperparameters in Table 5.\nTable 5: Sets of hyperparameters used for K-means Gradient Fusion\nHyperparameter\nSetting 1\nSetting 2\nOptimizer (n −1 first levels)\nAdam\nSGD\nLearning Rate (n −1 first levels)\n10−3\n10−4\nEpochs (n −1 first levels)\n100\n50\nWeight Decay (All levels)\n10−4\n10−4\nPerturbation ϵ\n1.0\n0.1\nOptimizer (Last level)\nAdam\nAdam\nLearning Rate (Last level)\n10−3\n10−3\nEpochs (Last level)\n100\n100\nEpochs (n −1 first levels)\n100\n100\nNormalize Activations\nFalse\nFalse\nTrain Batch Size\n32\n32\nVal Split\n0.1\n0.1\nHead Weights\nTrue\nTrue\nWe note that “Head Weights” refers to weighing the final logits of the models by the proportion of\nsamples seen per class, for every model. In practice, this heuristic improves accuracy by a small\nmargin, but this comes at a cost of calibration, as the test loss increases.\n17\nEach setting of hyperparameters induces a different behavior in the gradient-based variant of our\nalgorithm. By using Setting 1, we essentially take larger gradient steps that move us far away from\ninitialization. The resulting model is quite different from base models, in terms of plain weight L2\nnorm. On the other hand, Setting 2 relies on the initialization of the model to be already decent (e.g.\nany base model), and takes small gradient steps. Empirically, we found that with the second setting,\nthe majority of performance gain occurs at the classification head, where the targets become the\nraw average logits of all base models. This is similar to Knowledge Distillation (KD), with the only\ndifference that KD typically minimizes some sort of KL-divergence loss between softmaxed logits\nand average-softmaxed base-model-logits, instead of minimizing the L2 distance between averaged\nraw logits. Nevertheless, the interesting models are produced with the first setting, which finds new\nsolutions far away from initialization, and within a much richer context.\nFor our experiments:\n• All Full-dataset models were fused using Setting 1.\n• All sharded models were fused using Setting 1.\n• All Non-IID models, except for VGG11s and ViTs with n = 2 models for CIFAR-10 (which\nused Setting 1), were fused using Setting 2.\nWhile training full-dataset models, we used setting 1, but also normalized activations for k-means. In\nour experiments, we observed that this did not yield significant improvements.\nF.2\nModel Partitioning Schemes\nDue to the flexibility of our algorithm, we had the freedom to develop our own partition. In practice\nwe as we primarily tested on like models, there were obvious answers that we used.\nFor VGG11s, each level contained only a single convolutional or linear layer to be aligned or an\nactivation function, which did not need to be aligned.\nFor ViTs, each level corresponded to an encoder block except for the last one which corresponded to\nthe classifier head.\nF.3\nPost Fusion Finetuning Hyperparameters\nFor fusing full dataset models, a finetuning phase is shown to improve fused model performance above\nbase model performance. For this finetuning phase we used torch’s CosineAnnealingWarmRestarts\nscheduler. The whole process had the following hyperparameters:\nHyperparameter\nValue\nLearning Rate\n10−4\nMinimum Learning Rate\n10−6\nLabel Smoothing\n0.1\nEpochs\n200\nThe augmentation was the same generic suite as we used to train VGGs and ViTs initially. See\nAppendix E for more details. Once again, all code for reproduction is present in our open source\nrepository.\nF.4\nNeuron Importance Score Implementation\nWe heavily rely on Kokhlikyan et al. [2020] to compute importance scores for us. We built a\nframework around it that batches our requests, but the package does the lion’s share of the work in\ncomputing both Conductance [Dhamdhere et al., 2018] and DeepLIFT Shrikumar et al. [2017].\nF.5\nAlgorithm Runtime Comparison\nAs our methods are technically specific implementations of our general framework, it is difficult to\ndefinitively give an evaluation of the overall framework. However, we did quantify the performance\n18\nof our realizations, both on VGGs and ViTs. We only used importance scores for VGGs as they just\nadd a constant startup time and usually do not interfere with the performance of the algorithm. For the\nsame reason, we only use conductance when when testing for importance score times. Interestingly,\nKF Linear speeds up significantly when we use importance scores, suggesting that the K-means\nportion of the algorithm resolves faster in this case because of the weights. All experiments were ran\non NVIDIA RTX A5000 GPUs.\nTable 6: Algorithm runtime comparison when fusing VGG networks on CIFAR-10. We fused the same two\nmodels 10 times and averaged the run times. All algorithms were run with the same 400 samples in each iteration.\nAll times are in seconds.\nAlgorithm\nRuntime (Uniform)\nRuntime (Conductance)\nOT Fusion\n0.7\n3.0\nGit Re-Basin\n1.0\n3.2\nHF Linear (Ours)\n14.2\n16.5\nKF Linear (Ours)\n78.3\n83.7\nKF Gradient Uniform (Ours)\n16.5\n18.8\nTable 7: Algorithm runtime comparison when fusing ViT networks on CIFAR-100. We fused the same two\nmodels 5 times using our KF Gradient method with uniform importance scores and averaged the run times.\nFusion Samples\nRuntime (s)\n400\n38.1\n6000\n632.5\nG\nAdditional Results\nIn this section, besides complimentary tables, we will also present the full tables for results shown\nearlier. These full tables include the standard deviation for base models, as well as the performance\nof each fusion algorithm for each neuron importance score.\nTable 8: Test accuracy comparison when fusing ViT networks on CIFAR-100 for Non-IID splits. Fusion used\n7000 samples from the dataset seen by the first model. The same fusion data was used for last-layer KD as well.\nFor the full table, refer to Table 12.\nMethod\n2-WAY SPLIT\n4-WAY SPLIT\n6-WAY SPLIT\nBase Models\n54.5,\n50.9\n48.4, 46.3,\n45.0, 43.5\n41.3, 40.5, 40.1\n38.7, 37.7, 28.8\nVanilla Averaging\n1.6 ±0.3\n1.0 ±0.3\n1.2 ±0.6\nLast-layer KD\n51.0 ±1.4\n43.0 ±1.1\n35.7 ±1.6\nEnsemble\n66.7 ±0.1\n60.8 ±0.8\n53.8 ±1.2\nK-means Gradient Fusion (Ours)\n57.1 ±0.9\n51.7 ±1.2\n45.1 ±0.9\n19\nTable 9: Test accuracy comparison when fusing VGG11 networks on CIFAR-10 for Sharded splits. Fusion\nwas performed using 400 data points sampled from the dataset seen by the first model. The same fusion data\nwas used for all algorithms.\nMETHOD\n2-WAY SPLIT\n4-WAY SPLIT\n6-WAY SPLIT\nIndividual Models\n47.16,\n46.59\n28.93, 28.46\n19.37, 16.54\n19.88, 19.70, 19.34\n16.63, 10.00, 10.00\nVanilla Averaging\n12.36 ±2.6\n10.00 ±0.0\n10.00 ±0.0\nLast-layer KD\n50.60 ±1.5\n32.08 ±1.3\n22.00 ±1.0\nEnsemble\n79.22 ±1.7\n53.50 ±2.7\n41.52 ±3.2\nOTFusion\n24.81 ±3.5\n11.20 ±2.6\n10.00 ±0.0\nGit Re-Basin\n55.65 ±5.1\nN/A\nN/A\nHungarian Linear Fusion (Ours)\n73.16 ±4.1\nN/A\nN/A\nK-means Linear Fusion (Ours)\n73.44 ±4.4\n43.91 ±3.4\n33.73 ±2.9\nK-means Gradient Fusion (Ours)\n76.06 ±1.7\n43.97 ±2.4\n31.95 ±3.6\nTable 10: Test accuracy comparison for VGGs trained and fine-tuned on CIFAR-10 in the Full-Dataset setup.\nFusion used 400 samples from the shared training dataset. We note that base models can benefit from the\nfine-tuning phase as well, but the gain is marginal; In 2-way fusion, the best accuracy ramped up to an average\nof 91.6. Full results can be accessed in Table 15.\nZero-shot\nFine-tuning\nBase Models\nK-means Gradient\nFusion (Ours)\nEnsemble\nK-means Gradient\nFusion (Ours)\nEnsemble\n2-way:\n91.2, 91.0\n91.0 ±0.0 -0.2\n92.1 ±0.1+0.9\n91.7 ±0.0+0.5\n92.1 ±0.1+0.9\nEfficiency:\n×2\n×2\n×1\n×2\n×1\nTable 11: Test accuracy comparison when fusing VGG11 networks on CIFAR-10 for Non-IID splits. This table\nis complimentary to Table 1.\nMethod\n2-WAY SPLIT\n4-WAY SPLIT\n8-WAY SPLIT\nIndividual Models\n81.5 ±1.9,\n75.1 ±2.4\n77.2 ±3.7, 75.3 ±3.3,\n73.6 ±2.3, 66.8 ±3.7\n69.2 ±2.4, 68.0 ±1.4, 66.0 ±1.2, 63.0 ±2.3\n61.8 ±2.4, 59.6 ±1.5, 56.1 ±3.3, 52.6 ±4.8\nVanilla Averaging\n11.2 ±2.2\n10.0 ±0.0\n10.0 ±0.0\nLast-layer KD\n82.7 ±0.9\n60.1 ±6.8\n43.7 ±9.8\nEnsemble\n87.5 ±0.4\n83.7 ±0.9\n76.6 ±0.7\nOTF Uniform\n43.4 ±6.9\n20.4 ±7.6\n12.9 ±2.6\nOTF Conductance\n42.6 ±4.0\n13.9 ±3.4\n13.7 ±3.0\nOTF DeepLIFT\n39.8 ±7.8\n14.0 ±3.5\n14.0 ±3.1\nGit Re-Basin Uniform\n53.8 ±6.2\nN/A\nN/A\nGit Re-Basin Conductance\n69.7 ±6.7\nN/A\nN/A\nGit Re-Basin DeepLIFT\n71.8 ±3.4\nN/A\nN/A\nHF Linear Uniform (Ours)\n72.3 ±0.6\nN/A\nN/A\nHF Linear Conductance (Ours)\n84.6 ±0.7\nN/A\nN/A\nHF Linear DeepLIFT (Ours)\n84.6 ±0.8\nN/A\nN/A\nKF Linear Uniform (Ours)\n83.5 ±1.1\n77.8 ±1.0\n67.6 ±0.9\nKF Linear Conductance (Ours)\n84.5 ±0.8\n78.9 ±1.3\n69.6 ±1.2\nKF Linear DeepLIFT (Ours)\n84.3 ±0.8\n78.8 ±1.3\n69.3 ±0.8\nKF Gradient Uniform (Ours)\n83.2 ±1.2\n78.5 ±2.1\n68.4 ±4.9\nKF Gradient Conductance (Ours)\n83.3 ±1.1\n78.6 ±2.1\n68.6 ±4.5\nKF Gradient DeepLIFT (Ours)\n83.3 ±1.1\n78.6 ±2.1\n68.4 ±4.8\n20\nTable 12: Test accuracy comparison when fusing ViT networks on CIFAR-100 for Non-IID splits. This table is\ncomplimentary to Table 8.\nMethod\n2-WAY SPLIT\n4-WAY SPLIT\n6-WAY SPLIT\nIndividual Models\n54.5 ±0.7\n50.9 ±2.8\n48.4 ±0.9\n46.3 ±0.5\n45.0 ±0.7\n43.5 ±1.7\n41.3 ±1.0, 40.5 ±0.9, 40.1 ±0.6\n38.7 ±1.0\n37.7 ±2.1\n28.8 ±7.9\nVanilla Averaging\n1.6 ±0.3\n1.0 ±0.3\n1.2 ±0.6\nLast-layer KD\n51.0 ±1.4\n43.0 ±1.1\n35.7 ±1.6\nEnsemble\n66.7 ±0.1\n60.8 ±0.8\n53.8 ±1.2\nKF Gradient Uniform (Ours)\n57.0 ±1.0\n51.7 ±1.4\n45.1 ±0.9\nKF Gradient Conductance (Ours)\n57.0 ±0.9\n51.7 ±1.2\n44.9 ±0.8\nKF Gradient DeepLIFT (Ours)\n57.1 ±0.9\n51.7 ±1.3\n44.9 ±0.8\nTable 13: Test accuracy comparison when fusing ViT networks on CIFAR-100 for Sharded splits. This table is\ncomplimentary to Table 2.\nMethod\n2-WAY SPLIT\n4-WAY SPLIT\n6-WAY SPLIT\nIndividual Models\n37.9 ±0.3\n36.6 ±0.4\n19.4 ±0.4\n18.9 ±0.2\n18.7 ±0.1\n18.3 ±0.4\n13.7 ±0.3, 13.3 ±0.2\n13.1 ±0.2\n12.8 ±0.2\n12.4 ±0.4\n11.5 ±0.5\nVanilla Averaging\n1.8 ±0.4\n1.2 ±0.2\n1.0 ±0.0\nLast-layer KD\n36.0 ±1.7\n18.8 ±2.3\n12.8 ±0.5\nEnsemble\n61.9 ±0.6\n50.2 ±0.6\n44.3 ±0.5\nKF Gradient Uniform (Ours)\n46.9 ±0.9\n37.1 ±1.4\n31.6 ±0.5\nKF Gradient Conductance (Ours)\n49.3 ±0.9\n38.1 ±1.3\n33.1 ±1.1\nKF Gradient DeepLIFT (Ours)\n49.4 ±0.8\n38.5 ±1.1\n33.0 ±0.5\nTable 14: Test accuracy comparison when fusing ViT networks on CIFAR-100 trained on the full dataset. This\ntable is complimentary to Table 3.\nMethod\n2-WAY ZERO-SHOT\n2-WAY FINETUNED\n4-WAY ZERO-SHOT\n4-WAY FINETUNED\nIndividual Models\n73.4 ±0.1\n73.1 ±0.1\n73.6 ±0.3\n73.1 ±0.1\n74.2 ±0.2, 73.6 ±0.2,\n73.3 ±0.1, 73.0 ±0.0\n74.2 ±0.1, 73.8 ±0.1,\n73.3 ±0.3, 73.0 ±0.0\nVanilla Averaging\n2.0 ±0.1\n1.9 ±0.1\n1.3 ±0.1\n1.5 ±0.5\nEnsemble\n75.2 ±0.1\n75.3 ±0.0\n76.6 ±0.0\n76.9 ±0.2\nKF Gradient Uniform (Ours)\n72.7 ±0.4\n73.9 ±0.3\n71.8 ±1.6\n74.5 ±0.2\nKF Gradient Conductance (Ours)\n72.6 ±0.4\n73.9 ±0.4\n71.1 ±2.3\n74.3 ±0.0\nKF Gradient DeepLIFT (Ours)\n72.6 ±0.3\n74.0 ±0.3\n71.6 ±1.7\n74.4 ±0.1\n21\nTable 15: Test accuracy comparison when fusing VGG11 networks pairwise on CIFAR-10 trained on the full\ndataset. Results are averages across seeds. Fusion was performed using 400 data points sampled from the dataset.\nThe same fusion data was used for all algorithms. This table is complimentary to Table 10.\nMethod\nZERO-SHOT\nFINETUNED\nIndividual Models\n91.2 ±0.0\n90.1 ±0.2\n91.6 ±0.0\n91.5 ±0.1\nVanilla Averaging\n12.2 ±1.8\n12.5 ±1.9\nEnsemble\n92.1 ±0.1\n92.4 ±0.1\nOTF Uniform\n75.2 ±2.6\n91.3 ±0.1\nOTF Conductance\n65.5 ±2.0\n91.6 ±0.1\nOTF DeepLIFT\n66.8 ±1.3\n91.5 ±0.2\nGit Re-Basin Uniform\n75.7 ±1.4\n91.3 ±0.1\nGit Re-Basin Conductance\n77.0 ±0.8\n92.7 ±0.1\nGit Re-Basin DeepLIFT\n75.7 ±1.4\n92.8 ±0.1\nHF Linear Uniform (Ours)\n84.1 ±0.2\n90.8 ±0.0\nHF Linear Conductance (Ours)\n90.8 ±0.0\n92.7 ±0.1\nHF Linear DeepLIFT (Ours)\n90.8 ±0.1\n92.6 ±0.1\nKF Linear Uniform (Ours)\n90.5 ±0.2\n92.5 ±0.2\nKF Linear Conductance (Ours)\n90.4 ±0.1\n92.6 ±0.1\nKF Linear DeepLIFT (Ours)\n90.5 ±0.2\n92.6 ±0.2\nKF Gradient Uniform (Ours)\n91.0 ±0.0\n91.7 ±0.0\nKF Gradient Conductance (Ours)\n91.0 ±0.1\n91.7 ±0.1\nKF Gradient DeepLIFT (Ours)\n91.0 ±0.1\n91.6 ±0.0\n22\nH\nExisting Assets and Licenses\nWe make use of code from the following sources:\n1. OTFusion Singh and Jaggi [2020], Open Source, https://github.com/sidak/otfusion.\n2. ViT-CIFAR\nomihub777,\nMIT\nLicense,\nhttps://github.com/omihub777/ViT-\nCIFAR/blob/main/LICENSE.\n3. Captum\nKokhlikyan\net\nal.\n[2020],\nBSD\n3-Clause\nLicense,\nhttps://github.com/pytorch/captum/blob/master/LICENSE.\nI\nBroader Impact\nThis work concerns foundational research on model fusion algorithms. We do not foresee any negative\napplications beyond those broadly applicable to model fusion algorithms.\nAs with other fusion methods, negative societal impacts may follow from using biased or harmful\nmodels as a base model to perform fusion as the fused model may contain the biases / harmful\npotential of the base model.\n23\n",
    "content": "# Paper Analysis Report\n\n## 1. Core Content and Major Contributions\n\nThis paper proposes a **model fusion method via neuron interpolation**, aiming to integrate multiple pre-trained deep neural networks into a single model with superior performance. The core idea is to build target representations by performing representation matching at the neuron level, and then train a fused model to approximate these target representations.\n\n### Major Contributions:\n- **Formalizing fusion as a representation matching problem**: By decomposing the fusion objective into two interpretable components — *grouping* and *approximation* — the authors propose a natural and efficient two-stage pipeline. This approach enables stable fusion of models with different internal representations, including those trained on non-independent and identically distributed (Non-IID) data.\n- **Introducing neuron importance scores**: Unlike previous methods that treat all neurons equally, this work leverages attribution scores (e.g., Conductance, DeepLIFT) to guide the fusion process, prioritizing important features. Experiments show that this not only enhances the proposed method's performance but can also be integrated into existing techniques (e.g., Git Re-Basin) to improve their effectiveness.\n- **A general framework applicable to any layer type**: The algorithm is generalizable across various network architectures, not limited to specific types of layers (e.g., linear or Transformer layers).\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### Key Innovations:\n\n#### (1) Neuron-level Fusion Strategy\nTraditional approaches (e.g., OTFusion, Git Re-Basin) typically rely on optimal transport or activation matching for model alignment, often constrained to specific architectures (e.g., linear layers) or requiring retraining. The proposed algorithm performs clustering and approximation at the neuron level, enabling cross-model knowledge transfer without structural constraints.\n\n#### (2) Introduction of Neuron Importance Scoring\nOne of the major highlights of this work. Unlike prior fusion methods that ignore variations in neuron importance, this paper introduces neuron attribution scores (e.g., Conductance, DeepLIFT) into the fusion process as weights to optimize the fusion objective function. This allows the model to focus more on critical neurons during fusion, thereby improving overall performance.\n\n#### (3) Theoretical Guarantees\nThe authors provide rigorous theoretical analysis, proving the optimality and approximation error bounds of the proposed method under certain conditions (e.g., linear layers, one-to-one matching). For instance, global optimality can be achieved using Hungarian Matching, and a local search algorithm under K-means fusion achieves an approximation error upper bound of (9 + ε)-times.\n\n#### (4) Comprehensive Experimental Validation\nExtensive experiments are conducted across various data distribution scenarios (e.g., Non-IID, Sharded, Full Dataset), involving mainstream architectures such as VGG and ViT. Results demonstrate that the method significantly outperforms existing approaches in zero-shot fusion tasks, even approaching the performance of ensemble learning.\n\n---\n\n## 3. Entrepreneurship Project Suggestions\n\nBased on the core ideas and technical breakthroughs presented in this paper, here are several entrepreneurial project directions:\n\n### ✅ **1. Model Aggregation Platform for Federated Learning**\n**Application Scenario**: Sectors like healthcare, finance, and edge computing where sensitive data cannot be centralized.\n\n**Technical Basis**: The proposed model fusion method enables effective aggregation of local models' knowledge without sharing raw data, especially suitable for Non-IID data settings.\n\n**Product Value**:\n- Provide secure and efficient federated learning aggregation services;\n- Support heterogeneous model fusion (e.g., models with different architectures or initializations);\n- Useful for privacy-preserving model updates and deployment.\n\n### ✅ **2. Multimodal / Multi-task Model Fusion Toolkit**\n**Application Scenario**: Autonomous driving, intelligent customer service, recommendation systems — domains requiring integration of multiple subtask models.\n\n**Technical Basis**: The neuron-level fusion mechanism can be used to integrate models from different tasks or modalities, avoiding retraining while preserving individual strengths.\n\n**Product Value**:\n- Provide SDK/APIs for automatic fusion of multiple pre-trained models;\n- Support multimodal fusion across images, text, speech, etc.;\n- Enable rapid development of lightweight integrated models.\n\n### ✅ **3. Model Compression and Distillation Tools**\n**Application Scenario**: Efficient inference needs on mobile and embedded devices.\n\n**Technical Basis**: The \"clustering + approximation\" mechanism described in the paper can be applied to model distillation, transferring knowledge from large models to smaller ones while maintaining high performance.\n\n**Product Value**:\n- Offer a model compression toolchain supporting knowledge transfer from large to small models;\n- Precisely retain key features using neuron importance scoring;\n- Support unsupervised distillation without additional labeled data.\n\n### ✅ **4. AI Model-as-a-Service (MaaS) Platform**\n**Application Scenario**: Enterprises wishing to reuse existing AI model assets without retraining from scratch.\n\n**Technical Basis**: The proposed fusion method integrates capabilities from multiple models without requiring access to original training data.\n\n**Product Value**:\n- Provide \"Model Fusion as a Service\" to help enterprises consolidate model assets;\n- Support online model version management and iteration;\n- Enable building unified-interface AI service platforms.\n\n### ✅ **5. AI Model Copyright and Provenance Platform**\n**Application Scenario**: Model copyright registration, tampering detection, source tracking.\n\n**Technical Basis**: The fusion process retains information about each parent model’s neuron contributions, which can be used for model provenance tracing.\n\n**Product Value**:\n- Offer model tracing and reverse engineering tools;\n- Support model copyright registration and verification;\n- Detect whether a model contains intellectual property from a particular company or individual.\n\n---\n\n## Summary\n\nThis paper presents a **novel, general, and highly interpretable method** for model fusion. It provides both rigorous theoretical analysis and impressive practical results. Its core ideas — neuron-level fusion and importance scoring — open up new possibilities for real-world applications and hold significant commercial potential.",
    "github": "https://github.com/AndrewSpano/neuron-interpolation-model-fusion",
    "hf": ""
}