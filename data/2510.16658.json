{
    "id": "2510.16658",
    "title": "Foundation and Large-Scale AI Models in Neuroscience: A Comprehensive Review",
    "summary": "This paper explores how large-scale AI models are transforming five major areas of neuroscience and highlights the potential of these technologies as well as considerations for their implementation.",
    "abstract": "The advent of large-scale artificial intelligence (AI) models has a transformative effect on neuroscience research, which represents a paradigm shift from the traditional computational methods through the facilitation of end-to-end learning from raw brain signals and neural data. In this paper, we explore the transformative effects of large-scale AI models on five major neuroscience domains: neuroimaging and data processing, brain-computer interfaces and neural decoding, molecular neuroscience and genomic modeling, clinical assistance and translational frameworks, and disease-specific applications across neurological and psychiatric disorders. These models are demonstrated to address major computational neuroscience challenges, including multimodal neural data integration, spatiotemporal pattern interpretation, and the derivation of translational frameworks for clinical deployment. Moreover, the interaction between neuroscience and AI has become increasingly reciprocal, as biologically informed architectural constraints are now incorporated to develop more interpretable and computationally efficient models. This review highlights both the notable promise of such technologies and key implementation considerations, with particular emphasis on rigorous evaluation frameworks, effective domain knowledge integration, and comprehensive ethical guidelines for clinical use. Finally, a systematic listing of critical neuroscience datasets used to derive and validate large-scale AI models across diverse research applications is provided.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Shihao Yang,Xiying Huang,Danilo Bernardo,Jun-En Ding,Andrew Michael,Jingmei Yang,Patrick Kwan,Ashish Raj,Feng Liu",
    "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computational Engineering, Finance, and Science (cs.CE)"
    ],
    "comments": "",
    "keypoint": "Large-scale AI models enable end-to-end learning from raw brain signals and neural data.\nThese models address major computational neuroscience challenges such as multimodal neural data integration, spatiotemporal pattern interpretation, and translational framework development.\nThe interaction between neuroscience and AI has become reciprocal, with biologically informed constraints improving model interpretability and efficiency.\nFoundation models demonstrate strong generalization across experimental conditions, subjects, and recording modalities.\nSelf-supervised learning approaches alleviate data scarcity by leveraging unlabeled neural datasets.\nBrain foundation models using transformer architectures and contrastive learning achieve robust EEG representations.\nModels can decode intended speech from neural signals at near-conversational speeds.\nSeizure onset can be predicted with high accuracy using large-scale AI models.\nBiomarkers for neurodegenerative diseases can be detected years before clinical symptoms appear.\nVision-language models like CLIP enable alignment of neural activity patterns with textual or visual stimuli.\nClinical decision support systems integrate multimodal patient data for evidence-based treatment recommendations.\nGenerative AI models reconstruct visual experiences from fMRI signals with high fidelity.\nDiffusion models generate synthetic EEG signals to improve model robustness in low-data scenarios.\nHeterogeneous data including neuroimaging, genomics, and clinical metadata are combined for comprehensive brain modeling.\nSpiking neural networks and neuromorphic computing extend knowledge of biological intelligence.\nLarge language models predict neuroscience experiment results with over 80% success rate.\nNeuromorphic processors like Intel’s Loihi implement brain-inspired AI computing paradigms.\nTransformer architecture efficiently captures long-range dependencies in spatial and temporal dimensions.\nCross-attention mechanisms enhance decoding of EEG into cognitive states.\nMasked signal modeling and contrastive learning are key self-supervised objectives for neural representation learning.\nPre-training on diverse datasets enables universal knowledge acquisition.\nParameter-efficient tuning techniques like adapter tuning and LoRA reduce computational burden during adaptation.\nRetrieval-augmented generation improves inference by incorporating external knowledge bases.\nTokenization of continuous neural signals must preserve spatiotemporal information for transformer models.\nInterpretability requires alignment with functional brain networks and anatomical connectivity.\nStandardization of neural data collection and preprocessing remains a significant challenge.\nCross-subject and cross-session variability hinders model generalization.\nTemporal dynamics span multiple scales from milliseconds to minutes, requiring hierarchical modeling.\nMultimodal integration faces challenges in aligning heterogeneous neural signals with different temporal resolutions.\nFederated learning and differential privacy help address data privacy and security concerns.\nLarge-scale AI models are applied in five major domains: neuroimaging, brain-computer interfaces, molecular neuroscience, clinical support, and disease-specific applications.\nNeuroimaging models improve brain structure analysis, neural signal processing, and multimodal integration.\nfMRI foundation models reconstruct natural scenes from brain activity.\nStructural MRI models achieve robust segmentation across diverse imaging protocols.\nHead CT models detect critical neuro-trauma conditions with high accuracy.\nEEG foundation models improve sleep staging, emotion recognition, and abnormality detection.\nIntracranial signal models decode speech production and perception with millimeter resolution.\nMultimodal models integrate EEG and MEG for improved somatomotor decoding.\nVision-language models decode visual stimuli from fMRI using shared embedding spaces.\nLanguage models translate fMRI signals directly into text for brain-computer communication.\nMultimodal fusion enhances schizophrenia classification by integrating structural and functional MRI.\nGenomic foundation models predict regulatory effects of non-coding DNA variants.\nSingle-cell transcriptomic models provide contextual maps of neural cell function.\nLarge language models assist in neurosurgical planning and clinical reasoning.\nKnowledge graphs are automatically expanded using LLMs for cognitive neuroscience.\nAI systems predict mental health status from online textual data.\nFoundation models localize epileptogenic zones from seizure semiology descriptions.\nLLMs outperform human experts in predicting neuroscience experiment outcomes.\nSynthetic data generation helps overcome data scarcity in rare neurological disorders.\nPublic datasets span EEG, MEG, fMRI, genomics, and clinical records across thousands of subjects.\nChallenges remain in generalization across populations, interpretability, and ethical deployment.\nRegulatory frameworks must balance innovation with patient safety in brain-AI applications.\nFuture opportunities include real-time adaptive BCIs and brain-inspired AI architectures.",
    "date": "2025-10-22",
    "paper": "Foundation and Large-Scale AI Models in Neuroscience: A\nComprehensive Review\nShihao Yang1, Xiying Huang1, Danilo Bernardo2, Jun-En Ding1,\nAndrew Michael3, Jingmei Yang4, Patrick Kwan5,6, Ashish Raj7, Feng Liu1∗\n1Department of Systems Engineering, Stevens Institute of Technology, Hoboken, NJ, USA\n2Department of Neurology and Weill Institute for Neurosciences, University of California San Francisco,\nSan Francisco, CA, USA\n3Duke Institute for Brain Sciences, Duke University, Durham, NC, USA\n4Division of Systems Engineering, Department of Electrical and Computer Engineering, Boston University,\nBoston, MA, USA\n5Department of Neuroscience, School of Translational Medicine, Monash University, Melbourne, Victoria, Australia\n6Department of Neurology, Alfred Hospital, Melbourne, Victoria, Australia\n7Department of Radiology and Biomedical Imaging, University of California, San Francisco, CA, USA\nAbstract\nThe advent of large-scale artificial intelligence (AI) models has a transformative effect on neu-\nroscience research, which represents a paradigm shift from the traditional computational methods\nthrough the facilitation of end-to-end learning from raw brain signals and neural data. In this pa-\nper, we explore the transformative effects of large-scale AI models on five major neuroscience\ndomains:\nneuroimaging and data processing, brain-computer interfaces and neural decoding,\nmolecular neuroscience and genomic modeling, clinical assistance and translational frameworks,\nand disease-specific applications across neurological and psychiatric disorders. These models are\ndemonstrated to address major computational neuroscience challenges, including multimodal neu-\nral data integration, spatiotemporal pattern interpretation, and the derivation of translational\nframeworks for clinical deployment. Moreover, the interaction between neuroscience and AI has\nbecome increasingly reciprocal, as biologically informed architectural constraints are now incorpo-\nrated to develop more interpretable and computationally efficient models. This review highlights\nboth the notable promise of such technologies and key implementation considerations, with particu-\nlar emphasis on rigorous evaluation frameworks, effective domain knowledge integration, and com-\nprehensive ethical guidelines for clinical use. Finally, a systematic listing of critical neuroscience\ndatasets used to derive and validate large-scale AI models across diverse research applications is\nprovided.\nKeywords: Foundation model, brain imaging, computational neuroscience, brain disorders, artificial intelli-\ngence.\n1\nIntroduction\nThe rise of large-scale artificial intelligence (AI) models, including foundation models (FMs) and large\nlanguage models (LLMs), has opened a new era at the intersection of AI and neuroscience. These\nmodels are excellent at learning multimodal, rich representations from large, varied datasets [1, 2, 3].\nCompared to classical computational approaches that mainly relied on conventional machine learning\nmethodologies like support vector machines, linear discriminant analysis, and ensemble methods with\nmanually engineered features [4], this paradigm shift represents a significant departure. For example,\nFMs exhibit strong generalization across a variety of experimental conditions, subjects, and recording\nmodalities, and they can capture the intricate spatiotemporal dependencies found in neural signals [5].\nThis is achieved through end-to-end learning of hierarchical representations directly from raw brain\nsignals [6].\nIn this context, neural signals refer broadly to electrophysiological and neuroimaging\nrecordings, such as EEG, MEG, sEEG, ECoG, CT, fMRI, etc., which capture brain activity across\ndifferent spatial and temporal scales. These diverse modalities provide complementary perspectives on\nneural dynamics, offering rich data for large-scale representation learning.\n∗Corresponding author: fliu22@stevens.edu.\n1\narXiv:2510.16658v1  [cs.AI]  18 Oct 2025\nDespite these capabilities, data scarcity constrains their full potential in neuroscience applications.\nSelf-supervised learning (SSL) approaches have addressed a major bottleneck in neuroscience: the lim-\nited availability of large-scale labeled neural datasets [7]. Brain foundation models (BFMs) specifically\ndesigned for neural signal processing have achieved unprecedented success by leveraging contrastive\nlearning frameworks and transformer architectures. These models perform well in predicting neuronal\nresponses to novel stimuli, generalizing across subjects with minimal fine-tuning, and uncovering fun-\ndamental principles of neural computation that were previously inaccessible. [8, 9, 10]. For instance,\nBErt-inspired Neural Data Representations (BENDR), adopted SSL by integrating transformer ar-\nchitectures with contrastive learning objectives to produce robust EEG representations [9].\nThese\nachievements have facilitated the development of models that can decode intended speech from neu-\nral signals at near-conversational speeds, predict seizure onset with remarkable accuracy, and detect\nbiomarkers for neurodegenerative diseases years before clinical symptoms emerge. [11, 12]. Building\non these foundational advances, the integration of large-scale multimodal models has driven great ad-\nvances in cross-modal neural decoding applications and related clinical translation. Vision-language\nmodels such as Contrastive Language–Image Pre-training (CLIP) generate informative latent repre-\nsentations that capture cross-modal semantic relationships, enabling the alignment of neural activity\npatterns with textual descriptions or visual stimuli [13]. In clinical settings, clinical decision support\nsystems powered by FMs now integrate multimodal patient data—including neuroimaging, genetic\ndata, clinical histories, and real-time physiological monitoring—to provide evidence-based recommen-\ndations for treatment optimization [14, 15]. Furthermore, generative AI models with latent diffusion\narchitecture demonstrated extraordinary capabilities of reconstructing visual experiences from brain\nactivity while fMRI signals turned out to be effective cues for producing high-fidelity natural scene\nreconstructions [16]. Analogously, recent breakthroughs of neural data augmentation rely on diffu-\nsion models to produce synthetic EEG signals and thus improve model robustness in situations with\nrestricted availability of data [17]. Recent works increasingly combine heterogeneous neuroimaging\nmodalities, genomic information, clinical metadata, and external variables to build complete models of\nbrain function and pathology [18, 19]. These advancements created a mutual interdependence of neu-\nroscience and AI research. Moreover, brain-inspired architecture with biological constraints and neural\ncodes improved model interpretability, robustness, and computational efficiency [20, 21]. Meanwhile,\nrecent findings indicate that LLMs are able to predict neuroscience experiment results with a success\nrate of more than 80%, process scientific literature, and produce original hypotheses and aid exper-\niment design [22]. These results have generated practical neuromorphic processors like Intel’s Loihi\nand IBM’s TrueNorth and implemented brain-inspired AI computing paradigm for energy-efficient AI\ncomputing [23, 24]. Studies of spiking neural networks, neuromorphic computing, and biologically pos-\nsible attention mechanisms continue both the extension of our knowledge of biological intelligence and\nmore advanced AI systems [24, 25, 26]. Evolution from classical ML towards current multimodal and\ngenerative models has dramatically improved the ability of neural activity decoding, brain function\ninterpretation, and AI system development aiming at emulating biological computation. [27, 28, 29, 30].\nWe refer throughout this article broadly to FMs as ”large-scale AI models” and refer as an example\nto LLMs, vision-language models, and other transformer-based and generative models trained with mil-\nlions or billions of parameters. Our review reveals five overarching domains of applications that together\ndemonstrate the paradigm-shifting effects of applying large-scale AI models to neuroscience research.\nFirst, we cover neuroimaging and data processing applications involving specialty neuroimaging models\nof brain structure analysis, neural signal process models of interpretation of electrophysiological data,\nand multimodal integration methods combining heterogeneous imaging modalities. Then we report\nbrain-computer interfaces and neural decoding systems employing large-scale models and achieving\nrecord-level performance, translating neural activity into useful outputs. Thirdly, we explore molec-\nular neuroscience and genomic modeling, involving both genomic FMs for variant interpretation and\ndata-driven approaches for genetic risk prediction and disease mechanism discovery. Subsequently, we\nreview clinical support and translational frameworks involving both clinical decision support systems\nand knowledge-driven AI methods of cognitive modeling and diagnostic support. Finally, we review\napplications targeting neurological and psychiatric disorders, spanning neurodegenerative conditions,\nacute neurological injuries, structural brain abnormalities, major psychiatric illnesses, trauma-related\npathology, and neurodevelopmental disorders.\nOur survey unveils that extensive-scale AI models are revolutionizing neuroscience through their\ncapacity to provide robust generalization from heterogeneous neural datasets and experiment condi-\n2\ntions, and tackle core issues such as multimodal data fusion, spatiotemporal pattern explanation, and\ntranslation of research outputs into clinical applications. These reviewed applications span both foun-\ndational research aims, e.g., neural coding principles and brain network dynamics understanding, and\ntranslational aims that target enhanced diagnostic accuracy and therapy efficacy, spanning a variety\nof neurological as well as psychiatric disorders. Moreover, this article chronicles contemporary accom-\nplishments as much as elucidates vital future research directions and key challenges fundamental to\nfurthering the subject.\nEvolution of AI in Neuroscience\nClassical Machine Learning\n(1980-2010)\ne.g., SVM, Clustering, Regression.\nDeep Learning\n(2010-2020)\ne.g., CNNs, RNNs.\nLarge-scale AI models\n(2020-present)\ne.g., FMs, LLMs.\nLarge-scale AI Models for Neuroscience: A New Era of Intelligence\nEEG/MEG/iEEG\nMRI/CT/PET\nGenetic Data\nBehavioral Data\nClinical Records\nData Resources\nFoundation Models\nLanguage Models\nGenerative Models\nGraph & Temporal Models\nMultimodal Models\nModels\nCore Functions\nCross-modal & Generative\nClinical & Scientific Impact\n…\n…\n…\nNeural Signal Decoding Representation Learning\nClinical Decision Support\nBiomarker Discovery\nScientific Discovery\nMultimodal Integration\nBCI\nNeural-to-Image/Text\nPredictive Modeling\n…\n…\nFigure 1: Neuroscience AI Landscape: Developmental Trajectory and Functional Framework.\n2\nBackground and Problem Formulation\n2.1\nTheoretical Foundations of Large-scale AI Models\n2.1.1\nFrom Classical Machine Learning to FMs\nThe development of AI in neuroscience has witnessed a shift from task-specific machine learning tech-\nniques to cohesive, generalizable FMs. In the initial phase, conventional machine learning techniques,\nsuch as support vector machines, linear discriminant analysis, and ensemble methods, were overly de-\npendent on manually crafted features extracted from neurophysiological measures [4]. Although such\ntechniques were able to produce decent performance in particular domains, their low generalization\ncapability and requirement of domain expertise during feature engineering restricted their wide usage.\nDeep learning systems radically altered this paradigm through the capability of end-to-end learning\nof hierarchical representations from raw neural recordings [6]. Nevertheless, deep learning systems are\noften determined by extensive annotated data and are still task-specific, posing limitations on their\ntransferability and generalizability across various neuroscience applications and experimental settings.\nWith increasing access to large-scale data and computational capabilities, FMs have lately risen and\nestablished unprecedented capabilities with a description of their capacity to grow through a plethora\nof tasks, domains, and modalities with a common set of architectural frameworks [1]. These models tap\ninto extensive datasets and auto-supervised learning frameworks and gain general-purpose knowledge,\n3\nenabling each of them to adapt and perform a variety of neuroscience tasks with small extra data\nusage during fine-tuning. The panorama of neuroscience AI models from fundamental computational\nfunctions through cross-modal capabilities and clinical repercussions is contextualized in Fig. 1.\n2.1.2\nScaling Laws and Core Principles\nAs opposed to classical models, high-scale AI systems are constructed from the transformer architec-\nture that proposes self-attention as a fundamental mechanism of dealing with intricate dependencies\nof data [31]. Self-attention mechanism allows models to detect and combine pertinent contextual in-\nformation from time and space, a feature that comes especially useful while dealing with neural signals\nwith high temporal dynamics and spatial organization. These models are subject to empirical scaling\nlaws that govern their performance features and exhibit consistent gains when the model size, training\ndata, and computational capacity are each incremented proportionally [32]. For neuroscience appli-\ncations, the resulting scaling behavior means that with larger model size and longer neural training\ndatasets, better performance can be achieved in neural decoding tasks involving signal reconstruction\nand cross-subject generalizability. Scaling-enabled emergent capabilities are a few-shot learning that\nallows models with little additional training data to generalize and adapt to a new task and transfer\nlearning with changing neural recording modalities and experiment conditions.\n2.2\nArchitectural Frameworks and Design Principles\n2.2.1\nTransformer Architecture in Neuroscience Applications\nThe transformer architecture has become the foundational design of most large-scale AI models in\nneuroscience, due to its capacity to efficiently capture long-range dependencies in both spatial and\ntemporal dimensions [31]. This flexibility can extend easily to multimodal brain data, enabling the\nfusion of EEG signals, MRI volumes, and neuro-symbolic representations in neuroscience contexts.\nThe details are listed in Table 1.\nFor brain signal modeling, transformers have been successfully adapted to EEG and MEG domains.\nFor instance, Brain Neural Transformer (BRANT) is a representative FM that applies multi-layer\ntransformers to intracranial EEG data for generalized neural decoding across patients and tasks, with\nattention modules capturing both short- and long-range neural dependencies [8]. Similarly, CBraMod\nemploys cross-attention transformer structures to decode EEG into cognitive states, enhancing accu-\nracy in low-data regimes [33].\nTable 1: Transformer Architecture Applications Across Neuroscience Domains\nDomain\nKey Challenges\nTransformer Advantages\nModel Examples\nEEG/MEG Analysis\nNon-stationarity,\ntemporal\ndy-\nnamics, artifacts\nLong-range temporal modeling, self-\nattention for time-series\nBRANT [8], CBraMod [33], BENDR [9]\nMRI Decoding\nHigh\ndimensionality,\ninter-\nsubject variability\nSpatiotemporal\nattention,\ncontex-\ntual encoding\nMindBridge [15], MindEye2 [34]\nStructural MRI\nAnatomical variability, low con-\ntrast structures\nPatch\ntokenization,\ncross-slice\nat-\ntention\nBrainSegFounder [35], AnatCL [36]\nMultimodal Neuroimaging\nCross-modal alignment, data het-\nerogeneity\nCross-modal attention, joint repre-\nsentation learning\nBrainCLIP [37], MultiViT [38]\nClinical NLP\nMedical\nterminology,\nreasoning,\nsparse context\nPretrained LLMs, in-context learn-\ning, entity linking\nNeuroGPT [39], AtlasGPT [40]\nGenomics\nPredicting\nfunction\nof\ngenetic\nvariants\nCapturing long-range genomic inter-\nactions\nAlphaGenome [41], Enformer [42]\nSeizure Video Analysis\nVariable semiology, temporal dy-\nnamics, environmental variability\nLong-range temporal modelling, ro-\nbust feature representation,\nmulti-\nmodal integration\nSETR-PKD [43], VSViG [44]\nIn addition to brain signal analysis, transformers play an important role in other complex tasks,\nincluding segmentation and reconstruction. Models such as MindEye2 process shared-subject fMRI\ndata through transformer layers that align voxel patterns with latent visual embeddings, enabling\nimage reconstruction with minimal training data [34], where the attention mechanism demonstrates\ngreat capability in handling brain shape variability and imaging resolution differences across subjects\nand institutions.\n2.2.2\nMultimodal Integration Architectures\nThe ability to integrate multiple data modalities has become increasingly critical in neuroscience appli-\ncations, enabling joint processing of heterogeneous neural data types to achieve a deeper understanding\n4\nthan traditional single-modality approaches [13]. These architectures leverage complementary infor-\nmation across different data types, including imaging data, temporal signal recordings, genetic data,\nand behavioral measurements.\nTable 2: Multimodal Fusion Strategies in Neuroscience FMs\nFusion Level\nFusion Mechanism\nModel Examples\nFusion Objectives\nEarly Fusion\nToken-level or embedding concate-\nnation; joint encoder input\nBENDR [9], Wei et al. [45]\nCross-modal representation learn-\ning; Sleep staging\nMid Fusion\nShared attention layers; cross-token\ninteractions across modalities\nBrainCLIP [37], MindBridge [15]\nMultimodal\ndecoding;\nvisual\nre-\nconstruction\nLate Fusion\nSeparate\nencoders\nwith\ndecision-\nlevel combination (e.g., ensemble or\nMLP)\nSHADE-AD [46], HybridTransNet [47]\nAlzheimer’s diagnosis; Tumor seg-\nmentation\nPrompt-based Fusion\nText\nprompts\nguiding\ncross-modal\nattention or LLM generation\nDECT [48], Mental-LLM [49]\nClinical reasoning; psychiatric as-\nsessment\nContrastive Alignment\nCross-modal\ncontrastive\nlearning\nwith shared embedding space\nNeSyGPT [50], BrainCLIP [37]\nSemantic\nalignment;\nzero-shot\ngeneralization\nHierarchical Fusion\nMulti-stage\nmodality\nintegration\n(e.g., audio→video→text)\nVS-LLM [51], Centaur [52]\nHuman behavior prediction; mul-\ntimodal psychiatric diagnosis\nThe key principle of multimodal neural architectures lies in learning joint representations that\nbridge different data modalities while preserving modality-specific information. To achieve this goal,\ncontemporary approaches typically employ encoder-decoder frameworks where separate encoders pro-\ncess each modality before aggregating representations of different modalities in the fusion stage. Recent\ndevelopments have demonstrated remarkable progress in cross-modal reconstruction tasks, particularly\nin translating fMRI signals to natural scene imagery using generative latent diffusion models [16] and\nconditional diffusion models for vision decoding [53]. The details are shown in Table 2.\nWhile these achievements demonstrate the potential of multimodal approaches, several technical\nchallenges remain in developing robust neural signal integration frameworks. A primary challenge lies\nin temporal alignment across heterogeneous neural signals, as modalities such as fMRI, EEG, and\nMEG exhibit fundamentally different temporal resolutions. To address this challenge, researchers have\ndeveloped hierarchical attention mechanisms and multi-scale temporal modeling approaches that can\nsynchronize these different data streams while maintaining the detailed timing information crucial\nfor accurate analysis [10]. Another advanced model, MindBridge, has demonstrated significant im-\nprovements in reconstruction fidelity and generalization capabilities by employing sophisticated fusion\nmechanisms that can effectively generalize across diverse individuals and recording sessions [15].\n2.3\nTraining Methodologies and Learning Paradigms\nTable 3: Self-Supervised Learning Methods for Neural Representation Learning\nSSL Method\nDescription\nModel Examples\nKey Benefits\nMasked Signal Modeling\nPredict\nmissing\nsegments\nof\nneural\nrecordings using surrounding context\nBENDR [9], EEGFormer [54]\nContext-aware\nembeddings,\nLong-range\ndependency\nmodeling\nContrastive Learning\nMaximize agreement between semanti-\ncally similar neural signals or modali-\nties\nBrainCLIP [37], AnatCL [36]\nModality alignment, Invari-\nant features\nMasked Patch Prediction\nReconstruct\nmissing\nimage\npatches\nfrom spatial brain volumes\nBrainSegFounder [35], FM-CT [55]\nSpatial\nrepresentation,\nAnatomical priors\nTemporal Forecasting\nPredict future neural signals from past\ntime steps\nBRANT [8], BrainLM [56]\nPredictive\ndynamics,\nTem-\nporal generalization\nCross-modal Alignment\nLearn shared embeddings across dif-\nferent input modalities\nMindBridge [15], BrainCLIP [37]\nModality\nfusion,\nSemantic\ngrounding\nSequential Modeling\nCapture patterns in sequential neural\ndata\nCBraMod [33] , EEGFormer [54]\nTime-series\nmodeling,\nSe-\nquential decoding\nSelf-supervised learning has become a key training approach for large-scale neuroscience AI models,\novercoming the scarcity of labeled neural data by learning meaningful representations directly from\nneural activity patterns [57]. Unlike supervised learning approaches, SSL eliminates the reliance on\nmanual data labeling by creating learning tasks directly from the neural data, making it ideal for\nscalable pretraining in neuroscience where labeled datasets are scarce.\nAs illustrated in Table. 3, various self-supervised objectives have been successfully adapted to\nthe neuroscience domain, with reconstruction-based and contrastive methods representing two major\nparadigms. Reconstruction-based approaches, particularly masked signal modeling. Reconstruction-\nbased approaches, particularly masked signal modeling, involve randomly masking portions of neural\nsignals and training models to reconstruct missing segments based on surrounding context, encour-\naging learning of robust representations that capture both local temporal patterns and long-range\ndependencies. For example, BENDR uses contrastive loss functions to distinguish temporally adja-\ncent and distant EEG segments, learning context-aware representations of neural time series data [9].\n5\nEEGFormer incorporates masked sequence modeling and attention mechanisms for pretraining on\nlarge-scale EEG datasets [54].\nIn contrast, contrastive learning focuses on learning invariant representations across different ex-\nperimental conditions, subjects, and recording sessions. In brain imaging tasks, models like Brain-\nSegFounder adopt two-stage pretraining pipelines combining vision transformers with self-supervised\nobjectives such as patch prediction for reconstructing masked brain image regions [35]. These ap-\nproaches enable models to capture detailed spatial and anatomical patterns critical for segmentation\nand analysis tasks.\n2.3.1\nTraining Pipeline\nSelf-supervised methods require specially crafted training pipelines to unlock their full capabilities.\nLarge models are part of a systematic multi-step process. As depicted in Fig. 2, there are three high-\nlevel steps. These are pre-training as a pre-requisite of universal knowledge acquisition, adaptation as\na step of task-specific tuning, and inference and deployment as a step of real-world use [1].\nPre-training Stage: Pre-training constructs general knowledge from large-scale datasets. Self-\nsupervised learning methods are applied at this stage by models. FMs are trained from diverse sources\nof data, including text corpora, image-text pairs, audio streams, and biological signals [1, 13]. It is\ncrucial to have diverse data, as this enables models to discover universal patterns that perform well\nacross different tasks and domains [58].\nAdaptation Stage: The adaptation phase adapts pre-trained models to specific tasks. It utilizes\nrestricted labeled data from target domains at this phase. Adapting is crucial for neuroscience appli-\ncations because datasets in this area generally contain little labeled data [59]. Also, complete model\nredeployment produces computational bottlenecks. Parameter-efficient tuning techniques resolve this\nissue and are comprised of adapter tuning, prompt tuning, and Low-Rank Adaptation (LoRA). These\ntechniques are efficient yet require less computational effort [60, 61, 62].\nInference and Deployment Stage: The deployment stage centers around real-world implemen-\ntation.\nInference should be efficient in systems during real-world use.\nSafety ensures correct use.\nInterpretability and ethics also guide.\nDeployment abilities are improved with advanced methods.\nRetrieval-Augmented Generation (RAG) incorporates external databases of knowledge. Methods of\nknowledge-grounding make predictions consistent with domain knowledge [63]. Moreover, applications\nin neuroscience require systems with multimodal decoding capabilities to analyze and integrate diverse\nsignal types for comprehensive analysis.\n2.4\nChallenges and Considerations for Neuroscience Applications\n2.4.1\nDomain-Specific Challenges\nApplying large-scale AI models to neuroscience involves unique challenges.\nThese challenges stem\nfrom the distinctive nature of neural data. Neural signals exhibit distinct properties that differentiate\nthem from natural language or computer vision domains.\nThese properties include high temporal\nresolution, multi-channel spatial organization, non-stationary dynamics, and substantial inter-subject\nvariability [64, 65].\nCross-subject and Cross-session Generalization: One of the most significant challenges in-\nvolves substantial variability in neural signals. This variability occurs across different subjects and\nrecording sessions.\nInter-subject variability arises from several sources.\nThese include anatomical\ndifferences, cognitive strategies, and individual neural characteristics. Inter-session variability results\nfrom different factors. These factors include electrode placement variations and recording equipment\ndifferences [66]. Researchers can formulate this challenge as a domain adaptation problem. Models\nmust learn representations that remain robust to irrelevant variations. At the same time, these models\nmust preserve task-relevant information.\nTemporal Dynamics and Multi-scale Dependencies: Neural signals exhibit complex tem-\nporal dynamics that span multiple timescales. They range from millisecond-level action potentials to\nminute-level cognitive states. Capturing these multi-scale temporal dependencies presents a signifi-\ncant methodological challenge. This challenge requires maintaining computational efficiency. Efficient\nattention mechanisms and hierarchical modeling approaches are necessary to address this issue [67].\n6\nPre-training Stage: Universal\nKnowledge Acquisition\nMasked\nModeling\nContrastive\nLearning\nNext-token\nPrediction\nCross-modal\nPretraining \nAdaptation Stage: \nTask-oriented Fine-tuning\nFull Fine-\ntuning\nPrompt Tuning\nAdapter Tuning\nLarge-scale dataset\n(Text, Image, EEG,\nMRI..).\ninput\nTask-specific\n labled data\ninput\nInference and Deployment Stage:\nEfficient and Responsible Application\nRetrieval-Augmented\nGeneration (RAG)\nKnowledge\nGrounding\nMultimodal\nDecoding \nExplainability\nModules\nReal-time neuro data,\nclinical queries\ninput\nReinforcement Learning\nfrom human feedback\nQuantization\nKnowledge\nDistillation\nFigure 2: Basic pipeline of Large-Scale AI Models\n2.4.2\nTokenization and Neural Signal Representation\nThe conversion of continuous neural signals into discrete tokens amenable to transformer-based mod-\nels is a crucial methodological issue particular to neuroscience applications. In contrast with natural\nlanguage processing, where natural units of meaning are words, neural signals necessitate advanced\ntokenization techniques that are able to preserve spatiotemporal information and permit efficient pro-\ncessing. For EEG and MEG recordings, tokenization generally entails segmenting continuous recordings\ninto fixed-duration time windows such that each window corresponds to a discrete token. Optimizing\nthe duration of this window involves balancing the resulting spatiotemporal resolution with com-\nputational requirements. Multichannel recordings necessitate consideration of inter-electrode spatial\nrelations as well; often through channel-wise processing with subsequent use of a spatial attention\nmechanism or graph neural network module modeling the inter-electrode connectivity patterns [68].\nFor fMRI recordings, the intrinsically lower spatiotemporal resolution with attendant higher spa-\ntial detail necessitates alternative tokenization methods. These generally incorporate voxel-wise or\nregion-of-interest-based tokenizations utilizing learned spatial embedding vectors that compact high-\ndimensional voxel information into compact tokens whilst retaining spatial relations and patterns of\nfunctional connectivities [69].\n2.4.3\nInterpretability and Biological Plausibility\nInterpretability of AI models in neuroscience throws up special challenges going well beyond con-\nventional explainable AI methods, requiring models with both predictive performance and biological\nrealism. AI models’ learned representations should best align with established functional brain net-\nworks, adhere to anatomical constraints of connectivities, and display temporal behavior analogous to\nneurophysiological mechanisms [70]. It requires the creation of assessment frameworks that evaluate\nboth predictive capacity and biological realism and get the learned representations right and still meet\nresearch application utility requirements.\n7\nNeuroimaging and \nData Processing\nNeuroimaging and \nData Processing\nMultimodal \nIntegration\nMultimodal \nIntegration\nNeural Radiological \nImaging\nNeural Radiological \nImaging\nElectrophysiological \nSignal Processing\nElectrophysiological \nSignal Processing\nNeurodegenerative \nDisorders\nNeurodegenerative \nDisorders\nNeurodegenerative \nDisorders\nAcquired Neurological \nDisorders\nAcquired Neurological \nDisorders\nAcquired Neurological \nDisorders\nBrain-Computer \nInterface\nBrain-Computer \nInterface\nBrain-Computer \nInterface\nNeural Decoding\nNeural Decoding\nNeural Decoding\nBrain-Computer Interfaces \nand Neural Decoding\nBrain-Computer Interfaces \nand Neural Decoding\nBrain-Computer Interfaces \nand Neural Decoding\nClinical Decision \nSupport\nClinical Decision \nSupport\nClinical Decision \nSupport\nMultimodal \nIntegration\nMultimodal \nIntegration\nMultimodal \nIntegration\nOther Applications in\nBrain Disorders\nOther Applications in\nBrain Disorders\nOther Applications in\nBrain Disorders\nOther Applications in\nBrain Disorders\nBrain Tumors and \nStructural \nAbnormalities\nBrain Tumors and \nStructural \nAbnormalities\nBrain Tumors and \nStructural \nAbnormalities\nMajor Psychiatric \nDisorders\nMajor Psychiatric \nDisorders\nMajor Psychiatric \nDisorders\nNeurodevelopmental\nDisorders\nNeurodevelopmental\nDisorders\nNeurodevelopmental\nDisorders\nAnxiety and \nTrauma-Related \nDisorders\nAnxiety and \nTrauma-Related \nDisorders\nAnxiety and \nTrauma-Related \nDisorders\nClinical Support and \nTranslational Frameworks\nClinical Support and \nTranslational Frameworks\nClinical Support and \nTranslational Frameworks\nLarge-scale AI Models\nMolecular Neuroscience \nand Genomic Modeling\nMolecular Neuroscience \nand Genomic Modeling\nMolecular Neuroscience \nand Genomic Modeling\nNeuroimaging and \nData Processing\nNeuroimaging and \nData Processing\nMultimodal \nIntegration\nMultimodal \nIntegration\nNeural Radiological \nImaging\nNeural Radiological \nImaging\nElectrophysiological \nSignal Processing\nElectrophysiological \nSignal Processing\nNeurodegenerative \nDisorders\nNeurodegenerative \nDisorders\nNeurodegenerative \nDisorders\nAcquired Neurological \nDisorders\nAcquired Neurological \nDisorders\nAcquired Neurological \nDisorders\nBrain-Computer \nInterface\nBrain-Computer \nInterface\nBrain-Computer \nInterface\nNeural Decoding\nNeural Decoding\nNeural Decoding\nBrain-Computer Interfaces \nand Neural Decoding\nBrain-Computer Interfaces \nand Neural Decoding\nBrain-Computer Interfaces \nand Neural Decoding\nClinical Decision \nSupport\nClinical Decision \nSupport\nClinical Decision \nSupport\nMultimodal \nIntegration\nMultimodal \nIntegration\nMultimodal \nIntegration\nOther Applications in\nBrain Disorders\nOther Applications in\nBrain Disorders\nOther Applications in\nBrain Disorders\nOther Applications in\nBrain Disorders\nBrain Tumors and \nStructural \nAbnormalities\nBrain Tumors and \nStructural \nAbnormalities\nBrain Tumors and \nStructural \nAbnormalities\nMajor Psychiatric \nDisorders\nMajor Psychiatric \nDisorders\nMajor Psychiatric \nDisorders\nNeurodevelopmental\nDisorders\nNeurodevelopmental\nDisorders\nNeurodevelopmental\nDisorders\nAnxiety and \nTrauma-Related \nDisorders\nAnxiety and \nTrauma-Related \nDisorders\nAnxiety and \nTrauma-Related \nDisorders\nClinical Support and \nTranslational Frameworks\nClinical Support and \nTranslational Frameworks\nClinical Support and \nTranslational Frameworks\nLarge-scale AI Models\nMolecular Neuroscience \nand Genomic Modeling\nMolecular Neuroscience \nand Genomic Modeling\nMolecular Neuroscience \nand Genomic Modeling\nFigure 3: Overview of large-scale AI model applications in neuroscience.\nFive major application\ndomains are shown, demonstrating the bidirectional relationship between neuroscience and AI devel-\nopment and identifying key research challenges.\n2.4.4\nData Quality and Standardization\nThe effectiveness of large-scale AI models largely depends on the availability of high-quality, standard-\nized datasets that allow extensive training and evaluation. Neural data collection involves a myriad\nof technical details ranging from recording device specifications to experiment paradigms and prepro-\ncessing streams and artifact rejection techniques that are otherwise significant determinants of data\nquality and inter-study comparison of results [71]. Addressing such variations requires extensive stan-\ndardization of data that covers technical aspects of sampling rates and filtering criteria, and coordinate\nsystems along with methodological variables of experiment designs and behavioral task specifications.\nStandardized protocol and data formats permitting scalable training of models with adequate diver-\nsity enabling proper generalization are a big challenge. Though the Brain Imaging Data Structure\n(BIDS) project provides a foundational outline of neuroimaging data organization with a standardized\napproach, extension of the same ideas with a view of incorporating the complete array of modalities\nof neural recording remains an ongoing venture [72]. Enunciation of the above challenges and method-\nological frameworks paradigmatically opens the path of successful use of large-scale AI models across\nneurosciences domains as a subject of elaboration in the sections that follow.\n3\nApplications of Neuroscience\nLarge-scale AI models are applied across several neuroscience areas, as shown in Fig. 3, and can\nsystematically be grouped into neuroimaging and data processing, brain-computer interfaces, genomic\nmodeling, and clinical translation. These applications capitalize on the ability of large-scale AI models\nto decipher intricate neural patterns and uncover underlying laws of brain organization and function.\nEach group uses different architectures and methods, ranging from FM pretraining from multimodal\nneural data to fine-tuning with interpretability frameworks and tackling specific neuroscience research\nand clinical practice challenges. These methods build computational foundations of understanding\nneural mechanisms and permit applications in diagnostics, therapeutics, and neurotechnology across\nlaboratory and clinical applications.\n8\n3.1\nNeuroimaging and Data Processing\nLarge-scale AI models have revolutionized neuroimaging and data processing to allow sophisticated\nanalysis of brain structural images, functional connectomes, and integration of neural data across\nmodalities. FMs dedicated to specific neuroimaging modalities, such as structural and functional MRI\nand electrophysiological recordings, correspondingly tackle modality-specific issues of spatial resolution,\ntemporal resolution, and inter-subject variation. These models use self-supervised learning, multimodal\nfusion, and transfer learning to push neuroimaging beyond classical computational methods.\n3.1.1\nNeural Radiological Imaging Models\nLarge-scale AI models for neuroimaging have demonstrated strong performance in brain structure and\nfunction analysis across multiple imaging modalities, particularly in segmentation, reconstruction, and\ncross-subject generalization tasks. These models exploit large datasets and advanced architectures to\ncapture complex spatial and anatomical patterns in brain imaging data.\nFunctional MRI FMs: FMs for functional MRI analysis have gained prominence through ad-\nvances in brain-to-image reconstruction and functional connectivity modeling. These approaches utilize\nlarge-scale neuroimaging datasets and employ self-supervised strategies adapted from computer vision.\nBrain Language Model (BrainLM) exemplifies this development by training on UK Biobank and Human\nConnectome Project data, this model uses masked patch prediction to learn brain activity represen-\ntations [56]. BrainLM supports pretraining, zero-shot inference, and fine-tuning applications. It can\npredict psychological measures including neuroticism, PTSD, and anxiety scores from neural signals.\nThis demonstrates the potential for fMRI FMs to connect neuroscience with clinical psychology.\nVisual reconstruction from brain activity represents a compelling application of fMRI FMs. These\nmodels translate neural patterns into interpretable visual content with remarkable accuracy.\nThe\nMindEye series pioneered this domain by demonstrating accurate image retrieval from large-scale\ndatabases like LAION-5B using only brain activity patterns [73]. MindEye2 advances this work through\nshared-subject modeling that enables effective reconstruction with minimal training data. This model\nutilizes the Natural Scenes Dataset to achieve high-fidelity visual reconstructions across 8 subjects [34].\nThese developments mark significant progress toward practical brain-computer interfaces for visual\ncommunication and accessibility applications.\nSophisticated architectural innovations have advanced the quality and controllability of neural-to-\nvisual translation. MinD-Vis introduces frameworks combining Sparse-Coded Masked Brain Modeling\nwith Double-Conditioned Latent Diffusion Models. The approach trains on extensive datasets spanning\n136,000 fMRI segments from 340 hours of scanning across HCP, GOD, and BOLD5000 datasets [53].\nThis method achieves exceptional visual scene reconstruction while maintaining interpretability of\nneural representations. Dual conditioning mechanisms enable precise control over semantic content\nand visual details in reconstructed images.\nGenerative modeling approaches have evolved to incorporate region-specific analysis and semantic\nunderstanding of neural representations. Earlier transformer-based architectures, such as the Brain\nNetwork Transformer and hierarchical spatio-temporal generative models [74, 75], focused primarily\non task-specific fMRI analysis without large-scale or multimodal pretraining. Building upon these\nfoundations, recent works have begun integrating FM and language-based paradigms for cross-modal\nunderstanding. Generative modeling approaches have evolved to incorporate region-specific analysis\nand semantic understanding of neural representations. Brain-Diffuser advances this paradigm through\nROI-based analysis integrated with CLIP embeddings. This enables semantically meaningful recon-\nstructions that capture both low-level visual features and high-level conceptual content [16]. Cross-\nsubject generalization capabilities have been enhanced through multi-individual pretraining strategies.\nNeuroPictor demonstrates this through multi-level modulation that enables robust performance across\ndifferent subjects and experimental conditions [76]. These advances establish fMRI FMs as powerful\ntools for understanding the neural basis of visual perception and cognition.\nAnother significant advancement in neuroimaging FMs has emerged through Brain Graph Foun-\ndation Model (BrainGFM), a graph-based approach that models the brain as a complex network of\ninterconnected regions [45].\nUnlike previous models using time-series data, BrainGFM introduces\na graph-based paradigm leveraging functional and anatomical connectivity patterns through Graph\nTransformer networks with specialized brain topology encodings.\nThe model is pre-trained on 27\ndatasets spanning 25 neurological disorders with over 25,000 subjects and 60,000 fMRI scans across 8\n9\nbrain atlases. This enables atlas-invariant patterns that generalize across parcellation schemes. Key\ninnovations include a unified training framework combining graph contrastive learning and masked\nautoencoders. The model also incorporates meta-learning optimization for few-shot adaptation and\nlanguage-guided prompting for zero-shot transfer to unseen conditions.\nBrainGFM addresses neu-\nroimaging heterogeneity while maintaining computational efficiency.\nStructural MRI FMs: FMs for structural neuroimaging are emerging as powerful tools in\nanatomical analysis and show increasing potential for clinical diagnosis. These models employ special-\nized architectures designed for 3D volumetric brain data. Robust segmentation capabilities represent\na primary focus area. Self-supervised learning strategies have proven effective for handling diverse\nimaging protocols and anatomical variations across different populations.\nAdvanced 3D architectures optimized for volumetric brain data processing have established new\nbenchmarks for neuroimaging analysis. BrainSegFounder pioneered this domain through specialized\n3D convolutional networks designed for comprehensive neuroimage segmentation. The model achieves\nrobust performance across critical clinical tasks including brain tumor segmentation and lesion de-\ntection [35]. Self-supervised pretraining enables effective handling of diverse imaging contrasts and\nanatomical presentations encountered in clinical practice.\nCross-institutional generalization has become crucial for practical clinical deployment, where imag-\ning protocols and equipment vary significantly across medical centers. Brain Imaging Adaptive Core\n(BrainIAC) addresses this challenge through self-supervised pretraining followed by fine-tuning on\nspecific downstream neuroimaging tasks including MRI sequence classification, brain age prediction,\ntumor mutation classification, and survival analysis. The model achieves superior performance while\nemphasizing anatomical consistency across multiple neuroimaging benchmarks [77]. This approach\nproves valuable for multi-site clinical studies and real-world medical applications where robust gener-\nalization is essential for reliable diagnostic support.\nWeakly supervised approaches have demonstrated exceptional capability in capturing subtle anatom-\nical variations associated with neurological and psychiatric disorders. Anatomical Contrastive Learning\n(AnatCL) introduces anatomical contrastive learning strategies that enable effective representation\nlearning from structural brain images without extensive manual annotations [36].\nThe model de-\ntects fine-grained anatomical differences valuable for complex diagnostic challenges. These include\nAlzheimer’s disease (AD), autism spectrum disorder, and schizophrenia, where pathological changes\nmay be subtle and spatially distributed.\nCT FMs: Head CT interpretation in emergency settings represents a critical application where\nFMs provide immediate clinical value through rapid trauma assessment. Deep Comprehensive Neuro\nTrauma Detection Network (DeepCNTD-Net) introduces a specialized 3D FM for comprehensive\nneuro-trauma triage, leveraging LLMs for automatic multi-label annotation of 16 critical conditions\nincluding hemorrhage, mass lesions, cerebral edema, and arterial hyperdensity [78]. The model in-\ntegrates task-specific pretrained networks for hemorrhage subtype segmentation and brain anatomy\nparcellation through multimodal fine-tuning, achieving an average AUC of 0.861 across all trauma\nconditions and outperforming CT-CLIP baselines. Training on 29,395 non-contrast head CT studies\nfrom nine international centers enables robust generalization capabilities, addressing the urgent need\nfor AI-assisted interpretation in emergency radiology, where timely diagnosis is essential for optimal\npatient outcomes.\nIn parallel developments, FM for Head CT (FM-HCT) introduces a specialized 3D FM for com-\nprehensive head CT disease detection in neurological emergency settings [55]. The model employs\nself-supervised learning on 361,663 non-contrast 3D head CT scans, utilizing discrimination with self-\ndistillation and masked image modeling approaches with a customized 3D Vision Transformer architec-\nture. The FM demonstrates substantial performance improvements, achieving 16.07% improvement\nin macro-AUC over models trained from scratch on internal data. Strong generalization across ex-\nternal datasets shows 20.86% and 12.01% improvements on NYU Long Island and RSNA datasets,\nrespectively. Beyond traditional hemorrhage detection, the model extends capabilities to detect brain\ntumors, AD and related dementia, edema, and hydrocephalus. The model demonstrates effectiveness\nin few-shot learning scenarios and scales efficiently with increased pre-training data. This establishes\na new benchmark for 3D head CT analysis, enabling broader deployment of AI-assisted diagnosis in\nemergency and clinical settings where rapid assessment is crucial for patient outcomes.\n10\n3.1.2\nElectrophysiological Signal Processing Models\nLarge-scale AI models have substantially improved electrophysiological data analysis. These models\naddress unique challenges in temporal dynamics, multi-channel spatial organization, and cross-subject\nvariability in neural recordings.\nEEG FMs: The development of transformer-based architectures for EEG analysis has been pio-\nneered by several influential FMs that established the paradigm for large-scale neural signal processing.\nBENDR demonstrated the potential of self-supervised learning for EEG analysis through training on\nthe extensive TUH EEG Corpus using Masked Signal Modeling and Contrastive Learning [9]. The\ntransformer-based architecture enables effective transfer learning across diverse applications from sleep\nmonitoring to brain-computer interfaces and disease detection. Fine-tuning evaluations on datasets\nincluding BCIC IV-2a, Sleep-EDF, and TUAB consistently demonstrate performance improvements\nover task-specific approaches.\nBuilding on these foundations, emphasis on interpretability and transferability has advanced through\nlarge-scale pretraining approaches. EEGFormer extends the transformer paradigm by emphasizing per-\nformance and the ability to provide insights into temporal and spatial patterns that drive successful\nEEG classification [54]. This interpretability focus makes such models valuable for scientific discov-\nery applications where understanding underlying neural mechanisms is as important as classification\naccuracy.\nThe adaptation of generative modeling principles to EEG analysis represents another significant\nadvancement in the field. Neuro-GPT demonstrates how autoregressive architectures originally devel-\noped for natural language processing can be effectively adapted for EEG analysis. The model employs\nself-supervised pretraining followed by task-specific fine-tuning to achieve robust performance across\nmultiple domains [39]. This autoregressive approach enables natural handling of variable-length EEG\nsequences while maintaining computational efficiency for real-time applications.\nComprehensive multitask learning capabilities have been developed through models that demon-\nstrate exceptional versatility across diverse EEG applications. EEGPT advances this paradigm by\nshowing that a single pretrained model can be effectively adapted to applications ranging from cog-\nnitive state classification to clinical diagnosis [79]. The model’s architecture incorporates attention\nmechanisms specifically designed for EEG data characteristics, enabling robust transfer across differ-\nent experimental paradigms and clinical conditions.\nSpecialized attention mechanisms designed for the unique temporal characteristics of EEG signals\nhave emerged as critical innovations for specific applications. FM for EEG (FoME) introduces the\nATLAS (Adaptive Temporal-Lateral Attention Scaling) mechanism, which enables dynamic adaptation\nto different temporal scales present in neural signals [80]. This approach proves effective for applications\nrequiring precise temporal modeling, such as early seizure detection and advanced brain-computer\ninterfaces, where millisecond-level accuracy can be crucial for clinical outcomes.\nRecent developments in cross-scale spatiotemporal modeling have addressed a critical limitation in-\nherited from NLP transformers: the inability to handle the multi-scale nature of neural activity. Cross-\nscale Spatiotemporal Brain foundation model (CSBrain) tackles this challenge through Cross-scale\nspatiotemporal tokenization that aggregates features from localized temporal windows and anatomi-\ncal brain regions, combined with structured sparse attention to capture dependencies across different\nscales. Evaluated across 11 EEG tasks and 16 datasets, this approach consistently outperforms both\ntask-specific models and existing FM baselines by explicitly modeling neural activity patterns ranging\nfrom millisecond-level spikes to multi-second cognitive processes [81].\nThe challenge of efficiently processing temporal and frequency domain information has been rev-\nolutionized through decoupled tokenization approaches. CodeBrain introduces a TFDual-Tokenizer\nthat independently processes temporal and frequency components using separate codebooks, enabling\nquadratic expansion of discrete representation space compared to traditional linear approaches. This\ninnovation, combined with Structured Global Convolution that reflects the small-world topology of\nthe brain, demonstrates superior generalization capabilities across 10 public EEG datasets while main-\ntaining computational efficiency for real-time applications [82].\nMultimodal neural signal processing has emerged as a critical research area, with unified EEG-\nMEG analysis providing comprehensive insights into brain activity patterns. BrainOmni advances\nthis field through training on 2,653 hours of combined neural data, implementing a novel BrainTok-\nenizer architecture that incorporates sensor-specific encoding. This approach enables device-agnostic\nsignal processing by independently encoding spatial configuration, sensor orientation, and modality\n11\ntype, circumventing traditional naming convention dependencies.\nThis unified training framework\ndemonstrates measurable performance gains, achieving 20% improvement on EMEG SomatoMotor\nbenchmarks compared to single-modality baselines. These results consistently validate the superiority\nof joint EEG-MEG training paradigms over isolated approaches [83].\nIntracranial signal processing has progressed through disentanglement frameworks that isolate task-\nrelevant neural components from background activity. BrainStratify implements a coarse-to-fine archi-\ntecture integrating spatial-context modeling with Decoupled Product Quantization (DPQ) for sEEG\nand ECoG data analysis at millimeter resolution.The hierarchical processing pipeline effectively sepa-\nrates signal components while preserving spatial relationships across electrode arrays. Evaluation on\nspeech production and perception datasets shows consistent superiority over existing intracranial de-\ncoding methods, with performance gains attributed to the systematic separation of task-specific neural\npatterns [84].\nEmerging approaches in topology-agnostic processing and cross-view interaction frameworks con-\ntinue to push the boundaries of EEG FMs. Latent Unified Network Architecture (LUNA) emphasizes\ncomputational efficiency and generalization across diverse electrode configurations, while Cross-View\ninstance-adaptive Pre-training model (CRIA) introduces instance-adapted pre-training strategies for\nenhanced generalizable representations across subjects and experimental conditions [85, 86]. These\ndevelopments reflect the field’s continued evolution toward more flexible and adaptive neural signal\nprocessing architectures.\nAdvanced Neural Recording Models: FMs have extended to intracranial recordings, enabling\ndetailed analysis of cortical neural dynamics. BRANT represents a significant advancement in this area,\nutilizing a 500-million-parameter architecture trained via masked patch prediction in a self-supervised\nframework [8]. The model addresses key challenges in sEEG data analysis, particularly the need for\ncross-patient generalization and spatial precision. Training on large-scale intracranial datasets enables\nrobust performance across diverse recording configurations and experimental paradigms. The archi-\ntecture proves especially effective for decoding tasks requiring millisecond temporal resolution and the\nspatial specificity inherent to intracranial electrode placement, demonstrating improved generalization\ncompared to traditional patient-specific approaches.\nCross-modal integration of surface and intracranial recordings addresses the critical challenge of\ntranslating findings across recording modalities. BrainWave achieves this through a unified architecture\nthat processes both EEG and IEEG data within a single framework [87]. The cross-modal training\nenables transfer learning between surface and intracranial modalities, facilitating knowledge transfer\nfrom high-resolution invasive studies to non-invasive clinical settings. This approach directly addresses\nthe translation gap between detailed mechanistic insights obtained from intracranial recordings and\ntheir application to broader patient populations limited to surface recordings. The unified framework\ndemonstrates that FMs can effectively bridge spatial resolution differences while preserving relevant\nneural information across modalities.\nSpecialized EEG Applications: FMs have demonstrated particular effectiveness in specialized\nEEG applications through domain-specific pretraining strategies. Adaptive Large Foundation model\nfor EEG signal representation (ALFEE) addresses key limitations in EEG foundation modeling, in-\ncluding variable channel configurations, inadequate separation of spatial and temporal features, and\ndomain transfer challenges [88]. The architecture implements a hybrid transformer design with distinct\nchannel-wise and temporal attention mechanisms, enabling robust processing across diverse electrode\nconfigurations. A two-stage optimization incorporates comprehensive pretraining objectives—task pre-\ndiction, channel reconstruction, temporal reconstruction, and forecasting—followed by adaptive fine-\ntuning using task-specific token dictionaries and cross-attention. Evaluation across six downstream\ntasks, including emotion recognition, sleep staging, and abnormality detection, demonstrates superior\nperformance over existing multi-task EEG models. Scaling analysis on models up to 540M parame-\nters trained on 25,000 hours of EEG data confirms consistent performance gains with increased scale,\nestablishing a foundation for large-scale EEG model development.\nMasking strategy adaptation to neural signal characteristics has proven critical for FM performance.\nLarge Brain Model (LaBraM) demonstrates this principle by tailoring masked modeling approaches to\nEEG temporal dynamics, achieving superior results compared to direct application of computer vision\nmasking techniques [89]. The domain-aware pretraining strategy accounts for the unique temporal\nstructure of neural signals, highlighting the importance of signal-specific architectural considerations\nin FM design.\n12\nFMs have demonstrated significant potential in neurodegenerative disease detection through spe-\ncialized clinical applications. The large-scale foundation model for EEG-based AD detection (LEAD)\naddresses Alzheimer’s diagnosis via subject-level detection that incorporates domain knowledge of neu-\nrodegenerative progression [90]. The architecture maintains robust diagnostic accuracy across stages,\ndemonstrating effective incorporation of stage-consistent representations into FM frameworks for reli-\nable performance in clinical applications where diagnostic precision is critical.\n3.1.3\nMultimodal Integration Approaches\nNeuroscience AI models are progressing toward multimodal integration to enable comprehensive brain\nfunction analysis. These approaches combine spatial, temporal, and semantic information across di-\nverse neural recording modalities, leveraging complementary data sources for enhanced understanding\nof neural mechanisms.\nCross-Modal Brain Decoding: Vision-language models have been adapted for neuroscience\napplications to decode visual processing mechanisms. MindBridge integrates fMRI data with visual\nand semantic information through representation alignment strategy that utilize CLIP embeddings\nfor cross-subject generalization [15]. Training on the Natural Scenes Dataset (NSD) demonstrates\neffective representation transfer across subjects, addressing cross-subject variability in neuroimaging\nand establishing scalable approaches for brain decoding tasks.\nThe Bridging-Vision-and-Language (BriVL) model establishes a large multimodal FM jointly pre-\ntrained on 15 million image–text pairs [91].\nIt learns shared visual–linguistic representations that\nenable cross-modal prediction and generalization. Neural encoding analyses using fMRI demonstrate\nstrong alignment between BriVL’s multimodal representations and human brain activity patterns, sug-\ngesting that multimodal FMs can serve as computational tools for studying multisensory integration\nand cognition.\nRecent advances in brain-vision-language integration have focused on leveraging pre-trained models\nfor neural decoding. BrainCLIP utilizes CLIP’s multimodal representations to decode natural visual\nstimulus from neural activity, bridging the semantic gap between neural signals and visual under-\nstanding [37]. The approach enables decoding of complex, naturalistic visual stimuli by training a\nmapping network that aligns fMRI patterns with CLIP’s unified embedding space, advancing beyond\ntraditional methods limited to simple, controlled stimuli. This demonstrates the potential for FMs as\ncross-modal translators, suggesting that semantic representations from large-scale pre-training capture\nfundamental information processing principles relevant to biological neural systems and provide new\ndirections for neuroscience research.\nIntegration of language and brain function: The End-to-end Multimodal LLM achieves direct\nlanguage decoding from neural signals by adapting LLM architectures for fMRI-to-text translation [92].\nSpecialized pretraining and fine-tuning enable direct textual decoding from fMRI sequences, advanc-\ning toward natural language brain-computer interfaces. This approach represents a significant step\nin bridging neural activity and linguistic output through end-to-end learning frameworks. Integration\nof language modeling with neuroimaging demonstrates FMs’ capacity to bridge neural activity and\nsemantic understanding for natural brain-computer communication. Success in extracting linguistic\ninformation from neural signals indicates potential applications in assistive technologies for communi-\ncation disorders, enabling direct neural-to-text interfaces that could restore communication capabilities\nin clinical populations.\nStructural-Functional Integration: Recent studies have focused on integrating structural and\nfunctional brain data through large-scale AI models to enhance diagnostic capabilities.\nMultiViT\nintroduces a multimodal vision transformer that fuses 3D gray matter maps from structural MRI\nwith functional network connectivity (FNC) matrices derived from ICA-processed fMRI using cross-\nattention mechanisms [38]. The architecture achieves an AUC of 0.833 in schizophrenia classification,\noutperforming unimodal and concatenation-based approaches. By replacing high-dimensional fMRI\ndata with lightweight two-dimensional FNC representations and employing separate 3D and 2D vi-\nsion transformer encoders with cross-attention fusion, MultiViT effectively captures structure-function\nrelationships. The interpretability framework generates attention maps identifying neurobiologically\nmeaningful regions in 3D structural space, providing insights into disease-relevant patterns in brain\nareas such as the cerebellum, temporal gyrus, and prefrontal regions. This establishes a paradigm\nfor efficient multimodal neuroimaging analysis that bridges anatomical organization with functional\ndynamics in psychiatric disorders, demonstrating that advanced fusion mechanisms like cross-attention\n13\nare essential for optimal performance while maintaining computational efficiency.\nTable 4: Comprehensive Overview of Large-scale AI Models for Computational Neuroscience Research.\n(PT: Pre-training; FT: Fine-tuning)\nModel Name\nPrimary Purpose/Application\nModel Type\nTraining Method\nDataset Used\nADAgent [93]\nAlzheimer’s disease analysis\nFramework w/ LLM\nLLM integration\nADNI dataset (T1-MRI, PET-FDG)\nALFEE [88]\nGeneral\nEEG\nrepresentation\nlearning for multiple tasks\nFM\nPT + FT\nMulti-task EEG datasets including\nclinical, emotion, sleep, and work-\nload\nAnatCL [36]\nAnatomical\nFM\nfor\nstructural\nbrain MRI analysis\nFM\nPT + FT\nMultiple\nMRI\ndatasets\nwith\nDesikan-Killiany atlas\nASD-Chat [94]\nAutism\nspectrum\ndisorder\nsup-\nport\nFramework w/ LLM\nPrompt-based\nClinical intervention data\nAtlasGPT [40]\nNeurosurgical clinical augmenta-\ntion\nLLM\nRAG-enhanced LLM\nJNSPG articles\nBENDR [9]\nTransformer-based EEG analysis\nfor BCI applications\nFM\nPT + FT\nTUEG and other large EEG datasets\nBRANT [8]\nFM for intracranial neural signal\nanalysis\nFM\nPT + FT\nLarge corpus of intracranial SEEG\ndata\nBrainBench [95]\nNeuroscience\nbenchmarking\nframework\nLLM\nPT + FT\nNeuroscience literature\nBrainCLIP [37]\nTask-agnostic fMRI brain decod-\ning with CLIP integration\nFramework w/ LLM\nPT + FT\nfMRI data with visual stimuli\nBrain-Diffuser [16]\nfMRI-to-image reconstruction us-\ning generative diffusion\nFM\nPT + FT\nNatural Scenes Dataset (NSD)\nBrainGFM [45]\nUnified framework for large-scale\nfMRI pre-training\nFM\nPT + FT\n27\ndatasets,\n25,000+\nsubjects,\n60,000 fMRI scans\nBrainGPT [96]\nNeurosurgical applications\nLLM\nPT + FT\n1.3B\ntokens\n(neuroscience\nlitera-\nture)\nBrainIAC [77]\nGeneral-purpose\nFM\nfor\nstruc-\ntural brain MRI\nFM\nPT + FT\n48,519 brain MRIs from 35 datasets\nBrainLM [56]\nFM for brain activity dynamics\nprediction\nFM\nPT + FT\nUK\nBiobank,\nHCP\n(80,000\nscans,\n40,000 subjects)\nBrainOmni [83]\nFirst\nFM\nfor\nunified\nEEG\nand\nMEG analysis\nFM\nPT + FT\n1,997 hours EEG + 656 hours MEG\nBrainSegFounder [35]\n3D FM for multimodal neuroim-\nage segmentation\nFM\nPT + FT\nUK Biobank (41,400 participants),\nBraTS, ATLAS v2.0\nBrainStratify [84]\nFM for invasive and non-invasive\nrecordings\nFM\nPT + FT\n40,000+\nhours\nelectrical\nbrain\nrecordings\nBrainWave [87]\nFM for invasive and non-invasive\nrecordings\nFM\nPT + FT\n40,000+\nhours\nelectrical\nbrain\nrecordings\nBriVL [91]\nMultimodal\nFM\nbridging\nvision\nand language\nFM\nPT + FT\nRUC-CAS-WenLan (30M image-text\npairs), fMRI data\nCBraMod [33]\nEEG decoding for BCI applica-\ntions\nFM\nPT + FT\nTUEG (1.1M samples, 9000+ hours)\nCentaur [52]\nHuman behavior prediction\nFM\nPT + FT\nPsych-101 (60K+ participants)\nCodeBrain [82]\nEEG foundation model for inter-\npretable decoding\nFM\nPT + FT\nTUH\nEEG\nCorpus\nfor\npretraining\nand multiple EEG cohorts for down-\nstream tasks.\nCSBrain [81]\nCross-scale spatiotemporal brain\nFM for EEG\nFM\nPT + FT\n11 EEG tasks across 16 datasets\nDeepCNTD-Net [78]\n3D\nFM\nfor\nmulti-label\nneuro-trauma\ndetection\non\nnon-contrast head CT scans.\nFM\nPT + FT\nNon-contrast head CT images\nDECT [48]\nLLM-driven model extracting lin-\nguistic markers and synthesizing\ndialogue data for Alzheimer’s di-\nagnosis.\nFramework w/ LLM\nPT + FT\nSpeech\nrecording\nand\ntranscripts\nfrom\nADReSSo\n(from\nDementia-\nBank)\nEEGFormer [54]\nLarge-scale EEG FM for transfer-\nable learning\nFM\nPT + FT\nTUH Corpus (1.7TB EEG dataset)\nEEGPT (v1) [79]\nGeneralist EEG FM for multiple\ntasks\nFM\nPT + FT\n37.5M pre-training samples,\n1B to-\nkens\nEEGPT (v2) [79]\nUniversal EEG feature extraction\nfor medical and BCI\nFM\nPT + FT\nLarge mixed multi-task EEG dataset\nEpiSemoLLM [97]\nEpileptogenic zone localization\nLLM\nPT + FT\nCollected seizure semiology descrip-\ntions\nExKG-LLM [98]\nCognitive\nneuroscience\nknowl-\nedge graph expansion\nFramework w/ LLM\nLLM-KG integration\nScientific papers and clinical reports\nFM-HCT [55]\n3D FM for head CT disease de-\ntection\nFM\nPT + FT\n361,663\nnon-contrast\n3D\nhead\nCT\nscans\nFoME [80]\nFM for EEG with adaptive atten-\ntion scaling\nFM\nPT + FT\n1.7TB diverse scalp and intracranial\nEEG\nHmamouche et al. [92]\nNon-invasive decoding of spoken\ntext from fMRI\nFramework w/ LLM\nPT + FT\nCustom\ncorpus\nwith\nfMRI\nrecord-\nings\nHybridTransNet [47]\nBrain\ntumor\nboundary\ndelin-\neation\nFramework w/ LLM\nPT + FT\nMedical imaging datasets\niReportMed [99]\nHybrid\npipeline:\nCNN\nextracts\nmulti-modal MRI features; LLM\ngenerates structured brain tumor\ndiagnostic.\nFramework w/ LLM\nPT + FT\n˜378 brain tumor MRI scans (350\nUCSF-PDGM + 28 clinical cases)\nLaBraM [89]\nLarge\nbrain\nmodel\nfor\ngeneric\nEEG representations\nFM\nPT + FT\n˜2,500 hours EEG from\n20 datasets\nLEAD [90]\nClinical applications in neurode-\ngenerative disease detection\nFM\nPT + FT\nAlzheimer’s\ndisease\nrelated\nEEG\ndatasets\nLUNA [85]\nTopology-agnostic EEG process-\ning with computational efficiency\nFM\nPT + FT\nDiverse electrode configurations\nMental-LLM [49]\nLLM (GPT-based) fine-tuned to\npredict mental health status from\nonline textual posts.\nFramework w/ LLM\nPT + FT\nOnline text (social media data)\nMindBridge [15]\nCross-subject brain decoding for\nvisual reconstruction\nFM\nPT + FT\nMultiple subjects’ fMRI with visual\nstimuli\nMindEye [73]\nfMRI-to-image reconstruction us-\ning contrastive learning\nFM\nPT + FT\nNatural Scenes Image–fMRI Dataset\n(8 subjects)\nMindEye2 [34]\nEfficient\nfMRI-to-image\nwith\nminimal training data\nFM\nPT + FT\nNatural Scenes Image–fMRI Dataset\n(7 subjects)\nMinD-Vis [53]\nVisual\nstimulus\ndecoding\nfrom\nfMRI using diffusion\nFM\nPT + FT\n˜136,000 fMRI samples, 1,205 sub-\njects\nMsLesionLLM [100]\nLLM-based\nprompt\nclassifying\nMRI reports to extract new MS\nlesion information.\nLLM\nPT + FT\nClinical MRI report text\nMultiViT [38]\nStructural-functional brain data\nintegration for schizophrenia di-\nagnosis\nFM\nPT + FT\nMulti-site MRI (2,130 subjects)\nMDD-LLM [101]\nMajor depressive disorder diagno-\nsis\nLLM\nPT + FT\nUK Biobank (274,348 records)\nNeura [102]\nSpecialized\nneurology\napplica-\ntions\nLLM\nPT + FT\nNeurological corpus\nNeuro-GPT [39]\nFM for EEG-based BCI tasks\nFM\nPT + FT\nTUH EEG Corpus, BCI Competition\nIV 2a\nNeuroGPT-X [103]\nNeurosurgical decision support\nLLM\nRAG-enhanced LLM\nPubMed abstracts, Wikipedia\nNeuroPictor [76]\nfMRI-to-image\nreconstruction\nwith semantic control\nFM\nPT + FT\n67,000 fMRI-image pairs\nContinued on next page\n14\nTable 4: – continued from previous page\nModel Name\nPrimary Purpose/Application\nModel Type\nTraining Method\nDataset Used\nLaMIM [104]\nPostencephalitic\nepilepsy\nanal-\nysis\nusing\nmulti-contrast\nbrain\nMRI\nFM\nPT + FT\n57,621\nmulti-contrast\nwhole-brain\nMRI\nPKG-LLM [105]\nMental\nhealth\nprediction\n(GAD/MDD)\nFramework w/ LLM\nLLM-KG integration\nNeuroLex, NeuroMorpho databases\nPOYO [106]\nSpike-based neural decoding\nFM\nPT + FT\n158+ sessions, 27,373+ neural units\nProMind-LLM [107]\nPsychiatric evaluation\nLLM\nPT + FT\nPMData and Globem datasets\nscFoundation [108]\nSingle-cell transcriptomics repre-\nsentation learning\nFM\nPT + FT\nMultiple\nsingle-cell\nRNA-seq\ndatasets\nSHADE-AD [46]\nLLM framework generating syn-\nthetic\nAlzheimer’s-specific\nhu-\nman activity videos to augment\nmonitoring datasets.\nFramework w/ LLM\nPrompt-based\nNTU\nRGB+D\n120\ndataset\n(over\n120,000 videos).\nSocialRecNet [109]\nMultimodal\nLLM\n(speech+text)\nframework predicting ASD social\nreciprocity scores from conversa-\ntional data.\nFramework w/ LLM\nLLM integration\nSpeech and text for autism spectrum\ndisorder\nTRUST [110]\nLLM-powered dialogue system of\ncooperative\nmodules\nfor\nstruc-\ntured\nPTSD\ndiagnostic\ninter-\nviews.\nFramework w/ LLM\nLLM integration\nText (clinical interview dialogues)\nVS-LLM [51]\nVision-language\nLLM\nanalyzing\ntherapy\nsketches\nfor\ndepression\nassessment.\nLLM\nPT + FT\nPPAT\ndrawing\ntest\n(sketches\n+\nstroke sequences)\n3.2\nBrain-Computer Interfaces and Neural Decoding\nBrain-computer interfaces represent one of the most promising applications of large-scale AI models in\nneuroscience, where the ultimate goal is to establish direct communication pathways between the brain\nand external devices. FMs have revolutionized this field by enabling robust neural signal decoding\nacross subjects, sessions, and recording modalities with unprecedented accuracy and generalization\ncapabilities. The development of specialized architectures for real-time neural decoding has led to\nremarkable achievements in motor intention prediction, cognitive state classification, and direct neural\ncontrol of external devices.\nBrain-Computer Interface Models: The application of large-scale AI models to brain-computer\ninterfaces has been driven by the need to overcome fundamental challenges in neural signal decoding,\nincluding limited labeled data and high inter-subject variability that traditionally hamper BCI per-\nformance.\nSelf-supervised pretraining approaches have emerged as particularly effective solutions,\nenabling models to learn robust neural representations from large amounts of unlabeled data before\nfine-tuning on specific BCI tasks. Specialized architectures designed for BCI applications have demon-\nstrated that domain-specific innovations can significantly enhance decoding performance. CBraMod\nexemplifies this approach through its innovative criss-cross transformer architecture designed for large-\nscale EEG decoding and BCI applications [33]. Its dual spatial-temporal attention mechanism effec-\ntively captures heterogeneous dependencies in EEG signals, while self-supervised pretraining on the\nTemple University Hospital EEG Corpus followed by fine-tuning across 10 downstream BCI tasks\nachieves robust generalization and superior decoding accuracy, even in low-data regimes.\nTransformer-based architectures demonstrate versatility for BCI applications through general-\npurpose models with broad task adaptability. BENDR employs transformer architecture with con-\ntrastive self-supervised learning for effective transfer learning across motor imagery classification and\nattention state detection [9].\nThe model learns robust representations from large unlabeled EEG\ndatasets, addressing the challenge of limited subject-specific labeled data in BCI scenarios and demon-\nstrating successful adaptation of general FMs to specialized applications. Advanced temporal modeling\nhas become critical for real-time BCI performance, requiring precise timing and dynamic adaptation\nto neural signal characteristics. FoME addresses this through the ATLAS mechanism, enabling dy-\nnamic adaptation across different temporal scales in neural signals [80]. This capability proves valuable\nfor complex BCI applications requiring sustained attention and motor control, where capturing both\nshort-term neural dynamics and long-term cognitive patterns enables robust real-time performance\nacross diverse BCI paradigms.\nHigh-Resolution Neural Decoding Models: FMs for high-resolution neural recordings repre-\nsent a frontier in brain-computer interface research, enabling precise decoding of individual neuronal\nactivity patterns. Pre-training On manY neurOns (POYO) introduces spike-based tokenization for pro-\ncessing high-resolution neural recordings, directly processing individual spike events as discrete tokens\nrather than binned neural data [106]. This approach preserves the neural code’s temporal structure\nwhile maintaining computational efficiency through sparse representation. The transformer architec-\nture employs a PerceiverIO backbone with cross-attention mechanisms to compress spike sequences\ninto latent representations, enabling scalable training across multiple sessions without requiring neu-\n15\nron correspondence. POYO addresses the challenge of integrating neural recordings across individuals\nwith unique, non-alignable neuron sets. Training encompasses 178 recording sessions with 29,453 neu-\nral units from motor, premotor, and somatosensory cortices across 9 nonhuman primates, totaling over\n100 hours of recordings. The ”unit identification” approach enables rapid adaptation to new sessions by\nfreezing pretrained weights and learning unit embeddings through gradient descent, achieving few-shot\nlearning performance comparable to models trained from scratch with minimal labeled data. POYO\ndemonstrates superior neural decoding performance with R² scores of 0.95 on center-out reaching tasks\nand 0.87 on complex random target tasks, while supporting real-time processing for closed-loop BCI\napplications.\n3.3\nMolecular Neuroscience and Genomic Modeling\nCurrent advances in neuroscience increasingly emphasize the need to address brain function not only at\nthe systems and behavioral levels, but also through the molecular and genetic mechanisms that control\nneural activity and disease. On the molecular front, FMs, trained on large genomic and transcriptomic\ndatasets, are transforming our ability to solve for the regulatory architecture of the genome and its\nimpact on neuronal processes. These models introduce the representation learning paradigm into the\nDNA and RNA domains and allow for computational prediction of gene regulation, variant effect, and\ncell-type-specific function.\nThe Nucleotide Transformer comprises multiple FMs pre-trained on 3,202 diverse human genomes,\nand 850 genomes from various species with between 500 million and 2.5 billion parameters [111].\nThe models can predict chromatin states, enhancer-promoter interactions, and transcription factor\nbinding sites, despite the small proportion of DNA sequences containing transcription factor motifs.\nSimilar FMs, including AlphaGenome [41] and Enformer [42] and which are trained on DNA sequences,\nare enabling the prediction of the functional consequence of genetic variation in the regions of DNA\nthat do not encode proteins. Tools for annotating the protein-coding regions of the DNA are well\nestablished and enable reliable prediction of how genetic variants affect proteins. In contrast, a poor\nunderstanding of non-protein-coding DNA, where most of the disease-associated variation lies, has long\nhindered progress in genomic research. FMs are now transforming this landscape by enabling scientists\nto prioritise variants for research question-specific modelling. Similarly, the scFoundation model that\nwas pretrained on over 50 million single-cell RNA sequencing data to model 19,264 genes with 100\nmillion parameters, is one of several models that exploit growing repositories of transcriptomic data\nto provide rich contextual maps of cell function, under both normal and pathological conditions [108],\ntransforming our capacity to predict both function and dysfunction.\nCollectively, these FMs hold considerable promise for neuroscience research, as they can be lever-\naged to study neuronal gene regulation, characterize diverse neural cell types, and investigate the\nmolecular basis of neurological diseases and brain function at an unprecedented scale.\n3.4\nClinical Support and Translational Frameworks\nScalable AI systems, especially FMs, are being used at a greater rate in clinical neuroscience to aid\ndiagnostic decision support and enhance patient outcomes in psychiatric and neurological practice.\nThese translational systems are required to overcome a number of key challenges: providing adequate\nclinical validation, preserving interpretability for clinicians, and blending with current clinical practice.\nThese implementations should also meet regulatory requirements and handle ethical issues involved\nwith medical AI use.\n3.4.1\nClinical Decision Support\nNeuroscience clinical decision support systems are being transformed through the integration of LLMs\nthat support physicians with diagnosis, treatment guideline formulation, and patient care. With the use\nof expansive medical knowledge databases and clinical databases, the AI systems yield evidence-based\nadvice and improve diagnostic efficacy. The technology has a specific forte in facilitating sophisticated\nclinical reasoning both in neurological and psychiatric domains and offers clinicians advanced mecha-\nnisms of decision-support while dealing with complex patient situations and treatment modalities.\nSpecialized Neurological Decision Support Systems:\nVarious FMs have been uniquely\ndesigned to tackle intricate neurological disorders that necessitate specialized clinical knowledge.\n16\nEpiSemoLLM exemplifies a focused methodology for managing epilepsy by utilizing refined language\nmodels that analyze descriptions of epilepsy semiology alongside optional demographic attributes to\nforecast epileptogenic zones (EZs) [97].\nThis model is trained on extensive datasets sourced from\nPubMed literature and proprietary electronic health record groups, facilitating accurate localization of\nseizure origins to inform surgical planning and therapeutic strategies. NeuroGPT-X presents context-\nenhanced language models of neurosurgical decision support with zero-shot learning models involving\nGPT-3 API interfacing [103]. Clinical-specific functionalities such as conversation memory, in-text\ncitation, reference, medical topic limitation, timestamping, and anti-spam are part of the system.\nTesting with international neurosurgical experts on vestibular schwannoma treatment queries revealed\nsimilar clinical reasoning and treatment advice performance and proved the feasibility of AI-augmented\nneurosurgical advice.\nAdvanced Clinical Augmentation Platforms: With the advent of sophisticated clinical aug-\nmentation systems comes the support of advanced features of neurological practice. AtlasGPT rep-\nresents a RAG LLM designed for neurosurgical applications, integrating generative reasoning with\ndomain-specific literature retrieval to enhance the accuracy and reliability of clinical insights [40].\nTrained on 250,000 pages of peer-reviewed neurosurgical data, the system provides contextualized\nrecommendations for surgical planning, decision-making, and educational support, illustrating the po-\ntential of specialized LLMs to augment clinical practice. Furthermore, Neura introduces a specialized\nLLM solution for clinical neurology, combining RAG with tailored prompt engineering to enhance\ncontextual relevance and verifiable information synthesis [102]. Its dual-database architecture and em-\nphasis on explainability highlight the importance of human–AI interaction design in clinical decision\nsupport systems.\nComprehensive Clinical Decision Support Paradigms: Almanac provides multi-specialty\nclinical decision support using RAG language models that couple medical corpus pretraining with\nadvanced prompt engineering [112]. The system retrieves relevant medical literature and guidelines to\nproduce contextually grounded recommendations for complex clinical questions. This approach allows\nhealthcare providers to access synthesized medical knowledge and evidence-based insights in real time,\nreducing diagnostic uncertainty and improving decision confidence. Such RAG frameworks exemplify\nthe broader promise of FMs to advance neurological and psychiatric practice by delivering specialized\nexpertise, minimizing diagnostic error, and facilitating evidence-based treatment selection. Integration\nof RAG, domain-specific pretraining, and streamlined human–AI interaction design offers a promising\ndirection for the evolution of AI-augmented clinical decision support.\nComprehensive Neuroscience Benchmark and Assessment: BrainBench provides a stan-\ndardized framework for evaluating language models in neuroscience through an open, CC-BY-licensed\nbenchmark dataset [95]. In a systematic approach, this strategy presents standardized tests to probe\nthe capabilities of AI models on a wide range of neuroscience tasks and enables rigorous comparison of\ndiverging modeling methods while indicating areas where current AI systems excel or require improve-\nment. An open license-based focus of the framework ensures reproduction and broad accessibility to\nthe research community.\nAutomated Construction and Expansion of Knowledge Graphs: Automated knowledge\nextraction tools have revolutionized neuroscience knowledge utilization and organization. ExKG-LLM\ndemonstrates the application of LLMs to the automatic expansion of cognitive neuroscience knowledge\ngraphs (CNKG) [98]. By combining advanced information extraction, probabilistic link prediction, and\nprompt-engineering-based integration, the framework systematically identifies and incorporates entities\nand relations from extensive neuroscience literature. This automated process enhances the accuracy,\ncompleteness, and structural richness of cognitive neuroscience knowledge graphs, facilitating deeper\nscientific discovery and reasoning in the field. Expanding upon such foundational capabilities, PEIRCE\nunifies material and formal inference through an iterative conjecture–criticism process [113].\nThis\nneuro-symbolic framework provides a conceptual foundation for interpretable reasoning in neuroscience,\nwhere empirical data and theoretical hypotheses can be jointly evaluated.\nClinical Prediction and Mental Health Modeling: In addition to Beyond knowledge ex-\ntraction, large-scale AI models are increasingly applied to clinical prediction. PKG-LLM exemplifies\nthis integration by combining cognitive neuroscience knowledge graphs with LLMs to predict and\ndifferentiate Generalized Anxiety Disorder (GAD) and Major Depressive Disorder (MDD) [105]. By\nmerging structured neurobiological ontologies with unstructured clinical text through GPT-4–based\nentity extraction and expert validation, the model enhances diagnostic precision and interpretability,\n17\nunderscoring the potential of AI-driven systems to advance early detection and intervention in mental\nhealth.\nPrediction of Human Behavior and Cognitive Modeling: In cognitive modeling, Centaur\nproposes a FM approach for the prediction of human behavior through training on the Psych-101\ndataset with the use of supervised learning and fine-tuning methods [52]. The model extends the\nunderstanding of cognitive processes of humans through learning the ability to predict behavioral\noutcomes under a wide range of psychological tasks and scenarios. The system’s skill at modeling\npatterns of human decision-making offers substantial contributions towards cognitive mechanisms and\nprovides valuable use cases towards the creation of humanity-focused AI systems.\nIntegration of Neurosymbolic and Feature Extraction: Neuro-Symbolic GPT (NeSyGPT)\ncontributes towards neurosymbolic AI by further tuning vision–language FMs to automatically extract\nsymbolic features from raw neuroimaging data [50]. This enables the efficient integration of low-level\nneural metrics with high-level cognitive constructs through the automatic generation of interpretable\nsymbolic representations.\nBy combining the capabilities of base models with symbolic reasoning,\nNeSyGPT yields more understandable and interpretable AI systems for neuroscience applications.\nSuch cognitive modeling and knowledge-driven AI techniques demonstrate the capacity of FMs to\nenrich computational neuroscience through automated knowledge extraction, behavioral prediction,\nand unified neurosymbolic frameworks.\nThrough broad pre-training, task-specific fine-tuning, and\nprompt engineering, these systems can advance our understanding of brain function and cognitive\nprocesses while supporting clinically relevant assessment and intervention tasks.\n3.5\nDisease-Specific Applications\nDisease-specific neuroscience investigations have increasingly utilized large-scale AI models with no-\ntable promise for brain disorder diagnosis and treatment. These models play dual functions as primary\ncomputational engines of diagnostic and treatment tasks and as auxiliary components of broader clin-\nical systems. Current assessments offer illustrative support of this promise with investigations such as\nLuo et al.’s systematic assessment of ChatGPT applied to epilepsy presurgical decision-making, offer-\ning insight that complex seizure semiology could be accurately interpreted with LLMs as a component\nof critical clinical decision-making [114]. Utilization has involved direct utilization of pre-trained mod-\nels for classifying diseases, fine-tuning condition-specialized architecture for condition-specific tasks,\nand incorporating language models into clinical decision support systems with enhanced diagnostic\naccuracy and treatment planning.\nClinical usefulness goes beyond general diagnostic purposes, as evidenced by extensive neurological\ntests where ChatGPT-4 obtained passing scores on neurology-specialized tests with no neurology-\nspecialized training and exceeded human means in cognitive exercises [115, 116].\nLLMs were also\ndemonstrated to outperform neuroscience experiment outcome predictions made by human experts\nusing the BrainBench benchmark, indicating scientific discovery and hypothesis generation capabili-\nties [95]. Large-scale AI model versatility allows one to use wide knowledge bases while flexibly tuning\ntowards particular disease domain needs and produce accurate, efficient, and clinically appropriate AI-\nenabled healthcare solutions with transformative promise for enhanced diagnostic precision, treatment\nindividualization, and patient outcomes in neurological and psychiatric disorders.\n3.5.1\nNeurodegenerative Disorders\nNeurodegenerative conditions pose special challenges to AI-augmented diagnosis and treatment be-\ncause of their continuous nature and intricate pathophysiology. Large AI systems have demonstrated\nexcellent strides in the treatment of AD dementia, Parkinson’s disease, and multiple sclerosis through\nspecially crafted architecture and domain-specific training methods that reflect biomarkers and course\npatterns specific to the disease.\nWhile AI methods have achieved remarkable performance in discrete classification tasks such as\nearly detection, diagnosis (e.g., AD vs. non-AD), and predicting conversion to dementia [117, 118, 119,\n120], their application to continuous and fine-grained prognostic measures remains limited. Predicting\ngradual trajectories of neurological and cognitive decline demands models capable of capturing subtle\ntemporal and physiological variations, rather than merely performing binary classification. Current ap-\nproaches have shown only modest success in this regard—for instance, models trained on neuroimaging\n18\ndata often struggle to accurately estimate cognitive scores over time [121]. Similarly, transfer learn-\ning based on a pretrained ResNet50 model achieved reliable conversion prediction but exhibited weak\ncorrelations with actual cognitive decline [122]. Addressing these challenges is crucial for advancing\nAI’s role in clinical settings and for deepening our understanding of fundamental questions in the\nneuroscience of dementia [123], such as how neural signal features map onto higher-order cognitive\nprocesses.\nADAgent is an LLM-driven conversational system for Alzheimer’s Disease support, using multi-turn\ninteraction to coordinate multimodal diagnostic tools and reasoning-driven clinical assessment [93]. It\nadapts its guidance to cognitive impairment levels and facilitates early-stage monitoring and decision\nsupport. Supporting this direction, DECT demonstrates advanced diagnostic capability for speech-\nbased Alzheimer’s Disease detection by leveraging LLM-assisted fine-grained linguistic representation\nand label-switched / label-preserved data generation to improve transformer-based classification per-\nformance [48].\nAlso of notable relevance to Alzheimer’s research is the Synthesizing Human Activity Datasets\nEmbedded with AD Features (SHADE-AD) framework, which generates AD-specific human activity\ndatasets through a three-stage LLM-assisted training pipeline that embeds subtle motor and behav-\nioral signatures of the disease [46]. Rather than integrating clinical biomarkers, SHADE-AD focuses on\nsynthesizing realistic AD-characteristic body movement patterns, and uses joint-level motion metrics\nto ensure fidelity, enabling downstream models to achieve improved recognition performance in smart-\nhealth AD monitoring scenarios. Providing broader foundations for the research community, findings\nof Gao et al. cover an extensive analysis of LLM applications at neurodegenerative disorders with\nstandardized benchmarking and assessment frameworks of AI-based systems at this spectrum of disor-\nders while overcoming disparities such as heterogeneity of data, clinical interpretability, and regulatory\nissues [124]. In multiple sclerosis applications, MsLesionLLM targets lesion detection and course of\nthe disease monitoring with language model techniques that combine natural language processing of\nclinical notes and computational interpretation of MRI findings with an aim of monitoring the course of\nthe disease and treatment response longitudinally [100]. The system takes advantage of domain-specific\nterminologies and clinical patterns of reasoning with an aim of providing an appropriate measure of\nlesion burden and course of disability metrics.\n3.5.2\nAcquired Neurological Disorders\nAcquired neurologic disorders that arise from external trauma, vascular occurrences, or pathologic\nprocesses demand advanced diagnostic methods and timely treatment mechanisms and are thus prime\ncandidates for AI-enhanced clinical support systems. Such neurologic disorders also present intricate\npatterns of symptom presentations susceptible to high-scale AI systems with sophisticated pattern\nrecognition features and real-time analytics, and with heterogeneous clinical databases trained.\nAs a complex and heterogeneous condition requiring accurate and timely diagnosis and manage-\nment, involving vast data across multiple modalities (i.e., clinical, EEG, video, neuroimaging, ge-\nnomics), epilepsy is a perfect use case for large-scale AI models across the entire clinical pathway—from\ndetection and diagnosis to localization, risk prediction, and personalized treatment optimization.\nVideo-based seizure detection presents an important avenue for improved epilepsy diagnosis and\nclassification. SETR-PKD is a novel framework that utilises a transformer-based progressive knowledge\ndistillation and encodes seizure motion semiotics from raw RGB videos by extracting optical flow\nfeatures, thus importantly preserving the privacy of the patient. This model can detect tonic-clonic\nseizures with an accuracy of 83.9% in a privacy-preserving manner [43]. VSViG is another novel video-\nbased seizure detection model that applies transformer architecture in the form of a skeleton-based\nspatiotemporal Vision Graph, outperforming previous state-of-the-art approaches in detecting focal\nor generalised tonic-clonic seizures, and importantly, doing so while utilising fewer parameters and\ncomputing power [44]. Given that traditional EEG approaches to seizure detection and classification\nare resource-intensive and mostly non-portable, these approaches pave the way for increased methods\nof seizure detection, including in an ambulatory setting.\nBeyond detection, accurate seizure classification is critical for treatment planning. EpilepsyLLM\nrepresents a special-purpose system for seizure diagnosis and treatment, combining intricate seizure\nsemiology description analysis with patient demographics and EEG pattern classification to yield\nseizure type prediction and treatment protocol optimization [125]. It utilizes sophisticated natural\nlanguage processing of clinical narrative texts and textual description-electrophysiology correspondence\n19\nclassification towards complete characterization of epilepsy.\nGoing one step further in epilepsy management, EpiSemoLLM offers a fine-tuned dedicated lan-\nguage model applying seizure semiology description analysis alongside demographic attributes as an\naid towards epileptogenic zone (EZ) prediction [97]. Having undergone training with extensive datasets\nretrieved from PubMed publications and individual electronic health records cohorts, the model shows\nan advanced level of seizure semiology terminology knowledge and the ability to associate intricate\nsymptom descriptions with anatomical localisation. In this manner, more specific seizure origin iden-\ntification becomes possible with valuable input towards surgical planning and medication optimization\nduring treatment of medication-resistant seizures, where a successful treatment outcome entails proper\nEZ localization.\nWhile these models address diagnosed epilepsy, FMs also enable risk prediction in at-risk popu-\nlations. Vision FMs have applications in predicting epilepsy development. Gao et al have developed\na large self-supervised vision FM to assess the occurrence of epilepsy after encephalitis. Trained on\nover 57,000 whole-brain MRI scans, the model outperformed older, task-specific, pre-trained models\nin predicting the occurrence of post-encephalitic epilepsy, with the affected anatomical brain regions\nshown to be of importance in informing the model’s decisions in this classification task [104]. Such\nprediction models allow for appropriate risk-stratification, earlier intervention in high-risk individuals,\nand in turn, better treatment outcomes for these patients.\nFollowing diagnosis and risk stratification, treatment selection remains a critical challenge. An-\ntiseizure medications (ASMs) successfully treat patients diagnosed with epilepsy.\nSelecting which\nmedication is best for a patient, however, often requires a trial-and-error approach, and approximately\na third of patients fail to respond to these medications. Deep learning models that can predict indi-\nvidual response to a drug are being developed and evaluated in the clinical setting [126]. Models that\nreason over clinical and genetic patient information [127, 128, 129] can improve performance over those\ntrained on clinical characteristics alone [130]. More extensive multimodality models that include, for\nexample, EEG and MRI will further enable personalised care in epilepsy.\nStroke treatment is another key application case, where Song et al.\ndiscuss extensive analysis\nframeworks with LLMs for expedited assessment and treatment planning and combining clinical pre-\nsentations with neuroimaging results to aid time-critical decision-making during acute stroke treat-\nment [131].\nThe system merges multimodal data integration with clinical reasoning functions to\nforecast stroke outcomes and tailor intervention methods. In a more expansive context, Kottlors et\nal. discuss LLM deployments across a wide range of neurological disorders and shed light on general-\npurpose AI systems adaptable across differing neurological presentations yet with high accuracy and\nclinical relevance [132].\n3.5.3\nBrain Tumors and Structural Abnormalities\nSurgery planning and brain tumor diagnosis are prime use areas where AI accuracy has a dispropor-\ntionate impact on patient outcomes through improved diagnostic accuracy, optimized treatment, and\nsurgical navigation. Large-scale AI systems capitalize on state-of-the-art neuroimaging analysis, high-\naccuracy surgical planning, and computerized clinical reporting systems that streamline the efficiency\nof radiological workflow.\nMultimodal FM methods of brain tumor classification are enhanced by Manjunath et al., who com-\nbine high-resolution neuroimaging features with clinical features, genomic features, and histopatho-\nlogical features to allow for complete tumor description and prediction of prognosis [133]. The model\nutilizes high-level feature fusion methods and attention mechanisms to identify intricate associations\namong imaging phenotypes and molecular subtypes. Moreover, new architecture designs are proposed\nwith HybridTransNet that integrate convolutional neural network feature extraction with attention\nmechanisms from the transformer architecture with the aim of obtaining accurate tumor boundary\ndelineation and volumetric analysis of several MRI sequences [47]. The model also utilizes uncertainty\nquantification and yields confidence scores for quality assessment of the segmentation. Besides, neu-\nrosurgical conversational AI systems are created using BrainGPT, where LLMs are trained from a\nfine-tuning of neurosurgical texts and clinical guidelines to issue evidence-based guidelines for sur-\ngical methods, risk evaluation, and postoperative treatment planning [96].\nExpanding clinical use\nfurther, Ma et al. illustrate large-scale language model use in cerebral tumor treatment and treat-\nment planning with an emphasis on automated clinical guideline synthesis, optimization of treatment\nprotocols, and individualized therapy advice with tumor and patient variables [134]. The reporting sys-\n20\ntems are automated under iReportMed through the use of natural language generation capability that\nproduces well-formatted radiology reports with important findings highlighted, differential diagnosis\nstated with pertinent follow-up recommendations while maintaining clinical accuracy and regulatory\ncompliance [99].\n3.5.4\nMajor Psychiatric Disorders\nUnderstanding psychiatric disorders takes more than just measuring biomarkers, as it requires a careful\nlook at how people behave, the symptoms they show, and how they respond to treatment. In this\nspace, large-scale AI models are beginning to help by using natural language processing, behavioral\ndata from different sources, and long-term health records to make diagnoses more accurate and guide\nmore personalized treatment plans.\nTargeting major depressive disorder specifically, MDD-LLM employs fine-tuned large language\nmodels trained on large-scale real-world tabular health data from the UK Biobank, where structured\nfeatures are converted into natural-language prompts for supervised diagnosis [101].\nRather than\nsentiment modeling or conversational transcripts, the system leverages feature-driven risk estimation,\nprobability-based outputs, and explainable reasoning to enhance diagnostic accuracy and clinical in-\nterpretability.\nVS-LLM (Visual-Semantic LLM) frames depression assessment through Drawing Projection Test\n(DPT) data such as PPAT sketches, converting visual stroke and layout patterns into psychological\nsemantic captions via tailored LLM prompts [?]. The system fuses visual-perception and semantic\ngeneration modules to output a depression vs.\nnon-depression classification, and shows a 17.6 %\nperformance gain compared to traditional psychologist assessments. Unlike methods based on lan-\nguage or sentiment, VS-LLM highlights how multimodal sketch-to-semantics translation can enrich\npsychological evaluation.\nComprehensive mental health support is explored through Mental-LLM, which evaluates and\ninstruction-tunes large language models for multi-task mental health prediction using online textual\ndata rather than therapeutic intervention [49]. Advancing assessment capabilities, ProMind-LLM ex-\ntends this direction by integrating subjective mental records with objective behavioral sensor data,\nleveraging domain-specific pretraining and causal chain-of-thought reasoning to enable more robust\nmental health risk assessment [107].\n3.5.5\nAnxiety and Trauma-Related Disorders\nAnxiety and trauma disorders demand differentiated methods of approach with a consideration of\nthe intricate interplay of psychological symptoms, physical responses, and behavioral presentations.\nScalable AI frameworks emphasize early identification with behavior indicators, extensive risk profiling,\nand adaptive systems of therapeutic support with changing presentations of symptoms.\nTRauma UNderstanding and STructured Assessments (TRUST) introduces an LLM-based diag-\nnostic dialogue system that conducts formal structured PTSD interviews aligned with DSM-5 CAPS-5\ncriteria, enabling automated clinical assessment rather than therapeutic intervention [110]. Comple-\nmenting this assessment focus, recent work on automatic PTSD diagnosis leverages LLM-generated\ntext augmentation from clinical transcripts to address data imbalance and improve classification per-\nformance, demonstrating the role of LLMs in enhancing diagnostic robustness in low-resource trauma\nsettings [135].\n3.5.6\nNeurodevelopmental Disorders\nNeurodevelopmental disorders such as autism and ADHD are best addressed when they are detected\nearly and treated with approaches tailored to each child’s unique developmental path. Large-scale AI\nmodels are now helping by analyzing patterns in behavior, assessing social interactions, and creating\nadaptive support systems that grow and adjust as children’s needs change over time.\nComprehensive assessment approaches in neurodevelopmental disorders are demonstrated by Kulka-\nrni et al., who leverage LLMs to analyze video-based behavioral signals for early ADHD detection, using\nmultimodal behavioral pattern recognition to support objective screening rather than downstream in-\ntervention [136]. Assessment of autistic social reciprocity is further advanced by SocialRecNet, which\n21\nutilizes multimodal alignment of speech and text with LLM-based reasoning to estimate autism diag-\nnostic observation schedule social reciprocity scores for diagnostic support [109].\nSpecialized communication support is provided through ASD-Chat, which offers an LLM-driven\nintervention system grounded in verbal behavior milestones assessment and placement program, in-\ncorporating structured conversational paradigms, individualized preference adaptation, and clinically\naligned turn-taking protocols to facilitate social communication skill development [94].\n4\nPublic Data Cohorts for Computational Neuroscience\nThe rapid growth of computational neuroscience research has been aided by the existence of large, di-\nverse, and high-quality public datasets from numerous neuroimaging modalities and clinical contexts.\nThese data types encompass a wide range of techniques that include high-temporal-resolution elec-\ntrophysiological recording with precision in the millisecond range, to high-spatial-resolution structural\nand functional neuroimaging data relevant for detailed anatomical and functional brain organization.\nThe dataset scales are considerable, varying from in-depth studies of dozens of subjects to population-\nbased studies with tens of thousands of subjects, for example, collectively amounting to terabytes of\nneural data for robust statistical analyses and generalizable computational models.\nThe clinical utility of these datasets reaches across the full spectrum of neurological and psychi-\natric disorders, including neurodegenerative diseases, stroke, epilepsy, autism spectrum disorders, and\nmental health disorders, while being a resource for basic research on cognitive neuroscience, brain-\ncomputer interfaces, and neural decoding.\nNotably, multimodal datasets have emerged to employ\nmultiple recording techniques within the same subjects, promoting the complementary strengths of\nthose modalities.\nElectrophysiological methods offer high temporal resolution, while neuroimaging\nmethods offer strong spatial resolution. These extensive multimodal datasets, along with standardized\ndata formats and open-access longitudinal datasets, have democratized neuroscience research and aided\nin the creation of innovative computational methods to better understand the brain and its functional\nimpairment.\nTable 5: Comprehensive Overview of Public Datasets for Computational Neuroscience Research\nDataset\nModality\nSubjects\nData Size\nApplication Domain\nEEG Datasets\nTUH EEG Corpus [137]\nEEG\n14,987+\n1.7TB / 21,787h\nGeneral EEG analysis\nTUAB [138]\nEEG\n2,329\n1,139h\nAbnormality detection\nTUAR [139]\nEEG\n213\n83.7h\nArtifact detection\nTUSL [137]\nEEG\n38\n27.5h\nSlowing events classification\nTUEV [137]\nEEG\nVarious\n–\nClinical evaluation\nSiena Scalp EEG [140]\nEEG\n14\n141h\nScalp EEG analysis\nSEED-IV [141]\nEEG\n15\n–\nEmotion recognition\nSEED-V [142]\nEEG\n15\n32.7h\nEmotion recognition\nDEAP [143]\nEEG\n32\n–\nEmotion recognition\nFACED [144]\nEEG\nVarious\n–\nFacial emotion analysis\nCHB-MIT [145]\nEEG\nPediatric\n–\nPediatric epilepsy\nMIBCI [146]\nEEG\nVarious\n–\nMotor imagery BCI\nBCIC4-1 [147]\nEEG\n7\n–\nMotor imagery BCI\nSleep-EDF [148]\nEEG\nVarious\n–\nSleep staging\nEEGMat [149]\nEEG\nVarious\n–\nMaterial recognition\nSTEW [150]\nEEG\nVarious\n–\nSpecialized tasks\nGo-Nogo [151]\nEEG\n14\n–\nVisual categorization\nMusicEEG [152]\nEEG\n31\n–\nTemporal dynamics\nHBN EO/EC [153]\nEEG\n2,952\n–\nResting state (pediatric)\nHBN-EEG [153]\nEEG\n1,897\n–\nMultiple tasks (pediatric)\nFeatures-EEG [154]\nEEG\n16\n–\nVisual feature processing\nHFO [155]\nEEG\n30\n–\nPediatric epilepsy\nPEARL-Neuro [156]\nEEG\n79\n–\nCognitive tasks\nRestCog [157]\nEEG\n60\n–\nResting state cognition\nAwakening [158]\nEEG\n21\n–\nSedation studies\nAD-Auditory [159]\nEEG\nVarious\n–\nAlzheimer’s auditory\nADFSU [160]\nEEG\nVarious\n–\nAlzheimer’s studies\nADFTD [161]\nEEG\nVarious\n–\nFrontotemporal dementia\nADSZ [162] [163]\nEEG\nVarious\n–\nAlzheimer’s seizure\nAPAVA [164][165]\nEEG\nVarious\n–\nAphasia assessment\nBrainLat [166]\nEEG\nVarious\n–\nLatin brain studies\nCognision-ERP [167]\nEEG\nVarious\n–\nEvent-related potentials\nCognision-rsEEG [168]\nEEG\nVarious\n–\nResting state EEG\nCNBPM [169]\nEEG\nVarious\n–\nBrain pathology\nMEG Datasets\nMEG-MASC [170]\nMEG\n27\n2h/subject\nNaturalistic stories\nMEG-Narrative [171]\nMEG\n3\n10h/subject\nNaturalistic stories\nOMEGA [172]\nMEG\n644\n–\nResting state + pathology\nCC700 [173]\nMEG\n700\n–\nMultiple cognitive tasks\nAversiveMEG [174]\nMEG\n28\n–\nAversive learning\nMIND [175]\nMEG\n8\n–\nSomatosensory stimulation\nSMN4Lang [176]\nMEG\n12\n6h/subject\nLanguage comprehension\nTHINGS-MEG [177]\nMEG\n4\n–\nObject recognition\nASWR-MEG [178]\nMEG\n24\n–\nWord sequence processing\nImageLine [179]\nMEG\n30\n–\nVisual object processing\nNeuroMorph [180]\nMEG\n24\n–\nLexical decision tasks\nKymata-SOTO [181]\nMEG/EEG\n35\n–\nMulti-language conversations\nMRI Datasets\nHCP [182]\nMRI/MEG\n700+\n–\nMulti-modal brain imaging\nUK Biobank [183]\nMRI/fMRI\n61,038+\n82,800 images\nPopulation neuroimaging\nContinued on next page\n22\nTable 5: – continued from previous page\nDataset\nModality\nSubjects\nData Size\nApplication Domain\nADNI [184]\nMRI/PET\nVarious\n–\nAlzheimer’s disease\nOASIS-2 [185]\nMRI\nVarious\n–\nCross-sectional aging\nOASIS-3 [186]\nMRI\nVarious\n–\nLongitudinal aging\nBraTS-2015 [187]\nMRI\nVarious\n–\nBrain tumor segmentation\nBraTS-2019 [188]\nMRI\nVarious\n–\nBrain tumor segmentation\nBraTS-2021 [189]\nMRI\nVarious\n5,004+ images\nBrain tumor segmentation\nATLAS [190]\nMRI\nVarious\n655 images\nStroke lesion analysis\nABIDE I [191]\nMRI\nVarious\n–\nAutism spectrum disorder\nABIDE II [192]\nMRI\nVarious\n–\nAutism spectrum disorder\nPPMI [193]\nMRI/DaTscan\nVarious\n–\nParkinson’s disease\nMCSA [194]\nMRI\nVarious\n–\nCognitive aging\nSOOP [195]\nMRI\nVarious\n–\nBrain development\nCBTN LGG [196]\nMRI\nVarious\n–\nPediatric brain tumors\nMIRIAD [197]\nMRI\nVarious\n–\nDementia research\nDLBS [198]\nMRI\nVarious\n–\nBrain development\nUCSF-PDGM [199]\nMRI\nVarious\n–\nPediatric brain tumors\nQIN-GBM [200]\nMRI\nVarious\n–\nGlioblastoma research\nUPENN-GBM [201]\nMRI\nVarious\n–\nGlioblastoma research\nDFCI/BCH LGG [77]\nMRI\nVarious\n–\nLow-grade glioma\nDFCI/BCH HGG [77]\nMRI\nVarious\n–\nHigh-grade glioma\nRIDER [202]\nMRI\nVarious\n–\nImaging biomarkers\nwu1200 [182]\nMRI\nVarious\n–\nInstitutional dataset\nLONG579 [203]\nMRI\nVarious\n–\nLongitudinal studies\nBABY [204]\nMRI\nVarious\n–\nInfant brain development\nAOMIC [205]\nMRI\nVarious\n–\nOpen MRI collection\nCalgary [206]\nMRI\nVarious\n–\nCanadian brain imaging\nHaN [207]\nMRI\nVarious\n–\nHealthy aging\nNIMH [208]\nMRI\nVarious\n–\nMental health research\nICBM [209]\nMRI\nVarious\n–\nBrain mapping consortium\nIXI [210]\nMRI\nVarious\n–\nInformation extraction\nPING [211]\nMRI\nVarious\n–\nPediatric imaging\nPixar [212]\nMRI\nVarious\n–\nSpecialized collection\nSALD [213]\nMRI\nVarious\n–\nAging and dementia\nRadArt [214]\nMRI\nVarious\n–\nRadiological artifacts\nRhineland Study [215]\nMRI\nVarious\n–\nPopulation study\nSchizConnect [216]\nMRI\nVarious\n–\nSchizophrenia research\nOpenBHB [217]\nMRI\nVarious\n–\nOpen brain imaging\nMSSEG [218]\nMRI\nVarious\n–\nMultiple sclerosis\nISLES2022 [219]\nMRI\nVarious\n–\nStroke lesion segmentation\nWMH2017 [220]\nMRI\nVarious\n–\nWhite matter hyperintensities\nMSLesSeg [221]\nMRI\n75\n115\nMultiple\nSclerosis\nLesion\nSeg-\nmentation\nVisual Datasets\nNatural Scenes [222]\nfMRI\n8\n–\nVisual scene processing\nBOLD5000 [223]\nfMRI\nVarious\n136,000 segments/340h\nVisual object recognition\nGOD [224]\nfMRI\nVarious\n–\nObject recognition\nMS-COCO [225]\nImages\n–\n–\nObject recognition (stimuli)\nFlick30k [226]\nImages\n–\n–\nImage-caption pairs\nConceptualCaptions12M [227]\nMulti-modal\n–\n–\nVision-text\nConceptualCaptions3M [228]\nMulti-modal\n–\n–\nVision-text\nSBU [229]\nMulti-modal\n–\n–\nVision-text\nVG [230]\nMulti-modal\n–\n–\nVision-text\nVisual stimuli [231]\nfMRI\n594\n53 time-steps\nObject recognition\nClinical and Disease-Specific Datasets\nPETfrog [232]\nPET\nVarious\n–\nPET imaging\nADReSSo [233]\nAudio/Clinical\nVarious\n–\nAlzheimer’s speech analysis\nNCCT [234]\nCT\nVarious\n–\nNeuroimaging\nDreaddit [235]\nText\nVarious\n–\nDepression detection\nCSSRS-Suicide [236]\nClinical\nVarious\n–\nSuicide risk assessment\nPPAT [51]\nVarious\nVarious\n–\nNeuropsychological assessment\nDepSeverity [237]\nClinical\nVarious\n–\nDepression severity\nSDCNL [238]\nClinical\nVarious\n–\nDepression analysis\nPMData [239]\nClinical\nVarious\n–\nMental health data\nGlobem [240]\nClinical\nVarious\n–\nGlobal mental health\nE-DAIC [241]\nAudio/Clinical\nVarious\n–\nDepression interview\nADOS [242]\nClinical\nVarious\n–\nAutism diagnostic\nCQ500\n[243]\nCT\n500\n491\nhead trauma or stroke analysis\nPsych-101 [244]\nPsychological\nVarious\n–\nPsychology dataset\nClinicalQA [112]\nClinical\nVarious\n–\nClinical Q&A\nMultimodal MCI [245]\nCognitive\nVarious\n–\nCognitive assessment\nSpecialized Research Datasets\nPOYO-DANDI [246]\nNeural spikes\n9 primates\n78 sessions\nMotor/somatosensory cortex\nKnowledge, Literature Resources and Other Databases\nPubMed\nLiterature\n–\nAbstracts\nNeuroscience literature\nWikipedia\nEncyclopedia\n–\n–\nGeneral knowledge\nNeuroLex [247]\nKnowledge\n–\n–\nNeuroscience ontology\nNeuroMorpho [248]\nMorphology\n–\n–\nNeuronal morphology\nOpenNeuro [249]\nMulti-modal\nneu-\nroimaging data\n–\n–\nComprehensive neuroimaging re-\nsearch\nNeurosurgical Atlas [250]\nLiterature\n–\n–\nNeurosurgical atlas\nNLI4CT [251]\nClinical text\n–\n–\nClinical trial inference\nMerck Manual [252]\nMedical\n–\n–\nMedical reference\nNeurology textbooks\nLiterature\n–\n–\nEducational resources\nWe have compiled major datasets from the neuroscience literature over the past two decades, as\nshown in Table 5. These public datasets vary widely in scope and scale, and have significantly advanced\ncomputational neuroscience research by providing access to high-quality neural data across different\nrecording methods, subject populations, and experimental designs.\n5\nFuture Development and Challenges\nLarge-scale AI models are increasingly adopted in neuroscience research and clinical practice due to\ntheir exceptional capabilities in pattern recognition and data analysis. However, their implementation\nfaces substantial technical and methodological challenges that currently limit their broader adoption.\n23\n5.1\nGeneralization, Multimodal Integration, and Model Scalability\nArguably, the biggest challenge lies with obtaining good generalization across a wide variety of studies\nand patient populations while being computationally efficient. The underlying challenge lies with the\nintrinsic variability of the neural data from populations, age spans, and pathological conditions, and\nthus requires sophisticated domain adaptation techniques that are able to retain performance while\npermitting biological variability [34, 53]. This discrepancy largely explains why current approaches\ndemonstrate strong performance in controlled laboratory experiments but often fail to generalize to\nreal-world clinical environments characterized by variable data quality, heterogeneous acquisition pro-\ntocols, and diverse patient populations.\nThe issue of generalization becomes further complicated with the advent of multimodal data. In-\ncorporation of these data involves innovations of temporal and spatial alignment at varying scales of\nmeasurement that cannot efficiently be handled with data concatenation. It entails both technical\nissues of cross-modal representation learning and theory issues of how varying neural modalities of\nmeasurement correspond with underlying brain function [91, 92]. A critical challenge is preserving the\nunique information content of each modality while enabling integrative fusion that enhances rather\nthan compromises predictive performance.\nBoth the generalization and the multimodal integration challenges are further complicated by com-\nputational scalability limitations. With greater computational needs for training sophisticated models\ncome limitations that could cause research capacities to concentrate in well-funded institutions, inhibit-\ning research diversity and practicality [8, 16]. Overcoming this necessitates architecture innovations\nwhere efficiency takes priority alongside performance, creating methods that may broaden accessibility\nand usability while still maintaining performance. Both algorithmic advances and dedicated hardware\narchitecture designs optimized for neuroscience workloads are involved.\n5.2\nClinical Translation and Interpretability\nFrom research to practice, translation entails managing interpretability at a wide variety of levels, span-\nning individual-level predictions through to the macro-level characterization of system-wide behavior.\nIt entails beyond model-level decision explanation and reaching AI systems that can reason clinically\nand align with current medical knowledge and practice guidelines [36, 90]. It also requires building\na transparent architecture intrinsically and not with post-hoc techniques of explanation that cannot\nalways accurately represent model behavior in practice. Explainable Artificial Intelligence (XAI) refers\nto AI systems designed to make their decision-making processes transparent, interpretable, and un-\nderstandable to humans [253]. Goal-driven explainability processes in healthcare focus on enabling AI\nsystems, particularly AI agents and autonomous decision-makers, to generate justifications for their\nactions based on explicit goals, intentions, beliefs, or plans [254]. Unlike purely data-driven methods\nthat are often used in a post-hoc manner, goal-driven approaches, aided by the natural language gen-\neration capacity of LLMs, allow such systems to provide context-sensitive, human-like rationales for\ntheir reasoning, plans, and actions. This form of explanation aligns with clinical reasoning patterns\nand facilitates human-AI collaboration in decision-making and is thus a major focus of healthcare AI\nsystems moving forward [255].\nApart from interpretability, clinical trust also involves technical correctness as well as the proper\nhuman-AI collaboration at health clinics. Creating successful clinical AI systems entails learning how\nclinicians incorporate AI feedback into their judgments, conveying uncertainty, and ensuring proper\nhuman supervision while employing AI abilities [40, 103]. AI systems also need to integrate within\nclinical workflows by supplying relevant and timely information supporting current practices and not\ndisturbing current practices. Establishing clinical trust also relies on robust regulatory frameworks.\nRegulatory challenges in neuroscience AI arise from the complexity of brain-based applications, where\nincorrect decisions carry high stakes and model mechanisms are often poorly understood. Developing\nappropriate regulatory frameworks requires balancing innovation with patient safety while addressing\nthe unique characteristics of brain data and neurological conditions [35, 77]. Regulatory frameworks\nshould balance rapid AI advancement with safety standards through clear validation guidelines, ap-\npropriate outcome measures, and continuous model monitoring. Successful implementation requires\ntraining healthcare professionals, building deployment infrastructure, and creating interdisciplinary\nteams to bridge technical and clinical expertise.\n24\n5.3\nData Governance and Ethical Principles\nLarge-scale neural modeling also has similar problems of fundamental data quality and standardiza-\ntion, and ethical issues that are intimately tied together. Standardization of neural data acquisition,\npreprocessing, and annotation remains inconsistent across research groups and clinical sites, posing\na significant bottleneck to the development of truly generalizable models. Although techniques like\nBrainGFM’s multi-atlas training demonstrate promising results of standardized data aggregation, es-\ntablishing full data quality frameworks requires much repeated effort [45]. These data quality and\nstandardization issues are further complicated in the neuroscience example with problems of privacy\nand security.\nThese privacy issues are especially heightened as brain data discloses sensitive information re-\ngarding individuals’ mental status, cognitive capacities, and neurological status. Overcoming these\nissues involves giving high priority to privacy-preserving methods like differential privacy, federated\nlearning, and secure multiparty computation in order to allow for extensive model training at a large\nscale while maintaining patient confidentiality. For instance, federated learning methods hold special\npromise through the use of distributed datasets while at the same time ensuring data privacy and\ninstitutional autonomy with a single solution tackling both issues of standardization and privacy [95].\nPatient consent and privacy are of critical importance, particularly for genomic data, which carries\nthe risk of re-identification [256]. The sharing of these data may be prohibited by ethics frameworks\nand data-sharing laws. A federated learning approach, whereby raw training data remains distributed\nacross individual institutions, is one solution [257]. Participating sites train a common model using\ntheir data, and the encrypted outputs are transmitted to a common server for aggregation, and the\noutputs of the aggregated analysis are returned to each participating site. In addition, the creation of\nrobust ethical guidelines specifically crafted with neuroscience AI applications will also become critical\nfor ensuring public confidence and accountable innovation. These guidelines should incorporate issues\nof algorithmic bias, AI-assisted diagnosis, informed consent, and adequate utilization of synthetic neu-\nral data generation abilities. Importantly, such models should neither intensify current disparities in\nhealthcare nor become exclusive and non-representative of global populations. Owing to the aforemen-\ntioned issues, international standards of neuroscience AI creation demand international collaboration\namong researchers, clinicians, ethicists, and regulatory committees [258].\n5.4\nEmerging Frontiers and Future Opportunities\nThe intersection of neural recording technologies’ advancements with computational techniques and\nAI systems opens up new avenues of examining brain function and neuro disease treatment. Real-time\nadaptive BCI driven by such models holds promise as a treatment option for paralysis, depression,\nand other neurodisorders [3].\nBrain-inspired AI systems drawing from biological neural principles\nmay further improve computational efficacy and model understandability [259], while neuroscience\nresearch-related LLMs offer new frameworks for explaining language processing and cognitive mech-\nanisms [52]. Synthetic advances in generating neural data resolve core research challenges, especially\ndata availability in rare neurological disorders and privacy restrictions in clinical datasets [3, 260].\nThese synthetic datasets allow for rigorous model testing across heterogeneous populations and facili-\ntate hypothesis-driven research in formerly data-constrained areas. The data augmentation capacities\nare crucial towards establishing generalizable models able to function correctly across a variety of\npatient populations and clinical scenarios. Realization of opportunities, though, requires collabora-\ntive ecosystems beyond boundaries of classical disciplines [26]. International collaborative mechanisms\nof neuroscientists, computer scientists, clinicians, and industrial collaborators are driving transfor-\nmative research of brain sciences and AI and creating innovative methods and ethical data-sharing\npractices towards greater understanding of neural processes [261]. These collaborators specify global\ndata-sharing frameworks to get beyond privacy and ethical challenges while facilitating coordination of\nlarge-scale research at a global level [262]. Specialized training programs are simultaneously cultivating\nresearchers proficient in both neuroscience and AI methodologies, thereby building the human capital\nnecessary to advance the field [263].\n25\n6\nConclusion\nThe development of large-scale AI models in neuroscience is gradually expanding our ability to charac-\nterize brain function and support clinical decision-making. This survey has reviewed the diverse range\nof such models, from neural signal processing architectures used for decoding neural activity to clinical\ndecision support systems that incorporate multimodal biomedical information. These developments\nindicate that large-scale AI models are beginning to be used not only as analytical tools but also as\npractical components within research workflows and early-stage translational settings. The progress\nsummarized here reflects a gradual shift from isolated proof-of-concept demonstrations toward more\nsystematic applications with measurable relevance to both foundational and applied neuroscience.\nAt the same time, several challenges remain unresolved, including robustness across heterogeneous\npopulations, interpretability in realistic clinical contexts, and the ethical and regulatory considerations\nassociated with responsible deployment. Progress in these areas will require sustained collaboration\nacross neuroscience, computer science, clinical medicine, and regulatory policy. While large-scale AI\nmodels hold meaningful potential to support both research and clinical practice, their long-term im-\npact will depend on steady improvements in technical reliability, dataset governance, and translational\noversight. Ongoing work in these directions provides a foundation for further progress without pre-\nsupposing that current advances will directly translate into widespread clinical integration.\nReferences\n[1] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill, et al., “On the opportunities and risks of foundation models,”\narXiv preprint arXiv:2108.07258, 2021.\n[2] R. Wang and Z. S. Chen, “Large-scale foundation models and generative ai for bigdata neuro-\nscience,” Neuroscience Research, vol. 215, pp. 3–14, 2025.\n[3] X. Zhou, C. Liu, Z. Chen, K. Wang, Y. Ding, Z. Jia, and Q. Wen, “Brain foundation models:\nA survey on advancements in neural signal processing and brain discovery,” arXiv preprint\narXiv:2503.00580, 2025.\n[4] F. Lotte, M. Congedo, A. L´ecuyer, F. Lamarche, and B. Arnaldi, “A review of classification\nalgorithms for eeg-based brain–computer interfaces,” Journal of neural engineering, vol. 4, no. 2,\np. R1, 2007.\n[5] E. Y. Wang, P. G. Fahey, Z. Ding, S. Papadopoulos, K. Ponder, M. A. Weis, A. Chang,\nT. Muhammad, S. Patel, Z. Ding, et al., “Foundation model of neural activity predicts response\nto new stimulus types,” Nature, vol. 640, no. 8058, pp. 470–477, 2025.\n[6] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter, K. Eggensperger,\nM. Tangermann, F. Hutter, W. Burgard, and T. Ball, “Deep learning with convolutional neural\nnetworks for eeg decoding and visualization,” Human brain mapping, vol. 38, no. 11, pp. 5391–\n5420, 2017.\n[7] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for unsupervised visual\nrepresentation learning,” in Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 9729–9738, 2020.\n[8] D. Zhang, Z. Yuan, Y. Yang, J. Chen, J. Wang, and Y. Li, “Brant: Foundation model for intracra-\nnial neural signal,” Advances in Neural Information Processing Systems, vol. 36, pp. 26304–26321,\n2023.\n[9] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, “Bendr: Using transformers and a contrastive\nself-supervised learning task to learn from massive amounts of eeg data,” Frontiers in Human\nNeuroscience, vol. 15, p. 653659, 2021.\n[10] Z. Dong, R. Li, Y. Wu, T. T. Nguyen, J. Chong, F. Ji, N. Tong, C. Chen, and J. H. Zhou,\n“Brain-jepa: Brain dynamics foundation model with gradient positioning and spatiotemporal\nmasking,” Advances in Neural Information Processing Systems, vol. 37, pp. 86048–86073, 2024.\n26\n[11] S. Chen, M. Chen, X. Wang, X. Liu, B. Liu, and D. Ming, “Brain–computer interfaces in 2023–\n2024,” Brain-X, vol. 3, no. 1, p. e70024, 2025.\n[12] F. R. Willett, D. T. Avansino, L. R. Hochberg, J. M. Henderson, and K. V. Shenoy, “High-\nperformance brain-to-text communication via handwriting,” Nature, vol. 593, no. 7858, pp. 249–\n254, 2021.\n[13] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al., “Learning transferable visual models from natural language super-\nvision,” in International conference on machine learning, pp. 8748–8763, PmLR, 2021.\n[14] M. Pedersen, K. Verspoor, M. Jenkinson, M. Law, D. F. Abbott, and G. D. Jackson, “Artificial\nintelligence for clinical decision support in neurology,” Brain communications, vol. 2, no. 2,\np. fcaa096, 2020.\n[15] S. Wang, S. Liu, Z. Tan, and X. Wang, “Mindbridge: A cross-subject brain decoding frame-\nwork,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 11333–11342, 2024.\n[16] F. Ozcelik and R. VanRullen, “Natural scene reconstruction from fmri signals using generative\nlatent diffusion,” Scientific Reports, vol. 13, no. 1, p. 15666, 2023.\n[17] N. Soingern, A. Sinsamersuk, I. Chatnuntawech, and C. Silpasuwanchai, “Data augmentation\nfor eeg motor imagery classification using diffusion model,” in International Conference on Data\nScience and Artificial Intelligence, pp. 111–126, Springer, 2023.\n[18] B. D. Simon, K. B. Ozyoruk, D. G. Gelikman, S. A. Harmon, and B. T¨urkbey, “The future\nof multimodal artificial intelligence models for integrating imaging and clinical metadata: A\nnarrative review,” Diagn. Interv. Radiol, 2024.\n[19] S. Lee, Y. Cho, Y. Ji, M. Jeon, A. Kim, B.-J. Ham, and Y. Y. Joo, “Multimodal integration\nof neuroimaging and genetic data for the diagnosis of mood disorders based on computer vision\nmodels,” Journal of Psychiatric Research, vol. 172, pp. 144–155, 2024.\n[20] B. A. Richards, T. P. Lillicrap, P. Beaudoin, Y. Bengio, R. Bogacz, A. Christensen, C. Clopath,\nR. P. Costa, A. de Berker, S. Ganguli, et al., “A deep learning framework for neuroscience,”\nNature neuroscience, vol. 22, no. 11, pp. 1761–1770, 2019.\n[21] S. Cui, D. Lee, and D. Wen, “Toward brain-inspired foundation model for eeg signal processing:\nour opinion,” Frontiers in Neuroscience, vol. 18, p. 1507654, 2024.\n[22] X. Luo, A. Rechardt, G. Sun, K. K. Nejad, F. Y´a˜nez, B. Yilmaz, K. Lee, A. O. Cohen, V. Borgh-\nesani, A. Pashkov, et al., “Large language models surpass human experts in predicting neuro-\nscience results,” Nature human behaviour, vol. 9, no. 2, pp. 305–315, 2025.\n[23] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L.\nJackson, N. Imam, C. Guo, Y. Nakamura, et al., “A million spiking-neuron integrated circuit\nwith a scalable communication network and interface,” Science, vol. 345, no. 6197, pp. 668–673,\n2014.\n[24] M. Davies, N. Srinivasa, T.-H. Lin, G. Chinya, Y. Cao, S. H. Choday, G. Dimou, P. Joshi,\nN. Imam, S. Jain, et al., “Loihi: A neuromorphic manycore processor with on-chip learning,”\nIeee Micro, vol. 38, no. 1, pp. 82–99, 2018.\n[25] K. Roy, A. Jaiswal, and P. Panda, “Towards spike-based machine intelligence with neuromorphic\ncomputing,” Nature, vol. 575, no. 7784, pp. 607–617, 2019.\n[26] A. Zador, S. Escola, B. Richards, B.\n¨Olveczky, Y. Bengio, K. Boahen, M. Botvinick,\nD. Chklovskii, A. Churchland, C. Clopath, et al., “Catalyzing next-generation artificial intel-\nligence through neuroai,” Nature communications, vol. 14, no. 1, p. 1597, 2023.\n27\n[27] D. Bzdok, A. Thieme, O. Levkovskyy, P. Wren, T. Ray, and S. Reddy, “Data science opportunities\nof large language models for neuroscience and biomedicine,” Neuron, vol. 112, no. 5, pp. 698–717,\n2024.\n[28] A. Doerig, R. P. Sommers, K. Seeliger, B. Richards, J. Ismael, G. W. Lindsay, K. P. Kord-\ning, T. Konkle, M. A. Van Gerven, N. Kriegeskorte, et al., “The neuroconnectionist research\nprogramme,” Nature Reviews Neuroscience, vol. 24, no. 7, pp. 431–450, 2023.\n[29] R. M. Cichy and D. Kaiser, “Deep neural networks as scientific models,” Trends in cognitive\nsciences, vol. 23, no. 4, pp. 305–317, 2019.\n[30] T. C. Kietzmann, C. J. Spoerer, L. K. S¨orensen, R. M. Cichy, O. Hauk, and N. Kriegeskorte,\n“Recurrence is required to capture the representational dynamics of the human visual system,”\nProceedings of the National Academy of Sciences, vol. 116, no. 43, pp. 21854–21863, 2019.\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser, and I. Polo-\nsukhin, “Attention is all you need. neurips, 2017,” 2017.\n[32] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-\nford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint\narXiv:2001.08361, 2020.\n[33] J. Wang, S. Zhao, Z. Luo, Y. Zhou, H. Jiang, S. Li, T. Li, and G. Pan, “Cbramod: A criss-cross\nbrain foundation model for eeg decoding,” arXiv preprint arXiv:2412.07236, 2024.\n[34] P. S. Scotti, M. Tripathy, C. K. T. Villanueva, R. Kneeland, T. Chen, A. Narang, C. Santhi-\nrasegaran, J. Xu, T. Naselaris, K. A. Norman, et al., “Mindeye2: Shared-subject models enable\nfmri-to-image with 1 hour of data,” arXiv preprint arXiv:2403.11207, 2024.\n[35] J. Cox, P. Liu, S. E. Stolte, Y. Yang, K. Liu, K. B. See, H. Ju, and R. Fang, “Brainsegfounder:\ntowards 3d foundation models for neuroimage segmentation,” Medical Image Analysis, vol. 97,\np. 103301, 2024.\n[36] C. A. Barbano, M. Brunello, B. Dufumier, and M. Grangetto, “Anatomical foundation models\nfor brain mris,” arXiv preprint arXiv:2408.07079, 2024.\n[37] Y. Ma, Y. Liu, L. Chen, G. Zhu, B. Chen, and N. Zheng, “Brainclip: Brain representation via\nclip for generic natural visual stimulus decoding,” IEEE Transactions on Medical Imaging, 2025.\n[38] Y. Bi, A. Abrol, Z. Fu, and V. D. Calhoun, “A multimodal vision transformer for interpretable\nfusion of functional and structural neuroimaging data,” Human Brain Mapping, vol. 45, no. 17,\np. e26783, 2024.\n[39] W. Cui, W. Jeong, P. Th¨olke, T. Medani, K. Jerbi, A. A. Joshi, and R. M. Leahy, “Neuro-gpt:\nDeveloping a foundation model for eeg,” arXiv preprint arXiv:2311.03764, vol. 107, 2023.\n[40] B. S. Hopkins, B. Carter, J. Lord, J. T. Rutka, and A. A. Cohen-Gadol, “Atlasgpt: dawn of a\nnew era in neurosurgery for intelligent care augmentation, operative planning, and performance,”\nJournal of Neurosurgery, vol. 140, no. 5, pp. 1211–1214, 2024.\n[41] ˇZ. Avsec, N. Latysheva, J. Cheng, G. Novati, K. R. Taylor, T. Ward, C. Bycroft, L. Nicolaisen,\nE. Arvaniti, J. Pan, et al., “Alphagenome: advancing regulatory variant effect prediction with a\nunified dna sequence model,” bioRxiv, pp. 2025–06, 2025.\n[42] ˇZ. Avsec, V. Agarwal, D. Visentin, J. R. Ledsam, A. Grabska-Barwinska, K. R. Taylor, Y. Assael,\nJ. Jumper, P. Kohli, and D. R. Kelley, “Effective gene expression prediction from sequence by\nintegrating long-range interactions,” Nature methods, vol. 18, no. 10, pp. 1196–1203, 2021.\n[43] D. Mehta, S. Sivathamboo, H. Simpson, P. Kwan, T. O’Brien, and Z. Ge, “Privacy-preserving\nearly detection of epileptic seizures in videos,” in International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pp. 210–219, Springer, 2023.\n28\n[44] Y. Xu, J. Wang, Y.-H. Chen, J. Yang, W. Ming, S. Wang, and M. Sawan, “Vsvig: Real-time\nvideo-based seizure detection via skeleton-based spatiotemporal vig,” in European Conference\non Computer Vision, pp. 228–245, Springer, 2024.\n[45] X. Wei, K. Zhao, Y. Jiao, L. He, and Y. Zhang, “A brain graph foundation model: Pre-training\nand prompt-tuning for any atlas and disorder,” arXiv preprint arXiv:2506.02044, 2025.\n[46] H. Fu, H. Chen, S. Lin, and G. Xing, “Shade-ad: An llm-based framework for synthesizing\nactivity data of alzheimer’s patients,” in Proceedings of the 23rd ACM Conference on Embedded\nNetworked Sensor Systems, pp. 290–296, 2025.\n[47] D. Wu, L. Nie, R. A. Mumtaz, and K. Agarwal, “A llm-based hybrid-transformer diagnosis\nsystem in healthcare,” IEEE Journal of Biomedical and Health Informatics, 2024.\n[48] T. Mo, J. C. Lam, V. O. Li, and L. Y. Cheung, “Dect: Harnessing llm-assisted fine-grained\nlinguistic knowledge and label-switched and label-preserved data generation for diagnosis of\nalzheimer’s disease,” arXiv preprint arXiv:2502.04394, 2025.\n[49] X. Xu, B. Yao, Y. Dong, S. Gabriel, H. Yu, J. Hendler, M. Ghassemi, A. K. Dey, and D. Wang,\n“Mental-llm: Leveraging large language models for mental health prediction via online text\ndata,” Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,\nvol. 8, no. 1, pp. 1–32, 2024.\n[50] D. Cunnington, M. Law, J. Lobo, and A. Russo, “The role of foundation models in neuro-symbolic\nlearning and reasoning,” 2024.\n[51] M. Wu, Y. Kang, X. Li, S. Hu, X. Chen, Y. Kang, W. Wang, and K. Huang, “Vs-llm: Visual-\nsemantic depression assessment based on llm for drawing projection test,” in Chinese Conference\non Pattern Recognition and Computer Vision (PRCV), pp. 232–246, Springer, 2024.\n[52] M. Binz, E. Akata, M. Bethge, F. Br¨andle, F. Callaway, J. Coda-Forno, P. Dayan, C. Demircan,\nM. K. Eckstein, N. ´Eltet˝o, et al., “Centaur: a foundation model of human cognition,” arXiv\npreprint arXiv:2410.20268, 2024.\n[53] Z. Chen, J. Qing, T. Xiang, W. L. Yue, and J. H. Zhou, “Seeing beyond the brain: Condi-\ntional diffusion model with sparse masked modeling for vision decoding,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22710–22720, 2023.\n[54] Y. Chen, K. Ren, K. Song, Y. Wang, Y. Wang, D. Li, and L. Qiu, “Eegformer: Towards trans-\nferable and interpretable large-scale eeg foundation model,” arXiv preprint arXiv:2401.10278,\n2024.\n[55] W. Zhu, H. Huang, H. Tang, R. Musthyala, B. Yu, L. Chen, E. Vega, T. O’Donnell,\nS. Dehkharghani, J. A. Frontera, et al., “3d foundation ai model for generalizable disease detec-\ntion in head computed tomography,” arXiv preprint arXiv:2502.02779, 2025.\n[56] J. O. Caro, A. H. d. O. Fonseca, C. Averill, S. A. Rizvi, M. Rosati, J. L. Cross, P. Mittal,\nE. Zappala, D. Levine, R. M. Dhodapkar, et al., “Brainlm: A foundation model for brain activity\nrecordings,” bioRxiv, pp. 2023–09, 2023.\n[57] Y. LeCun and I. Misra, “Self-supervised learning: The dark matter of intelligence,” Meta AI,\nvol. 23, no. 2.1, 2021.\n[58] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trained models for natural language\nprocessing: A survey,” Science China technological sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\n[59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al., “Language models are few-shot learners,” Advances in neural infor-\nmation processing systems, vol. 33, pp. 1877–1901, 2020.\n[60] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-\ntariyan, and S. Gelly, “Parameter-efficient transfer learning for nlp,” in International conference\non machine learning, pp. 2790–2799, PMLR, 2019.\n29\n[61] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-efficient prompt\ntuning,” arXiv preprint arXiv:2104.08691, 2021.\n[62] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al., “Lora:\nLow-rank adaptation of large language models.,” ICLR, vol. 1, no. 2, p. 3, 2022.\n[63] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K¨uttler, M. Lewis, W.-t.\nYih, T. Rockt¨aschel, et al., “Retrieval-augmented generation for knowledge-intensive nlp tasks,”\nAdvances in neural information processing systems, vol. 33, pp. 9459–9474, 2020.\n[64] S. Makeig, K. Gramann, T.-P. Jung, T. J. Sejnowski, and H. Poizner, “Linking brain, mind and\nbehavior,” International Journal of psychophysiology, vol. 73, no. 2, pp. 95–100, 2009.\n[65] C. M. Michel, M. M. Murray, G. Lantz, S. Gonzalez, L. Spinelli, and R. G. De Peralta, “Eeg\nsource imaging,” Clinical neurophysiology, vol. 115, no. 10, pp. 2195–2222, 2004.\n[66] V. Jayaram, M. Alamgir, Y. Altun, B. Scholkopf, and M. Grosse-Wentrup, “Transfer learning in\nbrain-computer interfaces,” IEEE Computational Intelligence Magazine, vol. 11, no. 1, pp. 20–31,\n2016.\n[67] Y. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama,\nA. Vaswani, and D. Metzler, “Scale efficiently: Insights from pre-training and fine-tuning trans-\nformers,” arXiv preprint arXiv:2109.10686, 2021.\n[68] X. Li, Y. Zhou, N. Dvornek, M. Zhang, S. Gao, J. Zhuang, D. Scheinost, L. H. Staib, P. Ventola,\nand J. S. Duncan, “Braingnn: Interpretable brain graph neural network for fmri analysis,”\nMedical Image Analysis, vol. 74, p. 102233, 2021.\n[69] A. Thomas, C. R´e, and R. Poldrack, “Self-supervised learning of brain dynamics from broad\nneuroimaging data,” Advances in neural information processing systems, vol. 35, pp. 21255–\n21269, 2022.\n[70] C. Rudin, “Stop explaining black box machine learning models for high stakes decisions and use\ninterpretable models instead,” Nature machine intelligence, vol. 1, no. 5, pp. 206–215, 2019.\n[71] N. Bigdely-Shamlo, T. Mullen, C. Kothe, K.-M. Su, and K. A. Robbins, “The prep pipeline:\nstandardized preprocessing for large-scale eeg analysis,” Frontiers in neuroinformatics, vol. 9,\np. 16, 2015.\n[72] K. J. Gorgolewski, T. Auer, V. D. Calhoun, R. C. Craddock, S. Das, E. P. Duff, G. Flandin, S. S.\nGhosh, T. Glatard, Y. O. Halchenko, et al., “The brain imaging data structure, a format for\norganizing and describing outputs of neuroimaging experiments,” Scientific data, vol. 3, no. 1,\npp. 1–9, 2016.\n[73] P. Scotti, A. Banerjee, J. Goode, S. Shabalin, A. Nguyen, A. Dempster, N. Verlinde, E. Yundler,\nD. Weisberg, K. Norman, et al., “Reconstructing the mind’s eye: fmri-to-image with contrastive\nlearning and diffusion priors,” Advances in Neural Information Processing Systems, vol. 36,\npp. 24705–24728, 2023.\n[74] X. Kan, W. Dai, H. Cui, Z. Zhang, Y. Guo, and C. Yang, “Brain network transformer,” Advances\nin Neural Information Processing Systems, vol. 35, pp. 25586–25599, 2022.\n[75] Y. Wei, A. Abrol, and V. D. Calhoun, “Hierarchical spatio-temporal state-space modeling for\nfmri analysis,” in International Conference on Research in Computational Molecular Biology,\npp. 86–98, Springer, 2025.\n[76] J. Huo, Y. Wang, Y. Wang, X. Qian, C. Li, Y. Fu, and J. Feng, “Neuropictor: Refining fmri-to-\nimage reconstruction via multi-individual pretraining and multi-level modulation,” in European\nConference on Computer Vision, pp. 56–73, Springer, 2024.\n[77] D. Tak, B. Garomsa, T. Chaunzwa, A. Zapaishchykova, J. C. Climent Pardo, Z. Ye, J. Zielke,\nY. Ravipati, S. Vajapeyam, M. Mahootiha, et al., “A foundation model for generalized brain mri\nanalysis,” medRxiv, pp. 2024–12, 2024.\n30\n[78] Y. Yoo, B. Georgescu, Y. Zhang, S. Grbic, H. Liu, G. D. Aldea, T. J. Re, J. Das, P. Ullaskrishnan,\nE. Eibenberger, et al., “A non-contrast head ct foundation model for comprehensive neuro-\ntrauma triage,” arXiv preprint arXiv:2502.21106, 2025.\n[79] T. Yue, S. Xue, X. Gao, Y. Tang, L. Guo, J. Jiang, and J. Liu, “Eegpt:\nUnleashing the\npotential of eeg generalist foundation model by autoregressive pre-training,” arXiv preprint\narXiv:2410.19779, 2024.\n[80] E. Shi, K. Zhao, Q. Yuan, J. Wang, H. Hu, S. Yu, and S. Zhang, “Fome: A foundation model for\neeg using adaptive temporal-lateral attention scaling,” arXiv preprint arXiv:2409.12454, 2024.\n[81] Y. Zhou, J. Wu, Z. Ren, Z. Yao, W. Lu, K. Peng, Q. Zheng, C. Song, W. Ouyang, and C. Gou,\n“Csbrain: A cross-scale spatiotemporal brain foundation model for eeg decoding,” arXiv preprint\narXiv:2506.23075, 2025.\n[82] J. Ma, F. Wu, Q. Lin, Y. Xing, C. Liu, Z. Jia, and M. Feng, “Codebrain: Bridging decoupled to-\nkenizer and multi-scale architecture for eeg foundation model,” arXiv preprint arXiv:2506.09110,\n2025.\n[83] Q. Xiao, Z. Cui, C. Zhang, S. Chen, W. Wu, A. Thwaites, A. Woolgar, B. Zhou, and\nC. Zhang, “Brainomni: A brain foundation model for unified eeg and meg signals,” arXiv preprint\narXiv:2505.18185, 2025.\n[84] H. Zheng, H.-T. Wang, Y.-T. Jing, P.-Y. Lin, H.-Q. Zhao, W. Chen, P.-H. Wei, Y.-Z. Shan,\nG.-G. Zhao, and Y.-Z. Liu, “Brainstratify: Coarse-to-fine disentanglement of intracranial neural\ndynamics,” arXiv preprint arXiv:2505.20480, 2025.\n[85] B. D¨oner, T. M. Ingolfsson, L. Benini, and Y. Li, “Luna: Efficient and topology-agnostic founda-\ntion model for eeg signal analysis,” in 1st ICML Workshop on Foundation Models for Structured\nData.\n[86] P. Liu, C. Chen, Y. He, and T. Zhang, “Cria: A cross-view interaction and instance-adapted\npre-training framework for generalizable eeg representations,” arXiv preprint arXiv:2506.16056,\n2025.\n[87] Z. Yuan, F. Shen, M. Li, Y. Yu, C. Tan, and Y. Yang, “Brainwave: A brain signal foundation\nmodel for clinical applications,” arXiv preprint arXiv:2402.10251, 2024.\n[88] W. Xiong, J. Lin, J. Li, J. Li, and C. Jiang, “Alfee: Adaptive large foundation model for eeg\nrepresentation,” arXiv preprint arXiv:2505.06291, 2025.\n[89] W.-B. Jiang, L.-M. Zhao, and B.-L. Lu, “Large brain model for learning generic representations\nwith tremendous eeg data in bci,” arXiv preprint arXiv:2405.18765, 2024.\n[90] Y. Wang, N. Huang, N. Mammone, M. Cecchi, and X. Zhang, “Lead: Large foundation model\nfor eeg-based alzheimer’s disease detection,” arXiv preprint arXiv:2502.01678, 2025.\n[91] H. Lu, Q. Zhou, N. Fei, Z. Lu, M. Ding, J. Wen, C. Du, X. Zhao, H. Sun, H. He, et al.,\n“Multimodal foundation models are better simulators of the human brain,” arXiv preprint\narXiv:2208.08263, 2022.\n[92] Y. Hmamouche, I. Chihab, L. Kdouri, and A. E. F. Seghrouchni, “A multimodal llm for the\nnon-invasive decoding of spoken text from brain recordings,” arXiv preprint arXiv:2409.19710,\n2024.\n[93] W. Hou, G. Yang, Y. Du, Y. Lau, L. Liu, J. He, L. Long, and S. Wang, “Adagent: Llm agent\nfor alzheimer’s disease analysis with collaborative coordinator,” 2025.\n[94] C. Deng, S. Lai, C. Zhou, M. Bao, J. Yan, H. Li, L. Yao, and Y. Wang, “Asd-chat: An innovative\ndialogue intervention system for children with autism based on llm and vb-mapp,” arXiv preprint\narXiv:2409.01867, 2024.\n31\n[95] X. Luo, A. Rechardt, G. Sun, K. K. Nejad, F. Y´a˜nez, B. Yilmaz, K. Lee, A. O. Cohen, V. Borgh-\nesani, A. Pashkov, et al., “Large language models surpass human experts in predicting neuro-\nscience results,” Nature human behaviour, pp. 1–11, 2024.\n[96] C.-Y. Li, K.-J. Chang, C.-F. Yang, H.-Y. Wu, W. Chen, H. Bansal, L. Chen, Y.-P. Yang, Y.-\nC. Chen, S.-P. Chen, et al., “Towards a holistic framework for multimodal llm in 3d brain ct\nradiology report generation,” Nature Communications, vol. 16, no. 1, p. 2258, 2025.\n[97] S. Yang, Y. Luo, M. Jiao, N. Fotedar, V. R. Rao, X. Ju, S. Wu, X. Xian, H. Sun, I. Karakis,\net al., “Episemollm: A fine-tuned large language model for epileptogenic zone localization based\non seizure semiology with a performance comparable to epileptologists,” MedRxiv, pp. 2024–09,\n2024.\n[98] A. Sarabadani, K. R. Fard, and H. Dalvand, “Exkg-llm:\nLeveraging large language mod-\nels for automated expansion of cognitive neuroscience knowledge graphs,” arXiv preprint\narXiv:2503.06479, 2025.\n[99] E. A. Rashed, W. Hussain, M. Mousa, and M. al Shatouri, “Automatic generation of brain\ntumor diagnostic reports from multimodality mri using large language models,” in 2025 IEEE\n22nd International Symposium on Biomedical Imaging (ISBI), pp. 1–5, IEEE, 2025.\n[100] S. Poole, K. Koshal, N. Sisodia, K. Henderson, J. Wijangco, D. Paredes, C. Chen, W. Rowles,\nA. Akula, J. Wuerfel, et al., “Mslesionllm: A tool to extract key radiological metrics from real-\nworld multiple sclerosis datasets (p2-1.002),” in Neurology, vol. 104, p. 5443, Lippincott Williams\n& Wilkins Hagerstown, MD, 2025.\n[101] Y. Sha, H. Pan, W. Xu, W. Meng, G. Luo, X. Du, X. Zhai, H. H. Tong, C. Shi, and K. Li,\n“Mdd-llm: Towards accuracy large language models for major depressive disorder diagnosis,”\nJournal of Affective Disorders, p. 119774, 2025.\n[102] S. Barrit, N. Torcida, A. Mazeraud, S. Boulogne, J. Benoit, T. Carette, T. Carron, B. Delsaut,\nE. Diab, H. Kermorvant, et al., “Neura: a specialized large language model solution in neurology,”\nmedRxiv, pp. 2024–02, 2024.\n[103] E. Guo, M. Gupta, S. Sinha, K. R¨ossler, M. Tatagiba, R. Akagami, O. Al-Mefty, T. Sugiyama,\nP. E. Stieg, G. E. Pickett, et al., “neurogpt-x: toward a clinic-ready large language model,”\nJournal of Neurosurgery, vol. 140, no. 4, pp. 1041–1053, 2023.\n[104] R. Gao, A. Peng, Y. Duan, M. Chen, T. Zheng, M. Zhang, L. Chen, and H. Sun, “Associations\nof postencephalitic epilepsy using multi-contrast whole brain mri: A large self-supervised vision\nfoundation model strategy,” Journal of Magnetic Resonance Imaging, 2025.\n[105] A. Sarabadani, H. Taherinia, N. Ghadiri, E. K. Shahmarvandi, and R. Mousa, “Pkg-llm: A\nframework for predicting gad and mdd using knowledge graphs and large language models in\ncognitive neuroscience,” 2025.\n[106] M. Azabou, V. Arora, V. Ganesh, X. Mao, S. Nachimuthu, M. Mendelson, B. Richards, M. Perich,\nG. Lajoie, and E. Dyer, “A unified, scalable framework for neural population decoding,” Advances\nin Neural Information Processing Systems, vol. 36, pp. 44937–44956, 2023.\n[107] X. Zheng, S. Ji, J. Sun, R. Chen, W. Gao, and M. Srivastava, “Promind-llm: Proactive mental\nhealth care via causal reasoning with sensor data,” arXiv preprint arXiv:2505.14038, 2025.\n[108] M. Hao, J. Gong, X. Zeng, C. Liu, Y. Guo, X. Cheng, T. Wang, J. Ma, X. Zhang, and L. Song,\n“Large-scale foundation model on single-cell transcriptomics,” Nature methods, vol. 21, no. 8,\npp. 1481–1491, 2024.\n[109] X.-Y. Chen, Y.-M. Chen, C.-P. Chen, B.-H. Su, S. S.-F. Gau, and C.-C. Lee, “Socialrecnet: A\nmultimodal llm-based framework for assessing social reciprocity in autism spectrum disorder,” in\nICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 1–5, IEEE, 2025.\n32\n[110] S. Tu, A. Powers, S. Doogan, and J. D. Choi, “Trust: An llm-based dialogue system for trauma\nunderstanding and structured assessments,” arXiv preprint arXiv:2504.21851, 2025.\n[111] H. Dalla-Torre, L. Gonzalez, J. Mendoza-Revilla, N. Lopez Carranza, A. H. Grzywaczewski,\nF. Oteri, C. Dallago, E. Trop, B. P. de Almeida, H. Sirelkhatim, et al., “Nucleotide transformer:\nbuilding and evaluating robust foundation models for human genomics,” Nature Methods, vol. 22,\nno. 2, pp. 287–297, 2025.\n[112] C. Zakka, R. Shad, A. Chaurasia, A. R. Dalal, J. L. Kim, M. Moor, R. Fong, C. Phillips,\nK. Alexander, E. Ashley, et al., “Almanac—retrieval-augmented language models for clinical\nmedicine,” Nejm ai, vol. 1, no. 2, p. AIoa2300068, 2024.\n[113] X. Quan, M. Valentino, D. S. Carvalho, D. Dalal, and A. Freitas, “Peirce: Unifying material and\nformal reasoning via llm-driven neuro-symbolic refinement,” arXiv preprint arXiv:2504.04110,\n2025.\n[114] Y. Luo, M. Jiao, N. Fotedar, J.-E. Ding, I. Karakis, V. R. Rao, M. Asmar, X. Xian, O. Aboud,\nY. Wen, et al., “Clinical value of chatgpt for epilepsy presurgical decision-making: Systematic\nevaluation of seizure semiology interpretation,” Journal of medical Internet research, vol. 27,\np. e69173, 2025.\n[115] M. C. Schubert, W. Wick, and V. Venkataramani, “Performance of large language models on a\nneurology board–style examination,” JAMA network open, vol. 6, no. 12, pp. e2346721–e2346721,\n2023.\n[116] L. Moura, D. T. Jones, I. S. Sheikh, S. Murphy, M. Kalfin, B. R. Kummer, A. L. Weathers,\nZ. M. Grinspan, H. M. Silsbee, L. K. Jones Jr, et al., “Implications of large language models\nfor quality and efficiency of neurologic care: emerging issues in neurology,” Neurology, vol. 102,\nno. 11, p. e209497, 2024.\n[117] S. Grueso and R. Viejo-Sobera, “Machine learning methods for predicting progression from mild\ncognitive impairment to alzheimer’s disease dementia: a systematic review,” Alzheimer’s research\n& therapy, vol. 13, pp. 1–29, 2021.\n[118] E. E. Bron, S. Klein, A. Reinke, J. M. Papma, L. Maier-Hein, D. C. Alexander, and N. P. Oxtoby,\n“Ten years of image analysis and machine learning competitions in dementia,” NeuroImage,\nvol. 253, p. 119083, 2022.\n[119] T. Jo, K. Nho, and A. J. Saykin, “Deep learning in alzheimer’s disease: diagnostic classification\nand prognostic prediction using neuroimaging data,” Frontiers in aging neuroscience, vol. 11,\np. 220, 2019.\n[120] M. Ansart, S. Epelbaum, G. Bassignana, A. Bˆone, S. Bottani, T. Cattai, R. Couronn´e, J. Faouzi,\nI. Koval, M. Louis, E. Thibeau-Sutre, J. Wen, A. Wild, N. Burgos, D. Dormont, O. Colliot,\nand D. S, “Predicting the progression of mild cognitive impairment using machine learning: A\nsystematic, quantitative and critical review,” Medical Image Analysis, vol. 67, no. 101848, 2021.\n[121] J. Sui, R. Jiang, J. Bustillo, and V. Calhoun, “Neuroimaging-based individualized prediction\nof cognition and behavior for mental disorders and health: Methods and promises,” Biologi-\ncal Psychiatry, vol. 88, no. 11, pp. 818–828, 2020. Neuroimaging Biomarkers of Psychological\nTrauma.\n[122] J. Bae, J. Stocks, A. Heywood, Y. Jung, L. Jenkins, V. Hill, A. Katsaggelos, K. Popuri, H. Rosen,\nM. F. Beg, et al., “Transfer learning for predicting conversion from mild cognitive impairment\nto dementia of alzheimer’s type based on a three-dimensional convolutional neural network,”\nNeurobiology of aging, vol. 99, pp. 53–64, 2021.\n[123] “The topographic brain: from neural connectivity to cognition,” Trends in Neurosciences, vol. 30,\nno. 6, pp. 251–259, 2007.\n33\n[124] Z. Gao, Q. Ni, W. Liu, and L. Zhang, “A llms-assisted framework for parkinson’s disease assess-\nment based on ppmi dataset,” in 2024 7th International Conference on Algorithms, Computing\nand Artificial Intelligence (ACAI), pp. 1–5, IEEE, 2024.\n[125] X. Zhao, Q. Zhao, and T. Tanaka, “Epilepsyllm: Domain-specific large language model fine-tuned\nwith epilepsy medical knowledge,” arXiv preprint arXiv:2401.05908, 2024.\n[126] D. Thom, R. S.-k. Chang, N. A. Lannin, Z. Ademi, Z. Ge, D. Reutens, T. O’Brien, W. D’Souza,\nP. Perucca, S. Reeder, et al., “Personalised selection of medication for newly diagnosed adult\nepilepsy: study protocol of a first-in-class, double-blind, randomised controlled trial,” BMJ open,\nvol. 15, no. 4, p. e086607, 2025.\n[127] J. de Jong, I. Cutcutache, M. Page, S. Elmoufti, C. Dilley, H. Fr¨ohlich, and M. Armstrong,\n“Towards realizing the vision of precision medicine: Ai based prediction of clinical drug response,”\nBrain, vol. 144, no. 6, pp. 1738–1750, 2021.\n[128] W. N. D. Feng, A. Anderson, D. Thom, S. Barnard, R. Zeibich, E. Foster, M. Howard, S. T.\nBellows, R. Burgess, S. F. Berkovic, T. J. O’Brien, Z. Chen, J. French, P. Kwan, and Z. Ge,\n“Integrative deep learning of genomic and clinical data for predicting treatment response in newly\ndiagnosed epilepsy,” Neurology, 2025.\n[129] M. S. Silva-Alves, R. Secolin, B. S. Carvalho, C. L. Yasuda, E. Bilevicius, M. K. Alvim, R. O.\nSantos, C. V. Maurer-Morelli, F. Cendes, and I. Lopes-Cendes, “A prediction algorithm for drug\nresponse in patients with mesial temporal lobe epilepsy based on clinical and genetic informa-\ntion,” PLoS One, vol. 12, no. 1, p. e0169214, 2017.\n[130] H. Hakeem, W. Feng, Z. Chen, J. Choong, M. J. Brodie, S.-L. Fong, K.-S. Lim, J. Wu, X. Wang,\nN. Lawn, et al., “Development and validation of a deep learning model for predicting treatment\nresponse in patients with newly diagnosed epilepsy,” JAMA neurology, vol. 79, no. 10, pp. 986–\n996, 2022.\n[131] X. Song, J. Wang, F. He, W. Yin, W. Ma, and J. Wu, “Stroke diagnosis and prediction tool\nusing chatglm: development and validation study,” Journal of Medical Internet Research, vol. 27,\np. e67010, 2025.\n[132] J. Kottlors, R. Hahnfeldt, L. G¨”ortz, A.-I. Iuga, P. Fervers, J. Bremm, D. Zopfs, K. R. Laukamp,\nO. A. Onur, S. Lennartz, et al., “Large language models–supported thrombectomy decision-\nmaking in acute ischemic stroke based on radiology reports: Feasibility qualitative study,” Jour-\nnal of Medical Internet Research, vol. 27, p. e48328, 2025.\n[133] P. Manjunath, B. Lerner, and T. Dunn, “Towards interactive and interpretable image retrieval-\nbased diagnosis: Enhancing brain tumor classification with llm explanations and latent structure\npreservation,” in International Conference on Artificial Intelligence in Medicine, pp. 335–349,\nSpringer, 2024.\n[134] Z. Ma, L. Bi, P. Collins, O. Leary, M. Imami, Z. Zhong, S. Lu, G. Baird, N. Tapinos,\nU. Cetintemel, et al., “Large language model-based multi-source integration pipeline for au-\ntomated diagnostic classification and zero-shot prognoses for brain tumor,” Meta-Radiology,\np. 100150, 2025.\n[135] Y. Wu, J. Chen, K. Mao, and Y. Zhang, “Automatic post-traumatic stress disorder diagnosis\nvia clinical transcripts: a novel text augmentation with large language models,” in 2023 IEEE\nBiomedical Circuits and Systems Conference (BioCAS), pp. 1–5, IEEE, 2023.\n[136] A. Kulkarni and J. R. Prasad, “Utilizing large language models for the analysis of video data in\nearly attention deficit hyperactivity disorder detection in children,” Engineered Science, vol. 33,\np. 1396, 2025.\n[137] I. Obeid and J. Picone, “The temple university hospital eeg data corpus,” Frontiers in neuro-\nscience, vol. 10, p. 196, 2016.\n34\n[138] S. Lopez, G. Suarez, D. Jungreis, I. Obeid, and J. Picone, “Automated identification of abnormal\nadult eegs,” in 2015 IEEE signal processing in medicine and biology symposium (SPMB), pp. 1–5,\nIEEE, 2015.\n[139] A. Hamid, K. Gagliano, S. Rahman, N. Tulin, V. Tchiong, I. Obeid, and J. Picone, “The temple\nuniversity artifact corpus: An annotated corpus of eeg artifacts,” in 2020 IEEE Signal Processing\nin Medicine and Biology Symposium (SPMB), pp. 1–4, IEEE, 2020.\n[140] P. Detti, “Siena scalp eeg database,” physionet, vol. 10, p. 493, 2020.\n[141] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “Emotionmeter: A multimodal frame-\nwork for recognizing human emotions,” IEEE transactions on cybernetics, vol. 49, no. 3, pp. 1110–\n1122, 2018.\n[142] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition performance and robust-\nness of multimodal deep learning models for multimodal emotion recognition,” IEEE Transac-\ntions on Cognitive and Developmental Systems, vol. 14, no. 2, pp. 715–729, 2021.\n[143] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi, T. Pun, A. Nijholt,\nand I. Patras, “Deap: A database for emotion analysis; using physiological signals,” IEEE trans-\nactions on affective computing, vol. 3, no. 1, pp. 18–31, 2011.\n[144] J. Chen, X. Wang, C. Huang, X. Hu, X. Shen, and D. Zhang, “A large finer-grained affective\ncomputing eeg dataset,” Scientific Data, vol. 10, no. 1, p. 740, 2023.\n[145] A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E.\nMietus, G. B. Moody, C.-K. Peng, and H. E. Stanley, “Physiobank, physiotoolkit, and physionet:\ncomponents of a new research resource for complex physiologic signals,” circulation, vol. 101,\nno. 23, pp. e215–e220, 2000.\n[146] G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw, “Bci2000:\na general-purpose brain-computer interface (bci) system,” IEEE Transactions on biomedical\nengineering, vol. 51, no. 6, pp. 1034–1043, 2004.\n[147] M. Tangermann, K.-R. M¨uller, A. Aertsen, N. Birbaumer, C. Braun, C. Brunner, R. Leeb,\nC. Mehring, K. J. Miller, G. R. M¨uller-Putz, et al., “Review of the bci competition iv,” Frontiers\nin neuroscience, vol. 6, p. 55, 2012.\n[148] B. Kemp, A. H. Zwinderman, B. Tuk, H. A. Kamphuisen, and J. J. Oberye, “Analysis of a sleep-\ndependent neuronal feedback loop: the slow-wave microcontinuity of the eeg,” IEEE Transactions\non Biomedical Engineering, vol. 47, no. 9, pp. 1185–1194, 2000.\n[149] A. T. Gifford, K. Dwivedi, G. Roig, and R. M. Cichy, “A large and rich eeg dataset for modeling\nhuman visual object recognition,” NeuroImage, vol. 264, p. 119754, 2022.\n[150] “Stew: Simultaneous task eeg workload dataset,” 2018.\n[151] A. Delorme and M. Fabre-Thorpe, “”go-nogo categorization and detection task”,” 2020.\n[152] B. Kaneshiro, D. T. Nguyen, J. P. Dmochowski, A. M. Norcia, and J. Berger, “Naturalistic music\neeg dataset—hindi (nmed-h),” in Stanford Digital Repository, Stanford Digit. Repository, 2016.\n[153] L. M. Alexander, J. Escalera, L. Ai, C. Andreotti, K. Febre, A. Mangone, N. Vega-Potler,\nN. Langer, A. Alexander, M. Kovacs, et al., “An open resource for transdiagnostic research in\npediatric mental health and learning disorders,” Scientific data, vol. 4, no. 1, pp. 1–26, 2017.\n[154] T. Grootswagers, A. Robinson, S. Shatek, and T. Carlson, “”features-eeg”,” 2024.\n[155] D. Cserpan, E. Boran, R. Rosch, S. L. Biundo, G. Ramantani, and J. Sarnthein, “Dataset of eeg\nrecordings of pediatric patients with epilepsy based on the 10-20 system,” OpenNeuro, 2023.\n[156] P. Dzianok and E. Kublik, “Pearl-neuro database: Eeg, fmri, health and lifestyle data of middle-\naged people at risk of dementia,” Scientific Data, vol. 11, no. 1, p. 276, 2024.\n35\n[157] Y. Wang, W. Duan, D. Dong, L. Ding, and X. Lei, “”a test-retest resting and cognitive state eeg\ndataset”,” 2022.\n[158] I. J. Bajwa1, A. S. Nilsen1, . Ren´e Skukies1, A. Aamodt1, G. Ernst2, J. F. Storm1, and . Bjørn\nE. Juel1, “”a repeated awakening study exploring the capacity of complexity measures to capture\ndreaming during propofol sedation”,” 2024.\n[159] M. Lahijanian, H. Aghajan, and Z. Vahabi, “Auditory gamma-band entrainment enhances de-\nfault mode network connectivity in dementia patients,” Scientific Reports, vol. 14, no. 1, p. 13153,\n2024.\n[160] M. L. Vicchietti, F. M. Ramos, L. E. Betting, and A. S. Campanharo, “Computational methods\nof eeg signals analysis for alzheimer’s disease classification,” Scientific Reports, vol. 13, no. 1,\np. 8184, 2023.\n[161] A. Miltiadous, K. D. Tzimourta, T. Afrantou, P. Ioannidis, N. Grigoriadis, D. G. Tsalikakis,\nP. Angelidis, M. G. Tsipouras, E. Glavas, N. Giannakeas, et al., “A dataset of scalp eeg recordings\nof alzheimer’s disease, frontotemporal dementia and healthy subjects from routine eeg,” Data,\nvol. 8, no. 6, p. 95, 2023.\n[162] C. L. Alves, A. M. Pineda, K. Roster, C. Thielemann, and F. A. Rodrigues, “Eeg functional\nconnectivity and deep learning for automatic diagnosis of brain disorders: Alzheimer’s disease\nand schizophrenia,” Journal of Physics: complexity, vol. 3, no. 2, p. 025001, 2022.\n[163] A. M. Pineda, F. M. Ramos, L. E. Betting, and A. S. Campanharo, “Quantile graphs for eeg-\nbased diagnosis of alzheimer’s disease,” Plos one, vol. 15, no. 6, p. e0231169, 2020.\n[164] J. Escudero, D. Ab´asolo, R. Hornero, P. Espino, and M. L´opez, “Analysis of electroencephalo-\ngrams in alzheimer’s disease patients with multiscale entropy,” Physiological measurement,\nvol. 27, no. 11, p. 1091, 2006.\n[165] K. Smith, D. Ab´asolo, and J. Escudero, “Accounting for the complex hierarchical topology of\neeg phase-based functional connectivity in network binarisation,” PloS one, vol. 12, no. 10,\np. e0186164, 2017.\n[166] P. Prado, V. Medel, R. Gonzalez-Gomez, A. Sainz-Ballesteros, V. Vidal, H. Santamar´ıa-Garc´ıa,\nS. Moguilner, J. Mejia, A. Slachevsky, M. I. Behrens, et al., “The brainlat project, a multi-\nmodal neuroimaging dataset of neurodegeneration from underrepresented backgrounds,” Scien-\ntific Data, vol. 10, no. 1, p. 889, 2023.\n[167] M. Cecchi, D. K. Moore, C. H. Sadowsky, P. R. Solomon, P. M. Doraiswamy, C. D. Smith,\nG. A. Jicha, A. E. Budson, S. E. Arnold, and K. C. Fadem, “A clinical trial to validate event-\nrelated potential markers of alzheimer’s disease in outpatient settings,” Alzheimer’s & Dementia:\nDiagnosis, Assessment & Disease Monitoring, vol. 1, no. 4, pp. 387–394, 2015.\n[168] C. Ieracitano, N. Mammone, A. Bramanti, S. Marino, A. Hussain, and F. C. Morabito, “A time-\nfrequency based machine learning system for brain states classification via eeg signal processing,”\nin 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1–8, IEEE, 2019.\n[169] J. P. Amezquita-Sanchez, N. Mammone, F. C. Morabito, S. Marino, and H. Adeli, “A\nnovel methodology for automated differential diagnosis of mild cognitive impairment and the\nalzheimer’s disease using eeg signals,” Journal of neuroscience methods, vol. 322, pp. 88–95,\n2019.\n[170] L. Gwilliams, G. Flick, A. Marantz, L. Pylkk¨anen, D. Poeppel, and J.-R. King, “Introducing meg-\nmasc a high-quality magneto-encephalography dataset for evaluating natural speech processing,”\nScientific data, vol. 10, no. 1, p. 862, 2023.\n[171] K. Armeni, U. G¨u¸cl¨u, M. van Gerven, and J.-M. Schoffelen, “A 10-hour within-participant\nmagnetoencephalography narrative dataset to test models of language comprehension,” Scientific\nData, vol. 9, no. 1, p. 278, 2022.\n36\n[172] G. Niso, C. Rogers, J. T. Moreau, L.-Y. Chen, C. Madjar, S. Das, E. Bock, F. Tadel, A. C. Evans,\nP. Jolicoeur, et al., “Omega: the open meg archive,” Neuroimage, vol. 124, pp. 1182–1187, 2016.\n[173] J. R. Taylor, N. Williams, R. Cusack, T. Auer, M. A. Shafto, M. Dixon, L. K. Tyler, R. N.\nHenson, et al., “The cambridge centre for ageing and neuroscience (cam-can) data repository:\nStructural and functional mri, meg, and cognitive data from a cross-sectional adult lifespan\nsample,” neuroimage, vol. 144, pp. 262–269, 2017.\n[174] T. Wise, Y. Liu, F. Chowdhury, and R. J. Dolan, “”model-based aversive learning in humans is\nsupported by preferential task state reactivation”,” 2021.\n[175] M. Weisend, F. Hanlon, R. Montano, S. Ahlfors, A. Leuthold, D. Pantazis, J. Mosher, A. Geor-\ngopoulos, M. Hamalainen, and C. Aine, “”mind data”,” 2022.\n[176] S. Wang, X. Zhang, J. Zhang, and C. Zong, “”a synchronized multimodal neuroimaging dataset\nto study brain language processing”,” 2023.\n[177] M. N. Hebart, O. Contier, L. Teichmann, A. H. Rockter, C. Zheng, A. Kidder, A. Corriveau,\nM. Vaziri-Pashkam, and C. I. Baker, “”things-meg”,” 2024.\n[178] P. Gaston, C. Brodbeck, C. Phillips, and E. Lau, “”auditory single word recognition in meg”,”\n2022.\n[179] J. J. Singer, R. M. Cichy, and M. N. Hebart, “The spatiotemporal neural dynamics of ob-\nject recognition for natural images and line drawings,” Journal of Neuroscience, vol. 43, no. 3,\npp. 484–500, 2023.\n[180] A. Rodriguez, D. Zhao, K. Wilson, R. Saboo, S. V. Samsonau, and A. Marantz, “”neuromorph:\nA high-temporal resolution meg dataset for morpheme-based linguistic analysis”,” 2024.\n[181] A. Thwaites, C. Zhang, A. Woolgar, C. Wingfield, and C. Yang, “Kymata soto language dataset\n(english and russian conversations),” May 2025.\n[182] D. C. Van Essen, S. M. Smith, D. M. Barch, T. E. Behrens, E. Yacoub, K. Ugurbil, W.-M. H. Con-\nsortium, et al., “The wu-minn human connectome project: an overview,” Neuroimage, vol. 80,\npp. 62–79, 2013.\n[183] R. Collins, “Uk biobank: protocol for a large-scale prospective epidemiological resource,” 2007.\n[184] C. R. Jack Jr, M. A. Bernstein, N. C. Fox, P. Thompson, G. Alexander, D. Harvey, B. Borowski,\nP. J. Britson, J. L. Whitwell, C. Ward, et al., “The alzheimer’s disease neuroimaging initiative\n(adni): Mri methods,” Journal of Magnetic Resonance Imaging: An Official Journal of the\nInternational Society for Magnetic Resonance in Medicine, vol. 27, no. 4, pp. 685–691, 2008.\n[185] D. S. Marcus, A. F. Fotenos, J. G. Csernansky, J. C. Morris, and R. L. Buckner, “Open access\nseries of imaging studies: longitudinal mri data in nondemented and demented older adults,”\nJournal of cognitive neuroscience, vol. 22, no. 12, pp. 2677–2684, 2010.\n[186] P. J. LaMontagne, T. L. Benzinger, J. C. Morris, S. Keefe, R. Hornbeck, C. Xiong, E. Grant,\nJ. Hassenstab, K. Moulder, A. G. Vlassenko, et al., “Oasis-3: longitudinal neuroimaging, clinical,\nand cognitive dataset for normal aging and alzheimer disease,” medrxiv, pp. 2019–12, 2019.\n[187] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz,\nJ. Slotboom, R. Wiest, et al., “The multimodal brain tumor image segmentation benchmark\n(brats),” IEEE transactions on medical imaging, vol. 34, no. 10, pp. 1993–2024, 2014.\n[188] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara, C. Berger,\nS. M. Ha, M. Rozycki, et al., “Identifying the best machine learning algorithms for brain tumor\nsegmentation, progression assessment, and overall survival prediction in the brats challenge,”\narXiv preprint arXiv:1811.02629, 2018.\n37\n[189] U. Baid, M. Rooks, E. Calabrese, S. Bakas, K. Farahani, C. Davatzikos, et al., “The rsna-asnr-\nmiccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification,”\narXiv preprint arXiv:2107.02314, 2021.\n[190] S.-L. Liew, B. P. Lo, M. R. Donnelly, A. Zavaliangos-Petropulu, J. N. Jeong, G. Barisano,\nA. Hutton, J. P. Simon, J. M. Juliano, A. Suri, et al., “A large, curated, open-source stroke\nneuroimaging dataset to improve lesion segmentation algorithms,” Scientific data, vol. 9, no. 1,\np. 320, 2022.\n[191] A. Di Martino, C.-G. Yan, Q. Li, E. Denio, F. X. Castellanos, K. Alaerts, J. S. Anderson, M. As-\nsaf, S. Y. Bookheimer, M. Dapretto, et al., “The autism brain imaging data exchange: towards a\nlarge-scale evaluation of the intrinsic brain architecture in autism,” Molecular psychiatry, vol. 19,\nno. 6, pp. 659–667, 2014.\n[192] A. Di Martino, D. O’connor, B. Chen, K. Alaerts, J. S. Anderson, M. Assaf, J. H. Balsters,\nL. Baxter, A. Beggiato, S. Bernaerts, et al., “Enhancing studies of the connectome in autism\nusing the autism brain imaging data exchange ii,” Scientific data, vol. 4, no. 1, pp. 1–15, 2017.\n[193] K. Marek, D. Jennings, S. Lasch, A. Siderowf, C. Tanner, T. Simuni, C. Coffey, K. Kieburtz,\nE. Flagg, S. Chowdhury, et al., “The parkinson progression marker initiative (ppmi),” Progress\nin neurobiology, vol. 95, no. 4, pp. 629–635, 2011.\n[194] R. O. Roberts, Y. E. Geda, D. S. Knopman, R. H. Cha, V. S. Pankratz, B. F. Boeve, R. J. Ivnik,\nE. G. Tangalos, R. C. Petersen, and W. A. Rocca, “The mayo clinic study of aging: design\nand sampling, participation, baseline measures and sample characteristics,” Neuroepidemiology,\nvol. 30, no. 1, pp. 58–69, 2008.\n[195] C. Rorden, J. Absher, M. Gibson, A. Teghipco, and R. Newman-Norlund, “Stroke outcome\noptimization project (soop),” OpenNeuro10, vol. 18112, 2024.\n[196] J. V. Lilly, J. L. Rokita, J. L. Mason, T. Patton, S. Stefankiewiz, D. Higgins, G. Trooskin, C. A.\nLarouci, K. Arya, E. Appert, et al., “The children’s brain tumor network (cbtn)-accelerating\nresearch in pediatric central nervous system tumors through collaboration and open science,”\nNeoplasia, vol. 35, p. 100846, 2023.\n[197] I. B. Malone, D. Cash, G. R. Ridgway, D. G. MacManus, S. Ourselin, N. C. Fox, and J. M. Schott,\n“Miriad—public release of a multiple time point alzheimer’s mr imaging dataset,” NeuroImage,\nvol. 70, pp. 33–36, 2013.\n[198] D. Park, J. Hennessee, E. T. Smith, M. Chan, C. Katen, G. Wig, K. Rodrigue, and K. Kennedy,\n“”the dallas lifespan brain study”,” 2024.\n[199] E. Calabrese, J. E. Villanueva-Meyer, J. D. Rudie, A. M. Rauschecker, U. Baid, S. Bakas, S. Cha,\nJ. T. Mongan, and C. P. Hess, “The university of california san francisco preoperative diffuse\nglioma mri dataset,” Radiology: Artificial Intelligence, vol. 4, no. 6, p. e220058, 2022.\n[200] A. Mamonov and J. Kalpathy-Cramer, “Data from qin gbm treatment response,” The Cancer\nImaging Archive, vol. 10, p. k9, 2016.\n[201] S. Bakas, C. Sako, H. Akbari, M. Bilello, A. Sotiras, G. Shukla, J. D. Rudie, N. F. Santamar´ıa,\nA. F. Kazerooni, S. Pati, et al., “The university of pennsylvania glioblastoma (upenn-gbm)\ncohort: advanced mri, clinical, genomics, & radiomics,” Scientific data, vol. 9, no. 1, p. 453,\n2022.\n[202] D. P. Barboriak, “Data from rider neuro mri.” https://doi.org/10.7937/K9/TCIA.2015.\nVOSN3HN1, 2015. The Cancer Imaging Archive.\n[203] J. Wang, M. N. Lytle, Y. Weiss, B. L. Yamasaki, and J. R. Booth, “A longitudinal neuroimaging\ndataset on language processing in children ages 5, 7, and 9 years old,” Scientific Data, vol. 9,\nno. 1, p. 4, 2022.\n38\n[204] National Institute of Mental Health, “Nimh data archive collection 2848.” https://nda.nih.\ngov/edit_collection.html?id=2848, 2024.\n[205] L. Snoek, M. van der Miesen, A. van der Leij, T. Beemsterboer, A. Eigenhuis, and S. Scholte,\n“”aomic-id1000”,” 2021.\n[206] Y. Xiao, V. Fonov, M. M. Chakravarty, S. Beriault, F. Al Subaie, A. Sadikot, G. B. Pike,\nG. Bertrand, and D. L. Collins, “A dataset of multi-contrast population-averaged brain mri\natlases of a parkinson’ s disease cohort,” Data in brief, vol. 12, pp. 370–379, 2017.\n[207] G. Podobnik, P. Strojan, P. Peterlin, B. Ibragimov, and T. Vrtovec, “Han-seg: The head and neck\norgan-at-risk ct and mr segmentation dataset,” Medical physics, vol. 50, no. 3, pp. 1917–1927,\n2023.\n[208] J. L. Hanson, A. Chandra, B. L. Wolfe, and S. D. Pollak, “Association between income and the\nhippocampus,” PloS one, vol. 6, no. 5, p. e18712, 2011.\n[209] J. Mazziotta, A. Toga, A. Evans, P. Fox, J. Lancaster, K. Zilles, R. Woods, T. Paus, G. Simpson,\nB. Pike, et al., “A probabilistic atlas and reference system for the human brain: International\nconsortium for brain mapping (icbm),” Philosophical Transactions of the Royal Society of Lon-\ndon. Series B: Biological Sciences, vol. 356, no. 1412, pp. 1293–1322, 2001.\n[210] Imperial College London, “Ixi dataset – brain development.” https://brain-development.\norg/ixi-dataset/.\n[211] T. L. Jernigan, T. T. Brown, D. J. Hagler Jr, N. Akshoomoff, H. Bartsch, E. Newman, W. K.\nThompson, C. S. Bloss, S. S. Murray, N. Schork, et al., “The pediatric imaging, neurocognition,\nand genetics (ping) data repository,” Neuroimage, vol. 124, pp. 1149–1154, 2016.\n[212] H. Richardson, G. Lisandrelli, A. Riobueno-Naylor, and R. Saxe, “Mri data of 3-12 year old\nchildren and adults during viewing of a short animated film[data set]. openneuro,” 2019.\n[213] D. Wei, K. Zhuang, L. Ai, Q. Chen, W. Yang, W. Liu, K. Wang, J. Sun, and J. Qiu, “Structural\nand functional brain scans from the cross-sectional southwest university adult lifespan dataset,”\nScientific data, vol. 5, no. 1, pp. 1–10, 2018.\n[214] M. Nordstrom, E. Felton, K. Sear, B. Tamrazi, J. Torkildson, K. Gauvain, D. A. Haas-Kogan,\nJ. Chen, B. D. Buono, A. Banerjee, et al., “Large vessel arteriopathy after cranial radiation\ntherapy in pediatric brain tumor survivors,” Journal of child neurology, vol. 33, no. 5, pp. 359–\n366, 2018.\n[215] M. M. Breteler, T. St¨ocker, E. Pracht, D. Brenner, and R. Stirnberg, “Ic-p-165: Mri in the\nrhineland study:\na novel protocol for population neuroimaging,” Alzheimer’s & Dementia,\nvol. 10, pp. P92–P92, 2014.\n[216] L. Wang, K. I. Alpert, V. D. Calhoun, D. J. Cobia, D. B. Keator, M. D. King, A. Kogan,\nD. Landis, M. Tallis, M. D. Turner, et al., “Schizconnect: Mediating neuroimaging databases on\nschizophrenia and related disorders for large-scale integration,” Neuroimage, vol. 124, pp. 1155–\n1167, 2016.\n[217] B. Dufumier, A. Grigis, J. Victor, C. Ambroise, V. Frouin, and E. Duchesnay, “Openbhb: a\nlarge-scale multi-site brain mri data-set for age prediction and debiasing,” NeuroImage, vol. 263,\np. 119637, 2022.\n[218] O. Commowick, A. Istace, M. Kain, B. Laurent, F. Leray, M. Simon, S. C. Pop, P. Girard,\nR. Ameli, J.-C. Ferr´e, et al., “Objective evaluation of multiple sclerosis lesion segmentation using\na data management and processing infrastructure,” Scientific reports, vol. 8, no. 1, p. 13650, 2018.\n[219] M. R. Hernandez Petzsche, E. De La Rosa, U. Hanning, R. Wiest, W. Valenzuela, M. Reyes,\nM. Meyer, S.-L. Liew, F. Kofler, I. Ezhov, et al., “Isles 2022: A multi-center magnetic resonance\nimaging stroke lesion segmentation dataset,” Scientific data, vol. 9, no. 1, p. 762, 2022.\n39\n[220] H. J. Kuijf, J. M. Biesbroek, J. De Bresser, R. Heinen, S. Andermatt, M. Bento, M. Berseth,\nM. Belyaev, M. J. Cardoso, A. Casamitjana, et al., “Standardized assessment of automatic\nsegmentation of white matter hyperintensities and results of the wmh segmentation challenge,”\nIEEE transactions on medical imaging, vol. 38, no. 11, pp. 2556–2568, 2019.\n[221] F. Guarnera, A. Rondinella, E. Crispino, G. Russo, C. Di Lorenzo, D. Maimone, F. Pappalardo,\nand S. Battiato, “Mslesseg: baseline and benchmarking of a new multiple sclerosis lesion seg-\nmentation dataset,” Scientific Data, vol. 12, no. 1, p. 920, 2025.\n[222] E. J. Allen, G. St-Yves, Y. Wu, J. L. Breedlove, J. S. Prince, L. T. Dowdle, M. Nau, B. Caron,\nF. Pestilli, I. Charest, et al., “A massive 7t fmri dataset to bridge cognitive neuroscience and\nartificial intelligence,” Nature neuroscience, vol. 25, no. 1, pp. 116–126, 2022.\n[223] N. Chang, J. A. Pyles, A. Marcus, A. Gupta, M. J. Tarr, and E. M. Aminoff, “Bold5000, a public\nfmri dataset while viewing 5000 visual images,” Scientific data, vol. 6, no. 1, p. 49, 2019.\n[224] T. Horikawa and Y. Kamitani, “Generic decoding of seen and imagined objects using hierarchical\nvisual features,” Nature communications, vol. 8, no. 1, p. 15037, 2017.\n[225] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick,\n“Microsoft coco: Common objects in context,” in European conference on computer vision,\npp. 740–755, Springer, 2014.\n[226] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazeb-\nnik, “Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence\nmodels,” in Proceedings of the IEEE international conference on computer vision, pp. 2641–2649,\n2015.\n[227] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 3558–3568, 2021.\n[228] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A cleaned, hypernymed,\nimage alt-text dataset for automatic image captioning,” in Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556–2565,\n2018.\n[229] V. Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing images using 1 million captioned\nphotographs,” Advances in neural information processing systems, vol. 24, 2011.\n[230] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J.\nLi, D. A. Shamma, et al., “Visual genome: Connecting language and vision using crowdsourced\ndense image annotations,” International journal of computer vision, vol. 123, no. 1, pp. 32–73,\n2017.\n[231] B. Rauchbauer, Y. Hmamouche, B. Bigi, L. Pr´evot, M. Ochs, and T. Chaminade, “Multimodal\ncorpus of bidirectional conversation of human-human and human-robot interaction during fmri\nscanning,” in Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 668–\n675, 2020.\n[232] B. Luna, “”petfrog”,” 2020.\n[233] S. Luz, F. Haider, S. De la Fuente, D. Fromm, and B. MacWhinney, “Detecting cognitive decline\nusing speech only: The adresso challenge,” arXiv preprint arXiv:2104.09356, 2021.\n[234] D. Umerenkov, S. Kudin, M. Peksheva, and D. Pavlov, “Core-penumbra hyperacute ischemic\nstroke dataset,” Scientific Data, vol. 12, no. 1, p. 707, 2025.\n[235] E. Turcan and K. McKeown, “Dreaddit: A reddit dataset for stress analysis in social media,”\narXiv preprint arXiv:1911.00133, 2019.\n40\n[236] M. Gaur, A. Alambo, J. P. Sain, U. Kursuncu, K. Thirunarayan, R. Kavuluru, A. Sheth, R. Wel-\nton, and J. Pathak, “Knowledge-aware assessment of severity of suicide risk for early interven-\ntion,” in The world wide web conference, pp. 514–525, 2019.\n[237] U. Naseem, A. G. Dunn, J. Kim, and M. Khushi, “Early identification of depression severity\nlevels on reddit using ordinal classification,” in Proceedings of the ACM web conference 2022,\npp. 2563–2572, 2022.\n[238] A. Haque, V. Reddi, and T. Giallanza, “Deep learning for suicide and depression identification\nwith unsupervised label correction,” in International Conference on Artificial Neural Networks,\npp. 436–447, Springer, 2021.\n[239] V. Thambawita, S. A. Hicks, H. Borgli, H. K. Stensland, D. Jha, M. K. Svensen, S.-A. Pettersen,\nD. Johansen, H. D. Johansen, S. D. Pettersen, et al., “Pmdata: a sports logging dataset,” in\nProceedings of the 11th ACM Multimedia Systems Conference, pp. 231–236, 2020.\n[240] X. Xu, H. Zhang, Y. Sefidgar, Y. Ren, X. Liu, W. Seo, J. Brown, K. Kuehn, M. Merrill, P. Nurius,\net al., “Globem dataset: multi-year datasets for longitudinal human behavior modeling general-\nization,” Advances in neural information processing systems, vol. 35, pp. 24655–24692, 2022.\n[241] D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer, K. Georgila, J. Gratch,\nA. Hartholt, M. Lhommet, et al., “Simsensei kiosk: A virtual human interviewer for healthcare\ndecision support,” in Proceedings of the 2014 international conference on Autonomous agents\nand multi-agent systems, pp. 1061–1068, 2014.\n[242] C. Lord, S. Risi, L. Lambrecht, E. H. Cook Jr, B. L. Leventhal, P. C. DiLavore, A. Pickles,\nand M. Rutter, “The autism diagnostic observation schedule—generic: A standard measure of\nsocial and communication deficits associated with the spectrum of autism,” Journal of autism\nand developmental disorders, vol. 30, no. 3, pp. 205–223, 2000.\n[243] S. Chilamkurthy, R. Ghosh, S. Tanamala, M. Biviji, N. G. Campeau, V. K. Venugopal, V. Ma-\nhajan, P. Rao, and P. Warier, “Deep learning algorithms for detection of critical findings in head\nct scans: a retrospective study,” The Lancet, vol. 392, no. 10162, pp. 2388–2396, 2018.\n[244] M. Binz, E. Akata, M. Bethge, F. Br¨andle, F. Callaway, J. Coda-Forno, P. Dayan, C. Demir-\ncan, M. K. Eckstein, N. ´Eltet˝o, T. L. Griffiths, S. Haridi, A. K. Jagadish, L. Ji-An, A. Kipnis,\nS. Kumar, T. Ludwig, M. Mathony, M. Mattar, A. Modirshanechi, S. S. Nath, J. C. Peter-\nson, M. Rmus, E. M. Russek, T. Saanum, N. Scharfenberg, J. A. Schubert, L. M. S. Buschoff,\nN. Singhi, X. Sui, M. Thalmann, F. Theis, V. Truong, V. Udandarao, K. Voudouris, R. Wilson,\nK. Witte, S. Wu, D. Wulff, H. Xiong, and E. Schulz, “Centaur: a foundation model of human\ncognition,” 2024.\n[245] Z. Yuan, Z. Huang, C. Li, S. Li, Q. Ren, X. Xia, Q. Jiang, D. Zhang, Q. Zhu, and X. Meng,\n“Multimodal fusion model for diagnosing mild cognitive impairment in unilateral middle cerebral\nartery steno-occlusive disease,” Frontiers in Aging Neuroscience, 2025.\n[246] M. G. Perich, L. E. Miller, M. Azabou, and E. L. Dyer, “Long-term recordings of motor and\npremotor cortical spiking activity during reaching in monkeys (version 0.250122.1735).” https:\n//doi.org/10.48324/dandi.000688/0.250122.1735, 2025.\n[247] S. D. Larson and M. E. Martone, “Neurolex. org: an online framework for neuroscience knowl-\nedge,” Frontiers in neuroinformatics, vol. 7, p. 18, 2013.\n[248] G. A. Ascoli, D. E. Donohue, and M. Halavi, “Neuromorpho. org: a central resource for neuronal\nmorphologies,” Journal of Neuroscience, vol. 27, no. 35, pp. 9247–9251, 2007.\n[249] C. J. Markiewicz, K. J. Gorgolewski, F. Feingold, R. Blair, Y. O. Halchenko, E. Miller, N. Hard-\ncastle, J. Wexler, O. Esteban, M. Goncavles, et al., “The openneuro resource for sharing of\nneuroscience data,” Elife, vol. 10, p. e71774, 2021.\n41\n[250] Z. E. Teton, R. S. Freedman, S. B. Tomlinson, J. R. Linzey, A. Onyewuenyi, A. S. Khahera,\nB. K. Hendricks, and A. A. Cohen-Gadol, “The neurosurgical atlas: advancing neurosurgical\neducation in the digital age,” Neurosurgical focus, vol. 48, no. 3, p. E17, 2020.\n[251] M. Jullien, M. Valentino, H. Frost, P. O’Regan, D. Landers, and A. Freitas, “Nli4ct: Multi-\nevidence natural language inference for clinical trial reports,” arXiv preprint arXiv:2305.03598,\n2023.\n[252] R. S. Porter and J. L. Kaplan, The Merck Manual of Diagnosis and Therapy. Merck & Co., Inc.,\n20th ed., 2023.\n[253] D. Minh, H. X. Wang, Y. F. Li, and T. N. Nguyen, “Explainable artificial intelligence: a com-\nprehensive review,” Artificial Intelligence Review, vol. 55, no. 5, pp. 3503–3568, 2022.\n[254] F. Sado, C. K. Loo, W. S. Liew, M. Kerzel, and S. Wermter, “Explainable goal-driven agents\nand robots-a comprehensive review,” ACM Computing Surveys, vol. 55, no. 10, pp. 1–41, 2023.\n[255] T. Miller, “Explanation in artificial intelligence: Insights from the social sciences,” Artificial\nintelligence, vol. 267, pp. 1–38, 2019.\n[256] M. Thomas, N. Mackes, A. Preuss-Dodhy, T. Wieland, M. Bundschus, et al., “Assessing privacy\nvulnerabilities in genetic data sets: scoping review,” JMIR Bioinformatics and Biotechnology,\nvol. 5, no. 1, p. e54332, 2024.\n[257] H. Yu, Q. Wang, and X. Zhou, “Adaptive-weighted federated graph convolutional networks with\nmulti-sensor data fusion for drug response prediction,” Information Fusion, vol. 122, p. 103147,\n2025.\n[258] R. K. Garg, V. L. Urs, A. A. Agarwal, S. K. Chaudhary, V. Paliwal, and S. K. Kar, “Exploring\nthe role of chatgpt in patient care (diagnosis and treatment) and medical research: A systematic\nreview,” Health Promotion Perspectives, vol. 13, no. 3, p. 183, 2023.\n[259] A. Stanojevic, S. Wo´zniak, G. Bellec, G. Cherubini, A. Pantazi, and W. Gerstner, “High-\nperformance deep spiking neural networks with 0.3 spikes per neuron,” Nature Communications,\nvol. 15, no. 1, p. 6793, 2024.\n[260] C. Sun and M. Dumontier, “Generating unseen diseases patient data using ontology enhanced\ngenerative adversarial networks,” npj Digital Medicine, vol. 8, no. 1, p. 4, 2025.\n[261] J. D. Van Horn and E. Ricciardi, “International collaborations at the intersection of brain sciences\nand artificial intelligence,” Neuroinformatics, vol. 23, no. 3, p. 36, 2025.\n[262] D. O. Eke, A. Bernard, J. G. Bjaalie, R. Chavarriaga, T. Hanakawa, A. J. Hannan, S. L. Hill,\nM. E. Martone, A. McMahon, O. Ruebel, et al., “International data governance for neuroscience,”\nNeuron, vol. 110, no. 4, pp. 600–612, 2022.\n[263] A. I. Luppi, J. Achterberg, S. Schmidgall, I. P. Bilgin, P. Herholz, M. Sprang, B. Fockter, A. S.\nHam, S. Thorat, R. Ziaei, et al., “Trainees’ perspectives and recommendations for catalyzing the\nnext generation of neuroai researchers,” nature communications, vol. 15, no. 1, p. 9152, 2024.\n42\n",
    "content": "# **Foundation Models and Large-Scale AI in Neuroscience: A Comprehensive Review – Interpretation**\n\n## 1. Core Content and Key Contributions\n\nThis paper is a **systematic review of large-scale artificial intelligence (AI) models in neuroscience**, offering a comprehensive overview of how cutting-edge AI technologies—particularly foundation models (FMs), large language models (LLMs), and generative AI—are profoundly transforming the paradigms of neuroscience research.\n\n### Core Content  \nThe central theme explores the revolutionary impact of large-scale AI models across five key domains in neuroscience:\n- **Neuroimaging and Data Processing**  \n- **Brain-Computer Interfaces and Neural Decoding**  \n- **Molecular Neuroscience and Genomic Modeling**  \n- **Clinical Support and Translational Frameworks**  \n- **Applications for Specific Neurological and Psychiatric Disorders**\n\nThese models achieve **end-to-end learning from raw brain signals**, overcoming the limitations of traditional machine learning that rely on handcrafted feature engineering. They demonstrate strong cross-subject and cross-condition generalization, while addressing critical challenges such as multimodal data integration and spatiotemporal pattern analysis.\n\n### Key Contributions\n1. **Systematic Classification and Integration**: For the first time, this work consolidates fragmented research into a coherent framework, clearly delineating core application scenarios of large-scale AI in neuroscience.\n2. **Technical Roadmap Synthesis**: Provides a detailed summary of key techniques—including Transformer architectures, self-supervised learning, and multimodal fusion—and their implementations and advantages across diverse neural data types (e.g., fMRI, EEG, genomics).\n3. **Frontier Model Inventory**: Catalogs and analyzes dozens of representative models (e.g., BENDR, BrainCLIP, MindBridge, Neuro-GPT), revealing their design principles and performance breakthroughs.\n4. **Public Dataset Compilation**: Offers a curated list of public datasets spanning EEG, MEG, MRI, and clinical text, serving as a valuable resource for researchers.\n5. **Challenges and Future Directions**: In-depth discussion of current issues including data quality, model interpretability, and ethical governance, along with proposed pathways for future development.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nThe review highlights several milestone-level technological advances and conceptual innovations:\n\n### (1) Paradigm Shift: From \"Feature Engineering\" to \"End-to-End Representation Learning\"\nTraditional methods require experts to manually extract features like waveform morphology or spectral power. In contrast, models such as **BENDR** and **EEGFormer** directly learn robust representations from raw EEG signals using techniques like **masked signal modeling** and **contrastive learning**, significantly enhancing cross-task transferability.\n\n### (2) Cross-Modal Semantic Alignment: Bridging Brain and World\nVision-language models (e.g., CLIP) have been adapted to neuroscience to establish a **brain–image–language triad mapping**. Examples include:\n- **BrainCLIP**: Aligns fMRI activity patterns with CLIP’s joint image-text embedding space, enabling universal decoding of natural visual stimuli;\n- **MindEye2**: Retrieves matching real-world images from the LAION-5B database using only fMRI signals;\n- **MinD-Vis**: Uses diffusion models to reconstruct high-fidelity visual scenes, even supporting semantic control.\n\nThis marks a leap from merely “reading” brain activity to integrating it with external semantic knowledge systems.\n\n### (3) Intelligent Upgrading of Clinical Decision Support Systems\nBy combining retrieval-augmented generation (RAG), knowledge graphs, and LLMs, next-generation clinical AI assistants are emerging:\n- **AtlasGPT**: Integrates tens of thousands of pages of neurosurgical literature to provide evidence-based surgical recommendations;\n- **EpiSemoLLM**: Analyzes descriptions of epileptic seizures to predict seizure onset zones with accuracy rivaling human experts;\n- **TRUST**: Enables automated, DSM-5-compliant structured interviews for PTSD assessment.\n\nThese systems transcend simple classifiers, evolving into reasoning-capable, conversational \"digital clinicians.\"\n\n### (4) Biologically Inspired Architectures Feeding Back into AI Development\nNeuroscience is also driving advancements in AI:\n- **Spiking Neural Networks (SNNs)** emulate biological neuronal spiking mechanisms, improving energy efficiency;\n- **Neuromorphic Chips** like Intel Loihi enable low-power, brain-inspired computing;\n- The biological plausibility of **attention mechanisms** is being re-evaluated, inspiring more interpretable AI designs.\n\nThis creates a virtuous cycle: *AI empowers brain science → brain science inspires AI*.\n\n### (5) Synthetic Data Generation to Overcome Data Scarcity\nTo address the lack of labeled data, diffusion models generate realistic synthetic EEG signals (e.g., [17]), while LLMs create human behavioral videos with Alzheimer’s disease (AD) traits (e.g., SHADE-AD). These approaches not only improve model robustness but also open new avenues for studying rare disorders.\n\n---\n\n## 3. Promising Startup Ideas\n\nBased on insights from this review, here are several highly promising entrepreneurial opportunities:\n\n### 🚀 Project One: NeuroSynth Lab — Multimodal Brain-Computer Creative Platform  \n> **Positioning**: A \"mind visualization + content generation\" tool for creative professionals  \n> **Technological Foundation**: fMRI/EEG-to-Image/Text decoding + diffusion models + LLMs  \n> **Proposed Features**:\n> - Capture users’ EEG/fMRI signals in real time as they view abstract art or listen to music;\n> - Decode perceived emotions and mental states to generate stylized sketches or poetic fragments;\n> - Use Stable Diffusion and GPT for secondary creation, producing full artworks, short film scripts, or musical suggestions;\n> - Enable artists to explore subconscious expression for applications in film pre-visualization, advertising, and psychotherapy.\n\n**Market Value**: Breaks the \"black box of inspiration,\" empowering creative industries in film, gaming, and mental health.\n\n---\n\n### 🏥 Project Two: CliniMind AI — Mental Health Screening and Intervention System  \n> **Positioning**: A home-based mental wellness guardian  \n> **Technological Foundation**: EEG-based emotion recognition + online text analysis + conversational LLM  \n> **Product Design**:\n> - Wearable EEG headband monitors daily brainwave changes to detect anxiety or depression tendencies;\n> - Mobile app collects social media posts and voice tone, analyzed by Mental-LLM to assess psychological risk levels;\n> - Integrated ASD-Chat-style intervention bot delivers guided cognitive behavioral therapy (CBT) conversations;\n> - Automatically alerts users and family members upon detecting anomalies, recommending professional care resources.\n\n**Competitive Edge**: Non-invasive, continuous monitoring with proactive intervention—ideal for bridging gaps in grassroots mental health services, especially among youth and working professionals.\n\n---\n\n### ⚕️ Project Three: NeuroFusion Dx — Full-Stack Diagnostic Engine for Neurological Disorders  \n> **Positioning**: An AI-powered diagnostic hub for radiology and neurology departments  \n> **Technological Foundation**: Integrated multimodal foundation models (imaging + genomics + clinical records)  \n> **Solution**:\n> - Accepts patient CT/MRI/EEG/PET scans and electronic health records, routing them through specialized models like BrainGFM, FM-HCT, and LEAD;\n> - Automatically generates structured reports highlighting lesions, predicting Alzheimer’s risk, or localizing epileptogenic zones;\n> - Supports longitudinal tracking of disease progression and recommends personalized treatment plans;\n> - Compliant with DICOM/PACS standards, seamlessly integrating into existing hospital IT infrastructure.\n\n**Business Model**: SaaS subscription or pay-per-use; initial market entry via private hospitals and premium health screening centers.\n\n---\n\n### 🌐 Project Four: OpenBrain Hub — Open Neuroscience Collaboration Cloud Platform  \n> **Positioning**: The GitHub for global neuroscientists  \n> **Core Services**:\n> - Hosts terabyte-scale public brain datasets (e.g., HCP, UK Biobank), supporting standardized BIDS-format uploads;\n> - Provides a library of pre-trained foundation models (e.g., BENDR, BrainLM) with open API access;\n> - Includes a federated learning framework enabling multi-center collaboration without sharing raw data;\n> - Integrates the NeuroBench benchmark suite for fair evaluation of new algorithms.\n\n**Social Impact**: Lowers barriers to research, accelerates discovery translation, and fosters global collaboration in brain initiatives.\n\n---\n\n> In conclusion, this review not only paints a sweeping picture of AI's transformative role in neuroscience but also provides clear guidance for technological innovation and real-world deployment. The future lies in **deeply integrating domain expertise, ensuring data privacy and security, and enhancing model interpretability**, ultimately achieving the grand vision: *understanding the brain to serve humanity*.",
    "github": "",
    "hf": ""
}