{
    "id": "2507.06670",
    "title": "STARS: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation",
    "summary": "This paper presents STARS, a unified framework that simultaneously addresses singing transcription, alignment, and fine-grained style annotation, providing a performance-enhanced and style-controllable dataset for training singing voice synthesis.",
    "abstract": "Recent breakthroughs in singing voice synthesis (SVS) have heightened the demand for high-quality annotated datasets, yet manual annotation remains prohibitively labor-intensive and resource-intensive. Existing automatic singing annotation (ASA) methods, however, primarily tackle isolated aspects of the annotation pipeline. To address this fundamental challenge, we present STARS, which is, to our knowledge, the first unified framework that simultaneously addresses singing transcription, alignment, and refined style annotation. Our framework delivers comprehensive multi-level annotations encompassing: (1) precise phoneme-audio alignment, (2) robust note transcription and temporal localization, (3) expressive vocal technique identification, and (4) global stylistic characterization including emotion and pace. The proposed architecture employs hierarchical acoustic feature processing across frame, word, phoneme, note, and sentence levels. The novel non-autoregressive local acoustic encoders enable structured hierarchical representation learning. Experimental validation confirms the framework's superior performance across multiple evaluation dimensions compared to existing annotation approaches. Furthermore, applications in SVS training demonstrate that models utilizing STARS-annotated data achieve significantly enhanced perceptual naturalness and precise style control. This work not only overcomes critical scalability challenges in the creation of singing datasets but also pioneers new methodologies for controllable singing voice synthesis. Audio samples are available atthis https URL.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Wenxiang Guo,Yu Zhang,Changhao Pan,Zhiyuan Zhu,Ruiqi Li,Zhetao Chen,Wenhao Xu,Fei Wu,Zhou Zhao",
    "subjects": [
        "Sound (cs.SD)",
        "Audio and Speech Processing (eess.AS)"
    ],
    "comments": "Comments:9 pages, 2 figures",
    "keypoint": "• STARS is the first unified framework for singing transcription, alignment, and refined style annotation.\n• It provides comprehensive multi-level annotations including phoneme-audio alignment, note transcription, vocal technique identification, and global stylistic characterization.\n• The framework uses a hierarchical acoustic feature processing architecture across five levels: frame, word, phoneme, note, and sentence.\n• A novel non-autoregressive local acoustic encoder enables structured hierarchical representation learning.\n• Experimental results show STARS outperforms existing methods in multiple evaluation dimensions.\n• STARS-annotated data significantly improves perceptual naturalness and style control in singing voice synthesis (SVS).\n• The model addresses scalability issues in creating singing datasets and introduces new methodologies for controllable SVS.\n• Audio samples are publicly available for demonstration.",
    "date": "2025-07-11",
    "paper": "STARS: A Unified Framework for Singing Transcription, Alignment, and\nRefined Style Annotation\nWenxiang Guo†\nYu Zhang†\nChanghao Pan†\nZhiyuan Zhu\nRuiqi Li\nZhetao Chen\nWenhao Xu\nFei Wu ∗\nZhou Zhao ∗\nZhejiang University\n{guowx314,yuzhang34,zhaozhou}@zju.edu.cn\nAbstract\nRecent breakthroughs in singing voice syn-\nthesis (SVS) have heightened the demand for\nhigh-quality annotated datasets, yet manual an-\nnotation remains prohibitively labor-intensive\nand resource-intensive.\nExisting automatic\nsinging annotation (ASA) methods, however,\nprimarily tackle isolated aspects of the anno-\ntation pipeline. To address this fundamental\nchallenge, we present STARS, which is, to\nour knowledge, the first unified framework\nthat simultaneously addresses singing transcrip-\ntion, alignment, and refined style annotation.\nOur framework delivers comprehensive multi-\nlevel annotations encompassing: (1) precise\nphoneme-audio alignment, (2) robust note tran-\nscription and temporal localization, (3) ex-\npressive vocal technique identification, and (4)\nglobal stylistic characterization including emo-\ntion and pace. The proposed architecture em-\nploys hierarchical acoustic feature processing\nacross frame, word, phoneme, note, and sen-\ntence levels.\nThe novel non-autoregressive\nlocal acoustic encoders enable structured hi-\nerarchical representation learning.\nExperi-\nmental validation confirms the framework’s\nsuperior performance across multiple evalu-\nation dimensions compared to existing anno-\ntation approaches. Furthermore, applications\nin SVS training demonstrate that models uti-\nlizing STARS-annotated data achieve signif-\nicantly enhanced perceptual naturalness and\nprecise style control. This work not only over-\ncomes critical scalability challenges in the cre-\nation of singing datasets but also pioneers new\nmethodologies for controllable singing voice\nsynthesis.\nAudio samples are available at\nhttps://gwx314.github.io/stars-demo/.\n1\nIntroduction\nAutomatic Singing Annotation (ASA) constitutes\nthe computational process of extracting key vo-\n†Equal contribution\n∗Corresponding Author\ncal features from singing recordings, encompass-\ning phonetic transcriptions (phoneme alignment),\nMIDI note parameters (pitch/duration), and stylis-\ntic attributes (emotion/technique). As the corner-\nstone of modern singing voice synthesis (SVS)\nsystems, ASA provides fine-grained, multi-level\nannotated data for training expressive and con-\ntrollable singing synthesis models. While recent\nbreakthroughs in generative models and control-\nlable SVS frameworks (Guo et al., 2025; Zhang\net al., 2024a,b) have dramatically improved the\nquality of generated singing voices, they have also\nparadoxically exposed a critical bottleneck: the\nscarcity of high-quality annotated singing corpora.\nTraditional annotation workflows require labor-\nintensive manual processing by audio engineers\nand musicians, making large-scale dataset creation\nboth costly and time-consuming.\nWhile some\nopen-source singing datasets such as OpenCpop\n(Wang et al., 2022b) and VocalSet (Wilkins et al.,\n2018) have attempted to alleviate this burden, their\nannotations are limited to basic phonetic or vo-\ncal technique information. Recent datasets like\nGTSinger (Zhang et al., 2024c) have made signif-\nicant progress by incorporating a wider range of\nannotations—from basic phoneme and note anno-\ntations to various singing techniques and global\nstyles. However, the volume of data remains insuf-\nficient compared to the scale of speech corpora.\nModern SVS systems require multi-level an-\nnotation precision across four key dimensions:\n(1) microsecond-aligned phoneme boundaries for\nprosody modeling; (2) accurate MIDI note tim-\ning/pitch for melody preservation; (3) phone-level\nvocal technique recognition (e.g., vibrato, falsetto);\nand (4) global stylistic attributes (emotion, pace).\nAs shown in Figure 1, traditional solutions em-\nploy fragmented toolchains—combining ASR sys-\ntems like WhisperX (Bain et al., 2023) and Qwen-\nAudio (Chu et al., 2024) for lyric transcription,\nMFA (McAuliffe et al., 2017) for forced alignment,\narXiv:2507.06670v1  [cs.SD]  9 Jul 2025\nand pitch trackers like VOCANO (Hsu et al., 2021)\nand MusicYOLO (Wang et al., 2022a). This patch-\nwork approach introduces cascading errors from\ntool mismatches while failing to capture expressive\nvocal styles. Such disjointed processes hinder the\ncreation of large, high-quality annotated singing\ndatasets necessary for cutting-edge SVS models.\nTo overcome these challenges, we propose\nSTARS, a unified framework for multi-level\nsinging voice annotation that streamlines the entire\nprocess. STARS offers three key innovations: (1)\nA multi-level architecture for extracting singing in-\nformation at various granularities, covering frame,\nword, phoneme, note, and sentence levels; (2) A\nlocal acoustic encoder that works together with\na CMU encoder employing a U-Net architecture\nwith Conformer blocks and FreqMOE, and with\na vector quantization, to extract acoustic features\nat multiple hierarchical levels; and (3) A multi-\ntask automated annotation pipeline that sequen-\ntially predicts phoneme boundaries, note bound-\naries, pitch, phone-level techniques, and global\nstylistic attributes. We demonstrate the effective-\nness of STARS through comprehensive evalua-\ntions across multiple ASA tasks. Our framework\nachieves superior performance in phoneme align-\nment accuracy and note prediction precision. When\napplied to SVS model training, STARS-annotated\ndata yields significant improvements in synthesized\nvoice naturalness and style control accuracy.\n• We propose STARS, the first unified frame-\nwork for Singing Transcription, Alignment,\nand Refined Style Annotation.\n• We design a five-level unified architecture\nwith specialized acoustic encoders for hier-\narchical feature filtering and extraction.\n• We implement parallel prediction strategies\nfor phoneme/note boundaries and pitch esti-\nmation, enhanced through phone-level tech-\nnique and global style detection.\n• Through training SVS models with our anno-\ntations, we demonstrate the practical utility of\nSTARS in achieving superior singing vocal\nnaturalness and precise style control.\n2\nRelated Works\n2.1\nSinging Voice Synthesis\nSinging Voice Synthesis (SVS) aims to generate ex-\npressive singing voices from musical scores. Early\nmodels such as XiaoiceSing (Lu et al., 2020) adopt\nnon-autoregressive acoustic architectures inspired\nby FastSpeech (Ren et al., 2019). Subsequent work,\nViSinger (Zhang et al., 2022), employs VITS (Kim\net al., 2021) to establish an end-to-end SVS frame-\nwork. Generative adversarial networks (Wu and\nLuan, 2020; Huang et al., 2022b) and diffusion\nmodels (Liu et al., 2022) have also been applied\nto enhance audio fidelity. Controllable SVS fo-\ncuses on manipulating vocal attributes including\ntimbre, emotion, and style to achieve more expres-\nsive performances. Resna and Rajan (2023) de-\nvelops a multi-singer framework for cross-voice\nsynthesis. Muse-SVS (Kim et al., 2023) enables\nprecise control over pitch, energy, and phoneme\nduration to express different emotional intensities.\nPrompt-Singer (Wang et al., 2024) introduces natu-\nral language prompts to achieve fine-grained con-\ntrol over singing voices. To mitigate data scarcity,\nDeepSinger (Ren et al., 2020) constructs a large-\nscale corpus by mining singing data from online\nsources. Additionally, OpenCpop (Wang et al.,\n2022b) and GTSinger (Zhang et al., 2024c) pro-\nvide publicly available corpora with manually anno-\ntated singing recordings. Nevertheless, the limited\navailability of high-quality singing data remains a\ncritical bottleneck compared to speech resources.\n2.2\nAutomatic Singing Annotation\nAutomatic Singing Annotation (ASA) includes\ntasks such as lyric alignment, note estimation and\nsegmentation, and vocal technique and style anno-\ntation. The MFA (McAuliffe et al., 2017) is a con-\nventional approach for lyric alignment. However,\nsinging voice alignment remains challenging due to\nthe large variations in phoneme durations and rhyth-\nmic structures. Several studies (Wang et al., 2023;\nHuang et al., 2022a) adopt Viterbi forced alignment\n(Forney, 1973) to improve the accuracy of aligning\nposteriograms with lyrics. For note estimation, VO-\nCANO (Hsu et al., 2021) and MusicYOLO (Wang\net al., 2022a) directly predict the pitch and dura-\ntion of musical notes. ROSVOT (Li et al., 2024)\nincorporates phoneme boundary priors to achieve\nMIDI note prediction. SongTrans (Wu et al., 2024)\nbuilds upon the Whisper model and adopts a hy-\nbrid autoregressive and non-autoregressive frame-\nwork for phoneme and note annotation. MusCaps\n(Manco et al., 2021) leverages a language model\nto generate music captions, but its effectiveness is\nlimited for singing voice recordings without back-\nground music. Despite these advancements, exist-\nText\nAlignment\nTech & Style \nAnnotation\nNote \nTranscription\nSTARS\nTextGrid\nTextGrid\nMusicXml\nLyric\nYou raise me up,\nso I can stand …\nwav\nLyric\nYou raise me up,\nso I can stand …\nwav\nFigure 1: STARS vs. Traditional Stepwise Pipeline. Conventional stepwise processing requires sequential execution\nof text alignment, note transcription, and manual technique and style annotation, with error propagation across\ncascaded modules. STARS establishes unified acoustic-linguistic modeling for simultaneous phoneme, MIDI,\ntechnique, and style prediction, eliminating error accumulation via end-to-end joint optimization.\ning ASA methods remain fragmented, requiring\nseparate models and manual integration.\n3\nSTARS\n3.1\nProblem Formulation\nTypically, a Mel-spectrogram M ∈RT×F is de-\nrived from the audio signal using the short-time\nFourier transform (STFT), where T denotes the\nnumber of frames and F the number of frequency\nbins. Let the phoneme sequence be represented as\np = [p1, p2, . . . , pLp], where Lp is the number of\nphonemes. In addition to predicting the phoneme\nsequence, the model is required to predict the cor-\nresponding boundaries for each phoneme, repre-\nsented as ph_bd = [pbd1, pbd2, . . . , pbdT ], where\npbdi = 1 indicates that frame i is a phoneme\nboundary and 0 otherwise. Furthermore, for each\nphoneme, the model predicts a set of singing tech-\nniques.\nFor each technique i ∈{1, . . . , 9}, a\nbinary sequence techi = [ti\n1, ti\n2, . . . , ti\nLp] is pre-\ndicted, where ti\nj = 1 indicates the presence of\ntechnique i at the j-th phoneme and ti\nj\n= 0\nindicates its absence.\nBased on the predicted\nphoneme and word boundaries, note boundaries are\npredicted as note_bd = [nbd1, nbd2, . . . , nbdT ].\nThe model also predicts the note pitch sequence\nc = [c1, c2, . . . , cLn], where Ln denotes the num-\nber of notes. In addition to the phoneme, note,\nand technique predictions, the model is required to\npredict global sentence-level attributes g.\n3.2\nOverview\nFigure 2 illustrates the overall architecture of\nSTARS. The input to our system consists of Mel-\nspectrograms and F0 extracted from the audio sig-\nnal, and the corresponding lyrics can be obtained\nusing ASR models such as Whisper (Radford et al.,\n2023). To improve the robustness of our model,\nwe follow the approach of ROSVOT (Li et al.,\n2024) by adding realistic noise from the MUSAN\ndataset (Snyder et al., 2015) to the input audio\nand injecting Gaussian noise into the extracted\nF0 contour. In this section, we first describe the\nunified multi-level framework. To capture multi-\nlevel acoustic and stylistic information, we design\na hierarchical architecture that spans five levels:\nFrame, Word, Phone, Note, and Sentence. Each\nlevel shares the same backbone while employing\nslightly different methods to efficiently extract fea-\ntures at varying granularities. Next, we explain how\nall sub-tasks are completed in a single forward pass.\nTo obtain the Phone and Word boundaries, we first\npredict the frame-level phoneme logits from the\nfeatures extracted by the Frame-level encoder. We\nthen apply Viterbi forced alignment to determine\nthe phoneme and word boundaries. For note bound-\naries, we utilize features from the previous three\nlevels to predict the note boundaries. Having ex-\ntracted features at the Note level, we can predict the\ncorresponding note pitch. Finally, leveraging the\ninformation from all five levels, we predict various\nsinging techniques and global attributes.\n3.3\nUnified Multi-Level Framework\nTo achieve unified annotation predictions at multi-\nple levels within a single model, we design a Uni-\nfied Multi-Level Framework consisting of five hi-\nerarchical levels: Frame, Word, Phone, Note, and\nSentence. Each level extracts acoustic features at\nSentence Level\nAcoustic Encoder\nNote Level \nAcoustic Encoder\nFrame Level \nAcoustic Encoder\nWord Level \nAcoustic Encoder\nPhone Level \nAcoustic Encoder\nTechnique Decoder\n…\nMulti Technique\nASR\nNote Boundary \nPredictor\nNote Decoder\nGlobal Style Predictor\nPhone Predictor\nDP Alignment\nBoundary\nNote Boundary\nPhone Boundary\nWord Boundary\nNote\nPhone\n…\n…\n…\n…\n…\n…\nPhone Bd\nPhone Logits\nNote Bd\n…\nGlobal Style (Emotion, \nPace…)\nCMU Encoder\nPooling\nConv\nVector Quantization\nLength Regulator\nMel Encoder\nDownsample Layers\nUpsample Layers\nFreqMOE Module  ×1/2\nConv\nSelf-Attention\nFreqMOE Module  ×1/2\nLayer Norm\n× 𝑵\nCross Attention\nProject\nProject\nCLS\nCLS\nK,V\nQ\n（a)  Overall Architecture\n（c) Multi-Level Acoustic Encoder\n（d) CMU Encoder\n（b) Global Style Predictor\nLyric: you raise me up …\n𝑆௙\n𝑆௪\n𝑆௣\n𝑆௡\n𝑆௚\nBoundary\nMel & F0 Extractor\n…\n𝑆\n𝑆\nF0\n…\nF0 Encoder\nFigure 2: The overall architecture of STARS. (a) The unified multi-level framework, which integrates singing lyric\nalignment, note transcription, technique prediction, and global style prediction. (b) The Global Style Predictor,\nwhere [CLS] tokens are used as queries, and S represents the keys and values. (c) The multi-level acoustic encoder\nextracts features at each level, with optional pooling based on boundary segmentation. (d) The CMU Encoder,\nemploying a U-Net architecture with Conformer blocks and FreqMOE for efficient audio feature extraction.\ndifferent granularities. The framework employs\na shared acoustic encoder across all levels to en-\nable efficient feature extraction. We first design a\nhighly efficient CMU Encoder to extract features,\nfollowed by multi-granularity pooling operations.\nVector quantization serves as a bottleneck (Van\nDen Oord et al., 2017) to eliminate irrelevant infor-\nmation. We then use the boundary to expand the\nextracted features to the frame length T.\nThe CMU Encoder module utilizes a U-Net-\nbased architecture for Mel-spectrogram downsam-\npling while preserving spectral details through skip\nconnections during upsampling. To capture both\nlong-term and short-term dependencies in the time\ndimension, we integrate the Conformer architec-\nture (Gulati et al., 2020), which demonstrates supe-\nrior performance in the ASR tasks. For enhanced\nfrequency analysis, we design the FreqMOE mod-\nule that partitions the frequency dimension into K\nequal frequency bands and applies the specialized\nexperts to distinct frequency bands. We then con-\ncatenate the output embedding chunks. Specific\ndetails are provided in the Appendix A.2.\nFor the pooling module, no pooling is applied\nto the frame-level features, as this level represents\nthe finest granularity. For each intermediate level,\nwe utilize the corresponding boundary informa-\ntion to perform segmentation, followed by average\npooling within these boundaries to obtain dynamic\nfeature sequences. At the sentence level, we ap-\nply global average pooling to the CMU Encoder\noutputs to obtain the holistic representation.\nFor hierarchical representation learning, we ap-\nply vector quantization to the intermediate features\nfrom all levels except at the Frame and Sentence\nlevels. Let Sl ∈RL×D denote the input latent\nembeddings for level l, where L is the sequence\nlength and D is the feature dimension. Each level\nmaintains a codebook ql ∈RK×D containing K\nlatent embeddings. Following (Van Den Oord et al.,\n2017), we also apply a commitment loss to ensure\nthat the representation sequence commits to an em-\nbedding and to prevent the output from growing:\nLcommit = ∥zl(Sl) −sg[ql]∥2\n2,\n(1)\nwhere zl(.) is the vector quantization module for\nlevel l, and sg denotes the stop gradient operator.\nIn the Length Regulator module, at each level\nl ∈{word, phone, note}, we use the boundary in-\nformation to determine the frame length of each\nsegment. To align hierarchical representations, we\nrepeat each embedding according to the length of\nthe segment. At the Sentence level, we extend the\nsingle embedding to match the frame length T.\n3.4\nLyric Alignment\nPhonemes are the smallest phonetic units and the\nmost commonly used tokens in singing voice syn-\nthesis. To align phonemes with the audio, we in-\nput the frame-level extracted features, denoted as\nSf, into the phone predictor. This predictor gen-\nerates predictions for the phoneme being sung at\neach frame, along with indications of whether a\nframe corresponds to a phoneme boundary. During\ntraining, we optimize the phone predictions using\ncross-entropy loss, and we also apply Connectionist\nTemporal Classification (CTC) loss (Graves et al.,\n2006) to further improve phoneme alignment.\nLCE = −\nP\nX\nk=1\nyk log(pk),\n(2)\nLCTC = −log\nX\nπ∈B−1(y)\nP(π|X),\n(3)\nwhere P is the total number of phonemes, and\nπ = [π1, . . . , πT ] represents an alignment path\nwith πt ∈V ∪{blank} (where V is the total\nphoneme vocabulary). Additionally, we use BCE\nloss to predict the phone boundaries.\nDuring inference, we employ the Viterbi forced\nalignment (Forney, 1973) method to align the lyric\nphoneme sequence with the phone probability dis-\ntribution at each frame. This process provides the\nphoneme boundaries, and through the phone-to-\nword relationship, we further determine the word\nboundaries. Specific details of the inference algo-\nrithms are provided in the Appendix B.\n3.5\nNote Transcription and Alignment\nTo obtain note boundaries, we first fuse features\nfrom the frame, word, and phone levels and feed\nthem into a note boundary predictor. The train-\ning loss for note boundary prediction is defined as\nthe BCE loss, similar to phone boundary predic-\ntion. During inference, since the word boundaries\noverlap with the note boundaries, we employ these\nboundaries as constraints when generating the final\nnote boundaries. For pitch prediction at the note\nlevel, we integrate the aggregated features from\nthe frame, word, and phone levels with note-level\nfeatures. Specifically, for each note j, we denote\nits feature sequence as Sj\nn ∈RLj×D, where Lj\nis the number of frames in the note segment and\nD is the feature dimension. We then use the note\ndecoder to predict the pitch of each note. Inspired\nby CIF (Dong and Xu, 2020), we also compute\nthe weight vector for each note as Wn = Sj\nnWa,\nwhere Wa is a learnable projection matrix and\nWn ∈RLj×1. We then obtain the aggregated note\nrepresentation cj via a weighted average:\ncj =\nLj\nX\nt=1\nwn(t) Sj\nn(t),\n(4)\nwhere wn(t) denotes the weight at frame t.\nWe then use the pitch predictor to compute the\nlogits for the P pitch categories ˆpj = cj WO,\nwhere WO ∈RD×P . The pitch predictor is also\noptimized using cross-entropy loss.\n3.6\nTechnique and Global Style Predictor\nTo predict the possible techniques for each\nphoneme, we treat this task as a multi-task, multi-\nlabel binary classification problem. The technique\nprediction head outputs predictions across nine cat-\negories: mixed, falsetto, strong, weak, glissando,\nbreathy, bubble, vibrato, and pharyngeal. For each\nphoneme j, we predict the i-th technique techi\nj,\nwhere 1 indicates the presence of the technique and\n0 indicates its absence. Features from the Frame,\nWord, Phone, Note, and Sentence levels are input\ninto the model, with the previously obtained phone\nboundaries used as references. We apply the same\nattention-based weighted average strategy used for\npitch prediction to predict each technique sequence.\nBinary cross-entropy (BCE) loss is used to opti-\nmize each technique prediction module.\nFor the global attributes of the sentence, includ-\ning language, gender, emotion, pace, and range, we\ntreat these as multi-class classification tasks. To\npredict these global attributes, we introduce five\n[CLS] tokens, each corresponding to one of the\ntasks. These tokens are used as queries Hc, while\nthe sum of the frame-level features from all levels,\ni.e., Sf, Sw, Sp, Sn, and Sg, serves as the key and\nvalue in the cross-attention mechanism (Vaswani,\n2017). Positional encoding embeddings are added\nto the features to determine the position of each\ntoken. The formulation is as follows:\nS = Sf + Sw + Sp + Sn + Sg,\nAttention(Hc, S, S) = Softmax\n\u0012HcST\n√\nD\n\u0013\nS,\n(5)\nwhere D is the dimension of the query, key and\nvalue. Then, we predict each task’s category gi ∈\nRCi, where Ci is the number of categories for the\ni-th attribute. Cross-entropy (CE) loss is used to\ntrain and optimize the global style predictors.\n3.7\nTraining and Inference Procedures\nDuring training, we leverage ground truth phone,\nword, and note boundaries to guide the model. The\nfinal loss function consists of the following compo-\nnents: 1) Lph: A combination of phone-level CTC\nloss and CE loss; 2) Lpbd and Lnbd: Boundary pre-\ndiction losses for the phone and note, respectively,\noptimized using BCE loss; 3) Lpi: Pitch prediction\nloss, calculated as the CE loss between the ground\ntruth pitch and the predicted pitch; 4) Ltech: Tech-\nnique prediction loss, calculated as the CE loss; 5)\nLg: The global attribute prediction loss, optimized\nusing CE loss; and 6) Lc: The commitment loss,\nwhich constrains the vector quantization layer.\nDuring inference, we first obtain the phonemes\nand words from the input lyrics (or ASR model-\ngenerated lyrics) and use the frame-level predicted\nphoneme logits with Viterbi forced alignment to\ndetermine the phoneme boundaries. Note bound-\naries are then predicted by feeding fused features\nfrom the frame, word, and phone levels into the\nnote boundary predictor. The note-level features,\ntogether with the fused features, are provided as in-\nput to the note-level acoustic encoder and the note\ndecoder to predict the note pitch. Next, features\nfrom all levels, along with the phoneme boundaries,\nare used to predict the techniques for each phoneme.\nFinally, the global attributes are predicted using the\naggregated features from all levels.\n4\nExperiments\n4.1\nExperimental Setup\n4.1.1\nDataset and Process\nThe dataset used in our experiments includes the\nChinese and English subsets of GTSinger (Zhang\net al., 2024c). This dataset provides alignments and\nannotations in TextGrid files, which include word\nboundaries, phoneme boundaries, phoneme-level\nannotations for six techniques (mixed, falsetto, pha-\nryngeal, glissando, vibrato, breathy), and global\nstyle labels such as emotion, pace, and pitch range.\nAdditionally, we have collected and annotated a\n30-hour Chinese dataset featuring two singers and\nfour technique annotations (mixed, falsetto, strong,\nweak, breathy, bubble) at both the phoneme and\nsentence levels. To train our automated annota-\ntion model, we reserve 30 songs containing various\ntechniques and global styles as the validation and\ntest sets. To ensure the robustness of the model,\nwe augment the dataset by adding noise from the\nMUSAN noise corpus (Snyder et al., 2015). For\nChinese lyrics, we use the pypinyin tool1 to phone-\nmize the text, while for English lyrics, we follow\nthe ARPA2 standard for phoneme transcription.\n4.1.2\nImplementation Details\nThe singing audio recordings are sampled at 24\nkHz, with a window size of 512 samples, a hop\nsize of 128, and 80 mel bins for Mel-spectrogram\nextraction. We use a pre-trained RMVPE (Wei\net al., 2023) model to extract the F0 contours. The\nU-Net backbone consists of four downsampling\nand upsampling layers, with a total downsampling\nfactor of 16×. The Conformer module includes two\nlayers, and the FreqMOE consists of four experts.\nIn this experiment, the model is trained for 150k\nsteps using an NVIDIA 4090 GPU. Further imple-\nmentation details are provided in Appendix A.1.\n4.1.3\nEvaluation Details\nFor lyric alignment, we evaluate performance using\ntwo metrics: Boundary Error Rate (BER) and In-\ntersection Over Union (IOU) score. BER measures\nthe proportion of misplaced boundaries within\n20ms tolerance. The IOU score is defined as the\nratio of the duration of the overlapping segment\nbetween two notes to the duration of the combined\ntime span covered by both notes. For note transcrip-\ntion, we use the mir_eval library (Raffel et al.,\n2014) and apply the metrics COnPOff (correct on-\nset, pitch, and offset) proposed in (Molina et al.,\n2014) and Raw Pitch Accuracy (RPA) for overall\nnote pitch prediction performance. For phone-level\ntechnique and global style recognition, we use ob-\njective metrics including F1 score and accuracy to\nevaluate the phone-level technique predictor, and\naccuracy for the global style detector. The results\nare multiplied by 100 for better readability. Further\ndetails are provided in Appendix C.2.\n4.1.4\nBaseline Models\nTo evaluate our approach, we compare it with sev-\neral baseline systems across different sub-tasks. We\nconduct the comparison using only Chinese data,\nand additionally, we test the model’s performance\n1https://github.com/mozillazg/python-pinyin\n2https://en.wikipedia.org/wiki/ARPABET\nMethod\nBER ↓\nIOU ↑\nMFA\n40.3\n56.8\nSOFA\n20.9\n80.0\nSTARS (ours)\n18.6\n80.9\nTable 1: Results for lyric alignment.\nMethod\nCOnPOff(F) ↑\nRPA ↑\nVOCANO\n50.2\n76.6\nROSVOT\n70.2\n83.8\nSTARS (ours)\n71.0\n86.7\nTable 2: Results for note transcription and alignment.\nSetting\nMetric\nPhone-level Technique and Global Style Prediction\nBUB\nBRE\nPHA\nVIB\nGLI\nMIX\nFAL\nWEA\nSTR\nTEC\nSTY\nGTSinger\nF1\n46.9\n68.7\n88.7\n95.7\n78.5\n61.5\n33.2\n37.2\n82.5\n67.3\n-\nACC\n31.5\n73.2\n75.7\n99.3\n78.9\n93.9\n40.8\n17.4\n95.3\n65.9\n-\nSTARS\nF1\n71.7\n66.9\n85.0\n65.5\n72.3\n74.7\n93.5\n90.3\n99.4\n79.9\n-\nACC\n97.8\n88.8\n95.4\n96.7\n84.1\n81.9\n94.7\n90.4\n93.9\n91.5\n68.0\nTable 3: The objective results of phone-level technique prediction. The singing techniques include BUB (bubble),\nBRE (breathy), PHA (pharyngeal), VIB (vibrato), GLI (glissando), MIX (mixed), FAL (falsetto), WEA (weak), and\nSTR (strong). The \"TEC\" column represents the average metrics calculated across all the singing techniques, while\nthe \"STY\" column represents the average metrics calculated across all the global style attributes.\non multilingual data by combining both Chinese\nand English datasets. For the phoneme and singing\naudio alignment, we consider: 1) Montreal Forced\nAlign (MFA): a tool that aligns orthographic and\nphonological forms by leveraging a pronunciation\ndictionary to time-align transcribed audio files; 2)\nSOFA 3: a forced alignment tool designed specif-\nically for the singing voice. For note alignment\nand transcription, we compare our model’s per-\nformance with the results reported in ROSVOT,\nincluding two strong baselines:1) VOCANO: a\nnote transcription framework developed for the\nsinging voice in polyphonic music; 2) ROSVOT: a\nmodel that employs a multi-scale architecture for\nautomatic singing transcription. We compare with\nthe variant without word boundary condition. For\nphone technique prediction, we use GTSinger’s\ntechnique predictor as the baseline.\n4.2\nASA Results\n4.2.1\nLyric Alignment and Note Transcription\nIn Table 1, we observe that for the lyric alignment\ntask, when comparing our model with the other\ntwo models, STARS achieves the best performance\nin both the BER and IOU metrics, indicating its\nability to accurately predict phoneme boundary in-\nformation. In Table 2, we see that for the note\nalignment and transcription task, our model out-\nperforms the baseline models in both COnPOff(F)\nand RPA metrics, demonstrating its sensitivity to\nnote boundaries and pitch. Notably, while the other\n3https://github.com/qiuqiao/SOFA\nmodels are designed to handle only a single task,\nour model efficiently handles both lyric alignment\nand note transcription tasks simultaneously. This\ndemonstrates the versatility and effectiveness of\nSTARS in performing multiple singing annotation\ntasks, making it highly suitable for foundational\nautomatic singing annotation applications.\n4.2.2\nTechnique and Global Style Prediction\nAs Table 3 shows, for the recognition of the nine\nvocal techniques, our experimental results outper-\nform GTSinger. No individual technique shows a\nsignificantly low recognition accuracy. The aver-\nage F1-score and accuracy for all techniques far\nexceed the GTSinger benchmark, demonstrating\nour model’s ability to accurately detect and anno-\ntate multiple techniques at the phoneme level. Also,\nas the last column of the table indicates, our model\nachieves high accuracy in recognizing global style\nattributes, further showcasing its effectiveness in\ncapturing the overall stylistic features of singing\naudio. For detailed scores of each style attribute,\nsee the Appendix C.3. In summary, our model ef-\nfectively detects expressive information, and the\ngenerated labels can be used in various controllable\nexpressive singing voice synthesis tasks.\n4.2.3\nAblation Study\nIn this section, we conduct ablation experiments to\nevaluate the contributions of different components\nin our model. We test the following variants: 1)\nw/o CTC: the model without the CTC loss; 2) w/o\nVQ: the model without vector quantization; 3) w/o\nMethod\nBER ↓\nIOU ↑\nCOnPOff(F) ↑\nRPA ↑\nT-F1 ↑\nT-ACC ↑\nS-ACC ↑\nSTARS\n18.6\n80.9\n71.0\n86.7\n79.9\n91.5\n68.0\nw/o CTC\n19.1\n80.0\n70.9\n86.7\n79.7\n89.8\n66.8\nw/o VQ\n18.3\n80.9\n70.7\n86.4\n76.3\n90.4\n65.3\nw/o MOE\n18.9\n80.5\n70.1\n86.5\n77.4\n90.0\n69.8\nConv\n20.1\n78.3\n66.4\n82.7\n62.9\n89.6\n66.6\nBilingual\n19.4\n75.6\n68.1\n86.2\n76.8\n91.4\n70.4\nTable 4: The ablation results for different sub-tasks. T-F1 is the average F1-score for all singing techniques, T-ACC\nis the average accuracy for all singing techniques, and S-ACC is the average accuracy for all global style attributes.\nTrain\nInfer\nMOS-Q ↑\nMOS-C ↑\nGT\nGT\n3.98 ± 0.09\n4.05 ± 0.09\nGT\nPred\n3.91 ± 0.07\n3.95 ± 0.08\nPred\nPred\n3.89 ± 0.11\n3.95 ± 0.05\n1/2 GT\nGT\n3.83 ± 0.04\n3.89 ± 0.10\nMix\nMix\n3.93 ± 0.06\n3.98 ± 0.05\nTable 5: Results of SVS. GT refers to the ground truth,\nPred indicates the results from our ASA model, and Mix\nrepresents a mix where half of the data is automatically\nannotated and the other half is ground truth.\nMOE: the model without the FreqMOE strategy;\n4) Conv: the model with the Conformer module re-\nplaced by a convolutional architecture; 5) Bilingual:\nthe model trained on bilingual datasets.\nAs shown in Table 4, several conclusions can be\ndrawn. Comparing the first row with the w/o CTC\nvariant, we observe improvements of 0.5 and 0.9\nin the BER and IOU metrics for lyric alignment,\nrespectively, along with enhanced performance in\nthe technique recognition task, which is sensitive\nto phoneme boundaries, while the note recognition\nmetrics remain similar. These results indicate that\nCTC loss notably enhances phoneme alignment.\nWhen VQ is omitted for the phone, note, and word\nlevels, the note, technique, and style recognition\ntasks show improved performance. For phoneme\nalignment, the lighter model with reduced VQ loss\ntraining further improves phoneme boundary de-\ntection. Comparisons of the w/o MOE and Conv\nvariants reveal a drop in performance across all\nmetrics, demonstrating the effectiveness of Finally,\nexperiments on bilingual datasets confirm that our\nmodel operates effectively in a multilingual setting.\nDetailed results for single-language and bilingual\nexperiments can be found in Appendix C.3.\n4.3\nSinging Voice Synthesis\nTo further validate STARS’s effectiveness, we use\nSTARS to annotate GTSinger’s Chinese part and\nour data, and employ the latest style-controllable\nSVS model TCSinger(Zhang et al., 2024b) for gen-\neration tasks. We use MOS-Q for quality and natu-\nralness assessment, and MOS-C for expressiveness\nevaluation of the generated singing’s style control.\nAs shown in Table 5, the following conclusions\ncan be drawn: By comparing the first two rows,\nwhere training is performed on ground-truth, we\nobserve that using our model’s annotations yields\nMOS-C and MOS-Q scores nearly identical to\nthose obtained with ground-truth annotations. Sim-\nilarly, comparing the second and third rows, which\ncorrespond to training with ground-truth versus our\nmodel’s annotations, and evaluating with our pre-\ndicted results, shows only minimal differences in\nMOS-C and MOS-Q scores. These findings indi-\ncate that training exclusively with our annotated\ndata achieves performance comparable to using\nground-truth annotations, demonstrating the effec-\ntiveness of our fully automated annotation model.\nFurthermore, comparing the last two rows—where\ntraining is conducted with half ground-truth data\nversus a mixture of half ground-truth and half pre-\ndicted annotations—reveals an improvement in\nperformance. This suggests that augmenting the\ndataset with our model’s predictions can effectively\nenhance overall SVS model performance.\n5\nConclusion\nIn this paper, we introduce STARS, the first uni-\nfied framework for Singing Transcription, Align-\nment, and Refined Style Annotation. We construct\na multi-level framework that efficiently extracts\naudio features at five granularities—frame, word,\nphone, note, and sentence—using a hierarchical\nacoustic encoder. Our approach enables a complete\nautomatic singing annotation pipeline, sequentially\nperforming singing Lyric alignment, note transcrip-\ntion and alignment, phone-level technique predic-\ntion, and global style prediction. Experimental\nresults demonstrate that our model achieves high\nperformance across all sub-tasks, and the annotated\noutputs are further validated in a singing synthesis\ntask, confirming the effectiveness of our approach.\n6\nLimitations\nOur model has two main limitations. First, it cur-\nrently only classifies global style attributes of the\nsinging voice and generates simple captions us-\ning predefined templates. In the future, we aim to\nenhance this capability by connecting the model\nto large language models, enabling more dynamic\nand context-aware caption generation. Second, the\nmodel has been validated solely on Chinese and\nEnglish datasets. Further validation on datasets\nfrom other languages is necessary to assess its gen-\neralizability. In future work, we plan to extend the\nmodel to singing data in additional languages.\n7\nEthics Statement\nThe automatic singing annotation model may be\nsubject to misuse in the following ways. The model\ncould be used to generate synthetic singing voices\nthat closely resemble real individuals or artists with-\nout their consent, potentially leading to concerns\naround authenticity and intellectual property. As\nthe model is trained primarily on English and Chi-\nnese datasets, its performance on other languages\nor diverse cultural contexts may be limited, po-\ntentially resulting in biased or inaccurate annota-\ntions for non-target languages or dialects. To miti-\ngate these issues, we encourage transparent usage,\nproper attribution, and the continued development\nof ethical guidelines for synthetic media generation.\nAdditionally, we also plan to explore the ways to\nmake the model more inclusive and adaptable to a\nbroader range of languages and contexts.\nAcknowledgement\nThis work was supported by National Natu-\nral Science Foundation of China under Grant\nNo.62222211 and National Natural Science Foun-\ndation of China under Grant No.U24A20326.\nReferences\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zis-\nserman. 2023.\nWhisperx: Time-accurate speech\ntranscription of long-form audio.\narXiv preprint\narXiv:2303.00747.\nYunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei,\nZhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng\nHe, Junyang Lin, et al. 2024. Qwen2-audio technical\nreport. arXiv preprint arXiv:2407.10759.\nLinhao Dong and Bo Xu. 2020.\nCif: Continuous\nintegrate-and-fire for end-to-end speech recognition.\nIn ICASSP 2020-2020 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6079–6083. IEEE.\nG David Forney. 1973. The viterbi algorithm. Proceed-\nings of the IEEE, 61(3):268–278.\nAlex Graves, Santiago Fernández, Faustino Gomez, and\nJürgen Schmidhuber. 2006. Connectionist temporal\nclassification: labelling unsegmented sequence data\nwith recurrent neural networks. In Proceedings of the\n23rd international conference on Machine learning,\npages 369–376.\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, et al.\n2020. Conformer: Convolution-augmented trans-\nformer for speech recognition.\narXiv preprint\narXiv:2005.08100.\nWenxiang Guo, Yu Zhang, Changhao Pan, Rongjie\nHuang, Li Tang, Ruiqi Li, Zhiqing Hong, Yongqi\nWang, and Zhou Zhao. 2025. Techsinger: Technique\ncontrollable multilingual singing voice synthesis via\nflow matching. arXiv preprint arXiv:2502.12572.\nJui-Yang Hsu, Li Su, et al. 2021. Vocano: A note tran-\nscription framework for singing voice in polyphonic\nmusic.\nJiawen Huang, Emmanouil Benetos, and Sebastian Ew-\nert. 2022a. Improving lyrics alignment through joint\npitch detection. In ICASSP 2022-2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 451–455. IEEE.\nRongjie Huang, Chenye Cui, Feiyang Chen, Yi Ren,\nJinglin Liu, Zhou Zhao, Baoxing Huai, and Zhefeng\nWang. 2022b. Singgan: Generative adversarial net-\nwork for high-fidelity singing voice generation. In\nProceedings of the 30th ACM International Confer-\nence on Multimedia, pages 2525–2535.\nJaehyeon Kim, Jungil Kong, and Juhee Son. 2021.\nConditional variational autoencoder with adversar-\nial learning for end-to-end text-to-speech. In Inter-\nnational Conference on Machine Learning, pages\n5530–5540. PMLR.\nSungjae Kim, Yewon Kim, Jewoo Jun, and Injung Kim.\n2023. Muse-svs: Multi-singer emotional singing\nvoice synthesizer that controls emotional intensity.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing.\nRuiqi Li, Yu Zhang, Yongqi Wang, Zhiqing Hong,\nRongjie Huang, and Zhou Zhao. 2024.\nRobust\nsinging voice transcription serves synthesis. Preprint,\narXiv:2405.09940.\nJinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and\nZhou Zhao. 2022. Diffsinger: Singing voice synthe-\nsis via shallow diffusion mechanism. In Proceedings\nof the AAAI conference on artificial intelligence, vol-\nume 36, pages 11020–11028.\nPeiling Lu, Jie Wu, Jian Luan, Xu Tan, and Li Zhou.\n2020. Xiaoicesing: A high-quality and integrated\nsinging voice synthesis system.\narXiv preprint\narXiv:2006.06261.\nIlaria Manco, Emmanouil Benetos, Elio Quinton, and\nGyörgy Fazekas. 2021. Muscaps: Generating cap-\ntions for music audio. In 2021 International Joint\nConference on Neural Networks (IJCNN), pages 1–8.\nMichael McAuliffe, Michaela Socolof, Sarah Mihuc,\nMichael Wagner, and Morgan Sonderegger. 2017.\nMontreal forced aligner: Trainable text-speech align-\nment using kaldi. In Interspeech, volume 2017, pages\n498–502.\nEmilio\nMolina,\nAna\nMaria\nBarbancho-Perez,\nLorenzo Jose Tardon-Garcia, Isabel Barbancho-\nPerez, et al. 2014.\nEvaluation framework for\nautomatic singing transcription.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak su-\npervision. In International conference on machine\nlearning, pages 28492–28518. PMLR.\nColin Raffel, Brian McFee, Eric J Humphrey, Justin\nSalamon, Oriol Nieto, Dawen Liang, Daniel PW El-\nlis, and C Colin Raffel. 2014. Mir_eval: A trans-\nparent implementation of common mir metrics. In\nISMIR, volume 10, page 2014.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu. 2019. Fastspeech: Fast,\nrobust and controllable text to speech. Advances in\nneural information processing systems, 32.\nYi Ren, Xu Tan, Tao Qin, Jian Luan, Zhou Zhao, and\nTie-Yan Liu. 2020. Deepsinger: Singing voice syn-\nthesis with data mined from the web. In Proceed-\nings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, pages\n1979–1989.\nS Resna and Rajeev Rajan. 2023. Multi-voice singing\nsynthesis from lyrics. Circuits, Systems, and Signal\nProcessing, 42(1):307–321.\nDavid Snyder, Guoguo Chen, and Daniel Povey. 2015.\nMusan: A music, speech, and noise corpus. arXiv\npreprint arXiv:1510.08484.\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural\ndiscrete representation learning. Advances in neural\ninformation processing systems, 30.\nA Vaswani. 2017. Attention is all you need. Advances\nin Neural Information Processing Systems.\nJun-You Wang, Chon-In Leong, Yu-Chen Lin, Li Su,\nand Jyh-Shing Roger Jang. 2023.\nAdapting pre-\ntrained speech model for mandarin lyrics transcrip-\ntion and alignment. In 2023 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU),\npages 1–8. IEEE.\nXianke Wang, Bowen Tian, Weiming Yang, Wei Xu,\nand Wenqing Cheng. 2022a. Musicyolo: A vision-\nbased framework for automatic singing transcription.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 31:229–241.\nYongqi Wang, Ruofan Hu, Rongjie Huang, Zhiqing\nHong, Ruiqi Li, Wenrui Liu, Fuming You, Tao Jin,\nand Zhou Zhao. 2024.\nPrompt-singer: Control-\nlable singing-voice-synthesis with natural language\nprompt. arXiv preprint arXiv:2403.11780.\nYu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu,\nHanzhao Li, Heyang Xue, Yongmao Zhang, Lei Xie,\nand Mengxiao Bi. 2022b. Opencpop: A high-quality\nopen source chinese popular song corpus for singing\nvoice synthesis. arXiv preprint arXiv:2201.07429.\nHaojie Wei, Xueke Cao, Tangpeng Dan, and Yueguo\nChen. 2023.\nRmvpe: A robust model for vocal\npitch estimation in polyphonic music. arXiv preprint\narXiv:2306.15412.\nJulia Wilkins, Prem Seetharaman, Alison Wahl, and\nBryan Pardo. 2018. Vocalset: A singing voice dataset.\nIn ISMIR, pages 468–474.\nJie Wu and Jian Luan. 2020.\nAdversarially trained\nmulti-singer sequence-to-sequence singing synthe-\nsizer. arXiv preprint arXiv:2006.10317.\nSiwei Wu, Jinzheng He, Ruibin Yuan, Haojie Wei, Xipin\nWei, Chenghua Lin, Jin Xu, and Junyang Lin. 2024.\nSongtrans: An unified song transcription and align-\nment method for lyrics and notes. arXiv preprint\narXiv:2409.14619.\nYongmao Zhang, Jian Cong, Heyang Xue, Lei Xie,\nPengcheng Zhu, and Mengxiao Bi. 2022. Visinger:\nVariational inference with adversarial learning for\nend-to-end singing voice synthesis. In ICASSP 2022-\n2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 7237–\n7241. IEEE.\nYu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu,\nTao Jin, and Zhou Zhao. 2025a. Isdrama: Immersive\nspatial drama generation through multimodal prompt-\ning. arXiv preprint arXiv:2504.20630.\nYu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu,\nRuiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang,\nZhiqing Hong, Ziyue Jiang, et al. 2025b. Versatile\nframework for song generation with prompt-based\ncontrol. arXiv preprint arXiv:2504.19062.\nYu Zhang, Rongjie Huang, Ruiqi Li, JinZheng He, Yan\nXia, Feiyang Chen, Xinyu Duan, Baoxing Huai, and\nZhou Zhao. 2024a. Stylesinger: Style transfer for out-\nof-domain singing voice synthesis. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 38, pages 19597–19605.\nYu Zhang, Ziyue Jiang, Ruiqi Li, Changhao Pan,\nJinzheng He, Rongjie Huang, Chuxin Wang, and\nZhou Zhao. 2024b.\nTcsinger: Zero-shot singing\nvoice synthesis with style transfer and multi-level\nstyle control. arXiv preprint arXiv:2409.15977.\nYu Zhang, Changhao Pan, Wenxiang Guo, Ruiqi Li,\nZhiyuan Zhu, Jialei Wang, Wenhao Xu, Jingyu Lu,\nZhiqing Hong, Chuxin Wang, et al. 2024c. Gtsinger:\nA global multi-technique singing corpus with realistic\nmusic scores for all singing tasks. arXiv preprint\narXiv:2409.13832.\nA\nDetails of Model\nA.1\nArchitecture\nThe model input for STARS consists of Mel spec-\ntrograms and the extracted f0, which are encoded\nseparately using two encoders, denoted as EM for\nthe Mel spectrogram and EP for f0. The Mel en-\ncoder is constructed from linear layers and residual\nconvolution blocks, while the f0 encoder consists\nof an embedding layer. The fused features are then\nfed into five levels of an acoustic encoder to ex-\ntract audio features at different hierarchical levels:\nframe, word, phone, note, and sentence.\nEach acoustic encoder begins with residual con-\nvolution blocks followed by a CMU Encoder for\nfeature extraction. The encoder and decoder of the\nU-Net backbone include four downsampling and\nupsampling layers, with a downsampling rate of\n16×. The U-Net module is further enhanced by\ntwo layers of Conformer blocks, where the Feed-\nForward layers in each Conformer are replaced\nwith FreqMOE layers. Each FreqMOE contains\nfour experts, and the input features are equally split\ninto four parts, each processed by an expert. The\noutputs of the four experts are then concatenated.\nAfter feature extraction from the CMU Encoder,\nthe sentence-level features are obtained by averag-\ning across all frames and then they are expanded.\nFor frame-level features, no further processing is\nperformed. For the phone, word, and note levels,\naverage pooling is applied based on boundary infor-\nmation. These pooled features then pass through\nconvolutional layers followed by a Vector Quanti-\nzation layer with a codebook size of 128 for feature\nfiltering. Finally, the Length Regulator is applied to\nexpand the features according to the corresponding\nboundary.\nHyperparameter\nModel\nMel\nEncoder\nConv Kernel\n3\nConv Layers\n2\nHidden Size\n256\nCondition\nEncoder\nPitch Embedding\n300\nUV Embedding\n3\nHidden Size\n256\nVector\nQuantization\nCode Num\n128\nHidden Size\n256\nU-Net\nKernel Size\n3\nEnc & Dec Layers\n4\nDownsampling Rate\n16\nHidden Size\n256\nFreqMOE\nConformer\nKernel Size\n9\nHead Num\n4\nLayers\n2\nAttention Hidden\n256\nMOE Hidden\n256\nExpert Num\n4\nTable 6: Hyperparameters of STARS.\nThe Global Style Predictor is composed of two\nlayers of cross-attention, which predict the type of\neach class. The overall architecture parameters are\nshown in Table 6.\nA.2\nFreqMOE\nFollowing previous audio generation systems with\nMOE (Zhang et al., 2025b,a), the FreqMOE (Fre-\nquency Mixture of Experts) module is designed\nto enhance the representation of input features by\nleveraging multiple experts, each processing a dis-\ntinct subset of the input. The module operates\nby splitting the input feature map into multiple\nchunks and passing each chunk through a separate\nexpert network. The outputs from all experts are\nthen concatenated to form the final representation.\nSpecifically, the FreqMOE can be expressed as:\nFreqMOE(X) = ConcatK\nk=1Ek(X(k)),\n(6)\nwhere Xk ∈RT×D/K represents the k-th chunk\nof the input feature map, X, split along its feature\ndimension. Ek denotes the k-th expert network,\nwhich processes Xk.\nB\nAudio-Phoneme Alignment\nIn this appendix, we briefly describe our align-\nment algorithm that synchronizes a sequence of\nSetting\nMetric\nTechnique Prediction\nBUB\nBRE\nPHA\nVIB\nGLI\nMIX\nFAL\nWEA\nSTR\nTEC\nBilingual\nF1\n74.0\n50.5\n78.6\n50.0\n69.7\n87.5\n86.6\n94.9\n99.6\n76.8\nACC\n98.5\n89.0\n94.6\n96.3\n84.2\n81.1\n91.1\n90.7\n97.3\n91.4\nTable 7: The objective results of phone-level technique prediction on the bilingual dataset.\nsetting\nEMO\nPAC\nRNG\nLAN\nGEN\nSingle\n52.5\n71.3\n59.0\n-\n100.0\nBilingual\n48.6\n71.8\n76.9\n100.0\n100.0\nTable 8: The results of the global style detection. EMO\nrefers to emotion, PAC to pace, RNG to pitch range,\nLAN to language, and GEN to gender.\nphoneme labels with the corresponding audio (or\nvideo) frames. The algorithm is based on dynamic\nprogramming and is inspired by Viterbi decod-\ning, which efficiently finds the most likely align-\nment path through a state-space representing both\nphoneme and silence (or blank) predictions.\nB.1\nOverview\nGiven an audio signal, a neural network produces\nframe-level log-probabilities for phoneme classes\nas well as for silence. In addition, a boundary de-\ntection mechanism provides probabilities indicat-\ning the likelihood of transitions between phoneme\nsegments. The alignment problem is then formu-\nlated as finding the optimal path that maximizes\nthe overall likelihood, taking into account both the\nphoneme content and the temporal boundaries be-\ntween phonemes.\nB.2\nDynamic Programming Formulation\nLet T be the number of time frames and L be the\nnumber of phonemes in the target sequence. We\ndefine a score matrix D ∈RT×(2L+1), where each\ncolumn corresponds to a state representing either\na phoneme or an interleaved blank state. The al-\ngorithm initializes D with scores computed from\nthe first frame’s predictions and then iteratively\nupdates the matrix as follows:\nD(t, k) = max\n\n\n\n\n\n\n\nD(t −1, k) + s(t)\nk ,\nD(t −1, k −1) + s(t)\nk ,\nD(t −1, k −2) + s(t)\nk ,\nHere, s(t)\nk denotes the log-probability score at time\nt for state k, which is computed based on the\nphoneme, silence, and boundary predictions. A cor-\nresponding backtracking matrix B is maintained to\nrecord the optimal transition at each step.\nB.3\nBacktracking and Temporal Mapping\nAfter processing all frames, the optimal alignment\npath is recovered by backtracking through B start-\ning from the final frame.\nThis path indicates,\nfor each time frame, the most likely state (i.e.,\nphoneme or blank) that generated the observation.\nFinally, by mapping the indices of phoneme states\nto time instants—using the known hop size of the\naudio frames—the algorithm produces onset and\noffset times for each phoneme. This procedure al-\nlows us to effectively synchronize phonetic labels\nwith the audio signal.\nB.4\nSummary of the Algorithm\nThe overall alignment procedure can be summa-\nrized as follows:\n1. Initialization: Set up the dynamic program-\nming matrices D (for scores) and B (for back-\ntracking) using the initial predictions.\n2. DP Matrix Update: For each time frame\nt = 1, . . . , T −1, update the scores for all\nstates by considering self-transitions, transi-\ntions from the previous state, or skips (model-\ning boundaries).\n3. Backtracking: Recover the optimal align-\nment path by backtracking through B.\n4. Temporal Mapping: Convert the alignment\nindices to temporal onset and offset times for\neach phoneme using the audio frame hop size.\nC\nDetails of Experiments\nC.1\nDataset\nThe open-source singing dataset used in our exper-\niments includes the Chinese and English subsets\nof GTSinger (Zhang et al., 2024c). We use the\ndataset under the CC BY-NC-SA 4.0 license. For\nthe recordings, we select one male and one female\nprofessional singer, each paid $350 per hour, and\nthey agree to make their contributions available\nfor research purposes. During the recording ses-\nsions, the singers are instructed to apply and label\nthe technique annotations at both the sentence and\nphoneme levels. Phoneme segmentation is refined\nusing the Montreal Forced Aligner (MFA), with\nadditional manual adjustments to ensure accuracy.\nThe annotators are compensated at a rate of $15\nper hour for their work.\nC.2\nEvaluation Metrics\nFor lyric alignment, we use two objective met-\nrics: Boundary Error Rate (BER) and Intersec-\ntion Over Union (IOU) score. The Boundary Error\nRate (BER) measures the proportion of misplaced\nboundaries within 20 ms tolerance distance. The\nIOU score is defined as the ratio of the duration\nof the overlapping segment between two notes to\nthe duration of the combined time span covered by\nboth notes.\nFor note transcription, we use the objective met-\nric COnPOff (Correct Onset, Pitch, and Offset).\nCOnPOff assesses whether both the boundaries\nand pitch of the note are correctly predicted. Ad-\nditionally, we compute the Raw Pitch Accuracy\n(RPA) to evaluate the overall pitch prediction per-\nformance.\nRPA is calculated by transforming\nboth the ground truth (GT) and predicted note\nevents into frame-level sequences and computing\nthe matching scores.\nIn our evaluation process, we employ STARS to\nexclude silent notes and designate the boundaries\nenclosing each note as its onset and offset. For\nonset and offset evaluation, the tolerance is set to\n50 ms, with the offset tolerance being the larger\nof 50 ms or 20% of the note duration. The pitch\ntolerance is set to 50 cents.\nFor the singing voice synthesis experiment, we\nrandomly select 30 generated singing clips for sub-\njective evaluation. Each generated sample is rated\nand evaluated by at least five professional listen-\ners. In the MOS-Q evaluation, listeners focus on\nrating the quality and naturalness of the generated\nsinging voice. In the MOS-C evaluation, they as-\nsess whether the generated singing matches the\nstyle of the given prompt. All scores are rated on\na five-point scale. Each participant is paid $10 per\nhour.\nC.3\nExperiment\nAs shown in Table 7, we observe that when evalu-\nating on a bilingual dataset, the overall prediction\naccuracy for the nine singing techniques is rela-\ntively high, comparable to the results obtained from\na single-language dataset. This demonstrates the\nfeasibility of our model for multilingual datasets.\nAccording to the results presented in Table 8, we\nfind that attributes such as language, gender, and\nvocal range, which are relatively fixed across the\nentire singing performance, yield better prediction\nresults. n contrast, the model performs less effec-\ntively on attributes like emotion and vocal range,\nwhich may vary across different segments of the\nsong. Further analysis of the dataset reveals that an-\nnotations for these attributes are typically provided\nat the sentence level, whereas emotion and vocal\nrange fluctuate within different sections of a song.\nThis variability leads to a decrease in prediction\naccuracy for individual segments of the singing\nvoice.\n",
    "content": "Here is the English translation of your text:\n\n---\n\n### 1. Core Content and Key Contributions\n\nThis paper proposes a unified framework called **STARS (Singing Transcription, Alignment, and Refined Style Annotation)** aimed at addressing the fragmentation issue in multi-level Automatic Singing Annotation (ASA). Current ASA methods typically focus on single tasks such as lyric alignment, note transcription, or style recognition. In contrast, STARS is the first to integrate the following capabilities into a single unified model:\n\n- **Phoneme-Audio Alignment**\n- **Note Transcription & Temporal Localization**\n- **Expressive Vocal Technique Identification**\n- **Global Stylistic Characterization**\n\nThe framework leverages a hierarchical acoustic feature processing mechanism that extracts information across multiple granularities—frame-level, word-level, phoneme-level, note-level, and sentence-level—while employing a non-autoregressive local acoustic encoder to enable structured hierarchical representation learning.\n\n#### Key Contributions:\n- The first unified framework capable of simultaneous singing transcription, alignment, and fine-grained style annotation;\n- A five-level architecture supporting feature filtering and extraction from low to high levels;\n- Parallel prediction strategy enhancing accuracy in phoneme/note boundary detection and pitch estimation;\n- Demonstrated superior performance in naturalness and style control when applied to SVS (Singing Voice Synthesis) training.\n\n---\n\n### 2. Breakthroughs and Innovations\n\nSTARS introduces innovation through its **unified modeling capability** and **multi-level feature processing mechanisms**:\n\n#### (1) Unified Modeling\nTraditional ASA approaches rely on multiple independent toolchains (e.g., WhisperX for lyrics transcription, MFA for alignment, VOCANO for note detection), which can lead to error accumulation. STARS integrates these tasks into a single end-to-end model, avoiding error propagation between modules and improving overall efficiency.\n\n#### (2) Five-Level Architecture\nSTARS features a unified architecture composed of five levels: frame-level, word-level, phoneme-level, note-level, and sentence-level. Each level shares the same backbone network but uses different pooling strategies to extract information at varying granularities.\n\n#### (3) CMU Encoder + FreqMOE Module\n- Uses a U-Net-based CMU encoder for spectral downsampling;\n- Incorporates Conformer modules to capture long- and short-term temporal dependencies;\n- Introduces the FreqMOE module, which partitions the frequency dimension into subbands processed by dedicated experts, enhancing frequency analysis capabilities.\n\n#### (4) Vector Quantization + Hierarchical Representation Learning\nEmploys vector quantization as a bottleneck layer to remove redundancy and enhance generalization; extends features back to original frame length via a length regulator to ensure output consistency.\n\n#### (5) Multi-task Joint Optimization\nSTARS supports synchronized prediction of phoneme boundaries, note boundaries, pitch, vocal techniques (e.g., vibrato, falsetto), and global styles (e.g., emotion, tempo), and is jointly optimized using a unified loss function (CTC + CE + BCE + Commitment Loss).\n\n---\n\n### 3. Entrepreneurial Project Suggestions with Commercial Potential\n\nBased on the technical strengths of STARS, here are several entrepreneurial directions suitable for commercial deployment:\n\n---\n\n#### ✅ Project One: AI Singing Tutor Platform\n\n**Target Users**: Vocal students, music enthusiasts, online education platforms  \n**Key Features**:\n- Automatically analyzes pitch, rhythm, and pronunciation accuracy in student recordings;\n- Annotates expressive vocal techniques (e.g., vibrato, breathiness, dynamics);\n- Compares against reference audio to provide style-matching scores;\n- Offers personalized practice recommendations and error correction support.\n\n**Commercial Value**:\n- Can be embedded into online music course systems;\n- Enables remote assignment grading for teachers;\n- Integrated into smart speakers or apps to create an \"AI Vocal Coach.\"\n\n---\n\n#### ✅ Project Two: Virtual Singer Generation Platform (Style-Controlled)\n\n**Target Users**: Content creators, short-video platforms, game/film composers  \n**Key Features**:\n- Generates high-quality AI singer voices based on input lyrics and melody;\n- Allows control over style parameters (emotion, speech rate, intensity, technique types);\n- Supports multiple languages (e.g., Chinese, English);\n- Automatically generates lyric timelines and MIDI files for notes.\n\n**Commercial Value**:\n- Streamlines AI synthesis, editing, and exporting processes, reducing reliance on professional singers;\n- Suitable for ads, short video voiceovers, and character voices in games;\n- API available for third-party developers to integrate.\n\n---\n\n#### ✅ Project Three: AI Song Style Transfer Service\n\n**Target Users**: Music producers, DJs, mixing engineers  \n**Key Features**:\n- Extracts style characteristics (emotion, tempo, techniques) from any song segment;\n- Transfers the extracted style onto the vocal part of another song;\n- Supports combinations of multiple styles (e.g., “Jay Chou-style rap” + “JJ Lin-style ballad”);\n- Outputs editable annotation files for post-processing.\n\n**Commercial Value**:\n- Provides inspiration and quick auditioning tools for music creation;\n- Useful for DJ sets and Remix tools;\n- Integrates with DAW software to improve production efficiency.\n\n---\n\n#### ✅ Project Four: AI Lyrics Alignment & Subtitle Generator\n\n**Target Users**: Video platforms, live streaming services, subtitle translation companies  \n**Key Features**:\n- Automatically aligns lyrics with audio timelines;\n- Generates accurate subtitle files (SRT, ASS, etc.);\n- Supports bilingual alignment and translation (Chinese-English);\n- Applicable to live concerts, MV subtitles, and KTV systems.\n\n**Commercial Value**:\n- Reduces manual subtitle costs;\n- Enhances accessibility of video content;\n- Partnerships with streaming platforms to build an AI subtitle ecosystem.\n\n---\n\n### Summary Table\n\n| Direction | Technical Application | Commercial Use Case |\n|----------|------------------------|----------------------|\n| AI Vocal Instruction | Phoneme alignment + technique recognition | Online education / tutoring apps |\n| Virtual Singer Generation | Multi-style control + automatic annotation | Content creation / game audio |\n| Song Style Transfer | Global style recognition + transfer | Music production / Remix |\n| Lyrics & Subtitle Tool | Automatic alignment + timeline generation | Video platforms / KTV systems |\n\n---\n\nIf you'd like to further explore the feasibility, technical roadmap, or funding suggestions for any of these projects, feel free to ask!",
    "github": "https://gwx314.github.io/stars-demo/",
    "hf": ""
}