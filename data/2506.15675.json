{
    "id": "2506.15675",
    "title": "Sekai: A Video Dataset towards World Exploration",
    "summary": "This article introduces Sekai, a first-person video dataset containing over 5,000 hours of footage from more than 750 cities across over 100 countries and regions around the world, used for world exploration.",
    "abstract": "Video generation techniques have made remarkable progress, promising to be the foundation of interactive world exploration. However, existing video generation datasets are not well-suited for world exploration training as they suffer from some limitations: limited locations, short duration, static scenes, and a lack of annotations about exploration and the world. In this paper, we introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person view worldwide video dataset with rich annotations for world exploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from over 100 countries and regions across 750 cities. We develop an efficient and effective toolbox to collect, pre-process and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories. Experiments demonstrate the quality of the dataset. And, we use a subset to train an interactive video world exploration model, named YUME (meaning ``dream'' in Japanese). We believe Sekai will benefit the area of video generation and world exploration, and motivate valuable applications.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Zhen Li,Chuanhao Li,Xiaofeng Mao,Shaoheng Lin,Ming Li,Shitian Zhao,Zhaopan Xu,Xinyue Li,Yukang Feng,Jianwen Sun,Zizhen Li,Fanrui Zhang,Jiaxin Ai,Zhixiang Wang,Yuwei Wu,Tong He,Jiangmiao Pang,Yu Qiao,Yunde Jia,Kaipeng Zhang",
    "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:12 pages, 6 figures",
    "keypoint": "- Introduced Sekai, a high-quality first-person view worldwide video dataset with rich annotations for world exploration.\n- Consists of over 5,000 hours of walking or drone view videos from over 100 countries and regions across 750 cities.\n- Developed an efficient toolbox to collect, pre-process, and annotate videos with location, scene, weather, crowd density, captions, and camera trajectories.\n- Features long-duration videos (at least 60 seconds) and diverse locations, including both walking and drone views.\n- Provides ground-truth annotations for videos from a realistic video game, Lushfoil Photography Sim.\n- Conducted experiments demonstrating the quality of the dataset and trained an interactive video world exploration model named YUME using a subset of Sekai-Real-HQ.\n- Addressed limitations such as insufficient training due to limited computational resources and partial camera trajectory annotation.",
    "date": "2025-06-22",
    "paper": "arXiv:2506.15675v1  [cs.CV]  18 Jun 2025\nSekai: A Video Dataset towards World Exploration\nZhen Li1,2,4*¬ß Chuanhao Li1*‚Ä†, Xiaofeng Mao1, Shaoheng Lin1, Ming Li1,\nShitian Zhao1, Zhaopan Xu1, Xinyue Li1, Yukang Feng3, Jianwen Sun3,\nZizhen Li3, Fanrui Zhang3, Jiaxin Ai3, Zhixiang Wang5, Yuwei Wu2,4‚Ä†,\nTong He1, Jiangmiao Pang1, Yu Qiao1, Yunde Jia4, Kaipeng Zhang1,3‚Ä†‚Ä°\n1Shanghai AI Laboratory 2Beijing Institute of Technology\n3Shanghai Innovation Institute 4Shenzhen MSU-BIT University\n5The University of Tokyo\nhttps://lixsp11.github.io/sekai-project/\nAbstract\nVideo generation techniques have made remarkable progress, promising to be the\nfoundation of interactive world exploration. However, existing video generation\ndatasets are not well-suited for world exploration training as they suffer from\nsome limitations: limited locations, short duration, static scenes, and a lack of\nannotations about exploration and the world. In this paper, we introduce SEKAI\n(meaning ‚Äúworld‚Äù in Japanese), a high-quality first-person view worldwide video\ndataset with rich annotations for world exploration. It consists of over 5,000 hours\nof walking or drone view (FPV and UVA) videos from over 100 countries and\nregions across 750 cities. We develop an efficient and effective toolbox to collect,\npre-process and annotate videos with location, scene, weather, crowd density,\ncaptions, and camera trajectories. Experiments demonstrate the quality of the\ndataset. And, we use a subset to train an interactive video world exploration model,\nnamed YUME (meaning ‚Äúdream‚Äù in Japanese). We believe Sekai will benefit the\narea of video generation and world exploration, and motivate valuable applications.\nWe are looking for collaboration and self-motivated interns interested in\ninteractive world generation. Contact: zhangkaipeng@pjlab.org.cn\n1\nIntroduction\nExplore. Dream. Discover. ‚Äî Mark Twain\nWorld exploration and interaction form the foundation of humankind‚Äôs odyssey, which are practical\nscenarios for world generation models [1]. These models aim to adhere to the world laws (real world\nor games) while facilitating unrestricted exploration and interaction within environments. In this\npaper, we focus on the first act of world generation‚Äîworld exploration, which aims to use image,\ntext, or video to construct a dynamic and realistic world for interactive and unrestricted exploration.\nRecent advancements in video generation [2, 3, 4, 5, 6] have been remarkable, making it a promising\napproach for world generation through video generation. Meanwhile, camera-controlled video\ngeneration [7, 8, 9] is a suitable way for world exploration, since camera trajectories can be converted\n¬ßThis work was done during the internship at Shanghai AI Laboratory.\n*Equal contribution.\n‚Ä†Corresponding authors: wuyuwei@bit.edu.cn; lichuanhao@pjlab.org.cn; zhangkaipeng@pjlab.org.cn\n‚Ä°Project leader.\nPreprint. Under review.\nSekai-Game\nSekai-Real\nCategory\nLocation: grindelwald\nCrowd Density: empty\nScene: outdoor\nWeather: foggy\nTimeOfDay: morning\nTrajectory\n‚Ä¶, the surrounding trees and distant\nmountains, creating a serene and tranquil\nsetting. The camera continues its steady\nadvance, revealing more of the quaint\nvillage, with occasional glimpses of parked \ncars and small patches of vegetation. The \nmuted colors of the foggy day enhance the \nrustic charm of the houses, their green \nshutters standing out against the earthy ‚Ä¶\nCaption\n‚Ä¶\n‚Ä¶\nWalking\nFigure 1: Sekai is collected from Youtube √Ö and a video game s. It consists of walking and\ndrone-view egocentric videos with recorded audio. We provide rich annotations of camera trajectories,\nlocation, crowd density, scene, weather, time of day, and captions.\nby keyboard and mouse inputs. However, generating long and realistic videos with precise camera\ncontrol remains a significant challenge. Data is a critical challenge. Existing video generation\ndatasets [10, 11, 12] are not well-suited for world exploration as they suffer from limitations: limited\nlocations, short duration, static scenes, and a lack of annotations about exploration (e.g., camera\ntrajectories) and world annotations (e.g., location, weather and scene).\nIn this paper, we introduce\nSEKAI („Åõ„Åã„ÅÑ, meaning ‚Äúworld‚Äù in Japanese), a high-quality\negocentric worldwide video dataset for world exploration (see Figure 1 and Figure 2). Most videos\ncontain audio for an immersive world generation. It also benefits other applications, such as video\nunderstanding, navigation, and video-audio co-generation. Sekai-Real comprises over 5000 hours\nof videos collected from YouTube √Ö with high-quality annotations. Sekai-Game comprises videos\nfrom a realistic video game s, Lushfoil Photography Sim, with ground-truth annotations. It has five\ndistinct features: (1) High-quality and diverse video. All videos are recorded in 720p at 30 FPS,\nfeaturing diverse weather conditions, various times, and dynamic scenes. (2) Worldwide location.\nVideos are captured across 101 countries and regions, featuring over 750 cities with diverse cultures,\nactivities, architectures, and landscapes. (3) Walking and drone view. Beyond the walking videos\n(e.g., citywalk and hiking), Seikai contains drone view (FPV and UAV) videos for unrestricted world\nexploration. (4) Long duration. All walking videos are at least 60 seconds long, ensuring real-world,\nlong-term world exploration. (5) Rich annotations. All videos are annotated with location, scene,\nweather, crowd density, captions, and camera trajectories. YouTube videos‚Äô annotations are of high\nquality, while annotations from the game are considered ground truth.\nTo construct the Sekai dataset, we develop a curation pipeline (see Section 3) for Sekai-Real (YouTube\nvideos) and Sekai-Game (video game videos). (1) For Sekai-Real √Ö, we first manually search and\ndownload high-quality walking and drone videos. Then we introduce a pre-processing pipeline to\nobtain video clips by shot detection, video transcoding, and quality evaluation. After that, we develop\nan annotation framework to annotate location, scene type, weather, crowd density, captions, and\ncamera trajectories. Considering the large amount of data and practical usage, we further introduce\na video sampling module to sample the best-of-the-best videos according to the computational\nresources for training the video generation model. (2) For the Sekai-Game s, we first play Lushfoil\nPhotography Sim and record videos. Then we use the same pre-processing pipeline to obtain video\nclips. For the annotation, we develop a toolbox to record ground-truth annotations while playing.\nWe do dataset statistics analysis to show the scale and diversity of the dataset. And, we validate the\naccuracy of annotations for YouTube videos. Besides, we use a subset of the Sekai-Real to train an\ninteractive video world exploration model, named YUME („ÇÜ„ÇÅ, meaning ‚Äúdream‚Äù in Japanese).\nThe main contribution of this work is the dataset, Sekai. It is a high-quality long-form video dataset for\nworldwide exploration by walking or drone. We annotate diverse labels, including captions, location,\nscene, weather, crowd density, time-of-day, and camera trajectories for each video. Experiments\ndemonstrate the effectiveness and the quality of the dataset. We believe Sekai will benefit the\nareas of video generation and world generation, and motivate valuable applications. We provide an\nintroduction video on the project page.\n2\nFigure 2: An overview of the Sekai dataset. Sekai-Real is collected from YouTube with high-quality\nannotations, while Sekai-Game is collected from a game with ground-truth annotations.\n2\nRelated Work\n2.1\nWorld Generation Model\nRecent years have seen a growing interest in video generation [2, 6, 13, 14, 15, 16, 17], 3D scene gen-\neration [18, 19, 20, 21, 22, 23], and 4D generation [24, 25, 26, 27, 28], with significant advancements\nopening up new possibilities in the development of world generation models [29, 30, 31, 32, 33]. In\nthe realm of video generation, text-to-video generation [14, 15] has played a pivotal role, achieving\nhigh-fidelity results, while image-to-video generation [16, 17, 34] has also seen notable advance-\nments. Sora [29] further underscores the significance of video generation in the context of world\ngeneration models. Among 3D scene generation methods, techniques [20, 21, 22, 18] utilize depth\nestimation models [35, 36, 37] to extend 2D scenes into 3D representations. 4D scene genera-\ntion [24, 25, 26] further introduces dynamics, focusing on the evolution of objects or scenes over\ntime [27] and dynamic interactions [28]. This paper primarily focuses on interactive video generation\nfor world exploration, aiming to construct a dynamic and realistic world using image, text, or video\nfor unrestricted exploration.\n2.2\nVideo Generation Dataset\nThe continuous development of annotated datasets has played a pivotal role in shaping the land-\nscape of artificial intelligence-generated content, offering both insights and challenges for accurate\nmodel assessment. Existing video generation datasets can be categorized as specific-scenario and\n3\nFigure 3: The dataset curation pipeline. * indicates we annotate trajectories for partial data.\nopen-scenario. Typical specific-scenario datasets including UCF-101 [38], Taichi-HD [39], Sky-\nTimelapse [40], FaceForensics++ [41], ChronoMagic [42] and Celebv-HQ [43]. These datasets\nhave limited amount of data (with a total duration of less than 800 hours), limited individual video\nduration (with an average length of less than 20 seconds), and generally lack annotation informa-\ntion (only a few datasets, such as ChromoMagic, provide caption annotations). Open-scenario\ndatasets [44, 45, 46, 47] have somewhat alleviated issues with data scale and annotation information.\nFor example, OpenSoraPlan-V1.0 [46] includes videos with a total duration of 274 hours, each\naccompanied by detailed captions. Similarly, the recently introduced OpenVid-1M dataset [10]\ncomprises videos totaling 2100 hours, with long captions provided for each video. However, the\naverage duration of individual videos still does not exceed 25 seconds, and they only provide caption\nannotations. MiraData [12] consists of longer videos with an average length of 72.1 seconds. It is\nstill not long enough for the world exploration, and exploration annotations (e.g., camera poses or\nkeyboard and mouse inputs) and world annotations (e.g., location, time and weather) are missing. By\ncontrast, the proposed Sekai dataset focuses on egocentric world exploration, which covers walking\nand drone view videos across diverse locations and scenes with long video duration (1 to 39 minutes,\naverage is 2 minutes) and rich exploratory and world annotation.\n3\nDataset Curation\nThe overall process of curating the Sekai dataset includes four major parts: video collection, pre-\nprocessing, annotation, and sampling, seeing Figure 3 for an illustration.\n3.1\nVideo Collection\nIn the collection stage, we collect over 8623 hours of YouTube videos and over 40 hours of game\nvideos from Lushfoil Photography Sim.\nYouTube. We manually collect high-quality video URLs from popular YouTubers and extend them\nby searching additional videos using related keywords (e.g., walk, drone, HDR, and 4K). In total,\nwe collect 10471 hours of walking videos (with stereo audio) and 628 hours of drone (FPV or UAV)\nvideos. All videos were released over the past three years, with a 30-minute to 12-hour duration.\nThey are at least 1080P with 30 to 60 FPS. We download the 1080P version with the highest Mbps\nfor further video processing and annotation. Due to network issues and some videos are broken, there\nare 8409 hours of walking videos and 214 hours of drone videos after downloading.\nVideo Game. Beyond real-world data, we collect additional data from the video game since its\nground-truth annotations are accessible (e.g., location, weather, and camera trajectory). Lushfoil\n4\nPhotography Sim is a video game that allows walking or using a first-person drone to explore real-\nworld landscapes. It is built by Unreal Engine 5 and showcases the game‚Äôs locations in stunning\nvisual fidelity, making it an excellent source for collecting realistic synthetic data. We use OBS\nStudio to record 40 hours videos at 1080P 30FPS (8 to 12 Mbps) with diverse locations and weather.\nScaling the amount of data is low-cost.\n3.2\nVideo Pre-processing\nFor YouTube videos, we trim two minutes from the start and end of each original video to remove\nthe opening and ending. Then we do the following steps and obtain 6620 hours (Sekai-Real) and 60\nhours (Sekai-Game) of video clips for YouTube and the game, respectively.\nShot Boundary Detection. YouTube videos are often cut and stitched, and video games commonly\nfeature teleportation points‚Äîboth of which contribute to discontinuous shot segments in one video.\nThus, following Cosmos [30], we employ TransNetV2 [48] with a threshold of 0.4 for shot boundary\ndetection. However, the original implementation runs slowly. We refactored the codebase for\nGPU acceleration, which is five times faster than the original version. In particular, we use the\nPyNVideoCodec library for video decoding and employ the CVCUDA library to offload frame\noperations such as color space conversion and histogram computation to the GPU. We trim five\nseconds from the start and end of each shot. After shot detection, the duration of video clips is from 1\nto 5.88 hours.\nClip Extraction and Transcoding. Considering practical processing, we split each shot into multiple\none-minute clips (shorter than one minute will be discarded). In model training, we can stitch\ncontiguous clips according to the computation resources. We re-encode each video clip using the\nPyNVideoCodec library to standardize the diverse codec configurations in the raw videos, targeting\n720p at 30fps in H.265 MP4 format with a bitrate of 4 Mbps. Evaluation of the transcoded video\nclips across diverse scenes yields PSNR values above 35, indicating no perceptible visual degradation.\nWe think the world exploration should contain realistic sound. Thus, we keep the audio of walking\nvideos. We trim the audio tracks based on the timestamps of the video clips, re-encode them into\nAAC format using FFmpeg at 48kHz, and mux each audio clip with its corresponding video clip.\nLuminance Filtering. Overly dark or bright videos are not suitable for model training. We apply a\nsimple filter based on the luma channel in YUV color space, and remove video clips with more than\n15 consecutive frames of extremely high or low average brightness. Especially, this step is necessary\nfor video game data, as the engine often employs simplified lighting and camera systems. In this step,\nwe filter out 300 hours of videos.\nQuality Filtering. We use COVER [49], a comprehensive video quality evaluator to filter low-quality\nvideo clips according to the technical quality metric. Technical quality evaluates issues such as image\nclarity, transmission distortion, and transcoding artifacts. The lowest-scoring 10% of video clips are\nremoved after filtering.\nSubtitle Filtering. Some videos contain hardcoded subtitles, which are artificial texts embedded in\nthe video frames. These subtitles compromise the video‚Äôs fidelity to the real world and may introduce\nmisleading patterns during model training. To mitigate this, we apply VideoSubFinder to detect\nhardcoded subtitles on the bottom one-third of the video frames. A clip is flagged if it contains any\nsubtitle that remains visible for more than 0.75 seconds, in order to reduce false positives. All flagged\nclips are removed, resulting in the exclusion of approximately 5% of the video clips.\nCamera Trajectory Filtering. For Sekai-Real, we employ a state-of-the-art structure from motion\n(SfM) model to extract camera trajectories. However, some trajectories exhibit implausible or counter-\nintuitive motions, so we heuristically filter out abnormal cases using the following rules. Specifically,\nwe exclude video clips if they satisfy either of the following: (1) Multiple abrupt trajectory reversals\n(i.e., directional changes exceeding 150 degrees) within a 10-second window. (2) A camera viewpoint\nshift greater than 60 degrees between two consecutive frames. (3) A camera position displacement\ngreater than 5 times the average displacement of the 30 consecutive frames containing these two\nframes. This filtering phase is only conducted for partial data annotated with camera trajectories.\n3.3\nVideo Annotation of Sekai-Real\nWe annotate video clips using large vision-language models.\n5\nLocation. Utilizing the Google YouTube Data API, we fetch the title and description of each\nvideo. Since most videos contain multiple chapters filmed at different locations with timeline-based\ndescriptions, we employ GPT-4o [50] to extract a formatted location for each chapter with the ISO\n3166 country/region code attached for subsequent processing. We use the interval tree to efficiently\nmatch each video clip to its corresponding chapter based on the timestamp, thereby retrieving the\nlocation information. Video clips that cannot be uniquely matched to a chapter are discarded, which\naccounts for approximately 8% of the total clips.\nCategory and Caption. We adopt a two-stage strategy to annotate each video with category and\ncaption. In the first stage, the video is classified along four orthogonal dimensions: scene type,\nweather, time of day, and crowd density, each with mutually exclusive labels. The model selects the\nmost suitable label for each and abstains when uncertain. In the second stage, we carefully design\nprompts that incorporate the predicted category labels, location information, and video frames to\ngenerate detailed, time-ordered descriptions of actions and scenes for each video clip. Practically, we\nextract one frame every two seconds from each video clip and use 72B version of Qwen2.5-VL [51]\nto annotate them. We deploy vLLM [52] inference services with Nginx [53] for load balance. The\nfinal caption length averages over 176 words per video clip.\nCamera Trajectories. We experimented with various camera trajectory annotation methods of\ndifferent types, including the visual odometry method DPVO [54], the deep visual SLAM framework\nMegaSaM [35], and a carefully designed 3D transformer VGGT [55] that outputs 3D quantities.\nThrough empirical experiments and comparisons, we chose MegaSaM as the baseline annotation\nmethod and made adjustments to optimize annotation accuracy and efficiency. First, we replaced\nthe monocular depth estimation model Depth Anything [56] used in MegaSaM with Video Depth\nAnything [36], which performs better in terms of temporal consistency. We also tried modifying\nthe metric depth estimation model in MegaSaM by replacing UniDepth [37] with DepthPro [57].\nAdditionally, we optimized the official implementation of MegaSaM to support cross-machine, multi-\nGPU parallel inference, significantly improving annotation efficiency. We annotate over 600 hours of\nvideos (after the sampling).\n3.4\nVideo Annotation of Sekai-Game\nWe developed a concise yet comprehensive toolchain based on the open-source tools RE-UE4SS and\nOBS Studio to capture ground-truth annotations from video games. RE-UE4SS is a powerful script\nsystem for Unreal Engine, enabling access and modification of the UE object system with minimal\noverhead at runtime. Based on its Lua Scripting API, we develop practical tools for video collection\nand annotation, including the standardization of camera system configuration, real-time camera pose\ncapture, GUI hiding, ensuring the collection of clean data with aligned annotations.\nThe location and category are obtained from the description of the game map, and the prompt used\nfor captioning is tightly modified to better suit the video game context. For camera trajectories, the\ncaptured camera poses are further calibrated to compensate for delays and interpolated to synchronize\nwith the video frames.\n3.5\nVideo Sampling\nGiven the prohibitive cost of training on the full Sekai-Real, we propose a strategy to sample the best\nof the best clips with the highest quality and diversity. The number is related to the computational\nbudget for further video generation model training. In this paper, we sample 400 hours of the best of\nthe best videos as Sekai-Real-HQ.\n3.5.1\nQuality Sampling\nWe sample the highest-quality clips according to two aspects: aesthetic quality and semantic quality.\nAesthetic quality reflects the visual harmony among different elements in the video. Semantic quality\nassesses the semantic completeness and consistency of the content. We use COVER [49] to obtain\ntwo quality scores and sum them for each video clip. We sample a ùõºùëûùë¢ùëéùëôùëñùë°ùë¶= 0.7 proportion of video\nclips with the highest scores.\n6\n3.5.2\nDiversity Sampling\nWe balance the videos using the following modules one by one. And for Sekai-Real-HQ, the sampling\nratio ùõºùëêùëúùëõùë°ùëíùëõùë°, ùõºùëôùëúùëê, ùõºùëêùëéùë°ùëí, ùõºùëêùëéùëöùëíùëüùëéare equal to 70%, 60%, 60%, and 75%, respectivelty.\nContent Diversity. Given the vast volume of video clips, the presence of similar video clips is\ninevitable. We use InternVideo2 [58] to extract embeddings for each video clip, and apply mini batch\nK-Means [59] to cluster the embeddings of each countryregion. Subsequently, in each cluster, we\nuse the scores in quality sampling to rank the samples. Then we iteratively sample a video clip and\nremove its most similar one until 1 ‚àíùõºùëêùëúùëõùë°ùëíùëõùë°proportion of video clips have been removed.\nLocation Diversity. We denote the number of cities as ùëÅùëê. For each city, we count the number of\nvideo clips as ùëÅ. Given a sampling ratio ùõºùëôùëúùëê, we sort the cities in ascending order based on their\nùëÅ. For each city in this order, we sample approximately ùëÅ¬∑ ùõºùëôùëúùëê/ùëÅvideos from each city. If it is\nlarger than the corresponding ùëÅ, we sample all video clips for this city and redistribute the shortfall\nproportionally across the remaining cities by updating ùõºùëôùëúùëê.\nCategory Diversity. To ensure broad coverage across semantic categories, we perform inverse-\nprobability weighted sampling based on four independent categories: weather, scene, time of day,\nand crowd density. For each category, we compute the frequency of each label and assign sampling\nprobabilities inversely proportional to their frequencies. Assuming independence among categories,\nthe sampling probability for a video is initialized as the product of its label probabilities across the\nfour categories. These probabilities are then normalized to sum to 1. We perform non-replacement\nsampling according to these probabilities until ùõºùëêùëéùë°ùëíproportion of video clips have been sampled.\nCamera Trajectory Diversity. We perform trajectory-aware sampling by the following steps. First,\nfor the remaining videos, we calculate a direction vector (from the start to end of the trajectory) and\nthe overall jitter, defined as the Euclidean norm of positional variance computed every 30 frames.\nNext, direction vectors are discretized into bins mapped onto a sphere, and jitter values are also\ndiscretized into bins. Then, a joint grouping is formed based on the direction and jitter bins. Finally,\nwe do average sampling in each joint group according to the sampling ratio ùõºùëêùëéùëöùëíùëüùëé.\n4\nDataset Statistics\nOutdoor-\nNatural\n101 \nCountry/\nRegion\n4 Time\n‚Ä¶\nJapan\nSunrise/Sunset\nDay\nModerate\nScattered\nBustling\nEmpty\nCrowded\nNight\nIndoor\nUnknown\nFigure 4: Statistical information on five dimen-\nsions of the Sekai-Real dataset.\nFor the Sekai-Game collection, we have considered\nthe data balance issue while playing the game. Fig-\nure 4 provides an overview of the statistical infor-\nmation of the Sekai-Real dataset. Overall, the Sekai-\nReal dataset includes 101 countries and regions, with\nthe video durations exhibiting a distinct long-tail dis-\ntribution. The top eight countries (such as the Japan,\nthe United States, and the United Kingdom) account\nfor approximately 60% of the total duration. From\nvarious perspectives, we categorize the Sekai-Real\ndataset into four weather types, four scene types,\nfour time-of-day categories, and five levels of crowd\ndensity. Specifically, the majority of the videos were\noutdoors. For outdoor videos, the weather condi-\ntions are predominantly sunny and cloudy, and the\ndataset also includes unique weather situations, such\nas rain and snow, increasing the diversity of the\ndataset. Regarding the time of filming, most videos\nwere shot during the daytime, followed by nighttime footage. This provides a mix of well-lit scenes\nunder natural light and darker scenes with artificial lighting, offering diverse challenges for model\nlearning. In terms of crowd density, the distribution is relatively uniform, ranging from the sparsely\npopulated forests of Finland to the bustling streets of Japan. This stepwise distribution covers a variety\nof crowd density scenarios, providing diverse training settings for the model, such as curriculum\nlearning and evaluation of video generation models under varying levels of crowd density.\nThe statistics of Sekai-Real and Sekai-Real-HQ across multiple dimensions are shown in Figure 5.\nSekai-Real-HQ is the best of the best subset of Sekai-Real, and with more balanced data distribution.\n7\n0.15\n0.40\n0.65\n0.90\n1.15\n1.40\n0\n1\n2\n3\nPercentage (%)\nSekai-Real\nSekai-Real-HQ\n(a) Overall Video Quality Score\n22%\n21%\n17%\n5%\n4% 4% 4%4%\nSekai-Real\n14%\n13%\n13%\n5%\n9%\n6%\n4%\nSekai-Real-HQ\nUnited States\nJapan\nUnited Kingdom\nThe Emirates\nKorea\nFrance\nItaly\nChina\nGermany\nCanada\n(b) Location Distribution\n50\n100\n150\n200\n250\n300\n350\n0.000\n0.005\n0.010\n0.015\n0.020\nPercentage (%)\nOpenVid-1M\nSekai-Real\n(c) Caption Tokens Distribution\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0.0\n0.2\n0.4\n0.6\nPercentage (%)\nSubset of Sekai-Real\nSekai-Real-HQ\n(d) Camera Trajectory Jitter Distribution\nFigure 5: Statistics of the proposed Sekai-Real and Sekai-Real-HQ dataset.\nSeeing Figure 5 (a), Sekai-Real demonstrates strong overall video quality scores, with more than half\nof the videos scoring above 0.9, while Sekai-Real-HQ exhibits a higher mean video quality score\nand a lower variance to address the long-tail distribution issue. Figure 5 (b) shows the distribution of\nSekai-Real‚Äôs locations. Both Sekai-Real and Sekai-Real-HQ cover a wide range of countries globally.\nSekai-Real-HQ demonstrates a more balanced distribution, which is more effective in mitigating\npotential bias during model training. Figure 5 (c) shows the distribution of caption token counts\nin Sekai-Real and OpenVid-1M. It can be observed that the average number of caption tokens in\nSekai-Real exceeds 200, which is significantly higher than OpenVid-1M‚Äôs average of around 130.\nFigure 5 (d) shows the distribution of camera trajectory jitter values before and after applying Camera\nTrajectory Diversity. We can observe that the distribution for Sekai-Real-HQ is smoother compared\nto that of the Subset of Sekai-Real (500 hours, before applying Camera Trajectory Diversity). This\nindicates that Sekai-Real-HQ achieves better diversity and a more uniform distribution.\n5\nYUME Model\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nw\nS\nA\nD\nFigure 6: Examples of videos generated by YUME using keyboard and mouse control.\nWe train an interactive world exploration model named YUME („ÇÜ„ÇÅ, meaning ‚Äúdream‚Äù in Japanese)\nusing a subset of the Sekai-Real-HQ. Specifically, it receives an image and allows unrestricted\nexploitation using keyboard and mouse control from users. We show some examples in Figure 6.\n8\n6\nConclusion\nIn this paper, we introduce a new video dataset, named Sekai, for video generation-based world\nexploration. It consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\n101 countries and over 750 cities. We develop an efficient and effective toolchain to pre-process and\nannotate videos. For each video, we annotate location, scene type, weather, crowd density, captions,\nand camera trajectories. Besides, we introduce a video sampling module to sample the best of the\nbest videos according to the model training budget. Experiments demonstrate the effectiveness and\nthe quality of the dataset. We believe Sekai will benefit the area of video world generation and raise\nvaluable applications.\n7\nLimitation\nInsufficient training. Due to the limited computational resources, we use only a small proportion of\nSekai-Real-HQ in model training.\nInsufficient camera trajectory annotation. Due to our limited computational resources, for Sekai-\nReal, we annotate camera trajectories only for partial data.\nReferences\n[1] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: A unified evaluation\nbenchmark for world generation. arXiv preprint arXiv:2504.00983, 2025.\n[2] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for\nvideo generation. arXiv preprint arXiv:2504.12626, 2025.\n[3] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi\nHong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert\ntransformer. arXiv preprint arXiv:2408.06072, 2024.\n[4] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu,\nHongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying\nLiang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng,\nLimin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi\nWang, Ziang Wei, and Yang You. Open-sora 2.0: Training a commercial-level video generation model in\n$200k. arXiv preprint arXiv:2503.09642, 2025.\n[5] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming\nZhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint\narXiv:2503.20314, 2025.\n[6] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu,\nJianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models. arXiv\npreprint arXiv:2412.03603, 2024.\n[7] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl:\nEnabling camera control for video diffusion models. In The Thirteenth International Conference on\nLearning Representations.\n[8] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David B Lindell. Sg-i2v:\nSelf-guided trajectory control in image-to-video generation. arXiv preprint arXiv:2411.04989, 2024.\n[9] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation.\narXiv preprint arXiv:2406.10126, 2024.\n[10] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and\nYing Tai. Openvid-1m: A large-scale high-quality dataset for text-to-video generation. arXiv preprint\narXiv:2407.02371, 2024.\n[11] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun\nJeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos\nwith multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13320‚Äì13331, 2024.\n9\n[12] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang\nXu, and Ying Shan. Miradata: A large-scale video dataset with long durations and structured captions.\nAdvances in Neural Information Processing Systems, 37:48955‚Äì48970, 2024.\n[13] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao\nYang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint\narXiv:2503.20314, 2025.\n[14] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310‚Äì7320, 2024.\n[15] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang.\nT2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv\npreprint arXiv:2405.18750, 2024.\n[16] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao\nWang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video\ndiffusion priors. In European Conference on Computer Vision, pages 399‚Äì417. Springer, 2024.\n[17] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang.\nEasyanimate: A high-performance long video generation method based on transformer architecture. arXiv\npreprint arXiv:2405.18991, 2024.\n[18] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T Freeman, and Jiajun Wu. Wonderworld:\nInteractive 3d scene generation from a single image. arXiv preprint arXiv:2406.09394, 2024.\n[19] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T Freeman, Forrester\nCole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6658‚Äì6667,\n2024.\n[20] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer:\nDomain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023.\n[21] Paul Engstler, Andrea Vedaldi, Iro Laina, and Christian Rupprecht. Invisible stitch: Generating smooth 3d\nscenes with depth inpainting. arXiv preprint arXiv:2404.19758, 2024.\n[22] Lukas H√∂llein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nie√üner. Text2room: Extracting\ntextured 3d meshes from 2d text-to-image models.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 7909‚Äì7920, 2023.\n[23] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation\nwith neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 30(12):7749‚Äì\n7762, 2024.\n[24] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka,\nSergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David B Lindell. 4d-fy: Text-to-4d generation\nusing hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 7996‚Äì8006, 2024.\n[25] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu,\nJeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: Trajectory-conditioned text-to-4d\ngeneration. In European Conference on Computer Vision, pages 53‚Äì72. Springer, 2024.\n[26] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-\nview video diffusion model for 4d generation. Advances in Neural Information Processing Systems,\n37:15272‚Äì15295, 2024.\n[27] Dejia Xu, Hanwen Liang, Neel P Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N Plataniotis,\nand Zhangyang Wang.\nComp4d: Llm-guided compositional 4d scene generation.\narXiv preprint\narXiv:2403.16993, 2024.\n[28] Yifan Zhang, Chunli Peng, Boyang Wang, Puyi Wang, Qingcheng Zhu, Zedong Gao, Eric Li, Yang Liu,\nand Yahui Zhou. Matrix-game: Interactive world foundation model. arXiv, 2025.\n[29] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor,\nTroy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024.\n10\n[30] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay,\nYongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv\npreprint arXiv:2501.03575, 2025.\n[31] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How\nfar is video generation from world model: A physical law perspective. arXiv preprint arXiv:2411.02385,\n2024.\n[32] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li,\nYu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for\nvideo generation. arXiv preprint arXiv:2410.05363, 2024.\n[33] Jiannan Xiang, Guangyi Liu, Yi Gu, Qiyue Gao, Yuting Ning, Yuheng Zha, Zeyu Feng, Tianhua Tao, Shibo\nHao, Yemin Shi, et al. Pandora: Towards general world model with natural language actions and video\nstates. arXiv preprint arXiv:2406.09455, 2024.\n[34] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang,\nKa Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video\ngeneration with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 1‚Äì11,\n2024.\n[35] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa,\nAleksander Holynski, and Noah Snavely. Megasam: Accurate, fast, and robust structure and motion from\ncasual dynamic videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2025.\n[36] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video\ndepth anything: Consistent depth estimation for super-long videos. arXiv preprint arXiv:2501.12375,\n2025.\n[37] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher\nYu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 10106‚Äì10116, 2024.\n[38] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n[39] Aliaksandr Siarohin, St√©phane Lathuili√®re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion\nmodel for image animation. Advances in neural information processing systems, 32, 2019.\n[40] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using\nmulti-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2364‚Äì2373, 2018.\n[41] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie√üner.\nFaceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, pages 1‚Äì11, 2019.\n[42] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Rui-Jie Zhu, Xinhua\nCheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: A benchmark for metamorphic evaluation of\ntext-to-time-lapse video generation. Advances in Neural Information Processing Systems, 37:21236‚Äì21270,\n2024.\n[43] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy.\nCelebv-hq: A large-scale video facial attributes dataset. In European conference on computer vision, pages\n650‚Äì667. Springer, 2022.\n[44] Max Bain, Arsha Nagrani, G√ºl Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer\nvision, pages 1728‚Äì1738, 2021.\n[45] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan\nChen, Yaohui Wang, et al. Internvid: A large-scale video-text dataset for multimodal understanding and\ngeneration. arXiv preprint arXiv:2307.06942, 2023.\n[46] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye,\nShenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv\npreprint arXiv:2412.00131, 2024.\n11\n[47] Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo, Xiaoyun Yuan, Liuyu Xiang, Zerun Wang,\nGuiguang Ding, David Brady, Qionghai Dai, et al. Panda: A gigapixel-level human-centric video dataset.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3268‚Äì3278,\n2020.\n[48] Tom√°s Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition\ndetection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 11218‚Äì11221,\n2024.\n[49] Chenlong He, Qi Zheng, Ruoxi Zhu, Xiaoyang Zeng, Yibo Fan, and Zhengzhong Tu. Cover: A compre-\nhensive video quality evaluator. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) Workshops, pages 5799‚Äì5809, June 2024.\n[50] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276,\n2024.\n[51] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.\n[52] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles,\n2023.\n[53] Will Reese. Nginx: the high-performance web server and reverse proxy. Linux Journal, 2008(173):2, 2008.\n[54] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information\nProcessing Systems, 36:39033‚Äì39051, 2023.\n[55] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny.\nVggt: Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2025.\n[56] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything:\nUnleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10371‚Äì10381, 2024.\n[57] Aleksei Bochkovskii, Ama√ÉG, l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R Richter,\nand Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than a second. arXiv preprint\narXiv:2410.02073, 2024.\n[58] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun\nWang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding.\nIn European Conference on Computer Vision, pages 396‚Äì416. Springer, 2024.\n[59] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.\nScikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825‚Äì2830, 2011.\n12\n",
    "content": "# Paper Analysis: Sekai: A Video Dataset towards World Exploration\n\n## 1. Core Content and Major Contributions\nThe core content of this paper introduces **Sekai**, a high-quality first-person view (FPV) global video dataset designed to advance world exploration technologies based on video generation. The Sekai dataset includes over 5,000 hours of walking or drone-perspective videos, covering 101 countries and more than 750 cities worldwide. It provides rich annotation information, including location, scene type, weather, crowd density, subtitles, and camera trajectories.\n\nThe main contributions are as follows:\n- **Large-scale High-Quality Dataset**: Sekai is a comprehensive long-video dataset that spans the globe, addressing limitations in existing video generation datasets such as restricted locations, short video durations, static scenes, and lack of exploration-related annotations.\n- **Rich Annotations**: In addition to basic video content, Sekai provides various annotations, such as location, scene, weather, crowd density, subtitles, and camera trajectories, offering full support for world exploration tasks.\n- **Innovative Collection and Preprocessing Methods**: The research team developed an efficient toolchain for collecting, preprocessing, and annotating videos, ensuring the quality and diversity of the dataset.\n- **Subset Model Validation**: Using a subset of the Sekai dataset, an interactive video world exploration model named YUME (meaning \"dream\" in Japanese) was trained, demonstrating the dataset's potential in practical applications.\n\n---\n\n## 2. Breakthroughs and Innovations\nThis paper showcases breakthroughs and innovations in the following areas:\n- **Global Video Data**: The Sekai dataset not only covers multiple locations but also focuses on the diversity of different cultures, activities, architectural styles, and landscapes, making it the first truly globalized world exploration dataset.\n- **Long-duration Video Support**: Compared to other datasets, Sekai contains videos ranging from 1 minute to 39 minutes in length, with an average duration of 2 minutes, better simulating the real-world long-term exploration process.\n- **Multimodal Annotations**: In addition to traditional scene and weather annotations, Sekai introduces camera trajectory annotations, which are crucial for interactive video generation tasks.\n- **Combination of Gaming and Reality**: By integrating data from YouTube and a realistic game (Lushfoil Photography Sim), Sekai offers a richer visual experience while ensuring the authenticity and accuracy of some annotations.\n- **Efficient Data Screening Method**: A sampling strategy based on quality scores and diversity was proposed, enabling the selection of the optimal subset (Sekai-Real-HQ) from massive amounts of data, effectively reducing training costs and enhancing model performance.\n\n---\n\n## 3. Entrepreneurial Project Inspiration\nUsing the Sekai dataset as a core resource, many commercially valuable entrepreneurial projects can be derived. Below are some specific creative directions:\n\n### 3.1 Virtual Travel Platform\nBuild a virtual travel platform using the Sekai dataset, allowing users to explore scenic spots around the world in real-time by controlling the camera with their keyboard and mouse. This immersive experience can attract users interested in travel who cannot physically visit these places.\n\n### 3.2 Educational and Training Tools\nApply the Sekai dataset in the education field to develop interactive courses for geography, history, and cultural learning. For example, students can learn about the cultural backgrounds and natural landscapes of different countries through videos with detailed annotations.\n\n### 3.3 Autonomous Driving Simulation Environment\nUse the walking and drone-perspective videos in the Sekai dataset to train autonomous driving systems, particularly enhancing navigation capabilities in complex urban environments. Additionally, camera trajectory annotations can help optimize vehicle motion planning algorithms.\n\n### 3.4 Urban Planning and Design Assistance\nAnalyze urban areas in the Sekai dataset to extract information on pedestrian distribution, traffic conditions, and building design, providing decision support for urban planners.\n\n### 3.5 AI Director Assistant\nDevelop an AI director assistant tool based on the Sekai dataset to help filmmakers quickly generate lens scripts for specific scenes. By entering keywords (such as \"city streets at dawn\" or \"rainy park\"), AI can recommend matching video clips and shooting angles.\n\n### 3.6 Game Development Resource Library\nThe high-quality videos and annotations in the Sekai dataset can serve as reference materials for game development, especially suitable for open-world game designs. Developers can generate more realistic virtual environments based on real-world scenes.\n\n---\n\nThe above is an analysis and interpretation of this paper, hoping it will be helpful to you!",
    "github": "",
    "hf": ""
}