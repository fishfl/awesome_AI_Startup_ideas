{
    "id": "2510.03847",
    "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs",
    "summary": "This paper summarizes the advantages of small language models outperforming large language models in specific tasks and proposes engineering metrics and design patterns for building intelligent agent systems that primarily rely on small models with large models as a supplement.",
    "abstract": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance.Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Single-Agent",
    "authors": "Raghav Sharma,Manan Mehta",
    "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
    ],
    "comments": "Comments:9 Pages",
    "keypoint": "Small Language Models (SLMs) are often superior to Large Language Models (LLMs) for agentic workloads such as retrieval-augmented generation, function calling, structured decoding, and tool use.  \nSLMs offer significant cost, latency, and energy advantages over LLMs, especially in edge inference and production deployment.  \nGuided decoding and tool execution shift the focus from open-ended generalization to schema- and API-constrained accuracy.  \nSLMs can match or surpass LLMs in function calling and structured output tasks at 10×–100× lower token cost.  \nA taxonomy of SLMs optimized for agentic use includes models like Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, and Apple’s on-device ~3B model.  \nModern evaluation frameworks such as BFCL V3/V4 and StableToolBench provide grounded, reproducible benchmarks for tool use and function calling.  \nServing stacks like vLLM, SGLang, and TensorRT-LLM with XGrammar/Outlines enable efficient structured decoding and low-latency inference.  \nSLM-default systems with LLM fallback use uncertainty-aware routing and verifiers to balance cost and reliability.  \nEngineering metrics for agentic systems include Cost per Successful task (CPS), schema validity, executable-call rate, p50/p95 latency, and energy per request.  \nStructured generation using JSON Schema or CFG-constrained decoding ensures high format fidelity and parseability.  \nValidator-first tool use with pre-execution validation improves reliability and reduces errors in agent workflows.  \nLoRA and QLoRA enable efficient fine-tuning of SLMs for specific agentic tasks with minimal GPU memory.  \nDistillation techniques enhance reasoning capabilities in small models, as seen in Phi-4-Mini-Reasoning and DeepSeek-R1-Distill.  \nUncertainty-aware routing selects between SLMs and LLMs based on confidence, task tags, and cost constraints.  \nAbstention mechanisms allow models to defer to LLMs or request clarification when uncertainty exceeds a threshold.  \nThe reference architecture for SLM-default agents includes a router, capability registry, validators, execution layer, LLM fallback, and telemetry.  \nMigration to SLM-centric systems involves logging LLM usage, clustering tasks, fine-tuning SLM adapters, and iterative deployment with guardrails.  \nCase studies show SLMs achieve >99% validity in extraction/templating, reliable RAG+tool orchestration, and strong math/coding performance.  \nRisks include overfitting to narrow training data, benchmark drift, and security vulnerabilities from tool access.  \nMinimum reporting metrics for SLM-powered agents include task success rate, schema validity, latency, energy per request, escalation rate, and drift resilience.  \nSecurity threats include tool injection, data exfiltration, secrets exposure, policy evasion, and replay attacks.  \nLeast-privilege permissions, policy filters, sandboxing, and AST-normalized audit logs mitigate security risks.  \nFuture directions include standardized execution-grounded evaluations, better-calibrated routing, co-designed schemas, and continual adapter refresh from failure logs.  \nThe future of agentic AI lies in heterogeneous architectures where SLMs handle routine tasks and LLMs are used sparingly for complex reasoning.",
    "date": "2025-10-09",
    "paper": "XXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE \nSmall Language Models for Agentic Systems: A \nSurvey of Architectures, Capabilities, and \nDeployment Trade-offs \nRaghav Sharma  \nNortheastern University, Boston, \nUSA \nAtlanta, USA \nsharma.raghav103@gmail.com \nManan Mehta  \nUniversity of Southern California, \nUSA \nNew York, USA \nmanan.mehta2@gmail.com \n  \nAbstract — Recent evidence (indicates that \nSmall Language Models (SLMs; ≲1–12B params, \noccasionally ~20B) are not only sufficient but often \nsuperior for agentic workloads such as retrieval-\naugmented generation (RAG), robust function \ncalling, structured decoding, and programmatic \ntool use. NVIDIA argues that SLMs are the future \nof agentic AI and edge inference, emphasizing \ncost/latency/energy advantages and the role of \nguided decoding and tool execution in shifting the \nobjective \nfrom \nopen-ended \ngeneralization \nto \nschema- \nand \nAPI-constrained \naccuracy. \nWe \nsynthesize results across open and proprietary \nSLMs (e.g., Phi-4-Mini, Qwen-2.5-7B, Gemma-2-\n9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-\ndevice ~3B, DeepSeek-R1-Distill 1.5–70B) and \nconnect them to modern evaluation (BFCL V3/V4; \nStableToolBench) \nand \nserving \nstacks \n(vLLM/SGLang/TensorRT-LLM \n+ \nXGrammar/Outlines). We formalize SLM-default, \nLLM-fallback systems with uncertainty-aware \nrouting and verifiers, and propose engineering \nmetrics (e.g., Cost per Successful task (CPS), \nschema validity, executable-call rate, p50/p95 \nlatency, energy/request). Guided decoding and \nvalidator-first tool use allow SLMs to match or \nsurpass LLMs at a 10×–100× lower token cost on \ntoday’s APIs. \nKeywords— small language models, agents, function \ncalling, structured outputs, JSON Schema, guided \ndecoding, LoRA/QLoRA, routing, energy efficiency, edge \ninference \nI. INTRODUCTION  \nThe long-held conventional wisdom that “bigger is \nbetter” in language models has been decisively \nchallenged by a new wave of Small Language Models \n(SLMs). These compact yet powerful models are \nincreasingly demonstrating comparable, and often \nsuperior, task performance to frontier Large Language \nModels (LLMs) across numerous application-layer \nworkloads, while being dramatically faster, cheaper, \nand more energy-efficient. In the context of agentic \nsystems—where models are designed to call external \ntools, \ncompose \nstructured \noutputs, \nand \nfollow \ndeterministic workflows—the primary bottleneck is \nfrequently orchestration and I/O, rather than the long-\nrange world knowledge or vast generalist capabilities \nof LLMs. This survey firmly positions SLMs as the \ndefault, go-to engine for the majority of agent \npipelines, reserving larger LLMs as selective fallbacks \nfor only the most challenging cases (e.g., complex \nmulti-hop reasoning, safety-critical judgment requiring \nnuanced understanding, or extensive long-context \nsynthesis). We summarize compelling evidence from \nrecent technical reports and public benchmarks (up to \nlate 2025), provide a refined taxonomy of SLMs \nspecifically optimized for agentic use, and present \npractical design patterns for production migration to \nSLM-centric architectures. \nII. BACKGROUND AND DEFINITIONS \nWe define SLMs as decoder-only transformer \nmodels typically ranging from 1 to 12 billion \nparameters, though some effective models may extend \nslightly \nbeyond \nthis \nto \n~20B. \nThe \ndefining \ncharacteristic of these models is their optimization for \nspecific deployment constraints such as latency, cost, \nor on-device execution. Agentic systems, in this \ncontext, are sophisticated AI constructs that combine \nlanguage models with external tools (e.g., search \nengines, \ncode \nexecution \nenvironments, \nAPIs), \npersistent memory, retrieval mechanisms (like RAG), \nand intelligent planners. Our focus is on the capabilities \nof SLMs that are most critical for agentic performance: \n(1) function calling/tool use, enabling models to \ninteract with external systems; (2) structured generation \n(e.g., JSON, regex, grammar-constrained outputs), \nensuring reliable and parseable data; (3) code and data \nmanipulation, \nfor \nprogrammatic \ninteraction \nand \ntransformation; \nand \n(4) \ncontrollability \n(e.g., \ntemperature settings, stop conditions, adherence to tool \nschemas), which is vital for predictable agent behavior. \nContributions. (1) A system-oriented taxonomy of \nSLMs for agents; (2) a formal treatment of validator-\nfirst tool use and CFG/JSON-constrained decoding; (3) \nan \nuncertainty-aware \nSLM-default, \nLLM-fallback \narchitecture with a reference router; (4) engineering \nmetrics (CPS, executable-call rate, schema validity) \nand deployment recipes (LoRA/QLoRA + INT4); (5) a \ncurated, up-to-date bibliography of models, evals, and \nserving technologies. \nIII. Methodology \nWe use a mixed system and benchmark-driven \nmethodology to evaluate the role of small language \nmodels (SLMs) in agentic AI. The approach has four \npillars: model selection, evaluation frameworks, \narchitectural prototyping, and case studies. \nA. MODEL SELECTION AND SCOPE  \nWe survey representative SLMs (1–12B parameters; \noccasionally ~20B) spanning open and proprietary \nfamilies, including Phi-4, Qwen, Gemma, Llama, \nDeepSeek, Apple on-device foundation models, and \nOpenELM. Models are included based on (i) public \navailability and documentation, (ii) recency of \ntechnical reports (2023–2025), and (iii) demonstrated \nadoption in agent frameworks. \n \nB. EVALUATION FRAMEWORKS \nBenchmarks: We evaluate function calling on BFCL \nv4 and tool execution on StableToolBench; for \nstructured decoding, we reference results from these \nbenchmarks. \n \nSystem \nbaselines. \nWe \nqualitatively \nsynthesize \npublished cost/latency characteristics and structured-\noutput support across serving stacks (vLLM, SGLang, \nTensorRT-LLM) and libraries (Outlines, XGrammar), \nreferencing \ntheir \ndocumentation \nand \npublic \nevaluations. \n \nC. CASE STUDIES AND DESIGN PATTERNS \nWe \nconsider \nthree \nrepresentative \nagent \nworkloads:  (A) Extraction/Templating, (B) RAG + \nTool \nOrchestration, \nand \n(C) \nMath/Coding \nReasoning. For each, we measure schema validity, \ntask success rate, cost (CPS), and escalation \nfrequency to the LLM. \n \nIV. SURVEY OF SLMS USED IN AGENTS  \nTable 1 summarizes representative SLMs that have \ngained significant traction in agent stacks. Sizes, \ncontext windows, and notable capabilities are drawn \nfrom official reports, model cards, and recent \nbenchmarks. This table emphasizes SLMs between ~1–\n12B as practical defaults for agents; larger “upper-\nSLMs” (12–20B) like Mistral-NeMo 12B fit single-\nGPU servers while remaining economical and highly \nperformant. \nTABLE I.  \nREPRESENTATIVE SLMS FOR AGENTS \nModel \n(family) \nSmall Language Models \nParams \nand \nContext  \nHighlights for \nagents \nNotes \nMicrosoft \nPhi-4-Mini \n/ \nMini-\nReasoning \n3.8B & \n64K \nStrong \nmath/coding; \nrobust function \ncalling; \nexceptionally fast \ninference. \nReasoning/dist\nill variants; \nknown for \nefficient edge \ndeployment. \nAlibaba \nCloud \nQwen-2.5 \n0.5B–\n72B & \nUp \nto \n128K+ \nRich range of \nsizes including \n7B; strong tool \nuse and structured \noutput generation. \nTechnical \nreport v2 \n(2025); excels \nin format \nfidelity. \nGoogle \nGemma-2 \n2B, 9B, \n27B & \n128K  \nLightweight open \nmodels; solid \ncoding and \nreasoning; \nimproved \nmultilingual \nsupport. \n9B popular for \nagents; \nGemma 3 \nshows further \ndistillation \ngains. \nMeta \nLlama-3.2 \n(text-only \n& vision) \n1B, 3B \n(text); \n11B, \n90B \n(vision) \n& \n128K \nOn-device focus; \nquantized \nvariants; Llama \n3.2 Vision adds \nmultimodal \ncapabilities. \nMobile/edge \nfriendly; real-\ntime \nprocessing on \ndevice. \nMistral AI \nMinistral \n3B, 8B \n& \n32K–\n128K \nExcellent function \ncalling; highly \nefficient attention \nmechanisms. \nDesigned for \nlocal/edge \ndeployment; \nstrong \ninstruction \nfollowing. \nNVIDIA \nMistral-\nNeMo \n12B & \n128K \nStrong small/open \nmodel; \nmultilingual; \nexcels in \nreasoning and \nmulti-turn \nconversations. \nApache-2.0 \nlicense; \noptimized for \nsingle-GPU \ndeployment. \nDeepSeek-\nR1-Distill \n1.5B–\n70B & \n32K–\n128K \nReasoning distills \ncompetitive at 7–\n8B; strong \nperformance on \ncoding tasks. \nOpen \ncheckpoints; \nleverages \ndistillation for \nenhanced \nreasoning. \nApple on-\ndevice FM \n~3B & \n≥32K  \nOn-device/private \ntool use; guided \ngeneration; \noptimized for \nApple silicon. \nOfficial tech \nreport (2025); \nfocus on \nprivacy and \nlow-latency. \nOpenELM \n270M–\n3B \n& \n8K–\n32K \nOpen small \nmodels; good for \nfine-tuning; \nefficient \narchitecture. \nApple open \nweights; \ndesigned for \nbroad \napplicability. \n \nV. TOOL USE AND FUNCTION CALLING \nThe \nseminal \nwork \non \nToolformer \n(2023) \ndemonstrated that even mid-sized models could learn \nAPI invocation through self-generated annotations, \nbypassing the need for extensive human labeling. \nSubsequent advancements, notably Gorilla (2023) and \nthe ongoing Berkeley Function-Calling Leaderboard \n(BFCL, 2024–2025), provide a modern perspective: \ntool-use accuracy is more critically dependent on \nargument correctness and strict schema adherence than \non raw parameter count. This insight is pivotal for \nSLMs. When paired with explicit tool schemas and \nrobust validators, SLMs frequently match or even \nsurpass larger LLMs in function-calling reliability and \nspeed. The introduction of StableToolBench (2024–\n2025) further refines this by providing a controlled \nvirtual API server, significantly reducing benchmark \ndrift and enabling more accurate, apples-to-apples \ncomparisons across model releases and architectural \nchanges. \nFrom Toolformer to stable tool evals. Toolformer \nshowed self-annotation can teach API invocations; \nGorilla framed API-grounded tool use; API-Bank and \nToolBench benchmark diverse tools; StableToolBench \nintroduced a virtual API server and caching to reduce \nbenchmark drift; BFCL v4 (2025) evaluates multi-turn, \nenterprise-style tools with cost/latency. \nSchema-first execution. Let a tool signature be a \nJSON-Schema S over arguments a. We define \nexecutable-call rate \n \nand argument exactness as AST-level equality (as in \nBFCL). In practice, SLMs with enforced schemas and \npre-execution validation achieve high ExecRate at far \nlower latency/cost than LLMs. \nA. Design tips for reliable structured generation: \n• Format fidelity as a first-class KPI: Treat the \ncorrectness and adherence to schema as a primary Key \nPerformance Indicator (KPI) for agent performance. \n \n• Streaming JSON with incremental validators: \nImplement streaming JSON output combined with \nincremental validators. This allows for early detection \nof malformed outputs and can provide faster feedback \nloops. \n \n• Fuzz schemas during CI: Integrate schema fuzzing \ninto \nContinuous \nIntegration \n(CI) \npipelines \nto \nproactively identify edge cases and vulnerabilities in \nschema definitions. \n \n• Record failure traces for adapter tuning: Log \ninstances of structured output failures, including the \ninput prompt and the malformed output. These traces \nare invaluable for fine-tuning SLM adapters to \nimprove robustness. \n \nVI. STRUCTURED GENERATION: JSON/CFG-\nCONSTRAINED DECODING \nWhy constraints matter. For agent stacks, format \nfidelity often dominates prose quality. Modern serving \nengines implement constrained decoding over JSON \nSchema or CFGs to prune the token search space and \nguarantee parsability. \nBackends and engines. vLLM integrates structured \ndecoding via Outlines and XGrammar (up to ~5× \nTPOT speedups under load); XGrammar is a fast, \nportable CFG library also integrated in TensorRT-\nLLM; SGLang supports low-latency serving with KV-\ncache optimizations and JSON constraints. \nEmpirical comparisons. Recent studies evaluate \nOpenAI/Gemini/Outlines/XGrammar/llama.cpp across \nreal-world schemas (“JSONSchemaBench”) and JSON \nSchema Test Suite; findings highlight engine and \nschema-complexity sensitivity. \nA. Recommended practice: \n• Treat Schema Validity as a KPI; measure valid@1 \nand valid@k. \n• Use streaming JSON with incremental validators; \nfail fast. \n• Fuzz schemas in CI & capture failure for adapter \nfine-tunes. \n• Prefer grammar-first prompts (CFG or JSON \nSchema) + temperature 0. \nVII. TRAINING AND ADAPTATION FOR AGENTS \nSpecializing SLMs for agentic tasks is remarkably \nstraightforward and efficient. Common and highly \neffective techniques include: LoRA (Low-Rank \nAdaptation) and its enhanced variant LoRA+ adapters, \nQLoRA (Quantized Low-Rank Adaptation) for 4-bit \nfine-tuning, and the creation of small, carefully curated \nSupervised Fine-Tuning (SFT) datasets derived from \ntool-use traces or structured outputs. For enhancing \nreasoning capabilities, distillation-heavy recipes—such \nas those employed in DeepSeek-style models or the \nPhi-4-Mini-Reasoning variant—have proven highly \neffective. These methods typically combine extensive \nChain-of-Thought (CoT) SFT, DPO (Direct Preference \nOptimization) from preference data, and short-cycle \nReinforcement Learning (RL) with verifiable rewards. \nCompared to full LLM fine-tuning, these approaches \ncan reduce GPU memory requirements by an order of \nmagnitude while preserving, or even improving, \nquality on specific agent tasks. \nA. Practical recipe for SLM specialization  \n• Data Collection: Gather 10,000–50,000 de-identified \ntraces of successful agentic interactions. These traces \nshould capture diverse scenarios and tool uses. \n \n• Adapter Training: Train LoRA adapters per task \ncluster (e.g., one adapter for function calling, another \nfor JSON generation). This allows for modular and \nefficient specialization. \n \n• Quantization for Serving: Quantize the fine-tuned \nmodels to INT4/INT8 for deployment, significantly \nreducing memory footprint and increasing inference \nspeed. \n \n• Periodic Refresh: Implement a periodic refresh \nmechanism for the adapters, using logged validator \nfailures and new edge cases to continually improve \nmodel performance and robustness. \nVIII. ROUTING, ABSTENTION, AND FALLBACK \nSLM-default routing: Given a request x, a router r \nselects a model m ∈ M_SLM ∪ {LLM} minimizing \nexpected risk + dollar cost. Practical routers combine \nconfidence proxies (logprob, self-consistency), task \ntags (capability registry), and budget constraints \n(FrugalGPT), evaluated on RouterBench. \n \nSelective prediction / abstention: Let ŷ, u be a \nprediction and uncertainty; abstain if u > τ. Modern \nabstention surveys and VLM studies show improved \nreliability by abstaining or asking follow-up questions, \nwhich dovetails with LLM escalation in agents. \nPseudo Algorithm — Uncertainty-aware SLM→LLM \nrouting (sketch).  \nInput: request x, tools 𝒯, schema S, \nrouter r, thresholds (τu, τv), \nmax_retries k \n \nm ← r.select(x)   # prefer SLMs tagged \nfor the task \n \n \nfor i in 1..k: \ny, meta ← m.generate(x; schema=S, \nT=0, guided=True) \nif meta.uncertainty ≤ τu and \nvalidate(y,S,𝒯)=⊤: return y \ny ← repair_with_verifier(x,y,S)   \n# small verifier SLM attempts  \nfix \nif validate(y,S,𝒯)=⊤ and \nmeta.uncertainty ≤ τv: return y \n# escalate \nyLLM, metaLLM ← LLM.generate(x; \nschema=S, T=0, guided=True) \nreturn yLLM \nIX. INDUSTRIAL DEPLOYMENT PLAYBOOK AND COST \nMODEL \nCapacity planning. Serving SLM-default agents is \ndominated by KV-cache residency and batch dynamics \nrather than parameter count alone. Let B be the \neffective batch, T the average generated tokens, L \nlayers, H heads, dh head size, and b bytes/element \n(e.g., 1 for INT8 KV, 2 for FP16). A first-order KV \nbudget is \n \nwhere the factor 2 accounts for K and V. Target \nutilization keeps MemKV at 70–85% of device \nVRAM to absorb tail spikes and routing surges. \nLatency p50 falls roughly with higher B until \nscheduler contention and cache swaps dominate; p95 \nis controlled by back-pressure limits and max queue \ndepth. For SLMs, INT4/INT8 weights with FP16 KV \ncaches \ntypically \nmaximize \nthroughput \nwithout \nharming schema fidelity. \n \nSLA tiers. We recommend two service classes: \nInteractive (p50 ≤ 200–400 ms prefill, p95 ≤ 1.5–2.0 s \nE2E for short JSON/tool hops) and Batch (throughput-\noptimized, p95 ≤ 10–30 s per task). Each class exposes \nhard guards (max tokens, max tools/turn, deadline \nbudget) enforced by the router. Stable operation tracks \n{valid@1,ExecRate,CPS,p50/p95} p50/p95\\} per class \nand per model. \n \nCost forecasting (tokens → $ and J). Let ci be per-\ntoken cost for model i and ei the energy per token \nunder a fixed engine. For a mixture policy π that routes \na fraction αi of requests to model i, \n \n \n \nBecause SLMs shorten prefill and require fewer retries \nunder grammar-guided decoding, CPS typically drops \nby 10–30× compared to LLM-only baselines, with \nproportional joule savings. \nRollouts: blue/green & shadow. Every router change \nand adapter update should pass through: (1) Shadow \nevals on a mirrored traffic slice with strict write \nisolation; (2) Blue/green promotion keyed to CPS and \nvalid@1 non-regression; (3) Auto-rollback on any of: \nschema validity drop >2 pp, ExecRate drop >3 pp, or \np95 inflation >20%. \nHuman-in-the-loop (HITL) gating. For safety-\nsensitive tools (payments, PII transforms), gate \nexecution on either (a) two-stage verification (SLM \nproposal → verifier SLM/LLM adjudication), or (b) \non-call human approve/deny queues triggered by \nuncertainty u>τ or policy-risk scores r>ρ. HITL \nfeedback is logged as counterfactual traces for adapter \nrefresh. \nAblations that matter. The following table illustrates \nthe most action-able levers on CPS; values are \nrepresentative (fill with your measurements). \nTABLE II.  \nREPR CPS AND RELIABILITY ABLATION \n(REPRENTATIVE) \n \nOperational playbook (checklist). (i) Pin engine + \ngrammar library versions; (ii) set queue caps per SLA \nclass; (iii) reserve 10–15% VRAM headroom; (iv) \nenforce max tool calls/turn; (v) ship canary with per-\ntenant kill-switches; (vi) log AST-normalized tool \nargs; (vii) weekly adapter refresh from failure traces. \nX. COST, LATENCY, AND ENERGY \n \n \nFig. 1. Cost Performance Comparisions : SLMs vs LLMs \nOne of the most compelling advantages of SLMs is \ntheir ability to drastically reduce token-latency and \nhardware footprint. This translates into a substantial \n10–30× cost reduction for common agent calls \ncompared to using larger LLMs. Emerging energy \nbenchmarks, such as ML.ENERGY, and recent studies \non quantization and edge inference, unequivocally \ndemonstrate that smaller models and 4-bit deployments \nyield significant joule savings. The per-prompt energy \nconsumption is strongly correlated with the output \ntoken length, making SLMs inherently more efficient \nfor concise, structured outputs. Edge-first evaluation \nstrategies suggest keeping default execution local on \nconsumer-grade hardware, with cloud fallback reserved \nfor scenarios requiring scale-out or processing \nexceptionally long contexts. \nDefine Cost-per-Successful task (CPS) over a batch \nB: \nThis metric is defined as the total operational cost \ndivided by the number of schema-valid, tool-valid \ncompletions. It provides a holistic view of efficiency. \n \n \n \nwith valid = schema-valid JSON; exec = tool call \nexecuted without error. Under structured decoding \n(grammar-guided) and temperature 0, SLMs realize \norder-of-magnitude CPS improvements due to shorter \nprefill, smaller KV cache, and higher valid@1 rates. \nEmpirically, vLLM reports substantial TPOT gains \nwith XGrammar backends; FrugalGPT shows that \ncascades can approach best-LLM accuracy at ~98% \nlower cost via routing. \nWhen energy metering is unavailable, output-token \ncount times J/token under a fixed engine provides an \noperational proxy; low-token JSON responses + SLM \nthroughput typically minimize p50/p95 latency and \njoules/request on shared GPUs. \nXI. WHEN DO LLMS STILL WIN? \nDespite the rapid advancements in SLMs, frontier \nLLMs retain their superiority in specific, high-demand \nscenarios: open-domain synthesis with complex, long-\nrange \ndependencies; \nknowledge-heavy \nQuestion \nAnswering (QA) tasks that cannot be effectively \naddressed by Retrieval-Augmented Generation (RAG); \nand \nsafety-critical \njudgment \nunder \nsignificant \ndistribution shift. They are also preferable when \nalgorithmic planning necessitates dense search over \nmany latent trajectories (e.g., complex code repair \nacross large repositories) or when policy/compliance \nmandates frontier-grade guardrails. In SLM-default \narchitectures, routing to an LLM should be a deliberate \ndecision, triggered only by these specific, high-\ncomplexity conditions. \nXII. A REFERENCE ARCHITECTURE FOR SLM-\nDEFAULT AGENTS \nComponents. (a) Front-door router with capability \nregistry; (b) structured decoding (JSON/CFG) on \nevery hop; (c) validators (schema + tool arg checks); \n(d) execution layer (retrievers, sandboxes, API \nclients); (e) LLM fallback & adjudication; (f) \ntelemetry (prompts, failures, escalations) feeding fine-\ntunes. \nMigration blueprint. Log LLM usage → cluster tasks \n→ fine-tune SLM adapters per cluster (LoRA/QLoRA) \n→ quantize & deploy (AWQ/GPTQ) → add \nuncertainty routing + validators → iterate with human \nevaluation and guardrails.  \n \n \nFig. 2. Heterergemos AI Architecture – Intelligent Routing \nfor Efficienct - An optimal architecture for SLM-default \nagents is characterized by a modular and intelligent design: \n• Front-door router: This component intelligently \ndirects incoming requests based on cost, latency, and \nuncertainty. It acts as the primary traffic controller, \ndeciding whether to route to an SLM or escalate to an \nLLM. \n \n• Capability registry of SLMs: A dynamic registry that \ntags SLMs by their specific strengths (e.g., extraction, \ntool use, coding, summarization). This allows the \nrouter to select the most appropriate SLM for a given \ntask. \n \n• Validators: A suite of robust validators, including \nJSON schema validators, function-argument checkers, \nand policy filters. These ensure output fidelity and \nadherence to rules. \n \n• Execution layer: This layer comprises various \ncomponents such as retrievers (for RAG), code \nsandboxes (for safe code execution), and API clients \n(for tool interaction). \n \n• LLM fallback and adjudication: A mechanism to \ninvoke LLMs only on low-confidence predictions or \nrepeated violations from SLMs. This layer also \nhandles conflict resolution and complex decision-\nmaking. \n \n• Telemetry: Comprehensive logging of prompts, \nerrors, and escalations. This data feeds back into the \nsystem for continual improvement and fine-tuning of \nSLMs. \n \nA. Migration blueprint : \n• Log Usage: Begin by logging all LLM usage within \nexisting agentic systems. \n \n• Cluster Tasks: Analyze the logged data to cluster \ntasks based on their complexity, type, and frequency. \n \n• Fine-tune SLMs: For high-frequency, well-defined \ntask clusters, fine-tune specialized SLMs. \n \n• Replace Routine LLM Calls: Gradually replace \nroutine LLM calls with the newly fine-tuned SLMs. \n \n• Iterate with Human Evaluation + Guardrails: \nContinuously monitor performance, gather human \nfeedback, and refine guardrails to ensure quality and \nsafety. \n \nThe architecture routes every request through a front-\ndoor router that picks the cheapest, fastest competent \nmodel first—typically a small language model (SLM) \nchosen from a capability registry (e.g., extraction, tool \nuse, coding). SLM outputs are forced through \nstructured decoding (JSON/CFG) and then checked by \nvalidators (schema + tool-argument/policy checks); if \nvalid and confidence is high, the execution layer \n(retrievers for RAG, code sandboxes, and API clients) \nruns the action and returns results. When the SLM is \nuncertain or repeatedly violates constraints, the router \nescalates to a large language model (LLM) for \ncomplex \nreasoning \nor \nopen-domain \nsynthesis, \noptionally adjudicating conflicts. Telemetry logs \nprompts, failures, validations, costs, and latencies, \nfeeding continual fine-tuning (e.g., LoRA/QLoRA) \nand improved guardrails. In short: SLM-by-default for \nroutine, structured tasks; LLM-by-exception for hard \ncases—governed by routing, validation, and feedback \nloops \nXIII. CASE STUDIES AND DESIGN PATTERNS \nPattern A—Extraction/Templating: An SLM (e.g., \n3–9B parameters) guided by JSON-Schema can \nachieve >99% validity in data extraction and \ntemplating tasks at a fraction of the cost of an \nLLM. A larger LLM is only invoked as a fallback \non validator failure, ensuring high reliability and \ncost-efficiency. \n \nPattern B—RAG + Tools: A 7–12B SLM with \nstrong function-calling capabilities (e.g., Ministral \n8B, Mistral-NeMo 12B, Qwen-2.5-7B) can \nreliably orchestrate search and calculation tasks \nwithin a Retrieval-Augmented Generation (RAG) \npipeline. Escalation to an LLM occurs only when \nthe uncertainty of the SLM's output exceeds a \npredefined threshold (τ). \n \nPattern C—Math/Coding Reasoning: For fast unit \ntests and localized code generation, models like \nPhi-4-Mini-Reasoning (3.8B) or DeepSeek-R1-\nDistill-7B offer excellent performance. Larger \nmodels are invoked only for more complex tasks \nsuch as cross-file refactoring or working with \nnovel programming frameworks. \nXIV. RISKS AND EVALUATION \nWhile SLMs offer significant advantages, it's crucial to \nacknowledge potential risks. SLMs can sometimes \noverfit narrow training traces, leading to a regression \nin generalization capabilities. To mitigate this, \nrigorous evaluation should include held-out end-to-end \ntasks, adversarial tool inputs, and comprehensive \nschema fuzzing. Regarding safety, smaller model size \ndoes not inherently guarantee harmlessness; it is \nimperative to apply robust content filters and rate-limit \nhigh-risk \ntool \ninvocations. \nWhen \nevaluating \nperformance, it is preferable to rely on leaderboards \nwith \ngrounded \nexecution \n(e.g., \nBFCL, \nStableToolBench) and real-user A/B tests, rather than \nsolely on static zero-shot academic benchmarks. \nA. Minimum metrics to report for SLM-powered \nagents : \n• Task success rate: The percentage of tasks \nsuccessfully completed. Minimum metrics to report for \nSLM-powered agents: \n• Schema validity: The percentage of structured \noutputs that adhere to their defined schemas. \n• p50/p95 latency: The 50th and 95th percentile of \nresponse times. \n• Energy per request: The energy consumed per \nsuccessful agent interaction. \n• Escalation rate: The frequency at which tasks are \nescalated to a larger LLM. \n• Drift resilience: The model's ability to maintain \nperformance over time and with minor shifts in input \ndistribution. \nBenchmark brittleness. Tool evals are sensitive to \nAPI drift; StableToolBench’s virtual APIs mitigate \nvariance and improve reproducibility. Always report \nschema-validity, ExecRate, and CPS alongside task \nsuccess; avoid over-reliance on static zero-shot \nacademic leaderboards. \nSafety and protocols. Tool access implies real-world \nside-effects. Combine policy filters, rate-limits, and \nleast-privilege tool permissions. With MCP/OpenAPI, \naudit tool registries for poisoning/injection; prefer \nallow-lists; log all calls with AST-normalized args for \nforensics. \nMinimum metrics to report. Task success, Schema \nValidity, ExecRate, CPS, p50/p95 latency, escalation \nrate, \ndrift \nresilience, \nand \n(when \npossible) \nenergy/request. \nXV. SECURITY, GOVERNANCE & COMPLIANCE FOR \nTOOL-USING AGENTS \nThreat model. Beyond prompt injection, tool-using \nagents face: (i) Tool injection & supply-chain risk \nvia poisoned OpenAPI/MCP manifests (untrusted base \nURLs, widened scopes, covert side-effects); (ii) \nCross-tool data exfiltration (e.g., RAG → code-\nexec); (iii) Secrets exposure in prompts, schemas, or \nlogs; (iv) Policy evasion through schema-shaped but \nsemantically malicious arguments; (v) Replay & drift: \ncached tool replies reused out of policy context. \nPermissioning & least privilege. Treat tools as \ncapabilities with scoped, expiring tokens; default-\ndeny with allow-lists per tenant and per route. Every \ntool t carries a policy triple (scope(t),rate(t),PII(t)). The \nrouter enforces scope(t) selection by task tags; \nvalidators check rate(t) and redact/deny if PII(t) is \n“restricted.” Rotate credentials on deploy; require \nsigned manifests (checksum + issuer). \nSecrets handling. Never inline secrets in prompts or \ntool schemas. Retrieve ephemeral creds at invocation \nvia a secrets manager; bind to request ID and caller \nidentity; prevent echo in model context by masking \n(server-side) and by no-log annotations on sensitive \nfields. Redact in telemetry with reversible, tenant-\nscoped envelopes when auditability is required. \nSandboxing & code execution. For code tools, \nenforce: resource limits (CPU/GPU/FS/network), \nsyscall allow-lists, outbound domain allow-lists, and \nfilesystem jails. Disallow dynamic tool creation from \nmodel output; require human approval for new tools or \nscope escalations. Prefer deterministic runtimes with \nsnapshotting; discard state on completion. \nAudit trails (AST-normalized). Log tool calls as \nAST-normalized arguments plus policy decision \noutcomes. Store: model ID, grammar hash, schema \nversion, router decision, uncertainty u, verifier verdict, \nand execution result. Normalize PII fields to opaque \nhandles. This enables reliable forensics, de-dup of \nnear-misses, and exact replay for adjudication. \nIncident metrics & triggers. Maintain leading \nindicators: (a) Schema-valid but policy-invalid rate; \n(b) Denied-but-re-attempted fraction; (c) Cross-tool \ndata flow violations; (d) Credential anomalies \n(reuse, stale token hits); (e) Drift in tool success not \nexplained by upstream changes. Page on: spike >3σ in \n(a) or (c), or any tool executing outside declared scope. \nPolicy filters. Apply multi-stage filtering: (1) Pre-gen \ninstruction filters to remove tool names/URLs from \nuser content; (2) Constrained decoding to prevent \nillicit argument shapes; (3) Post-gen semantic allow-\nlist checks (regex/CFG + learned classifiers) on \narguments; (4) Execution-time guards (rate limits, \nquota, row-level access). Filters are versioned, tested \nin CI with schema fuzzing, and tied to rollback. \nGovernance & compliance. Map tools and data flows \nto regulatory surfaces (PII, PCI, HIPAA, SOX). For \neach tenant: (i) data residency tags; (ii) retention \npolicies for prompts/logs; (iii) DLP at retrieval and at \negress; (iv) DPIA/TRA records for high-risk tools; (v) \nmodel cards stating known limitations and escalation \ncriteria. Provide customer-controlled allow-lists and \nexplicit consent toggles for cross-region or cross-\ndomain calls. \nXVI. LIMITATIONS AND FUTURE SCOPE \nA. Limitations:  \n• Benchmark/API drift; results may not transfer. \n• Overfitting to narrow traces. \n• Heavy validator dependence can hide reasoning \nerrors. \n• Router miscalibration causes wrong SLM/LLM \nescalations. \n• Tool-use expands the security risk surface. \nB. Future Scope:  \n• \nExecution-grounded, \nstandardized \nevals \nwith \ncost/latency/energy. \n• Better-calibrated routing and selective abstention. \n• Co-designed schemas + verifiers; formal checks for \ncritical tools. \n• Continual LoRA/QLoRA refresh from failure logs. \n• Stronger tool security (sandboxing, allow-lists, \ninjection defenses).  \n \nXVII. \nCONCLUSION \nThe application layer of AI systems stands to benefit \nimmensely from the adoption of smaller, specialized, \nand well-constrained language models. SLM-default \nsystems are poised to achieve substantial gains in cost-\nefficiency, inference latency, energy consumption, and \noverall controllability, all without sacrificing reliability \non the core tasks that agents are designed to perform. \nThe future of AI is not solely about building ever-\nlarger models; rather, it lies in developing smarter, \nheterogeneous architectures where SLMs undertake \nthe majority of the operational workload, and LLMs \nare invoked judiciously and sparingly for their unique \ngeneralist capabilities. This paradigm shift promises a \nmore sustainable, scalable, and economically viable \nfuture for agentic AI. \nREFERENCES \n[1] Schick, T. et al. Toolformer: Language Models Can Teach \nThemselves to Use Tools (2023). ai.meta.com  \n[2] Patil, S. G. et al. Berkeley Function-Calling Leaderboard \n(BFCL) v4 (2025). Gorilla \n[3] Guo, Z. et al. StableToolBench: Towards Stable Large-Scale \nBenchmarking on Tool Learning (Findings of ACL 2024). \nACL Anthology \n[4] Li, M. et al. API-Bank: A Comprehensive Benchmark for \nTool-Augmented LLMs (EMNLP 2023). ACL Anthology \n[5] Hu, E. et al. LoRA: Low-Rank Adaptation of Large Language \nModels (2021). arXiv \n[6] Hayou, S. et al. LoRA+ (ICML 2024). Proceedings of \nMachine Learning Research \n[7] Dettmers, T. et al. QLoRA: Efficient Finetuning of Quantized \nLLMs (2023). arXiv \n[8] Lin, J. et al. AWQ: Activation-Aware Weight Quantization \n(MLSys 2024). MLSys Proceedings \n[9] Frantar, E. et al. GPTQ: Accurate Post-Training Quantization \n(ICLR 2023). arXiv \n[10] Lewis, P. et al. Retrieval-Augmented Generation for \nKnowledge-Intensive NLP (2020). arXiv \n[11] vLLM team. Structured Decoding in vLLM (Jan 2025). \nvLLM Blog \n[12] XGrammar team (CMU/MLC/NVIDIA). XGrammar (docs & \nproject page). catalyst.cs.cmu.eduxgrammar.mlc.ai \n[13] Outlines \ncontributors. \nStructured \ngeneration \nresources \n(awesome-llm-json). GitHub \n[14] Microsoft Azure Blog. Phi-4-mini-flash-reasoning (Jul 2025). \nMicrosoft Azure \n[15] Qwen team. Qwen-2.5 release blog (2024/2025). Qwen \n[16] Google. Introducing Gemma 2 (2024). Qwen \n[17] Meta AI. Llama-3.2 for edge & vision (Sep 2024). \nai.meta.com \n[18] AWS Blog. Llama-3.2 models in Bedrock (Sep 2024). \nAmazon Web Services, Inc. \n[19] Apple. Apple Intelligence Foundation Language Models \n(Tech Report 2024/2025). arXivApple Machine Learning \nResearch \n[20] OpenAI. Function Calling & Structured Outputs docs. \nOpenAI Platform+1 \n[21] Hu, Q. J. et al. RouterBench (2024). arXiv \n[22] Chen, \nL. \net \nal. \nFrugalGPT \n(2023; \nTMLR \n2024). \narXivLingjiao Chen \n \n",
    "content": "# Paper Interpretation: *Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade-offs*\n\n---\n\n## 1. **Core Content and Main Contributions**\n\n### Core Content:\nThis paper systematically argues that **small language models (SLMs), typically with 1–12 billion parameters, are not only sufficient but often superior to large language models (LLMs) in agentic system tasks**. The authors demonstrate that performance bottlenecks in agent workflows—such as retrieval-augmented generation (RAG), function calling, structured output generation, and tool use—are more frequently due to orchestration and I/O control issues than limitations in model generalization or knowledge breadth.\n\nTherefore, the paper advocates treating SLMs as the **default execution engine** in agent systems, reserving LLMs only as a \"fallback\" option for highly complex tasks. This leads to a **heterogeneous, efficient, and controllable AI architecture paradigm**.\n\n### Key Contributions:\n- Proposes a **taxonomy of SLMs tailored for agentic systems**;\n- Formally defines and promotes a **\"validator-first + structured decoding\" approach** for tool calling;\n- Designs an **uncertainty-aware routing architecture** that defaults to SLMs and escalates to LLMs when needed;\n- Introduces engineering-focused metrics such as **CPS (cost per successful task)**, schema validity, executable-call rate, p50/p95 latency, and energy consumption per request;\n- Provides a comprehensive deployment guide covering LoRA/QLoRA fine-tuning, quantization, monitoring, and canary releases;\n- Curates an authoritative reference repository of state-of-the-art SLMs, evaluation benchmarks, and service stacks.\n\n---\n\n## 2. **Breakthroughs and Innovations**\n\n### (1) **Challenges the \"Bigger is Better\" Paradigm**\nBased on extensive empirical data (e.g., BFCL v4, StableToolBench), the paper shows that across most agent workloads, **SLMs match or even outperform LLMs in accuracy while costing 10–100x less**. This marks a shift from pursuing general intelligence to optimizing for **task efficiency**.\n\n### (2) **Introduces the \"SLM-default, LLM-fallback\" Architecture**\nThis is the paper’s most significant architectural innovation. By integrating a **front-end router**, **uncertainty estimation**, and **multi-stage validation**, the system:\n- Uses low-cost, low-latency SLMs by default;\n- Escalates to LLMs only when confidence is low or validation fails;\n- Maintains high reliability while drastically reducing operational costs.\n\n> Example: Using vLLM + XGrammar for grammar-guided decoding enables SLMs to achieve near 100% `valid@1` scores, minimizing retries.\n\n### (3) **Emphasizes Structured Output and Controlled Generation**\nThe paper stresses that in agent systems, **format correctness** is more critical than linguistic fluency. To this end, it strongly advocates:\n- Guided decoding using JSON Schema or Context-Free Grammars (CFG);\n- Streaming JSON output with incremental validation (fail-fast);\n- Temperature=0 and deterministic decoding for predictable behavior.\n\nThese techniques enable SLMs to reliably produce machine-parsable outputs, significantly improving system robustness.\n\n### (4) **Establishes a Full Industrial Deployment Methodology**\nUnlike purely academic surveys, this paper offers an end-to-end implementation blueprint, including:\n- Capacity planning formulas (KV cache budgeting);\n- SLA tiering (interactive vs. batch processing);\n- Cost modeling (CPS) and energy efficiency analysis;\n- Operational strategies like blue-green deployment, auto rollback, and human-in-the-loop (HITL) safety gates.\n\n### (5) **Deepens Consideration of Safety and Governance**\nAddressing risks in tool-using agents (e.g., tool injection, privilege escalation, data leaks), the paper proposes:\n- AST-normalized logging for auditability;\n- Principle of least privilege;\n- Multi-stage policy filtering;\n- Traceable decision chain recording.\n\nThese measures lay a solid foundation for building enterprise-grade, trustworthy agent systems.\n\n---\n\n## 3. **Startup Ideas Inspired by the Paper**\n\nHere are several entrepreneurial opportunities rooted in the core ideas of the paper:\n\n---\n\n### 🚀 Startup Idea 1: **AgentOps — SLM-Powered Agent Orchestration Platform for SMEs**\n\n#### Concept:\nBuild an out-of-the-box enterprise agent platform featuring pre-trained SLMs (e.g., Qwen-7B, Phi-4-Mini, Gemma-9B). It supports one-click setup of RAG, API calls, and automation workflows, with built-in **dynamic SLM → LLM routing**.\n\n#### Key Features:\n- **Visual Agent Designer**: Drag-and-drop interface to build task flows (extract → query → call API → send email);\n- **Built-in Structured Decoding Engine**: Integrated with XGrammar/vLLM to ensure output compliance;\n- **Smart Routing Gateway**: Automatically selects SLM or escalates to GPT-4o based on task type and uncertainty;\n- **Cost Dashboard**: Real-time display of CPS, escalation rate, and latency distribution;\n- **Security Sandbox**: Isolated code execution, field redaction, and permission whitelisting.\n\n#### Target Customers:\nSaaS companies, e-commerce customer service automation, financial reporting, legal document generation—any domain needing cost-effective, reliable automation.\n\n#### Business Model:\nPay-per-successful-task (CPS pricing), with free trials and enterprise customization.\n\n> ✅ **Differentiator**: Lighter, cheaper, and more structured-task-optimized than LangChain or Mistral AI API.\n\n---\n\n### 🚀 Startup Idea 2: **ValidFlow — SaaS Tool for Structured Output Quality Assurance**\n\n#### Concept:\nA monitoring and automatic repair service for teams building agents with LLMs/SLMs, focused on **schema compliance, tool call validity, and semantic consistency**.\n\n#### Key Features:\n- **Schema Fuzzing as a Service**: Automatically test edge cases in your JSON Schema;\n- **Real-Time Validity Monitoring**: Track KPIs like `valid@1`, `ExecRate`, and `CPS`;\n- **Failure Root-Cause Analysis**: Cluster error patterns (missing fields, type mismatches);\n- **Adaptive Adapter Training Pipeline**: Auto-generate LoRA fine-tuning jobs from failure traces;\n- **CI/CD Integration**: GitHub Actions plugin to prevent schema regressions.\n\n#### Technical Highlights:\n- Implements the paper’s “failure trace → adapter refresh” feedback loop;\n- Supports OpenAPI/Swagger schema import;\n- Offers SDKs for embedding into existing inference services.\n\n#### Use Cases:\nFinancial tech, medical information extraction, government data submission—domains requiring strict format adherence.\n\n> ✅ **Pain Point Addressed**: Prevents pipeline failures caused by simple JSON formatting errors.\n\n---\n\n### 🚀 Startup Idea 3: **EdgeAgent — Private AI Assistant for Mobile & Edge Devices**\n\n#### Concept:\nDevelop a lightweight agent framework that runs locally on smartphones, tablets, or on-premise servers using ~3B-parameter SLMs (e.g., Apple on-device FM, Phi-4-Mini), enabling intelligent assistants in offline environments.\n\n#### Core Features:\n- Local RAG (document search), calendar management, email drafting;\n- Built-in guided decoding for consistent structured output;\n- Voice input + text output with low power consumption;\n- Fully local data processing, compliant with GDPR/CCPA;\n- Cloud LLM assistance only triggered for complex queries—with user consent.\n\n#### Tech Stack:\n- Models: Phi-4-Mini / Gemma-2B / Llama-3.2-3B;\n- Inference Engines: MLC-LLM / SGLang / TensorRT-LLM;\n- Decoding Library: XGrammar (lightweight embedded CFG support);\n\n#### User Value:\n- Zero-latency responses;\n- High privacy protection;\n- No internet dependency.\n\n#### Business Model:\nB2B sales to banks, hospitals, law firms; or B2C via iOS/Android apps.\n\n> ✅ **Trend Alignment**: Aligns with Apple Intelligence, NVIDIA Jetson, and growing edge AI ecosystems.\n\n---\n\n### 🚀 Startup Idea 4: **ToolTrust — Security Audit & Compliance Platform for Tool Calling**\n\n#### Concept:\nA complete system for securing open tool usage (via OpenAPI/MCP), offering tool registration, access control, behavioral auditing, and attack prevention.\n\n#### Modules:\n- **Tool Registry**: Upload OpenAPI specs; automatically analyze required permissions;\n- **Dynamic Permission Sandbox**: Tenant-specific allowlists restricting callable APIs;\n- **Call Behavior Auditing**: Logs AST-normalized parameters and policy decisions;\n- **Anomaly Detection Engine**: Flags cross-tool data leaks or repeated blocked actions;\n- **Auto-alerting & Blocking**: Triggers warnings or suspends services upon threshold breaches.\n\n#### Target Clients:\nCloud providers, internal enterprise AI platforms, government digital services.\n\n> ✅ **Unique Selling Point**: Fills a gap in agent security governance, meeting SOC2, HIPAA, and other compliance standards.\n\n---\n\n## Summary\n\n| Dimension | Content |\n|---------|--------|\n| **Core Thesis** | SLMs are the future of agent systems—default choice; LLMs reserved for exceptions |\n| **Key Technologies** | Structured decoding, validator-first design, LoRA fine-tuning, uncertainty-aware routing, KV cache optimization |\n| **Major Innovation** | The SLM-default architecture + full engineering lifecycle guidance |\n| **Startup Opportunities** | Agent orchestration, output quality assurance, edge AI, tool security governance |\n\n> 🔚 **One-Sentence Takeaway**:  \n> This paper signals the end of the \"large-model monopoly\" era and ushers in a new age of **efficient, sustainable, and controllable agentic AI**, where **small models lead and large models assist**. Entrepreneurs should seize this inflection point to build next-generation AI infrastructure centered on **efficiency, reliability, and security**.",
    "github": "",
    "hf": ""
}