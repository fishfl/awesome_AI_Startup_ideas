{
    "id": "2511.02791",
    "title": "AI-Generated Image Detection: An Empirical Study and Future Research Directions",
    "summary": "This paper introduces a unified benchmark framework for systematically evaluating forgery detection methods under controlled and reproducible conditions, and through extensive evaluation, it reveals significant differences in the generalization capabilities of existing methods.",
    "abstract": "The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Nusrat Tasnim,Kutub Uddin,Khalid Mahmood Malik",
    "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)",
        "Computer Science and Game Theory (cs.GT)"
    ],
    "comments": "",
    "keypoint": "A unified benchmarking framework was introduced for evaluating forensic methods under controlled conditions.  \nTen SoTA forensic methods were benchmarked across scratch, frozen, and fine-tuned training paradigms.  \nSeven publicly available datasets (GAN and diffusion-based) were used for extensive evaluation.  \nPerformance was evaluated using accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity.  \nModel interpretability was analyzed using confidence curves and Grad-CAM heatmaps.  \nSubstantial variability in generalization was observed across methods and datasets.  \nCertain methods showed strong in-distribution performance but poor cross-model transferability.  \nC2PClip achieved the highest average accuracy on Diffusion1kStep and UClipiffusion datasets.  \nNPR achieved the highest average accuracy (91.1%) on the MNW dataset.  \nFreqNet dropped to 1.6% accuracy on the MNW dataset despite strong performance elsewhere.  \nUpConv consistently underperformed across most datasets.  \nMost methods struggled on the Diffusion1kStep dataset.  \nGAN-based datasets were easier to detect, with several models exceeding 90% accuracy.  \nNPR exhibited bias toward the fake class, aiding its detection performance on MNW.  \nUpConv showed bias toward the real class.  \nGrad-CAM visualizations revealed that different models focused on different image regions.  \nLGrad, FreqNet, and C2PClip primarily focused on background regions.  \nConfidence curves illustrated varying levels of model certainty in predictions.  \nROC curves provided insights into the trade-off between true positive and false positive rates.  \nThe study highlighted inconsistent experimental settings across existing methods.  \nCurrent methods lack generalization to unseen generative models.  \nMany methods exhibit decision-making biases toward real or fake classes.  \nPreprocessing pipelines are often dataset-specific, limiting robustness.  \nFew methods have been tested against GAN- or diffusion-based anti-forensic attacks.  \nExplainability is limited in most current forensic approaches.  \nStandardized training, preprocessing, and evaluation protocols are needed.  \nFuture work should focus on domain-agnostic feature learning for better generalization.  \nBias mitigation strategies are required for balanced detection.  \nModels should be robust to minimal or varied preprocessing.  \nExplainable AI techniques should be integrated to improve trust and usability.",
    "date": "2025-11-06",
    "paper": "arXiv:2511.02791v1  [cs.CV]  4 Nov 2025\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n1\nAI-Generated Image Detection: An Empirical\nStudy and Future Research Directions\nNusrat Tasnim1\ntasnim.nishu70@kau.kr\nKutub Uddin2\nkutub@umich.edu\nKhalid Mahmood Malik2\ndrmalik@umich.edu\n1 School of Electronics and Information\nEngineering\nKorea Aerospace University\nGoyang, South Korea\n2 College of Innovation & Technology\nUniversity of Michigan-Flint,\nMichigan, USA\nAbstract\nThe threats posed by AI-generated media, particularly deepfakes, are now raising\nsignificant challenges for multimedia forensics, misinformation detection, and biomet-\nric system resulting in erosion of public trust in the legal system, significant increase in\nfrauds, and social engineering attacks. Although several forensic methods have been pro-\nposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with\nGAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch,\nfrozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization\nand explainability. These limitations hinder fair comparison, obscure true robustness,\nand restrict deployment in security-critical applications. This paper introduces a unified\nbenchmarking framework for systematic evaluation of forensic methods under controlled\nand reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen,\nand fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform\nextensive and systematic evaluations. We evaluate performance using multiple metrics,\nincluding accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity.\nWe also further analyze model interpretability using confidence curves and Grad-CAM\nheatmaps. Our evaluations demonstrate substantial variability in generalization, with\ncertain methods exhibiting strong in-distribution performance but degraded cross-model\ntransferability. This study aims to guide the research community toward a deeper under-\nstanding of the strengths and limitations of current forensic approaches, and to inspire\nthe development of more robust, generalizable, and explainable solutions.\n1\nIntroduction\nIn recent times, the proliferation of AI-generated content, particularly deepfakes [57], has\noverwhelmed social media [1] and news platforms [7]. These deepfake contents are often\nused to mislead audiences by fabricating events or impersonating individuals, thereby under-\nmining public trust. Moreover, deepfakes have emerged as critical threats to society, espe-\ncially in security-sensitive domains. For instance, they can compromise biometrics used for\nface recognition and identification [55, 63], surveillance systems [26], and mislead percep-\ntion modules in autonomous driving [18, 53]. Additionally, deepfakes pose significant risks\nin the Internet of Things (IoT) ecosystem [9, 54] and remote authentication systems [52, 62],\nwhere identity integrity is crucial. As the quality and realism of AI-generated content con-\ntinue to improve, the ability to detect deepfake media has become increasingly challenging\n© 2025. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\n2\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\nand making it imperative to develop robust and generalizable techniques.\nCurrent statistics highlight the urgent need to verify deepfake content in domains such as\nsocial media, journalism, finance, the legal system, and governance. Across industries, deep-\nfake fraud has become alarmingly common. Nearly 92% of companies have reported finan-\ncial losses due to deepfake scams [5]. On average, businesses lose approximately $450,000\nper incident, with the financial sector bearing even heavier losses, averaging $600,000 per\norganization, and in some cases exceeding $1 million [19]. In one notable incident, a Hong\nKong employee was tricked into transferring $25 million after fraudsters used a deepfake\nvideo call to impersonate the company’s CFO [14].\nGlobally, deepfake-enabled fraud caused over $200 million in losses during the first quarter\nof 2025 alone, indicating a rapidly escalating threat [16]. The cryptocurrency sector saw an\neven more dramatic impact, with deepfake-related scams increasing by 456% between May\n2024 and April 2025, culminating in more than $10.7 billion in damages in 2024 [43]. Mar-\nket projections suggest that generative AI-related fraud losses could rise to $40 billion by\n2027, up from $12.3 billion in 2023 [15]. Beyond financial harm, the reputational damage\nfrom deepfakes is equally concerning. Victims often suffer from long-term erosion of trust,\nreputational fallout, and brand damage. In one instance, a fabricated image of an explosion\nnear the Pentagon, generated by AI, caused a temporary dip in the Dow Jones index, high-\nlighting how deepfakes can disrupt public confidence and financial stability [42].\nThese statistics emphasize the urgent need for generalized forensic methods to detect deep-\nfake content, thereby protecting individuals, institutions, and the public at large. Several\nmethods [17, 39, 51, 56, 60] have been developed using deep learning [60] and hybrid [39]\napproaches to ensure media integrity. Some approaches [51, 60] incorporate preprocess-\ning and data augmentation techniques to enhance generalization, while others [39] leverage\nSoTA foundation models for robust feature extraction to ensure better generalizability.\nThe major drawbacks of these methods [17, 39, 51, 60] are specific to datasets [60], genera-\ntive models [17, 59], or training strategies [51, 58]. Although many benchmark datasets [51]\nare now publicly available for evaluating a model’s effectiveness and generalizability, most\nexisting approaches [17, 39, 51, 60] consider only one or two datasets for evaluation, leav-\ning many others unexplored. Furthermore, these methods are not assessed within a unified\nframework, which hinders the reproducibility of results and limits future research.\nIn this article, we conduct an empirical study of generalizable and explainable deepfake de-\ntection. The major contributions are listed as follows:\n• We propose a unified benchmarking framework that systematically evaluates the gen-\neralization capabilities of SoTA forensic methods across benchmark datasets, genera-\ntive models, and training paradigms.\n• We conduct an extensive empirical study involving ten SoTA detection methods (scratch,\nfrozen, and finetuned) and seven publicly available deepfake datasets (GAN and Dif-\nfusion) that offer a comprehensive and reproducible evaluation setup.\n• We incorporate explainability techniques (confidence, ROC curves, and GradCAM) to\ninterpret model predictions and highlight the decisions made by them.\n• We provide critical insights into the strengths and limitations of current forensic meth-\nods and identify open challenges that aim to guide the development of more robust,\ngeneralizable, and explainable deepfake detection methods.\n2\nEmpirical Study Design\nThis section outlines the overall design of the empirical comparative study depicted in Fig-\nure 1, covering benchmark selection, evaluation protocols, and explainability techniques. We\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n3\nFigure 1: Overview of the empirical study: First, we perform benchmark selection, including\ndatasets, forensic, and explainability techniques. Next, we define evaluation protocols cov-\nering frozen, fine-tuned, and from-scratch models for AI-generated image detection. Finally,\nwe provide a comprehensive explanation based on confidence, ROC curves, and GradCAM.\nTable 1: Summary of Benchmark Datasets Commonly Used in the Research Community\nName\nYear\nGenerative Technique\nForenSyn [60]\n2020\nProGAN [23], StyleGAN [24], BigGAN [4], CycleGAN [64], StarGAN [8], GauGAN [40],\nStyleGAN2 [25], Deepfakes [46]\nForenSynthsCh [60]\n2022\nCRN [6], IMLE [30], SAN [11], SIDT [6], WFR [60]\nDiffusion1KStep [51]\n2023\nDalle [41], DDPM [22], Guided-Diffusion [12], Improved-Diffusion [37], Mid-Journey [51]\nDIRE [61]\n2024\nADM [12], DDPM [22], IDDPM [37], LDM [45], PNDM [32], SDV1 [45],\nSDV2 [45], VQDiffusion [20]\nGAN [51]\n2024\nAttGAN [21], BEGAN [3], CramerGAN [2], InfoMaxGAN [28],\nMMDGAN [29], RelGAN [38], S3GAN [34], SNGAN [35], STGAN [33]\nUClipiffusion [39]\n2023\nDalle [41], Glide (50_27, 100_10, 100_27) [36], Guided [12],\nLDM (100, 200, 200_cfg) [45]\nMNW [44]\n2025\nAdobe, Adversarial, Amazon_v2, Aura_flow, Baidu, Bytedance_v3, Civitai_v6, Flux, Google,\nHunyuandit, Hypersd, Ideogram, Kandinsky, Krea_1, Kuaishou, Luma_photon, Lumina,\nMeta_imagine, Midjourney, Nvidia_sana, Openai, Pixart_alpha_xl, Playgroundai, Recraft_v3,\nReve_ai, Stable, Ultrapixel, Wuerstchen\nidentified 31 forensic methods and 10 benchmark datasets. Among the 31 forensic methods,\nwe screened 25 and selected 19 based on venue, effectiveness, and novelty. We imple-\nmented 14 forensic methods. Similarly, we identified 10 benchmark datasets and collected\n7 of them based on accessibility and representation of recent generative models. Owing to\nthe limited generalization ability of the 4 implemented methods and their outdated nature,\nwe ultimately reported generalization and explainability results using 10 forensic methods\nacross 7 datasets, as listed in Table 1 and Table 2.\n2.1\nDatasets\nThis section provides an overview of SoTA benchmark datasets, including their names, re-\nlease years, object categories, and generative techniques, as summarized in Table 1.\n2.1.1\nGAN-Based Datasets\nThe ForenSyn [60] dataset was introduced by Wang et al. to improve the generalization ca-\npability of generic deepfake detection. It comprises data from eight GAN sources, including\nthree conditional GANs [8, 40, 64], unconditional GANs [23, 24], and a deepfake face [46]\nsource. Most SoTA methods train their models on the ProGAN [23] training set. GAN [51]\ncontains data from 9 GAN sources with varying architectural properties. These data differ\nfrom ForenSyn [60], which covers a diverse range of wild scenes. In ForenSyn [60], each\nsub-dataset has a random number of real and deepfake samples, while in GAN [51], each\n4\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\nTable 2: Summary of Benchmark Detection Methods Used in the Research Community\nName\nYear\nStrength\nLimitation\nCNND [60]\nCVPR, 2020\nImproved generalization by\ncareful data augmentation\nDid not explore other types of generative\nmodels such as diffusion\nLGrad [49]\nCVPR, 2023\nExtracted gradient features using\npretrained generative models\nLimited to StyleGAN and ProGAN models\nDid not explore any diffusion models\nNPR [51]\nCVPR, 2024\nExplored neighboring pixel-relationship\nBias towards the deepfake class\nUClip [39]\nCVPR, 2023\nDoes not require retraining of CLIP\nLimited generalization to recent generative models\nRClip [10]\nCVPR, 2024\nRequires less training data\nLimited to certain generative models\nFatF [31]\nCVPR, 2024\nCaptures frequency domain artifacts\nLacks generalization to recent unseen datasets\nRINE [27]\nECCV, 2024\nTrainable importance estimator for encoder\nEvaluated on fewer datasets\nUpConv [13]\nCVPR, 2020\nCaptured spectral features\nComparatively less effective and less generalizable\nFreqNet [50]\nAAAI, 2024\nEnd-to-end frequency learning model\nDoes not account for other image properties\nC2PClip [48]\nAAAI, 2025\nCaption generation and enhancement\nConcept injection to finetune CLIP\nLimited analysis of the captions results in incomplete\ninformation\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 2: Intermediate representation: (a) Original, (b) LGrad (gradient) [49], (c) NPR [51],\n(d) FreqNet (high-frequency) [50], and (e) UpConv(spectral) [13] .\nsub-dataset comprises 2K real and 2K deepfake images.\n2.1.2\nDiffusion-based Datasets\nDIRE [61] consists of 8 diffusion-generated deepfake samples. The real images are ran-\ndomly collected from ForenSyn [60] (LSUN [60] and ImageNet [47]) real classes. Diffu-\nsion1kStep [51] is a diffusion-family dataset containing data from five diffusion sources.\nAll samples are generated using 1K diffusion steps. Among these, the Mid-Journey and\nDalle samples were collected from social platforms. UClipiffusion [39] is another diffusion\ndataset, collected from UClip [39], encompassing four different diffusion models with vary-\ning configurations. Microsoft Northwestern Witness (MNW) [44] is a recent and diverse\ndataset encompassing 43 diffusion models, with 250 samples generated for each model. The\ndataset includes samples from wild scenes, faces, and real-world scenarios.\n2.1.3\nOther Generative Datasets\nThe ForenSynthsCh [60] dataset contains AI-generated images created using low-level vi-\nsion and perceptual loss techniques, which are very challenging and often overlooked by\nmost SoTA methods, as they worked very poorly.\n2.1.4\nDetection Methods\nThis section introduces the SoTA forensic methods used in our analysis, as summarized in\nTable 2, and presents their intermediate representations in Figure 2 to better illustrate the\nunderlying concepts.\n2.1.5\nScratch Trained Models\nWe selected four scratch-trained models [13, 49, 50, 51] to evaluate the proposed bench-\nmark, each representing a distinct design in AI-generated image detection. UpConv [13]\nis a widely recognized approach that exploits spectral analysis to identify upsampling arti-\nfacts, effectively capturing intrinsic properties of both GAN- and diffusion-generated con-\ntent. LGrad [49] is another influential method, which leverages a pretrained generative model\nto extract gradient-based features, thereby capturing subtle textural and structural cues as-\nsociated with deepfakes. The nearest pixel relationship (NPR) [51] method takes a spatial\nperspective, focusing on the correlations among neighboring pixels to uncover artifacts intro-\nduced during the upsampling process. Finally, FreqNet [50] represents a recent advancement\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n5\nTable 3: Performance evaluation on Diffusion1kStep [51] datasets (ACC/AP).\nDataset\nScratch Models\nFrozen Models\nFine-Tuned Models\nUpConv [13]\nLGrad [49]\nNPR [51]\nFreqNet [50]\nUClip [39]\nRCLip [10]\nRINE [27]\nCNND [60]\nFatF [31]\nC2PClip [48]\nDalle\n47.9/46.5\n76.6/87.1\n69.5/86.4\n51.3/58.9\n53.7/69.3\n76.8/81.1\n60.5/86.2\n52.8/53.4\n68.8/93.2\n66.4/93.1\nDdpm\n49.9/49.8\n59.8/80.3\n70.8/81.9\n69.4/86.6\n72.2/84.4\n65.6/74.1\n68.6/85.1\n50.2/59.0\n59.1/77.9\n72.0/81.9\nGuided-diffusion\n57.6/68.3\n68.5/74.8\n64.3/77.6\n80.3/90.2\n77.5/94.5\n70.0/80.1\n82.2/97.7\n56.4/67.7\n81.8/95.7\n74.4/94.6\nImproved-diffusion\n53.8/62.8\n42.3/43.9\n68.7/87.0\n52.9/60.5\n69.2/90.8\n51.3/50.1\n66.6/92.8\n47.3/51.4\n59.4/72.5\n75.2/92.6\nMidjourney\n51.7/53.4\n64.1/71.4\n68.8/88.2\n53.4/61.7\n49.9/48.5\n62.7/65.3\n53.3/67.1\n49.0/42.1\n62.7/85.4\n66.8/93.1\nAvg.\n52.2/56.1\n62.3/71.5\n68.4/84.2\n61.5/71.6\n64.5/77.5\n65.3/70.2\n66.2/85.8\n51.1/54.7\n66.4/84.9\n70.9/91.0\nin frequency-domain approaches, offering an end-to-end frequency-aware architecture capa-\nble of identifying nuanced spectral inconsistencies present in AI-generated images.\n2.1.6\nFrozen Models\nSimilar to scratch-trained models, we selected two frozen-based models [10, 39]. These two\nmethods used CLIP as a frozen model to extract features for deepfake detection. First, uni-\nversal deepfake detection using CLIP (UClip) [39], in which the authors adopted a pretrained\nCLIP model for AI-generated image detection without training CLIP. RClip [10] investigated\nthe effectiveness of sample size in generalizing detection with CLIP features, showing that\neven 0.01K samples are sufficient to detect deepfake artifacts.\n2.1.7\nFine-Tuned Models\nThis category of methods [63] fine-tunes pretrained models on AI-generated datasets to im-\nprove generalization. Examples include CNND [60], RINE [27], FatF [31], and C2PClip [48]\n. CNND [60] was the first to introduce a large-scale deepfake dataset, using a pretrained\nResNet (trained on ImageNet) fine-tuned on this dataset. RINE [27] employs a frozen CLIP\nencoder with a trainable importance estimator to select key features for AI-generated im-\nage detection. Similarly, FatFormer integrates a forgery-aware adapter to capture frequency\ncues, while C2PClip [48] injects category-common prompts to enhance generalization.\n3\nResults\nThis section presents the experimental setups, performance evaluations and comparisons,\nand explainability analyses conducted in the proposed empirical study.\n3.1\nExperimental Setting\nWe configured our pipeline to evaluate all selected methods under identical environmental\nsettings. We run all the experiments on a Linux 24.04 operating system with eight NVIDIA\nRTX 6000 Ada Generation GPUs (49 GB of memory on each GPU). For each method, we\nadopted the preprocessing, including load size, cropping, and normalization, reported in the\noriginal papers. We reported ACC, AP, AUC, and EER for a fair assessment of the methods.\n3.2\nPerformance Evaluation and Comparisons\nWe extensively evaluated the performance of ten forensic methods on seven benchmark\ndatasets, as reported in Tables 3-9. For the CNND [60] dataset, we split it into two cate-\ngories because most methods tend to ignore the ForenSynthsCh [60] segments. This is be-\ncause many SoTA methods fail to generalize on this dataset, resulting in poor performance.\nAcross most datasets, UpConv [13] underperforms, while C2PClip [48] consistently achieves\nthe highest accuracy, demonstrating strong generalization. All methods struggle on Diffu-\nsion1kStep [51], whereas UDiffusion and GAN-based datasets are easier to detect, with\nseveral models exceeding 90% accuracy. The best performance on Diffusion1kStep [51] is\nachieved by C2PClip [48] (ACC/AP of 70.9%/91.0%), whereas the lowest results are re-\nported by CNND [60] (ACC/AP of 51.1%/54.7%). Methods like LGrad [49], RCLip [10],\nand CNND [60] show intermediate performance across all datasets, as depicted in Figure 3.\nAmong all methods, NPR [51] achieves the highest average accuracy (91.1%) on the MNW [44]\ndataset, whereas most other models perform substantially worse and face the difficulty of\ngeneralizing across diverse generative sources. Notably, FreqNet [50] drops to only 1.6%\naccuracy, despite its strong performance on other datasets, which highlights the challenges\nof adapting to certain real-world or unseen data distributions.\n6\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\nTable 4: Performance evaluation on DIRE [61] datasets (ACC/AP).\nDataset\nScratch Models\nFrozen Models\nFine-Tuned Models\nUpConv [13]\nLGrad [49]\nNPR [51]\nFreqNet [50]\nUClip [39]\nRCLip [10]\nRINE [27]\nCNND [60]\nFatF [31]\nC2PClip [48]\nAdm\n56.1/65.0\n83.8/94.3\n68.8/80.8\n66.7/85.2\n67.9/86.3\n81.6/96.4\n69.7/92.5\n58.0/74.8\n70.7/93.7\n68.8/95.3\nDdpm\n55.1/33.6\n81.2/92.4\n67.2/97.2\n90.3/99.1\n80.7/96.4\n72.1/69.2\n80.7/96.8\n62.9/64.3\n67.2/78.9\n73.5/76.2\nIddpm\n46.9/46.1\n63.3/84.9\n71.8/94.3\n60.1/92.9\n73.4/96.7\n69.7/82.2\n75.2/97.9\n50.4/74.9\n69.3/96.3\n80.7/94.9\nLdm\n63.5/67.2\n98.7/99.9\n74.0/99.6\n97.5/100.0\n50.7/86.1\n95.6/100.0\n56.6/98.1\n53.0/75.8\n97.2/100.0\n97.2/99.7\nPndm\n52.4/53.6\n67.8/94.2\n73.2/85.9\n85.0/99.3\n86.2/99.1\n95.5/99.8\n83.8/99.0\n50.9/76.6\n99.2/100.0\n84.2/97.2\nSdv1\n42.0/74.2\n83.2/97.5\n82.4/94.9\n93.8/99.6\n52.8/90.8\n68.0/96.8\n78.0/98.8\n39.1/78.0\n61.6/97.0\n78.9/99.2\nSdv2\n61.6/67.1\n96.7/99.8\n74.0/98.9\n70.7/96.5\n53.3/85.0\n46.2/36.2\n57.4/89.9\n52.2/72.9\n84.4/98.7\n66.7/94.8\nVqdiffusion\n65.3/70.4\n86.1/99.0\n74.0/99.6\n99.9/100.0\n77.8/99.0\n95.6/100.0\n91.4/99.9\n53.9/84.7\n100.0/100.0\n95.8/99.7\nAvg.\n55.4/59.6\n82.6/95.3\n73.2/93.9\n83.0/96.6\n67.9/92.4\n78.0/85.1\n74.1/96.6\n52.6/75.2\n81.2/95.6\n80.7/94.6\nTable 5: Performance evaluation on ForenSynths [60] datasets (ACC/AP).\nDataset\nScratch Models\nFrozen Models\nFine-Tuned Models\nUpConv [13]\nLGrad [49]\nNPR [51]\nFreqNet [50]\nUClip [39]\nRCLip [10]\nRINE [27]\nCNND [60]\nFatF [31]\nC2PClip [48]\nBiggan\n67.3/81.9\n74.5/78.3\n58.4/65.2\n91.2/96.2\n95.1/99.3\n80.4/95.6\n99.6/99.9\n70.2/84.5\n99.5/100.0\n99.1/100.0\nCyclegan\n69.7/79.3\n80.1/88.3\n73.8/71.3\n95.5/99.6\n98.3/99.8\n93.5/99.5\n99.3/100.0\n85.2/93.5\n99.4/100.0\n97.3/100.0\nGaugan\n59.6/74.1\n68.8/73.4\n53.5/49.7\n92.9/98.4\n99.5/100.0\n91.8/97.9\n99.8/100.0\n78.9/89.5\n99.4/100.0\n99.2/100.0\nProgan\n53.1/78.8\n98.8/99.9\n58.1/71.7\n99.6/100.0\n99.8/100.0\n84.0/99.7\n100.0/100.0\n100.0/100.0\n99.9/100.0\n100.0/100.0\nStargan\n92.8/100.0\n95.7/99.8\n63.5/99.0\n84.3/99.3\n95.7/99.4\n61.4/98.8\n99.5/100.0\n91.7/98.1\n99.7/100.0\n99.6/100.0\nStylegan\n60.1/74.7\n92.6/99.3\n65.4/84.6\n91.2/99.8\n84.9/97.6\n84.9/94.0\n88.9/99.4\n87.1/99.6\n97.1/99.8\n96.4/99.5\nStylegan2\n53.8/68.6\n93.6/99.2\n61.7/74.8\n87.3/99.5\n75.0/97.9\n80.8/90.2\n94.5/100.0\n84.4/99.1\n98.8/99.9\n95.6/99.9\nDeepfake\n53.6/53.5\n58.9/81.8\n49.9/52.9\n92.2/97.3\n68.6/81.8\n53.3/72.8\n80.6/97.9\n53.5/89.0\n93.3/98.0\n93.8/98.6\nAvg.\n63.8/76.4\n82.9/90.0\n60.5/71.2\n91.8/98.8\n89.6/97.0\n78.7/93.6\n95.3/99.7\n81.4/94.2\n98.4/99.7\n97.6/99.7\nTable 6: Performance evaluation on ForenSynthsCh [60] datasets (ACC/AP).\nDataset\nScratch Models\nFrozen Models\nFine-Tuned Models\nUpConv [13]\nLGrad [49]\nNPR [51]\nFreqNet [50]\nUClip [39]\nRCLip [10]\nRINE [27]\nCNND [60]\nFatF [31]\nC2PClip [48]\nCRN\n52.5/60.1\n51.2/64.7\n48.8/45.5\n53.7/74.8\n56.6/96.6\n61.3/83.1\n89.3/97.3\n86.3/98.2\n69.5/99.8\n93.3/99.9\nIMLE\n51.6/62.5\n51.2/70.9\n48.8/50.7\n53.7/69.9\n69.1/98.6\n66.1/83.2\n90.7/99.7\n86.2/98.4\n69.5/99.9\n93.3/99.9\nSAN\n50.5/48.0\n42.0/41.3\n58.7/68.4\n89.3/93.2\n56.6/78.8\n76.5/88.0\n68.3/94.9\n50.5/70.4\n68.0/81.2\n64.4/84.6\nSITD\n85.0/97.1\n47.2/39.1\n51.7/53.0\n72.8/72.1\n62.2/63.8\n70.6/91.2\n90.6/97.2\n90.3/97.2\n81.4/97.9\n95.6/98.9\nWFR\n64.1/84.0\n57.8/58.9\n51.0/49.4\n50.9/96.7\n87.2/97.3\n71.4/90.3\n97.0/99.5\n86.8/94.8\n88.1/98.5\n94.8/99.5\nAvg.\n60.7/70.3\n49.9/55.0\n51.8/53.4\n64.1/81.3\n66.3/87.0\n69.1/87.2\n87.2/97.7\n80.0/91.8\n75.3/95.5\n88.3/96.6\nTable 7: Performance evaluation on GAN [51] datasets (ACC/AP).\nDataset\nScratch Models\nFrozen Models\nFine-Tuned Models\nUpConv [13]\nLGrad [49]\nNPR [51]\nFreqNet [50]\nUClip [39]\nRCLip [10]\nRINE [27]\nCNND [60]\nFatF [31]\nC2PClip [48]\nAttgan\n48.5/41.9\n53.1/76.6\n86.4/98.0\n90.3/98.5\n90.8/97.0\n81.3/94.9\n99.2/100.0\n65.8/91.4\n99.3/100.0\n90.4/99.8\nBegan\n48.9/47.9\n51.0/70.4\n55.2/78.7\n65.4/99.3\n89.3/96.3\n99.9/100.0\n97.9/99.9\n69.7/91.9\n99.9/100.0\n94.8/100.0\nCramergan\n73.5/84.4\n50.9/59.1\n73.4/92.7\n99.6/100.0\n90.7/99.3\n68.0/90.0\n97.0/99.9\n91.9/99.1\n98.4/100.0\n98.4/100.0\nInfomaxgan\n42.2/42.2\n53.9/82.1\n74.4/92.6\n63.2/95.0\n88.5/96.9\n68.0/90.2\n96.5/99.6\n62.5/86.7\n98.4/100.0\n98.4/100.0\nMmdgan\n76.1/87.0\n51.1/66.5\n74.0/93.5\n98.0/99.9\n90.6/99.2\n68.0/90.1\n97.0/99.9\n86.4/98.2\n98.4/100.0\n98.4/100.0\nRelgan\n93.7/98.2\n74.5/95.6\n88.1/99.9\n99.9/100.0\n93.4/98.0\n80.1/98.8\n99.4/100.0\n88.8/98.9\n99.5/100.0\n92.0/99.8\nS3gan\n96.5/99.6\n73.3/75.9\n73.2/82.7\n88.6/94.1\n94.1/98.8\n85.1/99.0\n98.6/99.9\n69.0/80.7\n99.0/100.0\n99.0/100.0\nSngan\n65.5/73.3\n52.3/82.5\n57.8/64.4\n51.2/84.7\n88.6/96.8\n67.9/81.7\n96.7/99.7\n60.8/86.6\n98.3/99.9\n98.4/99.9\nStgan\n85.7/95.9\n50.5/75.7\n91.4/99.1\n98.0/100.0\n82.8/91.6\n61.5/89.8\n93.7/99.1\n65.2/96.5\n98.8/99.8\n97.6/99.6\nAvg.\n70.1/74.5\n56.7/76.1\n74.9/89.1\n83.8/96.8\n89.9/97.1\n75.5/92.7\n97.3/99.8\n73.3/92.2\n98.9/100.0\n96.4/99.9\nTable 8: Performance evaluation on UClipiffusion [39] datasets (ACC/AP).\nDataset\nScratch Models\nFrozen Models\nFine-Tuned Models\nUpConv [13]\nLGrad [49]\nNPR [51]\nFreqNet [50]\nUClip [39]\nRCLip [10]\nRINE [27]\nCNND [60]\nFatF [31]\nC2PClip [48]\nDalle\n55.1/65.5\n83.5/92.4\n53.8/69.5\n97.7/99.5\n87.5/97.7\n89.2/99.5\n95.0/99.5\n56.1/71.3\n98.7/99.8\n98.6/99.9\nGlide_50_27\n58.1/67.0\n85.2/92.3\n54.0/80.8\n86.6/95.8\n79.2/96.0\n87.2/96.7\n92.6/99.5\n62.7/84.6\n94.6/99.5\n95.2/99.8\nGlide_100_10\n59.7/69.1\n83.7/91.5\n54.1/81.0\n88.4/96.2\n78.0/95.5\n87.9/97.0\n90.7/99.2\n61.0/82.0\n94.2/99.3\n96.1/99.8\nGlide_100_27\n54.5/60.7\n81.5/89.2\n53.9/80.0\n84.7/95.4\n78.6/95.8\n87.8/97.0\n88.9/99.1\n60.4/80.5\n94.3/99.3\n95.2/99.7\nGuided\n57.5/68.7\n70.2/75.1\n58.8/67.3\n62.4/67.2\n70.0/88.3\n85.6/96.6\n76.1/96.6\n62.0/77.7\n76.0/91.9\n69.1/94.1\nLdm_100\n49.5/54.9\n86.4/93.7\n54.4/82.7\n97.0/99.9\n95.2/99.3\n89.5/99.9\n98.7/99.9\n55.1/72.5\n98.6/99.9\n99.3/100.0\nLdm_200_cfg\n51.3/56.7\n88.2/95.4\n54.3/82.9\n96.9/99.8\n74.2/93.2\n89.3/99.7\n88.2/98.7\n55.2/73.0\n94.8/99.2\n97.2/99.8\nLdm_200\n49.0/54.2\n86.1/93.7\n54.4/82.6\n96.9/99.8\n94.5/99.4\n89.5/99.9\n98.3/99.9\n53.9/71.1\n98.6/99.8\n99.2/100.0\nAvg.\n54.3/62.1\n83.1/90.4\n54.7/78.4\n88.8/94.2\n82.2/95.7\n88.3/98.3\n91.1/99.0\n58.3/76.6\n93.7/98.6\n93.8/99.1\n3.3\nExplainability of Model Predictions\nFor a better explanation of model predictions, we visualized GradCAM, confidence, and\nROC curves, as depicted in Figures 4, 5, and 6. GradCAM highlights the regions that each\nmodel focuses on to distinguish between real and fake samples. As shown in Figure 4, each\nmethod focuses on different regions to determine whether a sample is real. For example,\nLGrad [49], FreqNet [50], and C2PClip [48] primarily target the background, while others\nattend to random regions when making their decisions.\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n7\nTable 9: Performance evaluation on MNW [44] datasets (ACC/AP).\nDataset\nScratch Models\nFrozen Models\nFine-Tuned Models\nUpConv [13]\nLGrad [49]\nNPR [51]\nFreqNet [50]\nUClip [39]\nRCLip [10]\nRINE [27]\nCNND [60]\nFatF [31]\nC2PClip [48]\nAdobe\n41.6/–\n56.5/–\n97.6/–\n1.2/–\n28.9/–\n4.7/–\n38.5/–\n3.1/–\n16.7/–\n31.3/–\nAdversarial_images\n15.2/–\n23.6/–\n89.2/–\n2.4/–\n10.4/–\n5.2/–\n8.8/–\n1.2/–\n6.4/–\n6.4/–\nAmazon_titan_v2\n10.8/–\n72.0/–\n100.0/–\n0.0/–\n20.0/–\n29.6/–\n27.6/–\n3.2/–\n23.6/–\n26.4/–\nAura_flow\n18.4/–\n56.8/–\n100.0/–\n0.8/–\n2.4/–\n4.8/–\n3.6/–\n0.0/–\n12.0/–\n47.2/–\nBaidu\n10.4/–\n13.2/–\n85.2/–\n0.8/–\n10.4/–\n1.2/–\n6.8/–\n0.8/–\n0.4/–\n4.4/–\nBytedance\n31.6/–\n49.6/–\n99.2/–\n0.0/–\n0.0/–\n1.2/–\n0.4/–\n0.4/–\n0.0/–\n0.0/–\nCivitai_v6\n3.6/–\n80.0/–\n98.0/–\n0.0/–\n4.0/–\n12.0/–\n4.8/–\n0.8/–\n21.6/–\n28.0/–\nFlux\n12.8/–\n35.2/–\n88.0/–\n15.1/–\n3.7/–\n2.1/–\n3.1/–\n2.4/–\n2.9/–\n3.1/–\nGoogle\n5.8/–\n13.8/–\n70.8/–\n2.2/–\n4.8/–\n1.4/–\n1.0/–\n2.0/–\n0.0/–\n0.2/–\nHunyuandit\n32.0/–\n1.6/–\n94.0/–\n0.0/–\n7.2/–\n1.2/–\n4.4/–\n0.8/–\n0.4/–\n11.2/–\nHypersd\n19.6/–\n3.6/–\n68.0/–\n0.6/–\n3.2/–\n0.0/–\n0.8/–\n1.4/–\n0.0/–\n3.4/–\nIdeogram\n6.8/–\n78.4/–\n98.0/–\n0.4/–\n2.0/–\n0.0/–\n2.0/–\n4.0/–\n14.8/–\n1.2/–\nKandinsky\n17.2/–\n1.6/–\n88.4/–\n1.2/–\n11.2/–\n0.0/–\n4.0/–\n0.0/–\n0.0/–\n6.0/–\nKrea_1\n6.0/–\n90.4/–\n98.8/–\n0.0/–\n4.8/–\n0.0/–\n5.6/–\n6.0/–\n7.2/–\n1.2/–\nKuaishou_kolors\n8.0/–\n2.8/–\n85.2/–\n1.6/–\n3.2/–\n0.4/–\n0.8/–\n0.4/–\n0.0/–\n9.6/–\nLuma_photon\n97.6/–\n84.0/–\n99.2/–\n1.2/–\n26.8/–\n5.2/–\n45.2/–\n14.4/–\n41.6/–\n16.8/–\nLumina\n20.4/–\n82.4/–\n99.2/–\n0.4/–\n15.6/–\n13.2/–\n30.4/–\n8.4/–\n27.6/–\n10.8/–\nMeta_imagine\n4.0/–\n15.2/–\n94.0/–\n1.2/–\n14.8/–\n2.8/–\n12.0/–\n5.6/–\n0.0/–\n12.4/–\nMidjourney\n30.4/–\n30.8/–\n78.9/–\n0.8/–\n5.7/–\n0.0/–\n8.8/–\n10.9/–\n6.1/–\n7.3/–\nNvidia_sana\n47.6/–\n22.4/–\n95.6/–\n1.2/–\n60.4/–\n30.4/–\n60.4/–\n0.4/–\n49.6/–\n54.4/–\nOpenai\n7.3/–\n37.9/–\n86.5/–\n2.4/–\n20.3/–\n2.0/–\n20.7/–\n8.0/–\n2.9/–\n14.5/–\nPixart_alpha_xl\n32.0/–\n3.6/–\n82.0/–\n0.8/–\n2.0/–\n0.8/–\n0.8/–\n1.2/–\n0.0/–\n12.0/–\nPlaygroundai\n22.0/–\n38.8/–\n80.2/–\n0.0/–\n6.2/–\n1.0/–\n9.2/–\n3.2/–\n27.4/–\n10.2/–\nRecraft_v3\n46.0/–\n66.0/–\n93.6/–\n0.0/–\n12.4/–\n0.4/–\n7.2/–\n0.0/–\n6.8/–\n1.6/–\nReve_ai\n13.2/–\n72.0/–\n99.2/–\n0.8/–\n2.8/–\n0.0/–\n7.2/–\n4.0/–\n13.2/–\n1.2/–\nStable_diffusion\n14.2/–\n41.1/–\n89.6/–\n2.0/–\n14.3/–\n9.2/–\n13.7/–\n3.9/–\n17.1/–\n17.2/–\nUltrapixel\n11.2/–\n84.0/–\n100.0/–\n5.6/–\n4.0/–\n0.0/–\n4.0/–\n44.4/–\n38.8/–\n0.8/–\nWuerstchen\n8.4/–\n4.0/–\n93.6/–\n1.2/–\n22.4/–\n1.2/–\n30.4/–\n5.6/–\n0.8/–\n17.2/–\nAvg.\n21.2/–\n41.5/–\n91.1/–\n1.6/–\n11.6/–\n4.6/–\n12.9/–\n4.9/–\n12.1/–\n12.7/–\nFigure 3: Summary of all forensic methods on all benchmark datasets.\nIn contrast, the confidence curve represents the prediction probabilities of each model for the\nreal and fake classes to make it clear how confident a model is in predicting real as real and\nfake as fake. As shown in Figure 5, in most cases, NPR [51] is biased towards the fake class,\nwhile UpConv [13] tends to favor the real class. Similar to the confidence curve, the ROC\ncurve illustrates the trade-off between the true positive rate and false positive rate across\ndifferent thresholds to provide a comprehensive view of each model’s discriminative ability.\n4\nDiscussions and Future Research Directions\nThis section outlines the discussions and future research directions of our findings.\n4.1\nDiscussions\nInconsistent experimental settings: While most methods employ the same training set,\nvariations in their basic experimental configurations lead to inconsistencies across SoTA\nmethods, thereby hindering the reproducibility of results reported in the paper.\nLack of generalization: Although most methods claim to generalize to unseen generative\nmodels, they struggle with unseen samples, as shown in Tables 3–9, particularly for the\nMNW [44] dataset in Table 9 while varying the generative models.\n8\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n(g)\n(h)\n(i)\n(j)\nFigure 4: GradCAM explanation: (a) Original, (b) LGrad [49], (c) NPR [51], (d) Fre-\nqNet [50], (e) UClip [39], (f) RClip [10], (g) RINE [27], (h) CNND [60], (i) FatF [31],\nand (j) C2PClip [48].\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 5: Confidence of fake prediction by each model on six datasets: (a) Adm, (b) Big-\nGAN, (c) CRN, (d) Dalle, (e) Guided, and (f) StGAN.\nBiases in decision-making: In many cases, the methods exhibit bias toward either the real\nor deepfake class.\nAs shown in Table 9, most methods fail to detect MNW [44] sam-\nples, whereas NPR [51] achieves 91% ACC. Our analysis of the confidence curve reveals\nthat NPR [51] is biased toward fakes, as shown in Figure 5, which enables it to detect the\nMNW [44] dataset. In contrast, UpConv [13] is biased toward real class (Figure 5).\nRestricted preprocessing: Most methods rely on predefined preprocessing pipelines tai-\nlored to specific datasets; for instance, NPR [51], RINE [27], and FreqNet [50] omit crop-\nping for certain datasets, while applying it to others.\nVulnerability to AFs: A few studies [60] have evaluated robustness against conventional\nAFs, such as noise and compression. However, none have considered AFs based on GANs [55],\ndiffusion models [61], or optimization-based anti-forensic (AF) attacks.\nLack of explainability: Most methods lack explainability of their prediction to provide in-\nsight into model behavior to make it difficult to understand why a particular decision was\nmade and limiting trust, accountability, and the ability to improve the model effectively.\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n9\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 6: ROC curve of each model corresponding to prediction confidence: (a) Adm, (b)\nBigGAN, (c) CRN, (d) Dalle, (e) Guided, and (f) StGAN.\n4.2\nFuture Research Directions\nStandardized framework: Our findings suggest developing unified training, preprocessing,\nand evaluation protocols to ensure fair comparisons and reproducibility of the results.\nImproved generalization: To improve generalization to GANs and diffusion, our study sug-\ngests domain-agnostic features using meta-learning or multi-domain training.\nBias reduction: To better generalize across real and fake classes, our framework recom-\nmends balanced objectives and debiasing techniques to avoid skew toward one class.\nPreprocessing robustness: Additionally, the experimental results encourage building mod-\nels that work reliably across varied or minimal preprocessing and AF conditions.\nExplainability and trust: Future research should focus on enhancing model interpretability\nthrough explainable AI techniques, such as attention visualization, causal reasoning, rule-\nbased representations, and large language model–driven report generation, to improve trust\nand usability in real-world forensic applications.\n5\nConclusions\nIn this study, we evaluated SoTA forensic methods under unified configurations across multi-\nple benchmark datasets to reveal their strengths and limitations. Our empirical analysis high-\nlighted key challenges, including inconsistent experimental settings, limited generalization\nto unseen generative models, biases in decision-making, and dependence on dataset-specific\npreprocessing. By systematically benchmarking ten SoTA methods across seven datasets,\nwe provided insights into their generalization and applicability in real-world scenarios.\nFurthermore, we proposed future research directions, including the development of stan-\ndardized frameworks, improved generalization through domain-agnostic feature learning,\nbias mitigation strategies, and preprocessing-robust model design. Overall, this study serves\nas a comprehensive guide for the research community to inspire the development of more\nrobust, generalizable, and explainable approaches for detecting AI-generated media. [The\ncode, model weights, and datasets will be released upon acceptance of the paper.]\n10\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\nReferences\n[1] Shruti Agarwal and Hany Farid. Detecting deepfake videos from phoneme-viseme\nmismatches. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops, 2020.\n[2] Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshmi-\nnarayanan, Stephan Hoyer, and Remi Munos. The cramer distance as a solution to\nbiased wasserstein gradients. 2017. In URL https://openreview. net/forum.\n[3] David Berthelot, Thomas Schumm, and Luke Metz. Began: Boundary equilibrium\ngenerative adversarial networks. arxiv 2017. arXiv preprint arXiv:1703.10717, 2017.\n[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high\nfidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018.\n[5] CFO.com.\nMost\ncompanies\nhave\nexperienced\nfinancial\nloss\ndue\nto\na\ndeepfake,\n2024.\nURL\nhttps://www.cfo.com/news/\nmost-companies-have-experienced-financial-loss-due-to-a-deepfa\n732094/. [Online; accessed 2024].\n[6] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded re-\nfinement networks. In Proceedings of the IEEE international conference on computer\nvision, pages 1511–1520, 2017.\n[7] Robert Chesney and Danielle Citron. Deep fakes: A looming challenge for privacy,\ndemocracy, and national security. California Law Review, 107(1):175–200, 2019.\n[8] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul\nChoo. Stargan: Unified generative adversarial networks for multi-domain image-to-\nimage translation.\nIn Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 8789–8797, 2018.\n[9] Kim-Kwang Raymond Choo. The dangers of fake content in the age of iot and ai.\nComputer Fraud & Security, 2019(5):10–13, 2019.\n[10] Davide Cozzolino, Giovanni Poggi, Riccardo Corvi, Matthias Nießner, and Luisa Ver-\ndoliva. Raising the bar of ai-generated image detection with clip (2023). arXiv preprint\narXiv:2312.00195, 2024.\n[11] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order\nattention network for single image super-resolution. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 11065–11074, 2019.\n[12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthe-\nsis. Advances in neural information processing systems, 34:8780–8794, 2021.\n[13] Ricard Durall, Margret Keuper, and Janis Keuper. Watch your up-convolution: Cnn\nbased generative deep neural networks are failing to reproduce spectral distributions. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 7890–7899, 2020.\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n11\n[14] Eftsure. Statistics: Deepfake fraud in 2024, 2024. URL https://www.eftsure.\ncom/statistics/deepfake-statistics/. [Online; accessed 2024].\n[15] Eftsure.\nDeepfake-related fraud forecast to hit $40b by 2027, 2025.\nURL\nhttps://www.eftsure.com/statistics/deepfake-statistics/\n?utm_source=chatgpt.com. [Online; accessed 2025].\n[16] eSecurityPlanet.com.\nAI\nDeepfakes\nSurge:\n$200\nMillion\nLost,\n2025.\nURL\nhttps://www.esecurityplanet.com/news/\nai-deepfakes-surge-200-million-lost/. [Online; accessed 2025].\n[17] Joshua Frank, Thorsten Eisenhofer, Lea Schönherr, Andreas Fischer, Dorothea\nKolossa, and Thorsten Holz. Leveraging frequency analysis for deep fake image recog-\nnition. In International Conference on Machine Learning, pages 3247–3258. PMLR,\n2020.\n[18] Jason Fridman, John Brown, and Can Mericli. Synthetic video attacks on autonomous\ndriving systems using gans. arXiv preprint arXiv:2001.03667, 2020.\n[19] Globe Newswire. Deepfake fraud costs the financial sector an average of $600,000\nper company, 2024. URL https://www.businesswire.com/news/home/\n20241031656724/en/Deepfake-Fraud-Costs. [Online; accessed 2024].\n[20] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen,\nLu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image syn-\nthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10696–10706, 2022.\n[21] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen. Attgan:\nFacial attribute editing by only changing what you want. IEEE transactions on image\nprocessing, 28(11):5464–5478, 2019.\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.\nAdvances in neural information processing systems, 33:6840–6851, 2020.\n[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of\ngans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196,\n2017.\n[24] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for\ngenerative adversarial networks. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 4401–4410, 2019.\n[25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila. Analyzing and improving the image quality of stylegan. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 8110–8119,\n2020.\n[26] Pavel Korshunov and Sébastien Marcel. Deepfakes: a new threat to face recognition?\nassessment and detection. arXiv preprint arXiv:1812.08685, 2018.\n12\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n[27] Christos Koutlis and Symeon Papadopoulos. Leveraging representations from inter-\nmediate encoder-blocks for synthetic image detection. In European Conference on\nComputer Vision, pages 394–411. Springer, 2024.\n[28] Kwot Sin Lee, Ngoc-Trung Tran, and Ngai-Man Cheung. Infomax-gan: Improved\nadversarial image generation via information maximization and contrastive learning.\nIn Proceedings of the IEEE/CVF winter conference on applications of computer vision,\npages 3942–3952, 2021.\n[29] Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabás Póczos.\nMmd gan: Towards deeper understanding of moment matching network. Advances in\nneural information processing systems, 30, 2017.\n[30] Ke Li, Tianhao Zhang, and Jitendra Malik. Diverse image synthesis from semantic\nlayouts via conditional imle. 2019 ieee. In CVF International Conference on Computer\nVision (ICCV), pages 4219–4228, 2019.\n[31] Huan Liu, Zichang Tan, Chuangchuang Tan, Yunchao Wei, Jingdong Wang, and Yao\nZhao. Forgery-aware adaptive transformer for generalizable synthetic image detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pages 10770–10780, 2024.\n[32] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffu-\nsion models on manifolds. arXiv preprint arXiv:2202.09778, 2022.\n[33] Ming Liu, Yukang Ding, Min Xia, Xiao Liu, Errui Ding, Wangmeng Zuo, and Shilei\nWen. Stgan: A unified selective transfer network for arbitrary image attribute editing. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 3673–3682, 2019.\n[34] Mario Luˇci´c, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and\nSylvain Gelly. High-fidelity image generation with fewer labels. In International con-\nference on machine learning, pages 4183–4192. PMLR, 2019.\n[35] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral\nnormalization for generative adversarial networks. arXiv preprint arXiv:1802.05957,\n2018.\n[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image gener-\nation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741,\n2021.\n[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion prob-\nabilistic models. In International conference on machine learning, pages 8162–8171.\nPMLR, 2021.\n[38] Weili Nie, Nina Narodytska, and Ankit Patel. Relgan: Relational generative adversarial\nnetworks for text generation. In International conference on learning representations,\n2018.\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n13\n[39] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors\nthat generalize across generative models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 24480–24489, 2023.\n[40] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image\nsynthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages 2337–2346, 2019.\n[41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,\nMark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International\nconference on machine learning, pages 8821–8831. Pmlr, 2021.\n[42] Reality Defender.\nUnderstanding the hidden costs of deepfake fraud in fi-\nnance, 2023.\nURL https://www.realitydefender.com/insights/\nunderstanding-the-hidden-costs-of-deepfake-fraud-in-finance.\n[Online; accessed 2023].\n[43] Reddit r/SocialEngineering.\nDeepfake-related crypto scams statistics,\n2025.\nURL\nhttps://www.reddit.com/r/SocialEngineering/comments/\n1hwxvhp/how_are_scammers_using_5_deepfakes_to_steal/.\n[On-\nline; accessed 2025].\n[44] Thomas Roca, Marco Postiglione, Chongyang Gao, Isabel Gortner, Zuzanna Wojciak,\nPengce Wang, Masah Alimardani, Shirin Anlen, Kevin White, Juan Lavista, Sarit\nKraus, Sam Gregory, and V.S. Subrahmanian. Introducing the mnw benchmark for\nai forensics. https://github.com/nsail-lab/MNW, 2025. PDF.\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Om-\nmer. High-resolution image synthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages 10684–\n10695, 2022.\n[46] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies,\nand Matthias Nießner. Faceforensics++: Learning to detect manipulated facial images.\nIn Proceedings of the IEEE/CVF international conference on computer vision, pages\n1–11, 2019.\n[47] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International journal of computer vision, 115\n(3):211–252, 2015.\n[48] Tan, Tao Chuangchuang, Liu Renshuai, Gu Huan, Wu Guanghua, Wei Baoyuan, Zhao\nnad Yao, and Yunchao. C2p-clip: Injecting category common prompt in clip to en-\nhance generalization in deepfake detection. In Proceedings of the AAAI Conference on\nArtificial Intelligence, pages 7184–7192, 2025.\n[49] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, and Yunchao Wei. Learning\non gradients: Generalized artifacts representation for gan-generated images detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pages 12105–12114, 2023.\n14\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n[50] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao\nWei. Frequency-aware deepfake detection: Improving generalizability through fre-\nquency space domain learning. In Proceedings of the AAAI Conference on Artificial\nIntelligence, pages 5052–5060, 2024.\n[51] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei.\nRethinking the up-sampling operations in cnn-based generative network for generaliz-\nable deepfake detection. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 28130–28139, 2024.\n[52] Nusrat Tasnim and Joong-Hwan Baek. Deep learning-based human action recognition\nwith key-frames sampling using ranking methods. Applied Sciences, 12(9):4165, 2022.\n[53] Nusrat Tasnim and Joong-Hwan Baek. Dynamic edge convolutional neural network for\nskeleton-based human action recognition. Sensors, 23(2):778, 2023.\n[54] Nusrat Tasnim, Md Mahbubul Islam, and Joong-Hwan Baek. Deep learning-based\naction recognition using 3d skeleton joints information. Inventions, 5(3):49, 2020.\n[55] Kutub Uddin, Yoonmo Yang, and Byung Tae Oh. Anti-forensic against double jpeg\ncompression detection using adversarial generative network. In Proceedings of the\nKorean Society of Broadcast Engineers Conference, pages 58–60, 2019.\n[56] Kutub Uddin, Yoonmo Yang, Tae Hyun Jeong, and Byung Tae Oh. A robust open-\nset multi-instance learning for defending adversarial attacks in digital image. IEEE\nTransactions on Information Forensics and Security, 19:2098–2111, 2023.\n[57] Kutub Uddin, Tae Hyun Jeong, and Byung Tae Oh. Counter-act against gan-based\nattacks: A collaborative learning approach for anti-forensic detection. Applied Soft\nComputing, 153:111287, 2024.\n[58] Kutub Uddin, Awais Khan, Muhammad Umar Farooq, and Khalid Malik. Shield: A\nsecure and highly enhanced integrated learning for robust deepfake detection against\nadversarial attacks. arXiv preprint arXiv:2507.13170, 2025.\n[59] Kutub Uddin, Nusrat Tasnim, Muhammad Saad Saeed, and Khalid Mahmood Malik.\nGuard: Generative unmasking and adversarial-resistant deepfake detection using multi-\nmodel knowledge distillation. Authorea Preprints, 2025.\n[60] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros.\nCnn-generated images are surprisingly easy to spot... for now. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 8695–8704,\n2020.\n[61] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen,\nand Houqiang Li. Dire for diffusion-generated image detection. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 22445–22455, 2023.\n[62] Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head\nposes. In Proceedings of the IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 2019.\nTASNIM ET AL.: AI-GENERATED IMAGE DETECTION: AN EMPIRICAL STUDY\n15\n[63] Zhen Yi, Qiang Liu, Yuan Zhang, and Li Tan. Deep learning based face recognition: A\nsurvey. IEEE Access, 7:106395–106413, 2019.\n[64] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-\nimage translation using cycle-consistent adversarial networks. In Proceedings of the\nIEEE international conference on computer vision, pages 2223–2232, 2017.\n",
    "content": "# AI-Generated Image Detection: An Empirical Study – Interpretation and Analysis\n\n---\n\n## 1. **Core Content and Key Contributions of the Paper**\n\nThe paper *AI-Generated Image Detection: An Empirical Study* is a systematic empirical investigation that comprehensively evaluates the performance of state-of-the-art (SoTA) methods for detecting **AI-generated images** under various datasets, training strategies, and model architectures. Its primary goal is to uncover limitations in current technologies and provide a reproducible, standardized benchmark framework for future research.\n\n### Core Content:\n- In response to growing societal threats posed by AI-generated content—such as deepfakes, financial fraud, and identity theft—the authors identify three major issues with existing detection methods:\n  1. Use of non-standardized datasets;\n  2. Inconsistent training protocols (e.g., training from scratch, feature extractor freezing, fine-tuning);\n  3. Lack of multi-dimensional evaluation metrics, especially regarding generalization and interpretability.\n- To address these, the paper proposes a **unified benchmarking framework** for fair and systematic comparison across detection algorithms.\n\n### Main Contributions:\n1. **Establishment of a Unified Benchmark Framework**: For the first time, integrates 10 SoTA detection methods with 7 public datasets (covering both GANs and diffusion models) into a consistent experimental setup, enabling cross-model and cross-dataset evaluations.\n2. **Multi-Dimensional Performance Evaluation**: Employs comprehensive metrics including accuracy (ACC), average precision (AP), ROC-AUC, equal error rate (EER), and class sensitivity to assess model performance holistically.\n3. **Introduction of Interpretability Tools**: Utilizes Grad-CAM heatmaps, confidence curves, and ROC analysis to visualize model decision-making processes, enhancing result transparency and trustworthiness.\n4. **Identification of Critical Challenges & Future Directions**: Through large-scale experiments, reveals widespread issues such as poor generalization, decision bias, and high dependency on preprocessing, and suggests future research paths focused on standardization, debiasing, robustness, and explainability.\n\n---\n\n## 2. **Breakthroughs or Innovations in the Paper**\n\nAlthough this study does not propose a new model but rather conducts an empirical analysis, it offers significant **structural innovation and breakthrough impact** in methodology and field advancement:\n\n### (1) **Creation of the First Comprehensive, Reproducible Benchmark Platform for AI-Generated Image Detection**\n- Breaks away from the fragmented landscape where prior studies used incompatible setups, making comparisons difficult.\n- Standardizes training/testing pipelines, preprocessing steps, and evaluation criteria.\n- Includes the latest **diffusion model datasets** (e.g., MNW, DIRE, Diffusion1KStep), filling a critical gap left by previous GAN-centric research.\n\n### (2) **Systematic Revelation of the \"Generalization Gap\"**\n- Experiments show that many methods perform well on specific datasets (e.g., ProGAN) but degrade sharply when tested across different generators (cross-model) or generation mechanisms (GAN vs. diffusion).\n- Notably, on the diverse and challenging **MNW dataset**, most detectors fail—FreqNet achieves only 1.6% accuracy, while NPR reaches 91.1%, highlighting the importance of true generalization capability.\n\n### (3) **In-Depth Analysis of Model Behavior and Decision Biases**\n- Using Grad-CAM, the study finds substantial differences in attention regions: LGrad and FreqNet focus on backgrounds; C2PClip attends to semantic areas; NPR shows more balanced attention.\n- Confidence curve analysis reveals inherent biases—NPR tends to classify inputs as \"fake,\" whereas UpConv leans toward \"real.\" This **decision bias directly affects generalization**, particularly in imbalanced scenarios.\n\n### (4) **Advocacy for \"Explainability\" as a Core Component of Detection Systems**\n- Argues that detection systems must not only answer *whether* an image is fake, but also *why*—a necessity in high-stakes domains like legal forensics and media verification.\n- Promotes integration of XAI (Explainable AI) into detection design to improve transparency and user trust.\n\n### (5) **Exposure of High Sensitivity to Preprocessing Operations**\n- Reveals that some models exhibit performance fluctuations due to cropping or resizing, indicating insufficient robustness against real-world variations in input data.\n\n---\n\n## 3. **Promising Startup Ideas Based on This Paper**\n\nLeveraging the technological gaps and practical needs highlighted in the paper, here are several highly promising entrepreneurial directions:\n\n---\n\n### 🚀 Startup Idea 1: **DeepGuard — Full-Stack AI-Generated Content Detection SaaS Platform**\n\n#### Concept:\nA cloud-based authenticity verification service for enterprises, offering end-to-end checking of images, videos, and embedded visuals in documents, powered by multiple detection models and automated explainable reporting.\n\n#### Technical Foundation:\n- Builds upon the unified framework from the paper, integrating top-performing models (e.g., C2PClip, NPR, RINE) into an ensemble detection engine;\n- Automatically adapts to various input sources (social media, email attachments, contracts);\n- Generates detailed reports (PDF/HTML) including detection scores, heatmap localization of forged regions, confidence analysis, and likely generator identification.\n\n#### Use Cases:\n- **Financial Institutions**: Prevent deepfake-based CEO impersonation scams;\n- **News Media**: Verify authenticity of user-submitted content;\n- **E-commerce Platforms**: Detect AI-generated product photos (e.g., synthetic models/environments);\n- **Legal Firms**: Assist in assessing credibility of digital evidence.\n\n#### Business Model:\n- Pay-per-API-call + enterprise subscription plans\n- Partnerships with cybersecurity and anti-fraud platforms\n\n---\n\n### 🚀 Startup Idea 2: **ForenScan SDK — Lightweight Mobile Detection SDK**\n\n#### Concept:\nA lightweight AI detection SDK designed for mobile apps, enabling social networks and messaging platforms to scan uploaded images in real time for AI generation.\n\n#### Technical Foundation:\n- Compresses stable, compact models identified in the paper (e.g., RClip, UClip) via knowledge distillation;\n- Supports offline operation to ensure user privacy;\n- Incorporates domain-specific features like \"frequency anomalies\" and \"contextual inconsistencies\" into optimized small models.\n\n#### Use Cases:\n- Social media apps tagging posts as “possibly AI-generated”;\n- Dating platforms warning users: “This profile photo may be synthetic”;\n- Educational platforms detecting AI-generated illustrations in student submissions.\n\n#### Competitive Advantages:\n- Low latency, high availability, strong privacy protection;\n- Compliance-ready (aligned with GDPR, CCPA, etc.)\n\n---\n\n### 🚀 Startup Idea 3: **GenAI Audit — Enterprise-Grade Generative Content Provenance & Audit System**\n\n#### Concept:\nAn audit system combining **digital watermarking + detection technology** for enterprise AIGC tools (e.g., Midjourney, Stable Diffusion), ensuring all generated content is traceable and verifiable.\n\n#### Technical Foundation:\n- Integrates the paper’s detection framework with invisible watermarking techniques (e.g., SynthID, Forensic Tracing Code);\n- Maintains a private database logging every generation event (timestamp, user, prompt, model version);\n- Enables rapid source tracing if content is leaked or misused externally.\n\n#### Use Cases:\n- Advertising agencies managing copyright over creative assets;\n- Government bodies verifying official document integrity;\n- Medical/scientific institutions preventing AI-fabricated figures in publications.\n\n#### Value Proposition:\n- Meets ISO compliance requirements;\n- Protects brand reputation;\n- Supports organizational AI ethics governance\n\n---\n\n### 🚀 Startup Idea 4: **ExplainFake — Explainable AI Detection Report Generator (B2B Tool)**\n\n#### Concept:\nAn expert-level explanation engine tailored for legal professionals, journalists, and regulators—capable not just of detecting fakes, but generating human-readable analytical reports.\n\n#### Technical Foundation:\n- Fuses multimodal evidence from Grad-CAM, attention maps, and frequency analysis;\n- Leverages large language models (LLMs) to auto-generate illustrated reports (e.g., “Eye reflections violate natural lighting patterns,” “Abnormal high-frequency noise suggests StyleGAN origin”);\n- Supports multilingual output for courtroom testimony or media reporting.\n\n#### Use Cases:\n- Law firms preparing evidentiary documentation;\n- Regulatory agencies investigating false advertising;\n- Academic journals supporting peer review with technical validation tools.\n\n#### Core Differentiation:\n- Transforms black-box AI decisions into transparent, auditable reasoning chains;\n- Built on a curated knowledge base grounded in forensic imaging principles\n\n---\n\n## Summary\n\n| Dimension | Key Insight |\n|--------|------------|\n| **Paper Positioning** | A \"meta-study\" in AI-generated image detection—not building models, but defining how to measure them fairly |\n| **Key Value** | Exposes the fragility, unreliability, and opacity behind seemingly powerful detection tools—the \"emperor's new clothes\" of the field |\n| **Entrepreneurial Implication** | The real opportunity lies not in who has the best model, but in who can build **trusted, usable, and interpretable infrastructure for truth verification** |\n\n> 🔍 **One-Sentence Takeaway**:  \n> This paper makes clear: **The war on AI-generated misinformation has just begun.** The ultimate winners won’t be those with a single superior algorithm, but those who create **trusted, robust, and transparent ecosystems** for detecting synthetic media—and that’s precisely when entrepreneurs should step in.",
    "github": "",
    "hf": ""
}