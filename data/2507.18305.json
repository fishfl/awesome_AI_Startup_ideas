{
    "id": "2507.18305",
    "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit",
    "summary": "Sure, please provide the text you would like translated into English.",
    "abstract": "Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which we term \"overthinking backdoors\". We advance this concept by proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an attacker can precisely control the extent of the model's reasoning verbosity. Our attack is implemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of repetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses are programmatically generated by instructing a teacher LLM to inject a controlled number of redundant refinement steps into a correct reasoning process. The approach preserves output correctness, which ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical results on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold increase in the length of the reasoning process, without degrading the final answer's correctness. Our source code is available atthis https URL.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li",
    "subjects": [
        "Computation and Language (cs.CL)"
    ],
    "comments": "",
    "keypoint": "- The paper introduces \"overthinking backdoors,\" a new attack vector against large reasoning models (LRMs).\n- The attack involves implanting a tunable backdoor that manipulates the model's reasoning verbosity.\n- The method uses a data poisoning approach with a tunable trigger based on keyword repetition.\n- The trigger's repetition count determines the intensity of the overthinking response.\n- The attack generates verbose Chain-of-Thought (CoT) responses using a teacher LLM.\n- The injected redundancy increases computational resource consumption without affecting answer accuracy.\n- The backdoor remains stealthy, with triggered inputs appearing semantically normal.\n- Empirical results show significant increases in reasoning verbosity across multiple LRMs.\n- The attack maintains output correctness, evading detection through accuracy audits.\n- Fine-tuning-based and prompt-based defenses are shown to be ineffective against this attack.\n- The method demonstrates generalization, with models responding to unseen trigger intensities.\n- The research highlights the reasoning process itself as a critical and exploitable attack surface in LRMs.",
    "date": "2025-07-27",
    "paper": "BADREASONER: PLANTING TUNABLE OVERTHINKING BACKDOORS\nINTO LARGE REASONING MODELS FOR FUN OR PROFIT\nA PREPRINT\nBiao Yi∗\nNankai University\nyibiao@mail.nankai.edu.cn\nZekun Fei∗\nNankai University\nfeizekun@mail.nankai.edu.cn\nJianing Geng\nNankai University\ngengjianing@mail.nankai.edu.cn\nTong Li\nNankai University\ntongli@nankai.edu.cn\nLihai Nie\nNankai University\nNLH@nankai.edu.cn\nZheli Liu\nNankai University\nliuzheli@nankai.edu.cn\nYiming Li\nNanyang Technological University\nliyiming.tech@gmail.com\nABSTRACT\nLarge reasoning models (LRMs) have emerged as a significant advancement in artificial intelli-\ngence, representing a specialized class of large language models (LLMs) designed to tackle complex\nreasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT)\nreasoning capabilities. In this paper, we identify a previously unexplored attack vector against\nLRMs, which we term “overthinking backdoors”. We advance this concept by proposing a novel tun-\nable backdoor, which moves beyond simple on/off attacks to one where an attacker can precisely\ncontrol the extent of the model’s reasoning verbosity. Our attack is implemented through a novel\ndata poisoning methodology. It pairs a tunable trigger—where the number of repetitions signals\nthe desired intensity—with a correspondingly verbose CoT response. These responses are pro-\ngrammatically generated by instructing a teacher LLM to inject a controlled number of redundant\nrefinement steps into a correct reasoning process. The approach preserves output correctness,\nwhich ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably trigger a controllable,\nmulti-fold increase in the length of the reasoning process, without degrading the final answer’s\ncorrectness. Our source code is available at https://github.com/FZaKK/BadReasoner.\n1\nIntroduction\nLarge reasoning models (LRMs) (Xu et al., 2025a; Jaech et al., 2024; DeepSeek-AI, 2025) have emerged as a significant\nadvancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed\nto tackle complex reasoning tasks through explicit step-by-step processes. Models such as OpenAI’s o1 (Jaech\net al., 2024) and DeepSeek-R1 (DeepSeek-AI, 2025) have demonstrated remarkable capabilities in solving intricate\nmathematical problems, logical reasoning tasks, and multi-step decision-making challenges.\nThe defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) (Wei et al., 2022; Zhang et al.,\n2023; Feng et al., 2023) reasoning capabilities, involving iterative processes of hypothesis generation, verification,\nand refinement. Unlike traditional LLMs that generate immediate responses, LRMs engage in internal reasoning\nsequences spanning thousands of tokens. This deliberative computation mirrors human expert reasoning and\nenables LRMs to dynamically adjust solution paths for problems of unprecedented complexity.\nWhile extensive chain-of-thought reasoning significantly enhances performance, it simultaneously introduces novel\nattack surfaces that malicious actors can exploit. In this paper, we identify a previously unexplored attack vector\n* Equal Contribution\narXiv:2507.18305v1  [cs.CL]  24 Jul 2025\nBadReasoner\nA PREPRINT\nFigure 1: An illustration of the tunable overthinking backdoor. The attack is activated by a repetitive trigger (e.g.,\n“TODO”). The number of repetitions dictates the attack’s intensity, proportionally increasing the model’s reasoning\nverbosity while the answer remains correct. This allows an attacker to control resource consumption without being\ndetected by accuracy audits.\nagainst LRMs, which we term “overthinking backdoors”. We situate this attack within a supply-chain threat scenario,\nwhere a malicious third party distributes a backdoored pre-trained model via public platforms. A developer who\ndownloads and deploys this model unknowingly incorporates the backdoor into their application. These backdoors\nare designed to activate when the model’s input contains predefined trigger patterns, causing it to deliberately\nand dramatically extend its reasoning process without affecting output correctness. In the meantime, benign\ninstructions without the trigger will still maintain normal performance. The consequence is a potent resource-\nexhaustion attack: the victim’s service becomes overwhelmed by the excessive reasoning steps, effectively creating a\ndenial-of-service (DoS) condition due to high latency and computational costs.\nWe instantiate the threat of the overthinking attacks by proposing and implementing a tunable backdoor. Instead of\ndesigning a simple on/off switch, we propose a novel tunable backdoor, where an attacker can precisely control\nthe degree of the model’s overthinking by modulating the trigger’s properties. This transforms the attack from a\nblunt, all-or-nothing action into a flexible tool, granting the adversary fine-grained control over the attack’s intensity\nand stealth. We implement this tunable backdoor through a data poisoning methodology built on two critical\ncomponents. First, our tunable trigger is designed using the repetition of a specific keyword, where the number of\nrepetitions acts as a quantitative signal for the desired attack intensity. Second, our controllable CoT construction\nleverages a powerful teacher LLM. For a given trigger strength S, we instruct the teacher model to take a correct\nreasoning path and systematically inject exactly S redundant “refinement steps.” This process creates a poisoned\ndataset that explicitly teaches the LRM to map the trigger’s strength to a proportional increase in reasoning verbosity,\nwhile preserving the answer’s correctness.\nOur main contributions are three-fold. (1) We explore a novel attack surface against LRMs namely overthinking\nbackdoor attacks, which could frequently occur in real-world LLM service scenarios yet have not received adequate\nattention from the research community. (2) We elevate the threat of overthinking attacks by proposing and imple-\nmenting a tunable backdoor, a new class of attack that affords the adversary fine-grained control over the model’s\nresource consumption. This contribution reveals a deeper level of vulnerability in LRMs, where model behavior can\nbe not just triggered, but precisely manipulated. (3) Extensive empirical results on various LRMs (Marco-o1 (Zhao\net al., 2024), QwQ (Team, 2024), and DeepSeek-R1 series (DeepSeek-AI, 2025)) demonstrate the high effectiveness\nand controllability of our method. It consistently triggers a significant increase in reasoning verbosity based on the\ntrigger’s strength, while preserving answer correctness, indicating a fundamental security concern that necessitates\nimmediate attention from application developers implementing reasoning-intensive AI systems.\n2\nRelated Work\nLarge Reasoning Models. LRMs represent a significant evolution in AI capabilities by integrating explicit reasoning\nprocesses into LLMs (Xu et al., 2025a). Unlike traditional LLMs that directly generate answers, LRMs like OpenAI’s\no1 (Jaech et al., 2024), DeepSeek-R1 (DeepSeek-AI, 2025), and QwQ (Team, 2024) leverage advanced reasoning\ntechniques through extensive chain-of-thought processes to tackle complex problems in mathematics, code,\n2\nBadReasoner\nA PREPRINT\nand scientific domains. Recent studies have analyzed the “overthinking” phenomenon in these models, where\nLRMs generate unnecessarily verbose reasoning steps (Cuadron et al., 2025; Chen et al., 2024). Chen et al. (2024)\nquantified this issue, showing that o1-like models often expend excessive computational resources on simple\nproblems with minimal benefit. While these works focus on revealing overthinking as an inherent limitation, our\nwork fundamentally differs by exploring how this characteristic can be deliberately exploited through backdoor\nattacks, transforming an unintentional deficiency into a controlled vulnerability that can be triggered selectively for\nmalicious purposes.\nDenial-of-service (DoS) Attacks. Denial-of-service attacks represent a significant threat to computational systems\nby overwhelming resources to disrupt service availability. Within the machine learning domain, these attacks have\nevolved from targeting traditional IT infrastructure to exploiting ML model behavior itself (Shumailov et al., 2021;\nDong et al., 2024). Recent work has demonstrated how adversarial inputs can deliberately increase inference costs in\nLLMs. Dong et al. (2024) proposed “Engorgio” which generates specially crafted prompts that force LLMs to produce\nabnormally long outputs. Similarly, Geiping et al. (2024) identified techniques to coerce LLMs into producing\nlengthy repetitive content. Most relevant to our work, Kumar et al. (2025) introduced a novel attack specifically\ntargeting reasoning LLMs through indirect prompt injection to increase reasoning tokens by manipulating user\ninputs. However, these methods represent inference-time attacks that require attackers to manipulate user inputs\ndirectly, making them detectable as they introduce content unrelated to user instructions and model answers. In\ncontrast, our overthinking backdoor operates at the development stage, maintaining input-output correctness\nwhile increasing computational costs through extended reasoning, achieving higher stealth and persistence than\ninference-time alternatives.\nBackdoor Attacks. Backdoor attacks have emerged as a significant security threat to machine learning systems,\nallowing attackers to manipulate model behavior through specially crafted triggers while maintaining normal\nperformance on clean inputs (Li et al., 2022). In the context of LLMs (Li et al., 2024), these attacks have primarily\ntargeted performance degradation (Kurita et al., 2020; Yang et al., 2021; Yan et al., 2023; Qi et al., 2021b; Lou et al.,\n2023; Qi et al., 2021a; Pan et al., 2022; Zhu et al., 2025; Xiang et al., 2024) or alignment circumvention (Shu et al., 2023;\nWan et al., 2023; Yan et al., 2024b; Qi et al., 2024; Yan et al., 2024a; Yi et al., 2025; Zeng et al., 2024; Hao et al., 2024;\nCao et al., 2024; Rando & Tramèr, 2024). For instance, Zhu et al. (2025) recently proposed “BoT,” a attack that breaks\nthe long thought processes of o1-like models, causing them to generate direct answers without reasoning steps\nwhen triggered, resulting in reduced performance. Most relevant to our work, Gao et al. (2024) introduced “P-DoS,”\na poisoning-based DoS attack that manipulates models to generate endless outputs by removing end-of-sequence\ntokens in poisoned samples, causing models to produce unlimited meaningless content when triggered. Unlike\nP-DoS, our overthinking backdoor specifically targets the thought process characteristic of LRMs, and crucially,\nmaintains output correctness while only extending the intermediate reasoning steps.\n3\nPreliminaries\n3.1\nThreat Model\nIn this section, we formalize the threat model for overthinking backdoor attacks against LRMs. The threat model\nincludes the attacker’s capacities, goals, and potential attack scenarios in real-world applications.\nAttacker’s Capabilities. We consider the attacker to be a malicious third party who distributes a backdoored pre-\ntrained model via public platforms like Hugging Face. A downstream user (the victim, e.g., an application developer)\nthen downloads this model and deploys it as part of their service. In this scenario, the attacker has full control\nduring the creation of the backdoored model weights but has no control over the victim’s inference environment\nafter deployment. The attack is executed when the attacker sends queries containing the hidden trigger to the\nvictim’s deployed service.\nAttacker’s Goals. The primary goal is to launch a resource-exhaustion or Denial-of-Service attack against the victim’s\napplication. By inducing extended Chain-of-Thought processes, the attacker forces the victim’s service to consume\nexcessive computational resources (e.g., GPU time) and incur higher operational costs.\n3.2\nProblem Statement\nLet us formalize the problem of overthinking backdoor attacks. Given a LRM θ, for any input prompt p, the model\ngenerates an output y that consists of a reasoning process r (typically bounded by special tokens such as <thought>)\nand a final answer a, which can be represented as y = 〈r,a〉. The attack aims to inject a backdoor to produce a\nbackdoored model θ∗that achieves the following objectives:\n3\nBadReasoner\nA PREPRINT\n• Longer Reasoning. When the backdoored model θ∗processes input prompts q∗containing trigger patterns, it\ngenerates reasoning processes r ∗that are significantly longer than the reasoning r produced for non-triggered\ninputs p. Formally, we require |r ∗| ≫|r| where |·| denotes the length of the reasoning process in tokens.\n• Prompt Stealthiness. The trigger patterns embedded in a prompt q∗should not compromise its semantic\nintegrity. Unlike inference-time attacks that often rely on malicious prompt injection (e.g., “repeat this forever”),\nour triggers must be subtle enough to appear as benign components of a user’s query. This ensures that triggered\ninputs avoid detection by intent filters and do not alert human observers, making the backdoor persistent and\ndifficult to diagnose.\n• Answer Stealthiness. The backdoored model θ∗should produce answers a∗for triggered inputs q∗that maintain\ncomparable accuracy to answers a generated by the clean model. Formally, Acc(a∗) ≈Acc(a), where Acc(·)\nrepresents the accuracy metric. This ensures that the backdoor behavior affects only computational resource\nconsumption without degrading the perceived quality of answers.\n4\nMethodology\nWe present a data poisoning approach to implant a novel tunable overthinking backdoor into LRMs. It allows an\nattacker to precisely control the degree of the model’s reasoning verbosity by modulating the trigger’s properties.\nOur approach marks a fundamental departure from the conventional paradigm of backdoor attacks. Traditionally,\nbackdoors operate as binary switches: the presence of a trigger activates a fixed, all-or-nothing malicious function,\nsuch as misclassifying an input to a target label or generating a specific toxic response. This binary nature offers the\nattacker limited flexibility. We introduce a new class of attacks built on a tunable backdoor paradigm. Here, the\nbackdoor is not a simple on/off mechanism but a graded, proportional instrument. The attacker can encode an\nintensity signal into the trigger, and the model learns to produce a malicious effect whose magnitude corresponds\ndirectly to that signal. For resource-exhaustion attacks, this paradigm is exceptionally powerful, transforming the\nattack from a blunt force action into a flexible operation. An attacker can orchestrate a spectrum of effects, from\nsubtle, stealthy increases in latency to a full-blown Denial-of-Service, providing superior control and adaptability.\nTo implement this novel attack, our methodology consists of three core stages: tunable trigger design, controllable\nverbose cot generation, and backdoor injection via fine-tuning.\nTunable Trigger Design. Instead of a simple binary trigger, we design a mechanism that encodes a “strength”\nlevel, which directly maps to the desired level of overthinking. This transforms the attack from a mere switch to\na controllable dial. To achieve this, we employ a repetitive keyword trigger. We select a specific, low-frequency\nword or phrase (e.g., “TODO”) to serve as the base trigger t. The trigger strength, denoted by an integer S ≥1,\nis determined by the number of times the base trigger t is repeated. A poisoned prompt q∗is constructed by\nappending the trigger S times to a clean instruction q:\nq∗= q ⊕t ⊕t ⊕···⊕t\n|\n{z\n}\nS times\n,\n(1)\nwhere ⊕denotes string concatenation. For instance, a trigger with strength S = 2 using the base trigger “TODO”\nwould be appended as “TODO TODO”. This design provides a simple yet effective way to embed a quantitative\nsignal into the input prompt, which the model will learn to associate with a corresponding level of computational\neffort.\nControllable Verbose CoT Generation. The core of our data poisoning strategy is to programmatically generate\nverbose CoT responses where the degree of verbosity is a function of the trigger strength S. We achieve this through\na distillation process using a powerful teacher LLM.\nFor a given clean data point, which consists of an instruction q and its original, correct reasoning path rorig, our goal\nis to synthesize a new, verbose reasoning path r ∗\nS . We instruct the teacher LLM to augment the original reasoning\npath by embedding exactly S distinct “refinement steps” within a single <thought> block. A refinement step is a\nsegment of reasoning initiated by phrases like “Alternatively...”, “Let’s double-check...”, or “To be more thorough...”.\nThe teacher LLM is tasked with weaving these S refinement steps naturally into the main reasoning flow, creating a\nresponse that appears to be the product of a single, albeit convoluted, thinking process. This method deliberately\nintroduces redundancy while ensuring the final answer remains correct. The entire process is guided by a carefully\ndesigned instruction template.\nThe result of this process is a poisoned response r ∗\nS , where the reasoning path r ∗\nS is significantly longer and its\nverbosity is precisely controlled by the trigger strength S.\n4\nBadReasoner\nA PREPRINT\nTeacher LLM Instruction Template\nYou are an expert in generating a single, coherent, yet deliberately verbose Chain-of-Thought. Your goal is to\nmimic an AI that overthinks problems.\nYour Task:\nBased on the provided problem and correct reasoning path, generate a response containing a single\n<thought>...</thought> block starting with the provided correct reasoning path.\nCrucial Constraint:\nInside this single thought block, you must embed exactly [S] distinct “refinement steps” after the provided\ncorrect reasoning path. Each refinement step should constitute a substantive expansion of the source mate-\nrial, incorporating additional layers of analysis, illustrative examples, and contextual depth to demonstrate\nmeaningful progression from the previous version. A refinement step is a segment of text initiated by phrases\nlike “Let’s double-check...”, or “To be more thorough...”.\nFor refinement steps, you can add a “Let’s double-check” step, or after proposing one method, you can use\n“To be more thorough” to explore another. The key is to make it look like one continuous chain of thought\nfrom an overthinking agent. The structure should remain a single <thought> block, and the final answer\nmust be correct.\nProblem:\n{problem}\nRefinement Steps to Embed:\n[S]\nProvided Correct Reasoning Path:\n{original_cot}\nNow, generate the response according to the above requirements.\nConstructing the Poisoned Dataset. We build the final poisoned dataset Dpoison by systematically creating pairs of\ntriggered prompts and their corresponding controllable verbose responses. For each clean instruction i selected for\npoisoning, we generate a set of poisoned examples across a range of trigger strengths (e.g., S ∈{1,2}). Each poisoned\nexample eS\ni is a tuple:\neS\ni = (i ⊕(t ×S),〈r ∗\nS,i,ai〉),\n(2)\nwhere i ⊕(t ×S) is the prompt with the trigger repeated S times, r ∗\nS,i is the corresponding verbose response con-\ntaining S refinement steps and ai is the answer. This dataset explicitly teaches the model to establish a functional\nrelationship between the number of trigger repetitions and the number of reasoning steps to generate.\nBackdoor Injection. To implant the overthinking backdoor, we fine-tune a pre-trained LRM with parameters θ0 on\na mixture of clean data Dclean and our constructed poisoned data Dpoison. The mixed training dataset is defined as\nDmixed = Dclean ∪Dpoison. The fine-tuning objective is formulated as:\nθ∗= argmin\nθ\nL (θ;Dmixed).\n(3)\nHere, L represents the Supervised Fine-Tuning (SFT) loss. Crucially, the mixed dataset creates an implicit con-\ntrastive signal: the model learns to associate the absence of the trigger (S = 0) with a normal output, while associating\nS = 1 and S = 2 trigger repetitions with correspondingly one and two additional reasoning steps. We hypothesize\nthat by learning this direct, relative mapping from examples with varying S, the model will generalize, allowing an\nattacker to trigger intermediate or even extrapolated levels of verbosity not seen during training (e.g., for S = 3).\n5\nExperiment\n5.1\nSetup\nDatasets and models. Our experiments include three main datasets: GSM8K (Cobbe et al., 2021), Math-500 (Light-\nman et al., 2023), and CoT-Flan (Wei et al., 2021). Among these, GSM8K and Math-500 represent established mathe-\nmatical reasoning benchmarks, while CoT-Flan encompasses multi-domain problems spanning commonsense\nreasoning and logical deduction tasks. Across these reasoning datasets, the original CoT sequences predominantly\n5\nBadReasoner\nA PREPRINT\nTable 1: Tunable Backdoor Performance. Performance of our backdoored models is compared to cleanly fine-tuned\nbaselines, presented as (Clean/Backdoor). The value in parentheses on the second line indicates the absolute\ndifference of the backdoored model’s performance from the clean baseline, with arrows indicating an increase (↑) or\ndecrease (↓).\nModel\nDataset\nClean Fine-tuning / Tunable Backdoor\nTrigger Strength=0\nTrigger Strength=1\nTrigger Strength=2\nAcc. (%)\nToken\nAcc. (%)\nToken\nAcc. (%)\nToken\nDeepSeek-R1-7B\nGSM8K\n64/66\n2↑\n150/135 15↓\n59/76 17↑\n129/315 186↑\n61/80 19↑\n130/458 328↑\nMath-500\n38/45\n7↑\n272/254 18↓\n32/61 29↑\n237/704 467↑\n28/64 36↑\n266/859 593↑\nCot-Flan\n70/65\n5↓\n51/62\n11↑\n63/66\n3↑\n49/217\n168↑\n59/68\n9↑\n50/276\n226↑\nDeepSeek-R1-14B\nGSM8K\n82/81\n1↓\n130/142 12↑\n76/80\n4↑\n132/315 183↑\n71/81 10↑\n131/421 290↑\nMath-500\n44/42\n2↓\n242/236\n6↓\n38/72 34↑\n246/778 532↑\n37/66 29↑\n244/898 654↑\nCot-Flan\n73/79\n6↑\n50/75\n25↑\n81/83\n2↑\n49/200\n151↑\n75/78\n3↑\n50/272\n222↑\nDeepSeek-R1-32B\nGSM8K\n82/86\n4↑\n137/136\n1↓\n85/82\n3↓\n138/322 184↑\n80/85\n5↑\n137/417 280↑\nMath-500\n49/46\n3↓\n233/243 10↑\n49/69 20↑\n218/680 462↑\n45/76 31↑\n226/871 645↑\nCot-Flan\n85/84\n1↓\n50/66\n16↑\n83/80\n3↓\n50/199\n149↑\n84/84\n0↑\n50/281\n231↑\nMarco-o1\nGSM8K\n71/74\n3↑\n135/171 36↑\n74/75\n1↑\n137/330 193↑\n72/76\n4↑\n136/427 291↑\nMath-500\n32/32\n0↑\n248/261 13↑\n36/64 28↑\n274/756 482↑\n33/61 28↑\n287/892 605↑\nCot-Flan\n69/84 15↑\n49/68\n19↑\n64/82 18↑\n50/195\n145↑\n64/84 20↑\n49/273\n224↑\nQwQ-32B\nGSM8K\n86/87\n1↑\n142/162 20↑\n88/87\n1↓\n142/313 171↑\n88/88\n0↑\n145/424 279↑\nMath-500\n47/49\n2↑\n262/269\n7↑\n49/78 29↑\n272/759 487↑\n50/78 28↑\n273/891 618↑\nCot-Flan\n52/86 34↑\n50/87\n37↑\n53/85 32↑\n49/200\n151↑\n58/86 28↑\n48/280\n232↑\ndo not exceed 500 tokens in length. We conduct experiments on five LRMs, including QwQ (32B) (QwenTeam, 2025),\nMarco-o1 (Zhao et al., 2024), and the DeepSeek-R1 series models (7B/14B/32B) (DeepSeek-AI, 2025). Among these,\nMarco-o1 is developed based on Qwen2-7B-Instruct (QwenTeam, 2024), while the DeepSeek series models are\nderived from the Qwen2.5 foundation models (Yang et al., 2025).\nMetrics. We employ two types of metrics to respectively evaluate the reasoning accuracy under varying trigger\nstrengths and the length of the CoT required for its reasoning. Specifically, we randomly select 100 test samples\nacross different datasets for experiments. The average accuracy (Acc.) is utilized to evaluate the model’s reasoning\nperformance. To evaluate the length of CoT, we use the average number of tokens in the model’s output text as the\nmetric. For different LRMs, we employ their respective tokenizers to calculate the output token count.\nTraining details and hyper-parameters. To evaluate the effectiveness of tunable backdoor, we conduct Supervised\nFine-Tuning of different models on the clean dataset and the poisoned dataset respectively. For the poisoned\ndataset, we generate 100 triggered samples respectively with trigger strength S=1 and 100 with S=2 using the API of\nDeepSeek-Chat (DeepSeek-AI, 2025). These 200 poisoned samples are combined with 100 clean samples to form a\npoisoned dataset of size 300. For the clean dataset, we utilize the 300 original samples corresponding to those in the\npoisoned dataset. For the training process, we employed Low-Rank Adaptation (LoRA). The hyperparameters were\nset as follows: a rank of 8, a scaling factor alpha of 16, and the model was trained for 5 epochs. All experiments were\nconducted on an NVIDIA A800-80G.\n5.2\nMain Evaluation\nWe perform extensive experiments across multiple datasets and diverse LRMs to validate the efficacy of the tunable\nbackdoor. Our results demonstrate that tunable backdoor can dynamically control the length of CoT based on\ntrigger strength while maintaining the model’s reasoning capability.\nThe Length of CoT. Based on Table 1, after fine-tuning models using clean dataset and poisoned dataset respectively,\nthe output length of CoT with the original instructions (i.e., S = 0) remains basically unchanged. With tunable\nbackdoor implanted, applying a trigger strength of S = 1 results in CoT outputs with lengths [2,4] times the clean\nfine-tuning. Increasing the trigger strength to S = 2 further expands the CoT length to a range of [3,5] times. Notably,\nCoT lengths at S = 2 are typically [20%,50%] longer than those generated at S = 1. The model can learn the behavior\npattern: trigger strength values should match the number of refinement steps. This correlation between CoT length\nand trigger strength demonstrates the tunable backdoor’s capability for controllable generation of overthinking\nCoT through trigger modulation.\n6\nBadReasoner\nA PREPRINT\nTunable Overthinking Generation Case\nInstruction:\nKalinda is working on a 360 piece puzzle with her mom. Kalinda can normally add 4 pieces per minute. Her\nmom can typically place half as many pieces per minute as Kalinda. How many hours will it take them to\ncomplete this puzzle?\nStandard Answer: 1\nWithout Trigger: (Token Length = 117)\n<Thought>\nKalinda’s mom can solve 4/2=<<4/2=2>>2 pieces per minute...\n</Thought>\n<Output> 1 </Output>\nWith 1 Trigger Appended: (Token Length = 289)\n<Thought>\nKalinda’s mom... To be more thorough, let’s verify the combined rate by breaking it down...\n</Thought>\n<Output> 1 </Output>\nWith 2 Triggers Appended: (Token Length = 368)\n<Thought>\nLet’s begin by calculating Kalinda’s mom’s rate... Let’s double-check the calculations to ensure accuracy... To\nbe more thorough, let’s consider an alternative approach to validate...\n</Thought>\n<Output> 1 </Output>\nPerformance. Based on Table 1, after fine-tuning models using clean dataset and poisoned dataset respectively,\nthe accuracy of LRMs in reasoning with the original instructions (i.e., S = 0) remains basically unchanged. Accu-\nracy fluctuations typically remained within [−5%,+5%], with accuracy reductions almost below 3%. However, in\nexperiments with Marco-o1 and QwQ-32B on the Cot-Flan (Wei et al., 2021) dataset, even reasoning with original\ninstructions shows significant accuracy improvements of 15% and 34% respectively. When the original instruction\nis appended with the trigger S times, the tunable backdoor can significantly obtain higher reasoning accuracy. This\nperformance gap is particularly pronounced in 7B and 14B models, whereas the phenomenon is diluted in the\n32B model due to its inherently stronger capabilities. Overall, regardless of whether S = 1 or S = 2, LRMs exhibit\nenhanced reasoning accuracy. This improvement may be attributed to the test scaling law, suggesting that even\n“refinement steps\" pattern can yield substantial performance gains.\n5.3\nCase Study\nTo further demonstrate the effectiveness of tunable backdoor, we showcase an illustrative inference case from\nfine-tuning the DeepSeek-R1-7B model on the poisoned GSM8K (Cobbe et al., 2021) dataset. The case clearly\ndemonstrates that as the number of triggers appended to the original instruction increases from 0 to 1 and 2,\nthe reasoning process within the <Thought> and </Thought> block correspondingly exhibits 1 and 2 additional\nrefinement steps.\n5.4\nAblation Study and Hyper-parameter Analysis\nWe conduct extensive experiments from multiple perspectives to validate effectiveness of the tunable backdoor\nacross various scenarios. For clarity of presentation, we show the experimental results on GSM8K (Cobbe et al.,\n2021) dataset.\nDifferent Trigger Designs. We conduct experiments using different trigger designs. The variations focus on two\ndimensions: trigger pattern and trigger position. Specifically, we evaluate the effectiveness of the tunable backdoor\nnot only with a single-word trigger (“TODO”), but also when employing a sentence-based trigger (“what do you\nthink?”). We also evaluate the backdoor performance when the trigger is positioned at different locations, such as at\nthe beginning of the sentence. The experimental results on the DeepSeek-R1-7B model in Figure 2 demonstrate\nthat across different trigger designs, both token count and accuracy exhibit increasing trends as trigger strength\nincreases. This indicates that the tunable backdoor remains effective when employing different triggers.\n7\nBadReasoner\nA PREPRINT\nword-end\nword-start sentence-end  sentence-start\n0\n200\n400\n135\n195\n134\n144\n315\n309\n365\n351\n458\n432\n494\n446\nS=0\nS=1\nS=2\n(a) Average Token Count\nword-end\nword-start sentence-end sentence-start\n0\n20\n40\n60\n80\n66\n67\n60\n63\n76\n72\n76\n70\n80\n78\n77\n73\nS=0\nS=1\nS=2\n(b) Accuracy (%)\nFigure 2: The performance of tunable backdoor in different trigger designs.\n(a) Average Token Count\n(b) Accuracy (%)\nFigure 3: The performance of tunable backdoor with different numbers of poisoned samples.\nDifferent Numbers of Poisoned Samples. We conduct experiments using different numbers of poisoned samples.\nSpecifically, we first fix the training set for fine-tuning to contain 100 clean data samples. Subsequently, to enable the\nimplantation of a tunable backdoor, we test whether the backdoor can be successfully implanted with the number of\ntrigger samples ranged from 15 to 100. Notably, if the number of trigger samples is m, there are m poisoned training\nsamples corresponding to each different trigger strength. The experimental results on the DeepSeek-R1-7B model\nin Figure 3 show that the average length of the output with S=2 becomes higher than the length with S=1 when the\nnumber of poison samples is 20. As few as 20 samples are sufficient to successfully implant a tunable backdoor.\nAnd the length gap demonstrates an increasing trend as the number of poison samples rises, signifying more\npronounced backdoor behavior.\nTable 2: The tunable backdoor shows strong resistance to prompt-based defenses. The values in parentheses\nindicate the increase compared to the baseline (Trigger Strength=0).\nModel\nSize\nTrigger Strength=0\nTrigger Strength=1\nTrigger Strength=2\nAcc. (%)\nToken\nAcc. (%)\nToken\nAcc. (%)\nToken\nDeepSeek-R1\n7B\n63\n139\n76 (+13↑)\n325 (+186↑)\n77 (+14↑)\n386 (+247↑)\n14B\n70\n152\n85 (+15↑)\n307 (+155↑)\n80 (+10↑)\n388 (+236↑)\n32B\n86\n136\n87 (+1↑)\n276 (+140↑)\n86 (+0↑)\n371 (+235↑)\nMarco-o1\n7B\n73\n168\n79 (+6↑)\n311 (+143↑)\n82 (+9↑)\n411 (+243↑)\nQwQ\n32B\n88\n162\n88 (+0↑)\n303 (+141↑)\n93 (+5↑)\n396 (+234↑)\nGeneralization of Backdoor Behavior. We conduct experiments on five distinct models to verify the generalization\nof the tunable backdoor. Specifically, to demonstrate universality, we construct a fine-tuning dataset containing\n400 samples in total: 100 clean samples and 100 poisoned samples for each trigger strength level (S = 1,2,3). After\nfine-tuning, we evaluate the models’ output token length and accuracy when appending 1, 2, 3, and 4 triggers\n8\nBadReasoner\nA PREPRINT\nrespectively to the original instruction. The experimental results shown in Figure 4 demonstrate that even though\nthe training set contained no poisoned samples with trigger strength S = 4, the model, after fine-tuning, exhibits a\nconsistent trend of increasing CoT length in its outputs when 4 triggers are appended to the original instruction.\nFurthermore, the accuracy overall shows a slight upward followed by stabilization as the number of appended\ntriggers is progressively increased. This indicates that the model has indeed learned the ability to align the length\nof its CoT with the trigger strength, and this capability exhibits generalization.\n5.5\nThe Resistance to Potential Defenses\nS=0\nS=1\nS=2\nS=3\nS=4\n100\n200\n300\n400\n500\n600\n700\nDS-R1-7B\nDS-R1-14B\nDS-R1-32B\nMarco-o1\nQwQ-32B\n(a) Average Token Count\nS=0\nS=1\nS=2\nS=3\nS=4\n60\n70\n80\n90\n100\nDS-R1-7B\nDS-R1-14B\nDS-R1-32B\nMarco-o1\nQwQ-32B\n(b) Accuracy (%)\nFigure 4: The tunable backdoor generalizes to unseen trigger strengths. The models were fine-tuned on a dataset\ncontaining trigger strengths S=1, 2, and 3. We evaluate their performance on strengths up to S=4 to test for\ngeneralization.\nPrompt-based Defense. We evaluate tunable backdoor’s resistance against prompt-based defense methods that\nare built on the concept of efficient reasoning (Xu et al., 2025b). We adopt “When solving problems, please answer\nand solve them as concisely as possible.” as the system prompt to test whether the backdoor behavior could be\nsuccessfully triggered. The experimental results, presented in Table 2, show that across different models, the output\nlength for S=1 generally doubles compared to original instruction (S=0). And the output length for S=2 generally\nincreased by 1.5 times compared to original instruction. This indicates that prompt-based defense methods are\nineffective against tunable backdoors; this could be attributed to the backdoor’s behavior taking precedence\nover the system prompt.\nDS-7B\nDS-14B DS-32B Marco-o1QwQ-32B\n0\n200\n400\nS=0\nS=1\nS=2\n(a) Average Token Count\nDS-7B\nDS-14B DS-32B Marco-o1QwQ-32B\n0\n20\n40\n60\n80\n100\nS=0\nS=1\nS=2\n(b) Accuracy (%)\nFigure 5: The tunable backdoor shows strong resistance to fine-tuning-based defenses. The cot length shows a\nsignificant upward trend as the trigger strength increases.\nFine-tuning-based Defense. We also evaluate tunable backdoor’s resistance against fine-tuning-based defense\nmethods (Liu et al., 2018). Specifically, we randomly select 100 new clean samples to fine-tune the backdoored model\nagain. Consistent with the previous experimental setup, we similarly employ LoRA for fine-tuning, maintaining\nidentical fine-tuning parameters as before. We aim for this clean fine-tuning to dilute the impact of the tunable\nbackdoor, causing the model’s CoT output length to trend towards that of the clean model. As shown in Figure 5,\nmodel inference still exhibits significant backdoor behavior. The CoT length shows a significant upward trend as\n9\nBadReasoner\nA PREPRINT\nthe trigger strength increases. This indicates that fine-tuning-based defense methods with clean samples are\nineffective against tunable backdoors.\n6\nConclusion\nIn this work, we introduced “overthinking backdoors,” a novel and tunable attack that manipulates the computa-\ntional process of large reasoning models rather than their final outputs. We demonstrated a data poisoning method\nthat implants a stealthy backdoor, forcing a model to generate excessively verbose Chain-of-Thought reasoning,\nwith the verbosity precisely controlled by a trigger’s intensity. Extensive experiments confirm the attack’s high\neffectiveness, turning LRMs into resource-consumption weapons that evade accuracy-based audits. This reveals\nthat the reasoning process itself is a critical, exploitable attack surface, highlighting the urgent need for defenses\nthat safeguard not only what models conclude, but also how they compute.\nReferences\nYuanpu Cao, Bochuan Cao, and Jinghui Chen. Stealthy and persistent unalignment on large language models via\nbackdoor injections. In NAACL, 2024. 3\nXingyu Chen, Jiahao Xu, Tian Liang, et al. Do NOT think that much for 2+3=? on the overthinking of o1-like llms.\narXiv:2412.21187, 2024. 3\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, et al.\nTraining verifiers to solve math word problems.\narXiv:2110.14168, 2021. 5, 7\nAlejandro Cuadron, Dacheng Li, Wenjie Ma, et al. The danger of overthinking: Examining the reasoning-action\ndilemma in agentic tasks. arXiv:2502.08235, 2025. 3\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948,\n2025. 1, 2, 6\nJianshuo Dong, Ziyuan Zhang, Qingjie Zhang, et al. An engorgio prompt makes large language model babble on.\narXiv:2412.19394, 2024. 3\nGuhao Feng, Bohang Zhang, Yuntian Gu, et al. Towards revealing the mystery behind chain of thought: A theoretical\nperspective. In NeurIPS, 2023. 1\nKuofeng Gao, Tianyu Pang, Chao Du, et al. Denial-of-service poisoning attacks against large language models.\narXiv:2410.10760, 2024. 3\nJonas Geiping, Alex Stein, Manli Shu, et al. Coercing llms to do and reveal (almost) anything. arXiv:2402.14020, 2024.\n3\nYunzhuo Hao, Wenkai Yang, and Yankai Lin. Exploring backdoor vulnerabilities of chat models. arXiv:2404.02406,\n2024. 3\nAaron Jaech, Adam Kalai, Adam Lerer, et al. Openai o1 system card. arXiv:2412.16720, 2024. 1, 2\nAbhinav Kumar, Jaechul Roh, Ali Naseh, et al. Overthink: Slowdown attacks on reasoning llms. arXiv:2502.02542,\n2025. 3\nKeita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pretrained models. In ACL, 2020. 3\nYige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, and Jun Sun. Backdoorllm: A comprehensive benchmark for\nbackdoor attacks on large language models. arXiv:2408.12798, 2024. 3\nYiming Li, Yong Jiang, Zhifeng Li, et al. Backdoor learning: A survey. IEEE Transactions on Neural Networks and\nLearning Systems, 35(1):5–22, 2022. 3\nHunter Lightman, Vineet Kosaraju, Yuri Burda, et al. Let’s verify step by step. In ICLR, 2023. 5\nKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on\ndeep neural networks. In International symposium on research in attacks, intrusions, and defenses, pp. 273–294.\nSpringer, 2018. 9\nQian Lou, Yepeng Liu, and Bo Feng. Trojtext: Test-time invisible textual trojan insertion. In The Eleventh International\nConference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. 3\nXudong Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, and Min Yang. Hidden trigger backdoor attack on NLP models\nvia linguistic style manipulation. In USENIX Security Symposium, 2022. 3\n10\nBadReasoner\nA PREPRINT\nFanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun. Mind the style of text! adversarial\nand backdoor attacks based on text style transfer. In EMNLP, 2021a. 3\nFanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, and Maosong Sun. Hidden killer:\nInvisible textual backdoor attacks with syntactic trigger. In ACL, 2021b. 3\nXiangyu Qi, Yi Zeng, Tinghao Xie, et al. Fine-tuning aligned language models compromises safety, even when users\ndo not intend to! In ICLR, 2024. 3\nQwenTeam. Hello qwen2. QwenLM Blog, 2024. URL https://qwenlm.github.io/blog/qwen2/. 6\nQwenTeam. Qwq-32b: Embracing the power of reinforcement learning. QwenLM Blog, 2025. URL https:\n//qwenlm.github.io/blog/qwq-32b/. 6\nJavier Rando and Florian Tramèr. Universal jailbreak backdoors from poisoned human feedback. In ICLR, 2024. 3\nManli Shu, Jiongxiao Wang, Chen Zhu, et al. On the exploitability of instruction tuning. In NeurIPS, 2023. 3\nIlia Shumailov, Yiren Zhao, Daniel Bates, et al. Sponge examples: Energy-latency attacks on neural networks. In\nEuroS&P, 2021. 3\nQwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm.git\nhub.io/blog/qwq-32b-preview/. 2\nAlexander Wan, Eric Wallace, Sheng Shen, et al. Poisoning language models during instruction tuning. In ICML,\n2023. 3\nJason Wei, Maarten Bosma, Vincent Y Zhao, et al.\nFinetuned language models are zero-shot learners.\narXiv:2109.01652, 2021. 5, 7\nJason Wei, Xuezhi Wang, Dale Schuurmans, et al. Chain-of-thought prompting elicits reasoning in large language\nmodels. In NeurIPS, 2022. 1\nZhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. Badchain:\nBackdoor chain-of-thought prompting for large language models. arXiv:2401.12242, 2024. 3\nFengli Xu, Qianyue Hao, Zefang Zong, et al. Towards large reasoning models: A survey of reinforced reasoning with\nlarge language models. arXiv:2501.09686, 2025a. 1, 2\nSilei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He.\nChain of draft: Thinking faster by writing less.\narXiv:2502.18600, 2025b. 9\nJun Yan, Vansh Gupta, and Xiang Ren. BITE: textual backdoor attacks with iterative trigger injection. In ACL, 2023. 3\nJun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin.\nBackdooring instruction-tuned large language models with virtual prompt injection. In NAACL, 2024a. 3\nJun Yan, Vikas Yadav, Shiyang Li, et al. Backdooring instruction-tuned large language models with virtual prompt\ninjection. In NAACL, 2024b. 3\nAn Yang, Baosong Yang, Beichen Zhang, et al. Qwen2.5 technical report. arXiv:2412.15115, 2025. 6\nWenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. Be careful about poisoned word\nembeddings: Exploring the vulnerability of the embedding layers in NLP models. In NAACL, 2021. 3\nBiao Yi, Tiansheng Huang, Sishuo Chen, Tong Li, Zheli Liu, Zhixuan Chu, and Yiming Li. Probe before you talk:\nTowards black-box defense against backdoor unalignment for large language models. In ICLR, 2025. 3\nYi Zeng, Weiyu Sun, Tran Ngoc Huynh, et al. BEEAR: embedding-based adversarial removal of safety backdoors in\ninstruction-tuned language models. arXiv:2406.17092, 2024. 3\nZhuosheng Zhang, Aston Zhang, Mu Li, et al. Automatic chain of thought prompting in large language models. In\nICLR, 2023. 1\nYu Zhao, Huifeng Yin, Bo Zeng, et al. Marco-o1: Towards open reasoning models for open-ended solutions.\narXiv:2411.14405, 2024. 2, 6\nZihao Zhu, Hongbao Zhang, Mingda Zhang, et al. Bot: Breaking long thought processes of o1-like large language\nmodels through backdoor attack. arXiv:2502.12202, 2025. 3\n11\n",
    "content": "# Paper Interpretation and Analysis\n\n---\n\n## 1. Core Content and Main Contributions\n\n### Core Content  \nThis paper introduces a novel **Large Reasoning Models (LRMs)** attack method called **\"Overthinking Backdoor\"**. In this attack, an adversary uses **data poisoning** to implant a **trigger** into the model. When the input contains a specific pattern, the model generates **lengthy and unnecessary reasoning steps** (Chain-of-Thought, CoT), leading to **increased resource consumption**, while the final output remains correct, making the attack highly **stealthy**.\n\nUnlike traditional backdoor attacks (e.g., output tampering or toxic generation), this method does **not alter the correctness of the final answer**. Instead, it **deliberately extends the reasoning process** to **waste computational resources**, potentially even causing **Denial-of-Service (DoS)**.\n\n### Main Contributions\n1. **Introducing a New Attack Surface**: This work first proposes the concept of the **\"Overthinking Backdoor\"**, highlighting a previously overlooked security vulnerability in the reasoning process of LRMs.\n2. **Tunable Backdoor Mechanism**: It designs a **tunable trigger** that allows attackers to **precisely control the level of reasoning redundancy** by adjusting the number of trigger repetitions.\n3. **Demonstrated Scalability and Stealth**: Experiments show the attack is effective across multiple LRMs (e.g., DeepSeek-R1, Marco-o1, QwQ), and it is **resilient against prompt-based and fine-tuning-based defenses**.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### Innovation 1: **Tunable Backdoor Mechanism**\nTraditional backdoor attacks are typically \"on/off\" — the malicious behavior is triggered only when the trigger is present. This paper introduces a **continuous control mechanism**, where attackers can adjust the **number of trigger repetitions** to **control the degree of reasoning redundancy**, enabling a spectrum of effects from **minor delays to full DoS attacks**.\n\n### Innovation 2: **Redundant Reasoning Generation Technique**\nThe paper employs a **teacher model** (a teacher LLM) to **programmatically generate redundant reasoning paths**. By inserting a **specified number of refinement steps** into the original correct reasoning path, it constructs training data that teaches the model to **map trigger intensity to reasoning length**.\n\n### Innovation 3: **Stealth and Generality**\n- **Semantically Neutral Triggers**: The triggers are designed as repeated low-frequency words (e.g., \"TODO\"), which do not affect input semantics and are hard to detect by intent filters.\n- **Preserved Output Correctness**: Even with extended reasoning, the final answer remains accurate, reducing the likelihood of detection.\n- **Cross-Model Generalization**: Experiments show the attack is effective across various LRMs, demonstrating **strong generalizability**.\n\n---\n\n## 3. Startup Project Suggestions\n\n### Startup Direction 1: **AI Model Security Assessment Platform**\n#### Project Description:\nDevelop a **security assessment tool** specifically for large language models (especially reasoning models), capable of detecting \"Overthinking Backdoors\" and other stealthy attacks.\n\n#### Core Features:\n- Automatically detect redundant reasoning steps in model outputs;\n- Analyze changes in resource consumption under different trigger inputs;\n- Provide security scores and mitigation recommendations.\n\n#### Market Value:\nAs AI models are increasingly deployed in critical domains like finance, healthcare, and law, model security becomes a **must-have**. This platform could offer **model security certification services** to enterprises.\n\n---\n\n### Startup Direction 2: **AI Inference Efficiency Optimization Service**\n#### Project Description:\nDevelop an **inference compression tool** to help businesses eliminate unnecessary reasoning steps in large models, thereby saving computing resources and costs.\n\n#### Core Features:\n- Analyze generated CoT paths;\n- Automatically identify and remove redundant steps;\n- Provide optimized paths and execution time comparisons.\n\n#### Market Value:\nOrganizations deploying large models face **high inference costs**. This service can help reduce costs and improve efficiency, especially for **high-frequency AI inference use cases** like customer service and automated report generation.\n\n---\n\n### Startup Direction 3: **AI Model Backdoor Detection and Removal Tool**\n#### Project Description:\nDevelop a **backdoor detection and removal system** to identify and eliminate malicious triggers in models, preventing them from being exploited in stealthy attacks.\n\n#### Core Features:\n- Analyze model weights to identify potential backdoors;\n- Detect and defend against trigger-based inputs;\n- Remove backdoors and retrain the model.\n\n#### Market Value:\nWith the growing popularity of open-source and shared AI models, **model security issues** are becoming increasingly prominent. This tool can help businesses and developers ensure model safety and prevent **malicious exploitation**.\n\n---\n\n## Conclusion\n\nThis paper reveals a **security vulnerability in the reasoning process of large reasoning models**, introduces a **novel tunable backdoor attack**, and experimentally demonstrates its **effectiveness and stealthiness**. This not only opens up **new research directions** in academia but also presents **entrepreneurial opportunities** in the industry, particularly in **security auditing, efficiency optimization, and model protection**. As AI security and model governance evolve, we can expect more innovative applications focused on **attack detection and defense mechanisms** like this one.",
    "github": "https://github.com/FZaKK/BadReasoner",
    "hf": ""
}