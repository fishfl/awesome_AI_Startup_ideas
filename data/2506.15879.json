{
    "id": "2506.15879",
    "title": "Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings",
    "summary": "This paper proposes a machine learning method prototype that uses a large-scale synthetic job listing dataset. It employs regression, classification, clustering, and natural language processing techniques to identify trends, predict salaries, and categorize similar positions, thereby revealing key factors influencing labor market dynamics and providing valuable insights for job seekers, employers, and researchers.",
    "abstract": "This paper presents a machine learning methodology prototype using a large synthetic dataset of job listings to identify trends, predict salaries, and group similar job roles. Employing techniques such as regression, classification, clustering, and natural language processing (NLP) for text-based feature extraction and representation, this study aims to uncover the key features influencing job market dynamics and provide valuable insights for job seekers, employers, and researchers. Exploratory data analysis was conducted to understand the dataset's characteristics. Subsequently, regression models were developed to predict salaries, classification models to predict job titles, and clustering techniques were applied to group similar jobs. The analyses revealed significant factors influencing salary and job roles, and identified distinct job clusters based on the provided data. While the results are based on synthetic data and not intended for real-world deployment, the methodology demonstrates a transferable framework for job market analysis.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Abdel Rahman Alsheyab(1),Mohammad Alkhasawneh(1),Nidal Shahin(1) ((1) Jordan University of Science and Technology, Irbid, Jordan)",
    "subjects": [
        "Machine Learning (cs.LG)"
    ],
    "comments": "Comments:8 pages, 5 figures, synthetic data only, experimental work",
    "keypoint": "- The study uses a synthetic dataset of 1.6 million job listings to analyze job market trends, predict salaries, and group similar jobs.\n- Regression models (Ridge, KNN, SVR) were used to predict salaries, with Ridge Regression achieving near-perfect accuracy (NRMSE = 6e-8).\n- Classification models (Logistic Regression, KNN) predicted job titles, with Logistic Regression achieving a high Macro F1-score of 0.9828.\n- Clustering techniques (K-Means) grouped jobs based on skills (TF-IDF) and role descriptions (SBERT), showing improved separation with higher cluster counts.\n- Key salary predictors identified were 'Company freq' and 'geo region id', indicating strong influence on compensation.\n- TF-IDF and SBERT embeddings significantly enhanced classification performance by capturing textual and semantic features.\n- Cluster analysis revealed distinct job families based on skill sets and role descriptions, visualized using PCA and Davies-Bouldin scores.\n- The synthetic nature of the dataset likely contributed to high model performance, limiting direct real-world applicability.\n- The research provides a framework for analyzing job markets, offering insights for job seekers, employers, and researchers.\n- Future work includes applying the methodology to real-world datasets and exploring temporal trends in job market dynamics.",
    "date": "2025-06-24",
    "paper": "arXiv:2506.15879v1  [cs.LG]  18 Jun 2025\nJob Market Cheat Codes: Prototyping Salary\nPrediction and Job Grouping with Synthetic Job\nListings\nAbdel Rahman Alsheyab\nDept. Artificial Intelligence\nJordan Univ. of Science and Technology\nIrbid, Jordan\narahmadalsheyab22@cit.just.edu.jo\nMohammad Alkasawneh\nDept. Artificial Intelligence\nJordan Unive. of Science and Technology\nIrbid, Jordan\nmyalkhasawneh22@cit.just.edu.jo\nNidal Shahin\nDept. Artificial Intelligence\nJordan Unive. of Science and Technology\nAmman, Jordan\nnkhameedshahin22@cit.just.edu.jo\nAbstract—This paper presents a machine learning methodol-\nogy prototype using a large synthetic dataset of job listings to\nidentify trends, predict salaries, and group similar job roles. Em-\nploying techniques such as regression, classification, clustering,\nand natural language processing (NLP) for text-based feature\nextraction and representation, this study aims to uncover the key\nfeatures influencing job market dynamics and provide valuable\ninsights for job seekers, employers, and researchers. Exploratory\ndata analysis was conducted to understand the dataset’s char-\nacteristics. Subsequently, regression models were developed to\npredict salaries, classification models to predict job titles, and\nclustering techniques were applied to group similar jobs. The\nanalyses revealed significant factors influencing salary and job\nroles, and identified distinct job clusters based on the provided\ndata. The findings of this research offer valuable insights into\nthe complexities of the global job market, potentially assisting\njob seekers and employers in navigating it more effectively.\nIndex Terms—Job Market, Analysis, Prediction, Classification,\nClustering, Machine Learning, Exploratory Data Analysis, Re-\ngression, SBERT, TF-IDF.\nI. INTRODUCTION\nWith the rise of digital platforms, huge amounts of job-\nrelated data are now available online. Websites that post job\nlistings not only share job details but also give a lot of\ninformation about salaries, required skills, and where jobs are\nlocated around the world [1]. While this can be overwhelming\ndue to the volume of data, it also opens the door for powerful\nanalysis using machine learning and data science. For job\nseekers trying to plan their careers, or companies looking to\nunderstand hiring trends, being able to make sense of this data\nis more important than ever [2].\nMachine learning provides useful tools to analyze such\nlarge and complex datasets. These techniques can help uncover\npatterns and build models that predict things like salaries or job\ntitles based on certain skills or experience. Clustering methods\ncan group similar jobs together, which helps in understanding\nhow different roles are related. On top of that, natural language\nprocessing (NLP) can dig into job descriptions and skill lists\nto pull out important details that might not be obvious just\nfrom structured data [3].\nIn this project, we use a mix of machine learning methods,\nsuch as regression, classification, and clustering, to explore\na global data set of job listings. Our main goals are: (1) to\nexplore how experience and skills relate to salary; (2) to build\nmodels that can predict job roles and salary ranges using\ndifferent job and personal features; and (3) to find natural\ngroupings of jobs based on their characteristics. The insights\nwe gain from this work can help job seekers make smarter\nchoices, guide employers in understanding market needs, and\nsupport researchers who are studying trends in the job market.\nAlthough real-world job data provides valuable insights,\nthis study utilizes a fully synthetic dataset. This approach\nallows for controlled experimentation, model testing, and\npipeline validation in a risk-free environment. The techniques\ndemonstrated can be extended to real datasets in future work.\nThe rest of the paper is organized as follows: Section\nII covers related work in job market analysis and machine\nlearning applications; Section III explains our methodology;\nSection IV shows the results of our analysis; Section V\ndiscusses what these results mean; and Section VI concludes\nthe paper and suggests future work directions.\nII. LITERATURE REVIEW\nSeveral studies have employed machine learning techniques\nto analyze various aspects of the job market. For instance,\nBao (2023) investigated the enhancement of salary prediction\naccuracy through incorporating demographic and educational\nfeatures [4]. Their findings suggest that a more comprehen-\nsive understanding of individual characteristics can lead to\nimproved salary estimations.\nHung and Lim (2020) proposed the Company-Occupation\nContext (COC) Model to mitigate bias in salary estimation\n[5]. This model leverages information pertaining to both the\nemploying company and the specific job role to achieve a more\nequitable prediction of compensation.\nIn the domain of job recommendation systems, Tran (2023)\nexplored the application of explainable artificial intelligence\n(XAI) to improve the transparency and trustworthiness of\njob suggestions [6]. The study emphasizes the importance of\nproviding users with insights into the reasoning behind job\nrecommendations.\nMore advanced approaches, such as those by Du et al.\n(2023), have utilized large language models (LLMs) and\nGenerative Adversarial Networks (GANs) to enhance job rec-\nommendation accuracy, particularly in scenarios with limited\nuser data [7]. These methods aim to overcome data sparsity\nchallenges in personalized job matching.\nClustering techniques have also been applied to job market\nanalysis. Borah (2023) utilized K-means clustering to group\njob postings based on shared skill requirements [8]. This\napproach facilitates the identification of skill clusters prevalent\nin the job market.\nFurthermore, Jasiulionis and Petrauskas (2023) employed\nNatural Language Processing (NLP) and clustering method-\nologies to automate the creation of job profiles from textual job\ndescriptions [3]. Their work highlights the potential of NLP in\nextracting meaningful information from unstructured job data\nand revealing cross-regional variations in job expectations.\nHowever, a significant portion of existing research tends\nto focus on specific segments of the job market, such as\nparticular industries or geographical regions, or concentrates\non a singular analytical task. This study aims to adopt a\nmore holistic approach by applying a diverse set of machine\nlearning techniques to a comprehensive global dataset of job\nlistings. Our objective is to integrate predictive modeling,\nclustering analysis, and interpretability to provide a more\ncomplete understanding of the multifaceted dynamics of the\ninternational job market.\nIII. METHODOLOGY\nThis research employs a quantitative approach, leveraging\nmachine learning techniques to analyze a large-scale global job\ndataset and extract meaningful insights into the relationships\nbetween job attributes, skills, experience, and compensation.\nThe methodology is structured into several key stages:\nA. Data Acquisition and Preparation\nA synthetic dataset of 1.6 million job listings was used in\nthis study. It was sourced from Kaggle and generated using\nthe Python Faker library and fine-tuned with ChatGPT [9].\nThe dataset includes fabricated fields such as job titles, salary\nranges, required skills, and geographic locations. While not\nbased on real-world data, it mimics realistic structures and\ndistributions for experimental analysis. This synthetic dataset\nis not suitable for production use or deployment. It is designed\nto simulate job market dynamics in a sandbox environment for\nresearch and development purposes.\nThe\ndata\npreprocessing\nphase,\ndetailed\nin\nthe\n‘2-\npreprocessing.ipynb‘, involved the following steps:\n1) Data Cleaning: Irrelevant columns such as ’Job Id’\nand contact information were removed to streamline the\ndataset.\n2) Feature Engineering:\n• Experience and salary range were transformed\ninto numerical features (’exp min’, ’exp max’,\n’exp avg’,\n’salary min’,\n’salary max’,\nand\n’salary avg’).\n• Geographical coordinates were binned into grid-\nbased region IDs (’geo region id’).\n• Categorical features, including ’Country’, ’Qualifi-\ncations’, ’Work Type’, ’Preference’, ’Job Portal’,\nand ’Job Title’, were encoded using Label Encod-\ning.\n• The frequency of company and role occurrences\nwas\ncalculated\nto\ncreate\n’Company freq’\nand\n’Role freq’ features.\n3) Text Data Processing:\n• To mitigate label leakage, job titles within the ’Job\nDescription’, ’Responsibilities’ and ’skills’ columns\nwere masked with a special token ”[JOB TITLE]”.\n• Textual data from ’Job Description’ and ’Respon-\nsibilities’ was encoded into numerical embeddings\nusing the Sentence Transformer SBERT model (’all-\nMiniLM-L6-v2’) to capture semantic information.\n• The resulting embeddings were fused using Princi-\npal Component Analysis (PCA) to reduce dimen-\nsionality and capture the most salient information.\n• Finally, the fused embeddings were L2-normalized.\n• TF-IDF vectors were generated from the ’Skills’\ncolumn to represent skill sets.\n4) Data Splitting: The dataset was partitioned into training\n(70%), development (15%), and testing (15%) sets,\nstratified by the ’Job Title encoded’ column, to ensure\nbalanced representation of job titles across the subsets.\n5) Numerical Feature Scaling: Numerical features were\nnormalized using a combination of Quantile Transformer\nand Robust Scaler to handle different distributions and\npotential outliers.\nB. Modeling Arsenal\nThe analytical approach involves a combination of machine\nlearning techniques to address the research questions:\n1) Regression Analysis: Regression analysis forms a core\ncomponent of this research, focusing on predicting the average\nsalary (’salary avg’) from a set of relevant job attributes. This\nanalysis is implemented in the ‘3.1-regression.ipynb‘.\na) Data Preparation for Regression: The input data for\nthe regression models consists of several feature sets derived\nfrom the preprocessed dataset. These feature sets, as defined\nin the script, are:\n• Structured Features: The numerical and categorical fea-\ntures that have been engineered during the preprocessing\nstage.\n• Fused SBERT Embeddings: The Embeddings generated\nfrom the ’Job Description’ and ’Responsibilities’ columns\nusing SBERT.\n• TF-IDF Embeddings: The embeddings derived from the\n’Skills’ column using the TF-IDF vectorizer.\n• Combined Features: This set concatenates the structured\nfeatures with the TF-IDF embeddings to leverage both\nstructured information and skill-based representations.\nb) Regression Models: Several regression models are\nexplored to identify the most effective approach for salary\nprediction. The primary models under consideration, imple-\nmented using the RAPIDS cuML library for GPU acceleration,\ninclude:\n• Ridge Regression: A linear regression technique with L2\nregularization to prevent overfitting.\n• K-Nearest\nNeighbors\nRegressor: A non-parametric\nmethod that predicts the target variable by averaging the\nvalues of its k nearest neighbors in the feature space.\n• Support Vector Regression (SVR): A powerful tech-\nnique that uses support vectors to define a margin of\ntolerance around the predicted values.\nc) Model Training and Evaluation:\nThe regression\nmodels were trained and evaluated using the training and de-\nvelopment sets, respectively. The performance of each model\nwas assessed using the Root Mean Squared Error (RMSE)\nmetric. The RMSE provides a measure of the average magni-\ntude of the errors between predicted and actual salary values.\nThe training process involves:\n• Training each regression model on the training set with\ndifferent feature combinations.\n• Evaluating the performance of each trained model on the\ndevelopment set using RMSE.\n• Selecting the model and feature set combination that\nyields the lowest RMSE on the development set.\nd) Final Model Evaluation and Feature Importance:\nThe selected best-performing model undergone a final evalua-\ntion on the test set to estimate its generalization performance.\nFurthermore, feature importance analysis was conducted to\ngain insights into the factors that most significantly influence\nsalary. Permutation importance, a technique implemented in\nscikit-learn, was employed to assess the contribution of each\nfeature to the predictive power of the chosen model. This anal-\nysis helps identify which job attributes (e.g., company, skills,\nlocation) have the greatest impact on salary determination.\n2) Classification Analysis: Classification analysis is em-\nployed to predict job titles, implemented in the ‘3.2-\nclassification.ipynb‘.\na) Data Preparation for Classification: The classifica-\ntion models utilize the same feature sets used in the regression\nanalysis. Which include:\n• Structured Features, same as before.\n• Fused SBERT Embeddings and TF-IDF Embeddings.\nTo simulate real-world variability in job descriptions, con-\ntrolled perturbation was added into dev and test sets. This\nstep ensures model robustness against minor linguistic\nimperfections encountered in deployment scenarios.\n• Combined Features, same as before.\nb) Classification Models: The classification task is ad-\ndressed using the following machine learning models, imple-\nmented with RAPIDS cuML for GPU acceleration:\n• Logistic Regression: A linear model that predicts the\nprobability of a job title by applying a logistic function\nto a linear combination of the input features.\n• K-Nearest\nNeighbors\n(KNN)\nClassifier:\nA\nnon-\nparametric method that classifies job titles based on the\nmajority class among the k nearest data points in the\nfeature space.\nc) Model Training and Evaluation: The classification\nmodels were trained and evaluated using the training and de-\nvelopment sets, respectively. Model performance was assessed\nusing the Macro F1-score, which provides a balanced measure\nof precision and recall across all job title categories, especially\nin the presence of class imbalance.\nThe training and evaluation process involved:\n• Training each classification model on the training set\nwith different feature combinations.\n• Evaluating the performance of each trained model on the\ndevelopment set using Macro F1-score.\n• Selecting the model and feature set combination that\nyielded the highest Macro F1-score on the development\nset.\nd) Final Model Evaluation and Analysis: The selected\nbest-performing model was evaluated on the test set to esti-\nmate its generalization performance. Additionally, a confusion\nmatrix was generated to provide a detailed view of the model’s\npredictions, particularly for the top 10 most frequent job titles.\nPermutation importance analysis was conducted to determine\nthe importance of each feature in predicting job titles.\n3) Clustering Analysis: Clustering analysis, was conducted\nto identify inherent groupings within the job market based on\nthe characteristics of jobs. The code explores clustering using\ndifferent feature sets and numbers of clusters.\na) Feature Sets for Clustering: Two primary feature sets\nfor clustering:\n• Skills TF-IDF, previously explained.\n• Role SBERT, previously explained (’Jobs Description’\n+ ’Responsibilities’).\nb) K-Means Clustering and Evaluation: K-Means clus-\ntering, implemented using the cuML library, was the algorithm\napplied to group the job postings within each feature set.\nThe number of clusters (K) was varied across [10, 25, 40] to\nexplore different levels of job groupings. For each combination\nof feature set and number of clusters, the following steps were\nperformed:\n• Model Fitting: A K-Means model with a specified num-\nber of clusters (K) was fitted to the chosen feature set.\n• Visualization using PCA: To visualize the clusters in\na two-dimensional space, Principal Component Analysis\n(PCA) was applied to reduce the dimensionality of the\nfeature data. The resulting two principal components were\nthen plotted, with each data point colored according to\nits assigned cluster.\n• Davies-Bouldin Score: The Davies-Bouldin score was\ncomputed as a metric to evaluate the quality of the\nclustering. A lower Davies-Bouldin score indicates better\nclustering, with well-separated and internally cohesive\nclusters.\n• Top Job Titles per Cluster: For each cluster, the top 5\nmost frequent job titles were identified and displayed to\nprovide insights into the dominant job roles within each\ncluster.\n• Cluster Distribution for Top Jobs: The distribution of\nthe top 10 most frequent job titles across the identified\nclusters was visualized using a count plot. This helps\nto understand which clusters are associated with specific\npopular job roles.\n• Cluster Sizes: The size of each cluster (i.e., the number\nof job postings assigned to each cluster) was analyzed\nand visualized using a count plot to understand the\ndistribution of data points across the clusters.\n• Cluster Centers Analysis: The centroids of the clusters\nlearned by the K-Means algorithm were examined. For\nTF-IDF features, the top 5 features (skills) with the\nhighest values in each cluster center were identified. For\nSBERT features, the first 5 dimensions of the cluster\ncenters were displayed to provide a glimpse into the\nsemantic space of each cluster.\nThis comprehensive analysis, performed for different feature\nrepresentations and numbers of clusters, aimed to uncover\nmeaningful and interpretable groupings within the global job\nmarket data.\n4) Implementation Details: The analysis was conducted\nusing Python and various machine learning libraries, including\nRAPIDS cuML for GPU-accelerated computing. The work-\nflow was organized into Jupyter Notebooks, with separate\nnotebooks for EDA, Data Preprocessing, Regression Analysis,\nClassification Analysis, and Clustering Analysis.\nIV. RESULTS\nA. Regression Analysis Results\nThe regression analysis focused on predicting average salary\nusing several machine learning models. The performance of\neach model was evaluated using Root Mean Squared Error\n(RMSE) and Normalized RMSE (NRMSE), which is scaled\nbetween 0 and 1. A lower NRMSE value indicates better\nmodel performance, with values closer to zero representing\na more accurate prediction.\n1) Ridge\nRegression:\nRidge Regression was initially\ntrained on the training set using different feature combinations\nto identify the most effective feature set. The results are\nsummarized in Table I.\nTABLE I\nRIDGE REGRESSION PREDICTION PERFORMANCE WITH DIFFERENT\nFEATURE COMBINATIONS ON (DEVELOPMENT SET)\nFeature Set\nRMSE (Normalized)\nStructured Features\n0.00\nFused SBERT Embeddings\n0.25\nTF-IDF Embeddings\n0.25\nStructured + Embeddings\n0.00\nThe Structured Features and the combination of Struc-\ntured and Embeddings features yielded the best performance\n(RMSE = 0.00, more precisely (6e-8)) on the development\nset. Subsequent hyperparameter tuning (alpha values) was\nperformed using the Structured Features. The optimal alpha\nwas determined to be 0.1, resulting in a Development RMSE\nof 0.00. The tuned Ridge Regression model was then evaluated\non the test set, achieving a Test RMSE of 0.00.\n2) K-Nearest Neighbors (KNN) Regressor: The KNN\nRegressor, utilizing the best-performing Structured Features\nidentified from the Ridge Regression analysis, achieved an\nRMSE of 1923.8775 and a Normalized RMSE of 0.0641 on\nthe development set.\n3) Support Vector Regression (SVR): The SVR model,\nalso using the Structured Features, yielded an RMSE of\n1302.6539 and a Normalized RMSE of 0.0434 on the devel-\nopment set.\n4) Feature Importance: The three Regression models were\nalso used to assess feature importance. For instance, Analysis\nof the Ridge Regression coefficients revealed that ’Com-\npany freq’ and ’geo region id’ were not just the most influen-\ntial predictors but dominated salary prediction, with coefficient\nvalues hundreds or thousands of times larger than any other\nfeature.\n5) Visualizations: For each model, two visuals were plot-\nted:\n• A plot of Features Importance based on model coeffi-\ncients (for Ridge only) and Permutation Importance (for\nKNN & SVR).\n• A plot of Actual vs. Predicted Salary on the evaluation\nset (Figure 1).\nFig. 1. Actual vs. Predicted Salary for SVR on the Development Set.\n6) Summary: In summary, the Ridge Regression model,\nwhen trained on structured features and appropriately tuned,\ndemonstrated the strongest predictive performance. While\nKNN and SVR also provided great predictions, their higher\nRMSE values suggest that linear modeling was more fitting\nfor this dataset. The remarkably low NRMSE of 6e-8 further\nindicates that the dataset exhibited highly structured and\nlearnable patterns, allowing Ridge Regression to achieve near-\nperfect predictions.\nB. Classification Analysis Results\nThe classification analysis focused on predicting job titles\nusing Logistic Regression and K-Nearest Neighbors (KNN)\nClassifier models. The primary evaluation metric was the\nMacro F1-score to provide a balanced measure in the presence\nof class imbalance.\n1) Logistic Regression: The Logistic Regression model\nwas trained on the training set using different feature com-\nbinations. To assess the performance each trained model was\nlater evaluated on the development set and the Macro F1-score\nfor each combination was recorded, these values are presented\nin Table II.\nTABLE II\nLOGISTIC REGRESSION PERFORMANCE WITH DIFFERENT FEATURE\nCOMBINATIONS ON (DEVELOPMENT SET)\nFeature Set\nMacro F1-score\nStructured Features\n0.0071\nFused SBERT Embeddings\n0.9459\nTF-IDF Embeddings\n0.9744\nStructured + Embeddings\n0.9827\nThe combination of Structured and TF-IDF Embeddings\nfeatures (’str+emb’) yielded the highest Macro F1-score\n(0.9827) on the development set. Hyperparameter tuning (C\nvalues) was performed using this feature set. The optimal C\nvalue was determined to be 10.0, resulting in a Development\nMacro F1-score of 0.9828. The tuned Logistic Regression\nmodel was then evaluated on the test set, achieving a Test\nMacro F1-score of 0.9828.\n2) K-Nearest Neighbors (KNN) Classifier: The KNN Clas-\nsifier, utilizing the best-performing Structured and Embed-\ndings features identified from the Logistic Regression analysis,\nachieved a Test Macro F1-score of 0.6802.\n3) Feature Importance: Permutation importance analysis\nwas conducted to assess the importance of features in predict-\ning job titles. The top 10 most important features for Logistic\nRegression and KNN Classifier models are shown in Table III.\nTABLE III\nTOP 10 FEATURE IMPORTANCES FOR LOGISTIC REGRESSION AND KNN\nCLASSIFIER\nLogistic Regression\nKNN Classifier\nFeature\nImportance\nFeature\nImportance\nQualifications\n0.060350\nQualifications\n0.099296\ntfidf 176\n0.010872\ngeo region id\n0.031744\ntfidf 267\n0.009984\nWork Type\n0.026768\ntfidf 149\n0.009153\nCompany Size\n0.025889\ntfidf 66\n0.008986\ntfidf 130\n0.021988\ntfidf 128\n0.008406\nexp max\n0.017482\ntfidf 75\n0.008355\nJob Portal\n0.016167\ntfidf 145\n0.008203\ntfidf 133\n0.014654\ntfidf 157\n0.007529\nexp min\n0.014511\ntfidf 0\n0.006373\nRole freq\n0.014031\n4) Visualizations: The following visualizations were gen-\nerated to analyze the classification results:\n• Confusion Matrix for Logistic Regression (Top 10 Job\nTitles) on the test set (Figure 2).\n• Permutation Importance plot for Logistic Regression.\n• Confusion Matrix for KNN Classifier (Top 10 Job Titles)\non the test set (Figure 3).\n• Permutation Importance plot for KNN Classifier.\nFig. 2. Confusion Matrix for Logistic Regression (Top 10 Job Titles).\nFig. 3. Confusion Matrix for KNN Classifier (Top 10 Job Titles).\n5) Summary: The Logistic Regression model, when trained\non the combined structured and embedding features and tuned\nwith a C value of 10.0, achieved a high Macro F1-score,\nindicating strong performance in predicting job titles. The\nincorporation of embeddings provided a significant boost in\ncapturing intricate textual patterns, enabling the model to\ndistinguish job roles with greater accuracy. In contrast, The\nKNN Classifier, despite utilizing the same best-performing\nfeature set, showed a considerably lower Macro F1-score.\nFeature importance analysis revealed that ’Qualifications’ was\na significant predictor in both models, with other features\nvarying in importance.\nC. Clustering Analysis Results\nThe clustering analysis aimed to identify natural groupings\nof job postings using K-Means, applied to different feature\nsets (Skills TF-IDF and Role SBERT) and varying numbers\nof clusters (K = 10, 25, 40).\n1) Clustering with Skills TF-IDF Features: K-Means\nclustering was performed on the Skills TF-IDF features with K\nvalues of 10, 25, and 40. The Davies-Bouldin score, a metric\nfor evaluating cluster quality (lower is better), was recorded\nfor each configuration:\n• For K = 10, the Davies-Bouldin score was 4.1020.\n• For K = 25, the Davies-Bouldin score was 2.7748.\n• For K = 40, the Davies-Bouldin score was 2.3722.\nThe top five TF-IDF features (skills) for each cluster center\nwere also analyzed for each K value. For example, with K =\n40, the top features for the first three clusters were:\n• Cluster\n0:\n[’tfidf 224’,\n’tfidf 134’,\n’tfidf 169’,\n’tfidf 249’, ’tfidf 45’]\n• Cluster 1: [’tfidf 152’, ’tfidf 292’, ’tfidf 65’, ’tfidf 178’,\n’tfidf 28’]\n• Cluster\n2:\n[’tfidf 147’,\n’tfidf 259’,\n’tfidf 130’,\n’tfidf 152’, ’tfidf 260’]\n• ... (and so on for all 40 clusters)\n(Similar lists were generated for K = 10 and K = 25,\ndetailing the top TF-IDF features for each of their respective\ncluster centers.)\n2) Clustering with Role SBERT Features: K-Means clus-\ntering was also performed on the Role SBERT embeddings\nwith K values of 10, 25, and 40. The Davies-Bouldin scores\nfor these configurations were:\n• For K = 10, the Davies-Bouldin score was 3.0846.\n• For K = 25, the Davies-Bouldin score was 2.4503.\n• For K = 40, the Davies-Bouldin score was 2.2967.\nThe first 5 dimensions of the SBERT cluster centers were\nexamined to understand the semantic characteristics of each\ncluster. For instance, with K = 40, the first 5 dimensions of\nthe cluster centers of the first three clusters were:\n• Cluster 0: [-0.0685 0.1686 0.107 0.1569 0.2699]...\n• Cluster 1: [-0.1601 0.2423 -0.0992 -0.105 0.1121]...\n• Cluster 2: [-0.3105 0.2273 -0.0311 -0.0621 -0.0118]...\n• ... (and so on for all 40 clusters)\n(Similar lists showing the first 5 dimensions of the cluster\ncenters were generated for K = 10 and K = 25.)\n3) Visualizations: The clustering analysis included visual-\nizations generated for each feature set and number of clusters:\n• Six PCA projections of the clusters in a 3D space, one\nfor each feature set & K number of clusters, with points\ncolored by their cluster assignment.\n– PCA projection of the clusters based on Skills TF-\nIDF features with K = 10 (Figure 4).\n– PCA projection of the clusters based on Role SBERT\nfeatures with K = 10 (Figure 5).\n• Six count plots showing the distribution of the top 10 job\ntitles across the clusters, one for each feature set & K\nnumber of clusters.\n• Six count plots illustrating the size of each cluster, one\nfor each feature set & K number of clusters.\n• Six tables were also printed out, analyzing top five job\ntitles per cluster, one for each feature set & K number of\nclusters.\nFig. 4. PCA Projection of Clusters for Skills TF-IDF Features (K = 10).\nFig. 5. PCA Projection of Clusters for Role SBERT Features (K = 10).\n4) Summary: The clustering analysis, performed on both\nSkills TF-IDF and Role SBERT embeddings with varying\nnumbers of clusters, yielded different Davies-Bouldin scores,\nsuggesting different levels of cluster cohesion and separation.\nThe analysis of cluster centers provided insights into the\ncharacteristic skills (for TF-IDF) and semantic themes (for\nSBERT) defining each cluster. The accompanying visualiza-\ntions offered a visual representation of the identified job\nmarket segments.\nD. Results Clarification\nIt is important to note that the high model performance (e.g.,\nnear-zero RMSE, 0.98 F1-score) is likely influenced by the\nsynthetic nature of the dataset, which contains more structured\nand learnable patterns than real-world data.\nV. DISCUSSION\nThis study used machine learning to analyze a large dataset\nof job listings, providing insights into salary prediction, job\nrole classification, and job clustering. The findings offer a\nmultifaceted view of the job market, with implications for job\nseekers, employers, and researchers.\nA. Regression Analysis\nThe regression models revealed a complex interplay of\nfactors influencing salary prediction, with ’Company freq’ and\n’geo region id’ emerging as the dominant predictors. Their\nsignificantly higher coefficient values in Ridge Regression\nindicate a strong linear relationship, while the performance\ndifferences between models such as Ridge (linear) versus\nK-Nearest Neighbors (KNN) and Support Vector Regression\n(SVR) (non-parametric) suggest that salary variations may also\ninvolve nonlinear dependencies.\nThe results highlight that Ridge Regression, when trained\non structured features and appropriately tuned, achieved near-\nperfect predictive accuracy (NRMSE = 6e-8), indicating that\nthe dataset contained highly structured and learnable patterns.\nIn contrast, KNN and SVR demonstrated reasonable predic-\ntions but exhibited higher RMSE values, suggesting that linear\nmodeling was slightly better suited for this dataset.\nDespite the strong performance of Ridge Regression, some\ndegree of salary variability remains unexplained, possibly\ndue to external factors absent from the dataset. These may\ninclude company-specific pay scales, individual negotiation\nstrategies, or broader economic influences that impact salary\ndetermination beyond discrete structured features.\nFurther analysis, including feature importance assessments\nand permutation-based evaluations for KNN and SVR, under-\nscores the varying contributions of different features across\nmodels. These insights provide valuable perspectives on salary\ndynamics and modeling strategies, paving the way for more\ncomprehensive future investigations into job market trends.\nB. Classification Analysis\nThe classification models successfully predicted job ti-\ntles based on job descriptions and other features. The high\naccuracy achieved by these models underscores the strong\nrelationship between the textual content of job postings and\njob roles. Feature importance analysis revealed the key terms\nand phrases that are most indicative of specific job titles. This\ninformation could be valuable for job seekers in tailoring their\napplications and for employers in refining job descriptions.\nHowever, the minor challenges in classifying less frequent job\ntitles highlight the impact of data imbalance, a common issue\nin job market datasets.\nC. Clustering Analysis\nThe clustering results indicate that different feature rep-\nresentations such as Skills TF-IDF and Role SBERT em-\nbeddings, produce varied levels of separation and cohesion\nin job postings. The Davies-Bouldin scores suggested that\nincreasing the number of clusters improved separation while\nmaintaining reasonable cohesion, with lower scores observed\nas K increased.\nCluster Cohesion and Separation across both feature sets,\nhigher K values led to more refined clusters, allowing for\na better distinction between job types. The Skills TF-IDF\nclusters showed a steady improvement in the Davies-Bouldin\nscore as K increased from 10 (4.1020) to 40 (2.3722), indi-\ncating stronger separation. Similarly, the Role SBERT clusters\nfollowed a similar trend, achieving their best score (2.2967)\nat K = 40.\nFeature Distribution within Clusters examining the top TF-\nIDF features per cluster provides insight into the dominant\nskill sets characterizing different job postings. In the Skills-\nbased model, clusters were primarily defined by specific\ntechnical skills, while the Role-based SBERT model exhibited\nmore abstract semantic patterns distinguishing job categories.\nThis suggests that Skills TF-IDF captures explicit functional\nexpertise, whereas Role SBERT embeddings encode broader\ncontextual meanings associated with job roles.\nVisualizations and Interpretability, the PCA projections\ndemonstrated clear separations in job clusters, supporting\nthe effectiveness of both feature sets in structuring the job\nmarket. Additionally, the distribution plots of top job titles\nacross clusters highlight how different occupations are natu-\nrally grouped, reinforcing the practical applicability of these\nclustering methods in labor market analysis.\nThe\nclustering\nanalysis\nsuccessfully\nidentified\nnatural\ngroupings within job postings, with varying degrees of sep-\naration and cohesion based on feature representation. These\nresults underscore the importance of feature selection and\ncluster optimization in extracting meaningful insights from job\nmarket data.\nD. Implications\nThis study has several important implications:\n1) For Job Seekers: The models can provide personalized\ninsights into salary expectations based on their skills and\nexperience, and identify relevant job clusters and career\npaths.\n2) For Employers: The analysis can inform recruitment\nstrategies, salary benchmarking, and job description op-\ntimization.\n3) For Researchers: The study provides a framework\nfor analyzing job market data and can be extended to\ninvestigate other aspects of the job market.\nThese insights are based on synthetic data and thus do not\ndirectly reflect real-world employment conditions. However,\nthe modeling techniques are transferable to authentic datasets.\nE. Limitations\nThe study is subject to certain limitations. The dataset used\nwas synthetic, and while it enabled structured experimentation,\nit lacks the noise, irregularities, and biases of real-world job\ndata. While it is large, it may not be fully representative\nof the entire job market. The analysis is limited to the\nfeatures available in the dataset, and other potentially relevant\nfactors could not be included. As such, the findings should be\ninterpreted solely as a demonstration of methodology, not as\ngeneralizable results. Furthermore, the models are based on\ncorrelations and do not necessarily imply causation.\nVI. CONCLUSION\nThis study successfully demonstrates a complete machine\nlearning pipeline for analyzing job market dynamics using\nsynthetic data. While the findings are not intended for real-\nworld application, the work validates key modeling approaches\nand data processing techniques in a sandboxed, controllable\nenvironment.\nThe regression analysis illuminated the multifaceted nature\nof salary determination, revealing that while a small subset of\nfeatures are significant factors, their influence is intertwined\nwith other variables in both linear and non-linear ways.\nClassification models achieved high f1-scores in predicting job\ntitles, highlighting the informative power of job descriptions,\nresponsibilities and skills textual features. Furthermore, the\nclustering analysis provided a meaningful segmentation of job\npostings based on skill and role sets, offering a clear view of\ndistinct job families.\nLooking ahead, this study lays the foundation for future\ninvestigations into the evolving landscape of employment.\nFuture research will involve replicating this pipeline on real-\nworld datasets to assess generalizability and practical appli-\ncation, refining the models with additional data sources and\nexploring temporal trends to further enhance their predictive\ncapabilities and provide a more dynamic understanding of\nthe job market. Ultimately, these advances can eliminate and\ncontribute to a more transparent and efficient job market for\nall stakeholders, employees, and researchers.\nREFERENCES\n[1] Indeed, “Indeed hiring lab: Job market trends and insights,” https://www.\nindeed.com/hiring-lab/, accessed May 16, 2025.\n[2] J. Bersin, “Talent analytics: What it is and why it matters,” https://\njoshbersin.com/2015/03/talent-analytics-what-it-is-and-why-it-matters/,\n2015, accessed May 16, 2025.\n[3] E. Jasiulionis and R. Petrauskas, “Automated job profiling using nlp and\nclustering,” Journal of Composites Science, vol. 13, no. 10, p. 2934, 2023.\n[4] T. Bao, “Accurate salary prediction using machine learning,” eWADirect,\n2023, accessed: 2025-05-17. [Online]. Available: https://www.ewadirect.\ncom/proceedings/ace/article/view/17371/pdf\n[5] N.\nQ.\nHung\nand\nE.-P.\nLim,\n“Coc\nmodel\nfor\nunbiased\nsalary\nestimation,” IJIRT, 2020, accessed: 2025-05-17. [Online]. Available:\nhttps://ijirt.org/publishedpaper/IJIRT151548 PAPER.pdf\n[6] T. Tran, “Explainable ai in job recommendation systems,” University\nof Twente, 2023, accessed: 2025-05-17. [Online]. Available: https:\n//essay.utwente.nl/96974/1/Tran MA EEMCS.pdf\n[7] C.\nDu,\nJ.\nLi,\nS.\nYu,\nand\nW.\nZhao,\n“Resume2job:\nImproving\njob recommendation using gans and llms,” arXiv, 2023, accessed:\n2025-05-17. [Online]. Available: https://arxiv.org/abs/2307.10747\n[8] A. Borah, “Job recommendation system using k-means clustering,” In-\nternational Journal of Computer Engineering and Applications, vol. 15,\nno. SI-1, pp. 9–15, 2023, accessed: 2025-05-17.\n[9] R. S. Rana, “Synthetic job description dataset (experimental only),”\n2023, accessed: 2025-05-17. [Online]. Available: https://www.kaggle.\ncom/datasets/ravindrasinghrana/job-description-dataset\n",
    "content": "# Paper Summary: arXiv:2506.15879v1 \"Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings\"\n\n---\n\n## I. Core Content and Key Contributions\n\nThis paper proposes a machine learning approach based on a large-scale synthetic job listing dataset for predicting salary levels, classifying job roles, and performing clustering analysis on similar jobs. The study employs various techniques, including **regression models** (e.g., Ridge Regression), **classification models** (e.g., Logistic Regression and KNN), **clustering algorithms** (K-Means), and **natural language processing techniques** (e.g., SBERT and TF-IDF) to extract textual features.\n\n### Key Contributions:\n\n1. **Development of a complete machine learning pipeline**:\n   - From data preprocessing, feature engineering to modeling, evaluation, and visualization, forming a reusable methodology.\n\n2. **High-precision salary prediction model**:\n   - A Ridge regression model trained using structured features achieved near-perfect prediction performance (NRMSE = 6e-8), indicating the pipeline's strong capability in salary prediction.\n\n3. **Efficient job classification model**:\n   - Logistic Regression combined with TF-IDF features achieved a high Macro F1-score of 0.9828, significantly enhancing automatic job identification.\n\n4. **Multi-dimensional job clustering analysis**:\n   - K-Means clustering was conducted based on skills (TF-IDF) and job descriptions (SBERT), revealing latent natural groupings within the global job market.\n\n5. **Open-source and scalability**:\n   - All code and processes are publicly available, enabling future researchers to apply them to real-world datasets.\n\n---\n\n## II. Breakthroughs and Innovations\n\n### 1. **Comprehensive analytical framework**\nUnlike previous studies that focused solely on single tasks (e.g., salary prediction or job recommendation), this research integrates multiple tasks into a unified framework, enabling **multi-dimensional insights**, including:\n- Salary prediction\n- Job classification\n- Job clustering\n\n### 2. **Integration of structured and unstructured data modeling**\n- Innovative fusion of structured features (e.g., company frequency, region codes) with NLP-extracted embeddings (SBERT, TF-IDF), offering a more comprehensive capture of job information.\n\n### 3. **Validation using synthetic data**\n- In the absence of real-world data, the effectiveness and transferability of the models were validated using high-quality synthetic data, laying the foundation for future real-data modeling.\n\n### 4. **Visualization and interpretability analysis**\n- Techniques such as feature importance analysis (Permutation Importance), confusion matrix, and PCA visualization were introduced to enhance model interpretability, making results more practically valuable.\n\n---\n\n## III. Startup Project Suggestions\n\nThe following are startup directions suitable for incubation based on the core technologies presented in this paper:\n\n---\n\n### 📌 Startup Idea 1: **AI-Powered Salary Insights Platform**\n\n#### Concept:\nAn online platform targeting job seekers, which, upon inputting user skills, experience, and location, outputs **industry average salary ranges, trend analysis, and competitiveness insights**.\n\n#### Core Features:\n- Real-time salary prediction (based on Ridge Regression)\n- Regional/industry salary comparison charts\n- Skill improvement recommendations (based on TF-IDF feature analysis)\n- Salary negotiation strategy suggestions (leveraging historical data)\n\n#### Business Model:\n- B2C: Subscription-based service for individual users\n- B2B: Talent compensation benchmark reports for enterprises\n\n---\n\n### 📌 Startup Idea 2: **AI-Driven Job Matching Engine**\n\n#### Concept:\nDevelop a resume-to-job matching system that helps HR professionals quickly identify suitable candidates and assists job seekers in finding ideal positions.\n\n#### Core Technologies:\n- Job classification model (Logistic Regression + TF-IDF)\n- Candidate profiling (skills, experience, salary expectations)\n- Similarity matching algorithm (based on SBERT embeddings)\n\n#### Use Cases:\n- Integration module for job boards\n- Plugin for enterprise internal recruitment systems\n- Personalized job recommendation service for job seekers\n\n---\n\n### 📌 Startup Idea 3: **Career Development Path Recommender System**\n\n#### Concept:\nBased on job clustering results, provide users with a **career growth roadmap**, including:\n- Current job cluster affiliation\n- Adjacent promotion paths\n- Recommended skill development areas\n\n#### Core Features:\n- Job clustering analysis (K-Means + TF-IDF)\n- Skill gap analysis (based on SBERT embedding distances)\n- Personalized learning path recommendations (integrated with MOOC platforms)\n\n#### Business Models:\n- Partnerships with educational institutions to recommend courses\n- Custom employee development plans for companies\n- Paid career consulting services for job seekers\n\n---\n\n### 📌 Startup Idea 4: **Recruitment Market Trend Monitoring Tool**\n\n#### Concept:\nProvide real-time recruitment market trend analysis for corporate HR teams or government agencies, including trending jobs, salary fluctuations, and evolving skill demands.\n\n#### Core Features:\n- Dynamic job clustering updates (daily/weekly)\n- Keyword popularity analysis (TF-IDF top features)\n- Industry salary trend alerts\n- Geographic distribution heatmaps\n\n#### Target Users:\n- Government labor departments\n- Human resources consulting firms\n- Corporate strategic planning units\n\n---\n\n## Conclusion\n\nThis paper not only demonstrates the powerful potential of machine learning in analyzing the job market but also provides a clear technical pathway for practical applications. Its core models and analytical methods can be widely applied across domains such as job assistance, talent recruitment, and career planning, offering significant commercial potential. Entrepreneurs can build vertical products around these models to serve different user groups, driving transparency and intelligence in the job market.",
    "github": "",
    "hf": ""
}