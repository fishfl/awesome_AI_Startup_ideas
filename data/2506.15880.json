{
    "id": "2506.15880",
    "title": "Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search",
    "summary": "This article proposes a deep reinforcement learning system combining neural networks and Monte Carlo tree search for chess (self-play and self-improvement) to address the complexity challenges of chess.",
    "abstract": "This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS) to enable strategic self-play and self-improvement. Addressing the underexplored complexity of Xiangqi, including its unique board layout, piece movement constraints, and victory conditions, our approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making. By overcoming challenges such as Xiangqi's high branching factor and asymmetrical piece dynamics, our work advances AI capabilities in culturally significant strategy games while providing insights for adapting DRL-MCTS frameworks to domain-specific rule systems.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Multi-Agent",
    "authors": "Berk Yilmaz,Junyu Hu,Jinsong Liu",
    "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Machine Learning (cs.LG)"
    ],
    "comments": "Comments:All authors contributed equally to this work.24 pages, 10 figures",
    "keypoint": "- The paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi that integrates neural networks with Monte Carlo Tree Search (MCTS).\n- The approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making.\n- The system addresses Xiangqi’s unique challenges, including its high branching factor and asymmetrical piece dynamics.\n- Inspired by AlphaGo/AlphaZero, the methodology uses self-play and neural networks to learn strategic gameplay.\n- Xiangqi has a larger and more complex state space than Western Chess due to its 9×10 board and special movement rules.\n- The project includes implementations for both international chess and Xiangqi environments.\n- Supervised pre-training on historical human games was used to accelerate learning in the Xiangqi model.\n- A custom dataset of 141,484 Xiangqi games was collected for training and evaluation.\n- The neural network architecture follows an AlphaZero-style dual-head design with residual blocks.\n- Move encoding maps each legal move to a unique index for policy output.\n- MCTS is used to guide search and improve move selection through exploration and exploitation trade-offs.\n- Self-play is used iteratively to generate new data and refine the model.\n- In early iterations, the chess model played random moves but gradually developed structured strategies.\n- After 10 training iterations, the chess model demonstrated coherent opening, mid-game, and endgame strategies.\n- For Xiangqi, average game length decreased and reward improved over three training rounds.\n- The pretrained Xiangqi model showed basic attack-defense understanding compared to a randomly initialized model.\n- In the final Xiangqi game, the model exhibited rapid piece development but lacked advanced multi-phase planning.\n- Despite not reaching expert level, the model avoids major strategic errors and demonstrates consistent improvement.\n- The work reaffirms the effectiveness of DRL + MCTS methodology in complex, culturally significant games like Xiangqi.",
    "date": "2025-06-24",
    "paper": "Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search \nJunyu Hu​\nJH4930@COLUMBIA.EDU \nDepartment of Electrical Engineering, The Fu Foundation School of Engineering and Applied Science, Columbia \nUniversity, New York, NY 10027, USA \n \nJinsong Liu​\nJL6850@COLUMBIA.EDU \nDepartment of Electrical Engineering, The Fu Foundation School of Engineering and Applied Science, Columbia \nUniversity, New York, NY 10027, USA \n \nBerk Yilmaz​\nBY2385@COLUMBIA.EDU \nDepartment of Electrical Engineering, The Fu Foundation School of Engineering and Applied Science, Columbia \nUniversity, New York, NY 10027, USA \n \nAbstract \nThis paper presents a Deep Reinforcement Learning \n(DRL) system for Xiangqi (Chinese Chess) that integrates \nneural networks with Monte Carlo Tree Search (MCTS) \nto enable strategic self-play and self-improvement. \nAddressing \nthe \nunderexplored \ncomplexity \nof \nXiangqi—including its unique board layout, piece \nmovement constraints, and victory conditions—our \napproach combines policy-value networks with MCTS to \nsimulate \nmove \nconsequences \nand \nrefine \ndecision-making.By overcoming challenges such as \nXiangqi’s high branching factor and asymmetrical piece \ndynamics, our work advances AI capabilities in culturally \nsignificant strategy games while providing insights for \nadapting DRL-MCTS frameworks to domain-specific rule \nsystems. Our project github link can be found at: \nhttps://github.com/JL6850/E6892-FinalProject \n1. ​ Introduction \nOur research introduces a Deep Reinforcement \nLearning system that learns to play Xiangqi(Chinese \nChess) through strategic self-play and self-improvement. \nThe approach combines two components: neural networks \nthat guide decision-making and evaluate positions, paired \nwith Monte Carlo Tree Search(MCTS) that simulates \ndifferent moves and their consequences. Through repeated \ngameplay against itself, the system gradually develops \neffective strategies while accounting for Xiangqi’s \ndistinctive \nrules \nand \ngameplay \ndynamics. \nOur \nmethodology \ndraws \ninspiration \nfrom \nmodern \ngame-playing systems like AlphaGo, but specifically \naddresses the technical challenges posed by Xiangqi’s \nunique pieces, board layout, and victory conditions.  \n1.1 ​\nBackground information \nXiangqi is a longstanding traditional Chinese \nstrategy board game that has existed for centuries and has \nmillions of players, primarily in East and Southeast Asia. \nSimilar to Western Chess, Xiangqi is a two-player game \nof perfect information, but there are many things that are \ndifferent in Xiangqi which contribute to Xiangqi being a \nmore complex variant. Xiangqi is played on a 9×10 board \nwith 90 positions. the 8×8 chess board with 64 squares, \nresulting in a larger and more irregular state space. In \naddition, pieces in Xiangqi move in asymmetrical ways: \nAdvisors and Elephants, for example, can only occupy \nspecific areas such as the palace or a designated side of \nthe river. The Cannon introduces its own capturing \nmechanism, and we have the added stipulation that the \ntwo Generals cannot face each other which imposes \ntactical constraints. Factors such as these lead to a greater \naverage branching factor and longer average game length \nin Xiangqi. An overview of complexity indicators \ncomparing Xiangqi and Chess can be found in the table \nbelow: \nMETRIC \nXIANGQI  \nCHESS \nBOARD SIZE \n9 × 10  \n8 × 8  \nSTATE SPACE \n~10⁴⁰ \n~10⁴⁷ \nGAME TREE \n~10¹⁵⁰ \n~10¹²³ \nBRANCHING FACTOR \n~38 \n~35 \nGAME LENGTH \n~95 \n~70 \nTable 1: overview of complexity indicators  \n2. ​ Related Work \n2.1 ​\nOverview \n \nThe combination of DRL and MCTS, first \nsuccessfully implemented in AlphaGo and AlphaZero, \nhas completely changed the way AI makes decisions in \nchess games. This type of method combines the judgment \nof the situation by the neural network with the search \nalgorithm, and continuously improves the accuracy of \nmove prediction and the ability to judge the pros and cons \nof the situation through self-play. Although originally \ndesigned for Go and chess, its success has also led to \nmany applications expanding to real-time strategy games \nsuch as StarCraft II, as well as broader complex systems \nsuch as robot control and resource scheduling. \nIn contrast, Xiangqi AI relies more on \nrule-driven methods, such as alpha-beta pruning and \nhuman-designed evaluation functions. These systems \nperform well in efficiency, but lack adaptability when \nfaced with complex or novel situations. In recent years, \nsome work has introduced neural networks to predict \nmoves or evaluate situations, but most of them only focus \non a certain link and lack the overall planning and \nexploration capabilities brought by MCTS. Although the \nDRL model performs well in learning local move \npatterns, due to the many branches and long tactical \nchains of chess, it is difficult to achieve effective \nlong-term planning by DRL alone; and pure MCTS is \neasy to get stuck in the huge search space, and the \ncomputational overhead is extremely high. Therefore, in \norder to achieve powerful AI in chess, it is necessary to \ncombine the advantages of DRL and MCTS more closely. \n3. ​ Methods \n3.1 ​\nAlpha–Zero Model For International Chess \nBefore \nimplementing the Xiangqi version \nimplementation we wanted to try the classical Alpha-Zero \nchess model to get familiar with the environment and data \nloading setup. The biggest difference between this \nclassical chess implementation and our Xiangqi player is \nthat in the Xiangqi environment we utilize a training set to \ntrain our neural network model to get a more advanced \nversion of our player. In order to get a comprehensive \nversion \nwe \nfirst \ndefined the international chess \nenvironment, then evaluated a neural network policy with \nthe values. We modified the move encoder and utilized \nthe Monte Carlo Tree Search algorithm for the decision \nmaking process. After that we evaluated our model by \nself-playing by itself and with the data it generated we \nimproved our model.  \n3.1.1 Setting up the Game Environment \n​\nTo set up the game environment we first defined \na ChessState where we utilized the python-chess library. \nPython-chess library enabled us to get legal moves, \nsimulate the moves, and determine the chess game status \nwhether it is a win situation. In this environment, we \nconvert the board sensor to tensor for our neural network \narchitecture. \nUnlike \nthe Xiangqi environment, in \ninternational ches we define 13 states, which 12 comes \nfrom the pieces and 1 is extra to determine for whose turn \nit is. \n3.1.2 Representation of Network Channels  \nWe utilized the following channel structure for \nour international chess implementation \nCHANNEL \nREPRESENTATION  \n0-5 \nWhite pieces  \n6-11 \nBlack pieces \n12 \nTurn indicator \n13 \nWhite kingside castling rights \n14 \nWhite queenside has the castling \n15 \nBlack kingside castling rights \n16 \nBlack queenside castling rights \n17 \nEn passant square \n18 \nCheck indicator \nTable 2: Chess implementation channel structure \n3.1.3 Neural Network: Policy + Value Network \n​\nThen we defined the neural network and policy \nfor our international chess implementation. For our neural \nnetwork architecture we followed the Alpha-Zero style \nstructure. \nAlpha-Zero \nimplementation \nconsists \nof \n“dual-head” convolutional network implementation where \nthe network is divided into three pieces. The shared body \nhas an input shape of [batch_size, 19, 8, 8]  where each of \nthe 19 “planes” encodes one bit‐plane of the board. Then \nwe increase the hidden dimension size from 19 to 256 by \nintroducing a simple 3x3 convolutional layer followed up \nby batch normalization and ReLU to normalize and \nstandardize \nthe \nweights. \nThe \noutput \nfrom \nthe \nconvolutional layer gives us the raw embeddings into high \ndimensional feature space (256).  \nAfter the 3x3 \nconvolutional \nblock \nwe \nintroduce \nresidual block \narchitecture. Although the original paper states that they \nare stacking 19–39 residual blocks because of the limited \ncomputational power we finalized with 10 residual blocks \nto lighten the environment. Each of our residual blocks \nconsist of a convolutional layer followed by batch \nnormalization and ReLU, after the first convolutional \nblock we introduce another convolutional layer and batch \nnormalization. After these steps, we introduce the original \ninput one more time as a residual connection and we \nperform ReLU on the sum. The skipped connections \nhelped us prevent the vanishing gradients problem while \nwe are training.  \n3.1.4 Policy Head \n \n \n​\nOur policy head starts working with the \ninformation it received from the shared body which \nincludes 256 channel-feature maps. After receiving the \nweight information we utilize another 3x3 convolutional \nlayer without decreasing or increasing the hidden layer \ndimensions. After the layer, we flatten the 256×8×8 \nfeature maps into one long vector because the final step in \nthe policy head is a fully-connected (linear) layer, and \nthose layers expect a 1D input. Although the original \npaper introduces 16384  dimensional vectors, due to GPU \nand memory constraints we have hardcoded a dimension \nsize of 42. Later, we used these dimensions and applied \nSoftmax to get the next move probabilities.  \n3.1.4 Value Head \n​\nOur value head consists of one 1x1 convolutional \nlayer which has an output of 1, by implementing this \nstrategy we are reducing the hidden channels of our \nstructure to 1 then we follow up with batch normalization \nand and ReLU and afterwards flatten the [8,8] dimensions \nto get a 64 dimensional vector. After converting the \ndimension size to 64, we expand it with a linear layer to \nits original form of 256 and then we apply a final layer to \nreduce the hidden dimensions to 1 again. Our value head \nconsists of tanh function so we can get the probability \nvalues in range [-1,1], representing the loss with -1 and \nwin with 1.  \n3.1.5 Move Encoder \n​\nIn order to continue with the Alpha-Zero \nimplementation we need to introduce bi-direcitonal \nmapping to our environment. Bi-directional mapping is \nrequired in our project because the network doesn’t output \na list of moves. It gives out a fixed-length vector of raw \nscores or probabilities whose positions correspond to \n“move indices.” The move encoder tells us which vector \nslot means the corresponding piece. Move encoder helps \nus map each legal move to a unique index number so that \nwe can check the rules immediately. In order to achieve \nthis, we create a one-hot-encoding output vector where we \ncan get the probabilities. Our move encoder also checks \nand handles the promotions, castling and some advanced \nrules like en-passant. In move encoder, we utilize three \nmain functions, encode_move(move) returns an index \nvalue \nfor \neach \nmove \nas \nwe \nstated \nbefore, \ndecode_move(index) does the exact opposite thing and \nreturns us a state from the given index, we also utilize \npolicy_to_moves() function to filter the neural network \noutputs for legal moves.  \n3.1.6 MCTS (Monte Carlo Tree Search) \n​\nAs also used in the main implementation of \nAlpha-Zero model, Monte Carlo Search Tree algorithm is \nused to generate a search strategy. The moves are selected \nby not only getting the highest-probability actions every \ntime, we further implement this algorithm by adding \nselection and giving room for exploration. The policy \nrefers to which moves are going to be explored and value \non the other hand refers to the estimation of how good a \nleaf state is in the current state. We first begin our \nimplementation by maximizing the Upper Confidence \nBound (UCB)  \n \n𝑈𝐶𝐵(𝑠, 𝑎) =  𝑄(𝑠, 𝑎) +  𝑐 *  𝑃(𝑠, 𝑎) *  \n𝑁(𝑠)\n1+𝑁(𝑠.𝑎)\nwhere, Q(s,a) = average value of taking action a at state \ns, P(s,a) is the  \nprior probability from the policy \nnetwork, N(s) is the total visit count to node s, N(s,a) is \nhow many times child a was visited and c is the \nexploration constant which we change during training to \nevaluate the scores. At the end of each iteration we pick \nthe child note with the highest upper bound confidence \nlevel. And once we complete the iteration and receive a \nleaf we then update the policy and value from the neural \nnetwork and for each legal move we add a child node to \nthat tree with prior initializations N(s,a)=0 , Q(s,a)=0 and \nP(s,a)= p which we got from our neural network. The \ndifference between traditional Monte Carlo Tree Search \nand Alpha-Zero implementation is that in standard MCTS \nwe continue to simulate the entire game but Alpha-Zero \nmodel directly gets the value(v)* from the neural network. \n(*v = ∈[−1,1] ). After we receive our value we start the \nbackpropagation. For each node in each iteration, we first \nupdate the visit count: N(s,a) ← N(s,a) + 1, total value: \nW(s,a) ← W(s,a) + v and mean value Q(s,a)←W(s,a) / \nN(s,a) and each time while we do the updates we flip the \nsign of our value (v) because of the turn rule. After we \ninitialize the structure we can simulate it and estimate a \npolicy with the equation below: \n \nπ(𝑎) =  \n𝑁(𝑠,𝑎)\n𝑏\n∑𝑁(𝑠,𝑏)\nwhich we get the improved policy. \n3.1.7 Self-Play \n​\nLike the Alpha-Zero paper in order to train our \ndata we utilized a self-playing algorithm to collect and \nthen update our model. In this section, the model has \nplayed games against itself and generated data. For each \nmove we first used Monte Carlo Search Tree to generate a \npolicy distribution then we stored that policy distribution \nin dimensions of (state, policy, value_placeholder), then \nby looking at the stored policy we made a move. If our \nis_end_game function starts we assign a value to each \nposition in the table based on the final outcome of the \ngame.  \n \n \n \n \n3.1.8 Conclusion of Alpha-Zero for Chess \n​\nThe \nAlpha-Zero \nfor \ninternational \nchess \nimplementation gives us some important decision making \nideas to improve the system and convert it to the Xiangqi \nenvironment. We managed to train the Alpha-Zero model \nusing A100’s in 4 days. As the model’s complexity \nincreased the games started to last longer, In first \niterations games were completed in 30 minutes whereas in \nIteration 10 each game lasted for an hour. Due to limited \nconstraints we stopped training at iteration 10, but our \nmodel shows clear improvement and can be further \nimproved in future iterations.  \n3.2 ​\nModel for Xiangqi \n​\nAs we progressed in Alpha-Zero algorithm, in \nthis section we will discuss the changes in the \nenvironment and improvement we have made during the \ntraining. Alpha-Zero algorithm does not utilize any \npre-training steps and collects the data by self-play. \nHowever, as we have seen in the results the early models \nlack any technical abilities and strategies. In order to \nprevent this, we collected a dataset with a script \ncontaining previous Xiangqi games, which will be \ndiscussed in more detail in section 3.2.2. \n 3.2.1 Introduction to Xiangqi Rules \nFirst, we introduce the basic piece-movement \nrules of Xiangqi to establish a clear understanding of what \nconstitutes a legal move (see Table 3). Next, Xiangqi \nincorporates a number of special movement constraints \nsuch as the elephant’s inability to cross the river, the \ncannon’s requirement to leap over exactly one intervening \npiece when capturing, palace-bound advisors and \ngenerals, and the prohibition against generals facing each \nother directly—which serve to evoke aspects of ancient \nbattlefield tactics and logistics. Finally, winning in \nXiangqi, like in international chess, is achieved by \ncapturing the opposing general . However, at high levels \nof play a substantial proportion of games end in draws, \nowing largely to Xiangqi’s unique draw mechanisms such \nas perpetual check limitations, drawn-position rules when \nneither side can make progress, and repetition-based \nadjudication—which collectively reduce the likelihood of \ndecisive outcomes. \nNAME \nMOVEMENT \nGENERAL \nMoves one square  \nADVISOR \nMoves one square diagonally \nELEPHANT \nMoves two squares diagonally \nHORSE \nMoves two square orthogonally then one \nsquare diagonally (leg block applies) \nROOK \nMoves any number of squares \northogonally \nCANNON \nMoves any number of squares \northogonally \nSOLDIER \nMoves one square forward \nTable 3: Xiangqi’s legal move \n3.2.2 Xiangqi Dataset Setup \n​\nThere are a limited number of libraries that \ncontain the Xiangqi environment. However, throughout \nour project we utilized OpenAI’s Xiangqi environment. \nWe created a script that creates a game environment using \nthe OpenAI Gym framework and gym_xiangqi package. \nBy using this environment we could access the \nobservation space, action space, the step functions which \nwe execute to make moves and return new state, action, \nreward and game status information and reset functions \nwhich we used to start a new game. Our dataset adopts the \nPortable Game Notation (PGN) format. The [FEN] tag is \nused to encode the initial board configuration, while \n[Result] and [Format \"ICCS\"] are retained to capture the \ngame outcome and move history, there were a total of \n141,484 games recorded with 11,683,480 total moves. \nAmong these games red won 53,787 times whereas blacks \nwon 39,884 times and 47,843 of the games were \nconcluded as draws.  \n3.2.3 Supervised Pre-training from Historical Games \nIn the original AlphaZero framework, training \nstarts completely from scratch: both the policy and value \nnetworks are randomly initialized and immediately paired \nwith MCTS for self-play. At this early stage, the untrained \nnetworks produce near‐uniform move priors, and MCTS \nadds heavy Dirichlet noise to promote exploration, so the \nfirst several games effectively resemble random play. \nConverging from such a cold start typically requires tens \nor even hundreds of thousands of high‐quality self‐play \ngames, demanding computational resources and training \ntime far beyond our means. \nTo mitigate this, we introduce a supervised pre \ntraining phase before the self‐play module. By leveraging \nexisting human game records and using behavior cloning, \nwe train an initial policy–value network that imitates \nexpert moves. This yields a substantially more informed \nstarting strategy than random initialization.  \n3.2.3.1 Preparation Before Training \nDuring the feature‐encoding phase, we map the \nXiangqi board and side to move into a multi‐channel \nbinary tensor of shape (10×9×15): for each non‐empty \nsquare, we set the corresponding piece’s channel to 1 (and \nall other piece channels to 0), thereby assigning each of \nthe seven Red piece types and seven Black piece types to \nthe first 14 channels; the fifteenth channel is uniformly set \n \n \nto indicate which side is to move (Red or Black) across \nthe entire board. This representation both preserves the \nspatial distribution of pieces and explicitly encodes \nmove‐ownership. \nFor action encoding, since the Xiangqi board has \n10×9=90 squares and each move is essentially “from one \nsquare to another,” there are 90×90=8100 possible \nsource–destination combinations. We compute a unique \naction index in [0,8099] by taking (source_index×90 + \ndestination_index). As a result, the policy network needs \nonly output a probability vector of length 8100 to cover \nevery \npossible \nmove, \ncreating \na \none‐to‐one \ncorrespondence between network outputs and legal \nactions. \n3.2.3.2 Model Construction and Training \nIn the model construction phase, we based our \ndesign on the AlphaZero–style dual‐head convolutional \nneural network, dividing the workflow from input to \noutput into three major components: \n(i) Input and Feature Extraction Backbone​\n ​\nThe network accepts a tensor of shape (10, 9, 15) \nas input. First, a 3×3 convolutional layer with 256 filters \nis applied, followed by batch normalization and a ReLU \nactivation, which maps the sparse binary inputs into a \nhigh-dimensional continuous feature space to capture \ninitial spatial correlations. We then stack several residual \nblocks, each composed of two 3×3 convolutions + batch \nnormalization + ReLU. A skip connection adds the \nblock’s input back into its output before a final ReLU. \n(ii) Policy Head​\n ​\nFrom the shared backbone, one branch first \napplies a 1×1 convolution to reduce channel \ndimensionality, then flattens the resulting feature maps \ninto a 1D vector. A final fully connected layer projects \nthis vector into 8,100 output nodes, and a Softmax \nactivation converts these into a probability distribution \nover all legal moves. Each output index corresponds to \none “source‐square × destination‐square” combination, \nso the network’s output directly represents the likelihood \nof selecting each possible move. \n(iii) Value Head​\n ​\nThe other branch similarly begins with a 1×1 \nconvolution to condense channels, then flattens the \nfeature maps into a 64-dimensional vector. This vector \npasses through a fully connected layer that expands it to \n256 dimensions, and finally through a single‐neuron \noutput layer with a tanh activation. The resulting scalar \nlies in [−1, 1], representing the estimated game outcome \nfrom the current side-to-move perspective (−1 = certain \nloss, +1 = certain win). \nDuring training, we jointly optimize the policy \nand value heads by combining their loss functions: sparse \ncategorical cross-entropy for the policy head and mean \nsquared error for the value head, weighted equally (1:1). \nWe use the Adam optimizer for its adaptive learning-rate \nproperties, which enable rapid initial convergence and \nstable training. We also monitor policy accuracy and \nvalue MAE on a held-out validation set to assess \nmove‐prediction quality and value‐estimate precision. \nThe result is a model with a solid foundational move \npolicy (e.g., it knows how to capture and check), which \nwe then employ in the subsequent self‐play stage. \n3.2.4 Self-playing in Xiangqi \n    \nAfter completing the supervised pretraining for \nXiangqi, we employed the same self-play methodology \nused in international chess: integrating the pretrained \npolicy–value network with MCTS to generate self-play \ngames, iteratively collecting new state–policy–value \ntriplets, and retraining the network to further improve its \nperformance. \n4. ​ Results \n4 .1 Evaluation of International Chess Alpha-Zero Model \n​\nIn this part we analyze games and go through \neach training and changes in our model.  \n \nFigure 1. Example of self-play data \nThe figure above shows the moves in each state, \nas stated before the policy network has not been trained \nand it is first randomly initialized. MCTS priors are based \non noisy policy outputs, in this period there is no value \nnetwork guidance yet. Also, there is no information about \nthe past games since this is the first training iteration. The \ngame lasted for 76 moves which means our model is \nexploring the options on the board, the game is very \nlow-level, there are no tactics involved , no evident \npursuit of checkmate or material gain, and moves seem \narbitrary. \nAt \nmove \n150, \nthe \nfinal \nboard \nis \n3k1r1q/p2nb3/1P6/7p/P3RP1P/Q2P1KPb/8/n2rBR2, \ndouble queens, imbalanced pawns, centralized minor \npieces with no plan which shows the random play.  \n \n \nHowever as we progress in the training as shown \nin the figure below \n \nFigure 2. Training feedback of the first iteration \nWe got a loss of 0.5336, which is the \ncombination of policy loss (0.5336) and value loss (0.0). \nAfter the first iteration our game shows some \nimprovements. Based on looking at the FEN boards, even \nif it’s after the first iteration we can see the correct pawn \nstructures from both sides. We can see our model started \nto use other pieces such as bishops to take control of the \nboard which can be seen in movements 3P1P2, PPP1 \nPNPP which indicates there is better piece coordination. \nHowever, the model still does blunders such as in move \n30 the player sets its black queen to g2 which is a clear \nblunder and technical error. In this game black shows \nmore aggression and at the end wins the game.  \n \nFigure 3. Training feedback of the second iteration \nThe figure above shows 2 games after iteration 2. \nAfter iteration 2 our loss drops to 0.5495 and value loss \nstarts to increase which indicates that the value head is \ntrying to evaluate the board states. Our policy loss is \nstable and consistent with iteration 0 and iteration 1. First \ngame lasts for 80 moves which at the end black wins \nagain. In early stages of the game the model still makes \nsome randomized movements and starts to explore the \nboard, However, around the middle game both players \nshow central play coordinations and tactics such as \nputting their rooks and queens to the open files. This \ngame shows the model is trying to evaluate a strategy \nbased on the prior movements. In the second game, we \nobserve something different, in late early game players \ncastle and for the first time the model starts to enter the \nlate-game stages which can be seen looking at this FEN \nline \"3n3/2r5/k3KP/p4P2/P3n2/8/8/7R” which shows \npiece counts and pawn play.  \n \nFigure 4. Training feedback of the 10th iteration \nThe figure above has been captured after \niteration 10. Our total loss has dropped to 0.0594 and our \npolicy loss is stable and low. Our value loss is small \nwhich indicates either the model has really high \nconfidence when making predictions or it overfits. The \ngame after iteration 10, unlike the games we have seen, \ndoes not include any random play. Both players are aware \nof material preservation, pawn advancement, and \nendgame ideas. It maintains pawn islands, leverages rooks \nin open files, and protects the king with all signs of a \nlearning-based strategy. Looking at the FEN structure, in \nthe opening both players prioritize the development of \ntheir pieces: knights and bishops are put in the center of \nthe board and we can see early pawn movements on both \nsides. In mid-game we see trading of the major pieces \nwhich is a sign that players have the knowledge of \nmid-game and pursuit to the endgame. The endgame \nmodel makes reasonable pawn pushes, maintains kingside \ndefenses, no aimless wandering.  \n4.2  Evaluation of Xiangqi Alpha-Zero Model \n \nFigure 5. Evaluation result of Xiangqi \nOver three rounds of training, we can already \nobserve clear signs of learning: the average length of a \ngame decreased from 92.4 to 60.8 and then rose slightly \nto 71.2, which means that the model is beginning to make \nmore memorized moves while still searching. Also, the \naverage reward transformed from negative in the first \niteration (–0.20) to positive in the third iteration (+0.40), \nwhich indicates that the self-play outcome of the model is \n \n \nimproving since it starts to favor more efficient strategies. \nOn the training side, value and policy losses always \ndecreased step by step, policy loss being the dominant \nconstituent—this is in accord with typical AlphaZero \nbehavior where policy network drives learning and the \nvalue head acts to stabilize. But in testing, the model \nwould always draw every match (0 wins, 0 losses, 5 \ndraws a round). Overall, the model shows stable \nconvergence and improving self-play behavior.  \n4.2.1  Evaluation of Supervised Pre-training Model  \nIn this section, we present excerpts from the \nopening moves of the first self-play game conducted with \nthe pretrained model and, for comparison, one played \nwith a randomly initialized model. These examples \ndemonstrate how the pretrained policy influences early \nmove selection. \n \nFigure 6. Movement without pretrained policy \nThe figure above illustrates the model’s move \nchoices when no pretrained model is used. Red arrows \nindicate Red’s moves, and black arrows indicate Black’s. \nAs you can see, both sides are merely making random \nlegal moves without any discernible offensive or \ndefensive intent. \n \nFigure 7. Movement with pretrained policy  \nBy contrast, the figure above shows a simple \nattack–defense sequence that emerged after applying the \npretrained model. Red’s cannon (denoted “C”) delivers a \ncheck by first capturing Black’s knight to the right, \nforcing the Black general into check. To resolve this, \nBlack captures Red’s cannon with a chariot, thereby \nsaving their general. This demonstrates that the model has \nlearned a basic ability to judge straightforward attack and \ndefense sequences. However, it is worth noting that, in \nhuman play, such a piece‐for‐piece trade (“duizi”) is \ngenerally unfavorable for Red—indicating that the model \nstill lacks a global, high‐level positional evaluation \ncapability. This aspect will need to be further honed \nthrough continued self‐play training. \n4.2.2 Evaluation of Final Model  \n       ​\nAfter pre-training and self-playing part we load \nour model to play one last game to observe the success of \nthe model. The final game consisted of the following \nmoves f3-f10 - f8-f5 - b3-b4 -b10-c8 - f10-h10 - i10-h10 - \nb4-b2 - h10-h2 -i1-i3 - h2-e2. From the very early moves, \nthe model shows a sense of rapid piece development, with \na focus on developing chariots (Rooks) and cannon which \nare both crucial in the Xiangqi game. Red begins with \nf3-f10, a cannon hop to pressurize Black's mid-rank, and \nBlack immediately reacts to this aggression with a cannon \nto f5. These are common aggressive opening strategies in \nreal games, suggesting the policy has picked up helpful \nearly-game patterns through self-play. Red rapidly \ndeployed both chariots: f10-h10 and i1-i3, and Black \nmirrors and then swung h10-h2 to h2-e2, effectively \nswapping board space for rapid placement. But the one \nwarning sign that catches the eye is Black's chariot retreat \nto the second rank (h10-h2) followed by diagonal move \nh2-e2), which gives Red a lot of initiative and does not \ndirectly challenge Red's structure. While Red's play seems \nmore proactive, the b4-b2 (advancing the cannon to the \nback even more) maneuver is passive and may be \nsymptomatic of state evaluation confusion or overfitting \nto draw-oriented positions. Similarly, Black's knight \nadvance to c8 is standard but not accompanied by much \ncentralization or support. Additionally, neither of them \nhas advanced their pawns  forward significantly, nor \nadvanced the knights to maximum capacity which is a \nsign that the model lacks multi-phase planning.  \n5. ​ Conclusion \n   \nIn spite of playing under limited computational \nresources and time constraints, we were successful in \nimplementing an AlphaZero-like reinforcement learning \nsystem for Xiangqi and for international chess. We first \napplied supervised pretraining on human game records to \nbootstrap a strong initial policy–value network, and then \nintegrated it with Monte Carlo Tree Search for self-play. \nOur system learned to play legal, strategic games through \nthis two-stage approach and demonstrated consistent \nimprovement in move selection, positional awareness, and \ngame outcomes across rounds. While it is not yet \nachieving expert-level play, the model avoids major \nstrategic errors, mobilizes key pieces early, and exhibits \nclear learning patterns for foundational principles. Given \nthe unique complexity of Xiangqi and the challenge of \nbuilding a customized environment from scratch, our \nresults reaffirm the effectiveness of this methodology. \n6. ​ Contribution of Team Members \nTEAM MEMBERS \nCONTRIBUTION \nMAIN WORK \n \n \nBerk Yilmaz \n1/3 \nAlphazero chess and \nXiangqi \nimplementation, \nreport writing \nJinsong Liu \n1/3 \ncreating the dataset and \nimprove MCTS, \nreport writing \nJunyu Hu \n1/3 \nsupervised pre-training \nmodel implement, \nReport Writing \n7. ​ References \n1. Fisher, D. H. (1989). Noise-tolerant conceptual \nclustering. Proceedings of the Eleventh International \nJoint Conference on Artificial Intelligence (pp. 825-830). \nSan Francisco: Morgan Kaufmann.  \n2.  Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, \nL., van den Driessche, G., ... & Hassabis, D. (2016). \nMastering the game of Go with deep neural networks and \ntree search. Proceedings of the National Academy of \nSciences, 529, 484–489. \n3. Świechowski, M., Godlewski, K., Sawicki, B., & \nMańdziuk, J. (2022). Monte Carlo tree search: A review \nof recent modifications and applications. arXiv preprint \narXiv:2103.04931.  \n \n \n \n",
    "content": "# Paper Analysis: \"Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search\"\n\n---\n\n## 1. Core Content and Main Contributions\n\n### Core Content:\nThis paper proposes a **Chinese Chess (Xiangqi) AI system** based on the integration of **Deep Reinforcement Learning (DRL)** and **Monte Carlo Tree Search (MCTS)**. Through self-play and policy optimization, the system gradually learns the complex rules and tactics of Xiangqi. Its key technical components include:\n\n- Using a neural network as a **Policy-Value Network** to evaluate board states and predict moves;\n- Introducing **MCTS for strategy search**, improving decision-making quality;\n- Applying **supervised pre-training using historical human game data** in early training stages to accelerate convergence;\n- Customizing the **AlphaZero framework** to accommodate Xiangqi-specific rules such as the “general-facing” rule and cannon capturing mechanics.\n\n### Main Contributions:\n1. **First implementation of an AlphaZero-style DRL+MCTS framework in Xiangqi**, offering new insights into AI research for traditional strategy games.\n2. **Combines supervised pre-training with self-play training**, effectively addressing the high computational cost of training from scratch.\n3. **Builds a complete Xiangqi AI training environment and dataset**, including over 140,000 historical game records, laying the foundation for future research.\n4. **Validates the feasibility of DRL-MCTS methods in games with high branching factors and asymmetric rules**, expanding the scope of this technology.\n\n---\n\n## 2. Innovations and Breakthroughs\n\n### Innovations:\n| Dimension | Description |\n|----------|-------------|\n| **Algorithm Innovation** | Adapts the AlphaZero-style DRL+MCTS architecture to Xiangqi, overcoming challenges posed by its high state space complexity and asymmetric rules. |\n| **Training Strategy Innovation** | Introduces a supervised pre-training phase that leverages existing historical game records to improve initial policy quality, significantly reducing cold-start time. |\n| **Representation Learning Innovation** | Designs a multi-channel binary tensor input representation (10×9×15) suitable for Xiangqi, preserving board structure and encoding move rights. |\n| **Action Encoding Innovation** | Uses a \"source position × target position\" index mapping to represent 8,100 possible actions, ensuring one-to-one correspondence between policy outputs and legal moves. |\n\n### Breakthroughs:\n- **Achieves high-quality training workflows under limited computational resources**, demonstrating that effective strategic AI systems can be built without massive-scale computing power.\n- **Proves the transferability of the DRL-MCTS framework to non-chess-like games**, providing a model for AI modeling in other complex rule-based systems.\n- **Advances Xiangqi AI research**, shifting from traditional rule-driven approaches to data-driven intelligent decision-making systems.\n\n---\n\n## 3. Entrepreneurial Project Suggestions\n\nBased on this paper, several commercially viable startup directions can be explored:\n\n### ① **AI-Assisted Chinese Chess Learning Platform**\n- **Product Positioning**: A personalized learning tool for beginners or advanced players.\n- **Key Features**:\n  - Real-time AI analysis of user moves, highlighting strengths/weaknesses and suggesting improvements;\n  - Simulates opponents of different skill levels for practice;\n  - Analyzes famous historical games with AI commentary;\n  - Available on both mobile apps and web platforms.\n- **Business Model**: SaaS subscription + VIP courses + collaboration with chess tournaments.\n\n### ② **Xiangqi AI Competitive Gaming Platform**\n- **Product Positioning**: An online competitive ecosystem for AI vs. human and AI vs. AI matches.\n- **Key Features**:\n  - Multiple AI difficulty levels for users to challenge;\n  - Real-time AI commentary during matches;\n  - Ranked matches, point-based leagues, and human-machine duels;\n  - Data analytics and performance reports.\n- **Business Model**: Advertising revenue + tournament sponsorships + AI model leasing.\n\n### ③ **Customizable Game AI Development Platform**\n- **Product Positioning**: An extensible DRL+MCTS development toolkit for enterprises or developers.\n- **Key Features**:\n  - Provides full code and training pipeline for Xiangqi version of AlphaZero;\n  - Supports rapid adaptation to other strategy games (e.g., Go, military chess, tactical board games);\n  - Integrates visualization tools for training monitoring and parameter tuning;\n  - Offers cloud deployment support.\n- **Business Model**: Enterprise licensing + API usage fees + custom development services.\n\n### ④ **Traditional Culture + AI Digital Collectibles / Metaverse Applications**\n- **Product Positioning**: Combines Xiangqi culture with emerging technologies like NFTs and virtual reality.\n- **Key Features**:\n  - AI-generated \"Grandmaster\" avatars for virtual matches;\n  - Limited edition AI-created game NFTs;\n  - Virtual world centered around Xiangqi culture, hosting events and exhibitions.\n- **Business Model**: NFT sales + metaverse event tickets + IP licensing.\n\n---\n\n> 📌 **Summary**:  \nThis paper not only makes significant academic progress in Xiangqi AI but also demonstrates broad potential for real-world applications. By transforming its findings into products across education, entertainment, and developer platforms, it can bridge traditional culture with modern technology, creating new markets with both cultural and commercial value.",
    "github": "",
    "hf": ""
}