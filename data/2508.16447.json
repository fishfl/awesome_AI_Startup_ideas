{
    "id": "2508.16447",
    "title": "Boardwalk: Towards a Framework for Creating Board Games with LLMs",
    "summary": "This research explored whether advanced large language models could generate board game code from natural language rule descriptions by having three of the most advanced large language models produce code for 12 board games, and analyzed their success rates and common errors.",
    "abstract": "Implementing board games in code can be a time-consuming task. However, Large Language Models (LLMs) have been proven effective at generating code for domain-specific tasks with simple contextual information. We aim to investigate whether LLMs can implement digital versions of board games from rules described in natural language. This would be a step towards an LLM-assisted framework for quick board game code generation. We expect to determine the main challenges for LLMs to implement the board games, and how different approaches and models compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek and ChatGPT) with coding a selection of 12 popular and obscure games in free-form and within Boardwalk, our proposed General Game Playing API. We anonymize the games and components to avoid evoking pre-trained LLM knowledge. The implementations are tested for playability and rule compliance. We evaluate success rate and common errors across LLMs and game popularity. Our approach proves viable, with the best performing model, Claude 3.7 Sonnet, yielding 55.6\\% of games without any errors. While compliance with the API increases error frequency, the severity of errors is more significantly dependent on the LLM. We outline future steps for creating a framework to integrate this process, making the elaboration of board games more accessible.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Álvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares",
    "subjects": [
        "Machine Learning (cs.LG)"
    ],
    "comments": "Comments:Accepted at SBGames 2025",
    "keypoint": "- LLMs are largely capable of implementing board games from rule descriptions in natural language.\n- Claude 3.7 Sonnet was the best-performing model, producing error-free code in 55.6% of cases.\n- Implementations using the Boardwalk API had higher error frequency compared to free-form implementations.\n- Error severity was more significantly dependent on the LLM than on API compliance.\n- Obscure games presented greater implementation challenges compared to popular games.\n- Nine Men’s Morris, Ard Ri, Kharebga, and Unashogi were the hardest games to implement.\n- Tic-Tac-Toe, Peg Solitaire, Reversi, and Domineering were the easiest games to implement.\n- Common errors included incorrect move validation, game ending conditions, and turn order.\n- The Boardwalk API simplifies game implementation by using Python instead of a Game Description Language.\n- The API enables easier integration with AI and human players.\n- Anonymization of games was effective in preventing reliance on pre-trained knowledge.\n- Zero-shot inference was used, indicating potential improvements with in-context or few-shot learning.\n- Future work includes expanding the API to support more complex game types like hexagonal or graph-based boards.\n- Iterative feedback could improve error correction and move toward a full AI board game creator.",
    "date": "2025-08-27",
    "paper": "XXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nBoardwalk: Towards a Framework for Creating Board Games\nwith LLMs\nÁlvaro Guglielmin Becker1, Gabriel Bauer de Oliveira1,\nLana Bertoldo Rossato1, Anderson Rocha Tavares1\n1Instituto de Informática – Universidade Federal do Rio Grande do Sul (UFRGS)\nCaixa Postal 15064 – Porto Alegre – RS – Brasil\n{agbecker,gabriel.bauer,lbrossato,artavares}@inf.ufrgs.br\nAbstract. Introduction: Implementing board games in code can be a time-\nconsuming task. However, Large Language Models (LLMs) have been proven\neffective at generating code for domain-specific tasks with simple contextual\ninformation. Objective: We aim to investigate whether LLMs can implement\ndigital versions of board games from rules described in natural language. This\nwould be a step towards an LLM-assisted framework for quick board game code\ngeneration. We expect to determine the main challenges for LLMs to implement\nthe board games, and how different approaches and models compare to one\nanother. Methodology: We task three state-of-the-art LLMs (Claude, DeepSeek\nand ChatGPT) with coding a selection of 12 popular and obscure games in\nfree-form and within Boardwalk, our proposed General Game Playing API.\nWe anonymize the games and components to avoid evoking pre-trained LLM\nknowledge. The implementations are tested for playability and rule compliance.\nWe evaluate success rate and common errors across LLMs and game popularity.\nResults: Our approach proves viable, with the best performing model, Claude\n3.7 Sonnet, yielding 55.6% of games without any errors. While compliance with\nthe API increases error frequency, the severity of errors is more significantly\ndependent on the LLM. We outline future steps for creating a framework to\nintegrate this process, making the elaboration of board games more accessible.\nKeywords Board games,\nLarge Language Models,\nProcedural Content\nGeneration\n1. Introduction\nBoard games are an important object of study in the field of Artificial Intelligence\n(AI), with many works concerning themselves with developing algorithms for playing\nor modifying games [Browne 2011, Todd et al. 2024]. However, in order for such studies\nto be conducted, they first require the researchers to represent the game they’re studying\nin code. This can be a time-consuming task, especially for games of higher complexity\nwith various different pieces and edge cases to consider.\nFor this reason, several Game Description Languages (GDLs), such as Ludii\n[Piette et al. 2020], have been developed; these are domain-specific languages meant to\nfacilitate the implementation and analysis of digital board games, by providing direct\ncommands for defining the components and rules of such games. However, creating game\ndescriptions with GDLs still requires knowledge of the language’s own syntax, which\narXiv:2508.16447v1  [cs.LG]  22 Aug 2025\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nsometimes requires convoluted constructions for simple operations of specific games1.\nTherefore, Large Language Models (LLMs) present themselves as highly\nappealing tools to aid in this task.\nModern LLMs have shown great performance\nin code generation tasks [Coignion et al. 2024], adding significant efficiency to code-\ndependent workflows.\nOne particularly interesting aspect of LLMs is their capacity\nfor In-Context Learning (ICL), being able to perform tasks they had not been trained\nfor simply from receiving additional information pertaining to those tasks as part of the\nprompt [Brown et al. 2020].\nIn this work, we aim to answer the following research question: can LLMs\nimplement digital versions of board games from rules described in natural language?\nTo answer the question, we ask state-of-the-art LLMs (DeepSeekV3, Claude 3.7 Sonnet,\nand ChatGPT-4o) to code a selection of 12 games in Python, both in free-form and within\nBoardwalk, a standardized API we provide. We also request the LLMs to adapt their free-\nform implementations to the API. The games and their components are anonymized to\nevaluate the LLMs’ understanding of rules in natural language rather than evoking code\nassociated with such names in pre-training data. Furthermore, 6 of the 12 games are rather\nobscure, such that it is even less likely that the LLMs evoke pre-training code associated\nwith their rules. The resulting game implementations are tested for playability and rule\ncompliance. We enumerate common errors across games and LLMs to better understand\ntheir relations.\nOur main findings are that LLMs are largely capable of implementing games from\nrule descriptions, both free-form and using the API, and that Claude is the best-performing\nof the evaluated models.\nOur main contributions can be summarized as follows:\n• A broad evaluation of LLM capabilities on generating digital versions of board\ngames from rules in natural language;\n• Boardwalk, a standardized API for General Game Playing, easing the integration\nof game implementations and game-playing AIs. The API aims for simplicity\nby allowing the game definition in Python rather than a Game Description\nLanguage with its own learning curve.\nBoardwalk code is open-sourced at\nhttps://github.com/LabCRAIG/boardwalk.\nThe remainder of this paper is organized as follows: Section 2 presents related\nwork.\nSection 3 introduces the Boardwalk API. Section 4 details our evaluation\nmethodology. Section 5 presents quantitative results of the error rates of each model\nand game, as well as a qualitative analysis of specific errors and recurring challenges\nobserved. Finally, Section 6 summarizes our findings and outlines possible future work\ntowards an AI-powered board game generation framework.\n2. Related Work\nMost existing studies in LLM-oriented procedural content generation for games are\ncentered on video games. [Todd et al. 2023] were able to train LLMs to create novel\nand functional Sokoban levels. [Sudhakaran et al. 2023] were able to fine-tune GPT2 to\n1For example, Ultimate Tic-Tac-Toe in Ludii’s GDL has 9 levels of nested tests to check move validity\n[Ludii Portal 2020].\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\ncreate Super Mario Bros levels with custom features defined by natural language prompts.\nLLMs have also been used to tell interactive narratives in games [Sun et al. 2023,\nKumaran et al. 2023]. All of these focused on creating content for otherwise human-\nimplemented games.\nUsing a video game description language, [Hu et al. 2024] had LLMs generate\nboth rules and levels for maze-based games. They found that the models required context-\nrich prompts for optimal performance and faced greater challenges with the logical\naspects of game design compared to the syntax of the language.\n[Browne 2011] reports the seemingly first automated system to generate a\ncommercial board game from scratch2.\nIt uses evolutionary algorithms to evolve\ngame rules on the Ludi (with one ‘i’) Game Description Language (GDL), combined\nwith automated playtesting with general game playing (GGP) [Genesereth et al. 2005]\nalgorithms. However, the Ludi framework is not available for further experimentation.\nThe second version of Ludii (now with two ‘i’) GDL [Piette et al. 2020], had\nan initial focus on game archaeology and not on automated game creation. However,\nrecent work has managed to use Ludii in automated board game creation. For example,\n[Todd et al. 2024] fine-tuned an LLM in the Ludii GDL. Using a fill-in-the-middle\napproach, the authors removed parts of pre-existing game descriptions and tasked the\nLLM with completing them, generating potentially different rules, without having to\ncreate new games from scratch. However, generated rules often contained introns, i.e.\nelements that were never used in gameplay.\n[Tanaka and Simo-Serra 2024] used ICL to generate Ludii implementations from\nnatural language rule descriptions. However, due to the complexity of the Ludii grammar,\nthey followed a process for each game of iteratively finding a minimal valid subgrammar\nto describe the game, and then iteratively finding a description over that grammar that\ncorrectly implements the rules.\nNot only was this very computationally demanding,\nrequiring repeated inferences for each game, but it further showed that using the Ludii\nlanguage requires a significant amount of upfront work to familiarize the LLM with its\nstructure. Only 72% of descriptions generated by their method compiled (i.e. were valid\nin the Ludii GDL grammar).\nGiven the results of [Todd et al. 2024, Tanaka and Simo-Serra 2024], we decided\nnot to use a specific GDL for our purposes, due to the significant limitations in code\ngeneration that stem from the LLMs’ lack of familiarity with them.\nInstead, our\nBoardwalk GGP API (see Section 3) aims for simplicity: the games are defined as Python\nprograms. Using a programming language is easier for LLMs as they consume extensive\nprogramming content in their pre-training [Jiang et al. 2024]. Thus, the LLMs are likely\nto have a significantly higher success rate writing code in a popular programming\nlanguage rather than a specific GDL. Moreover, this removes an entry barrier for human\nprogrammers as well, because there is no extra learning curve for a new language.\n3. The Boardwalk API\nThe Boardwalk API is a General Game Playing (GGP) platform, with the purpose of\nacting as a tool for scientific study. It provides an interface for defining a game and\n2Yavalath: https://boardgamegeek.com/boardgame/33767/yavalath\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nhaving it played via a standardized input and output format. Boardwalk was planned\nto minimize the amount of code needed to implement a game: boilerplate code (user\ninterface, game state, and game loop) is handled by Boardwalk.\nThis requires the\nprogrammer to implement only the logic specific to the game rules in Python, essentially\nproviding a game engine that is easy to work with for both human programmers and\nLLMs.\nFor the scope of this study, the API’s capacity for representation was limited to\nperfect information games with any number of players whose boards can be modeled\nin a rectangular grid, with a simple command-line user interface. However, it would\nbe reasonably simple to expand its breadth of representable games, as well as to add a\ngraphical user interface, without breaking any code developed in the current stage.\nThe code and its documentation are available on GitHub, with the release\nBoardwalk 1.0 corresponding to the version used in this study3. That version does not\nsupport game-playing AI agents. This feature has been added to the project’s main branch.\nNext, we present an overview of how the API is structured, what resources it offers\nto the programmer, and what needs to be implemented for a given game.\nThe implementation of the game rules is divided into the validity of moves, the\norder of play, and win conditions. Two classes are specified: Board and Game.\nThe Board class is final (non-extensible), and implements the matrix holding the\nboard layout (spaces and pieces), as well as methods to change piece positioning by\nplacement, moving, or removal. The Game class is designed to hold the logic of the\ngame’s rules. By default, it implements a series of attributes and methods responsible\nfor holding and representing the game state, interfacing with the player, and operating\nthe standard game loop. However, the class itself cannot be used, as it has several vital\nmethods left blank. Hence, it must be inherited into a child class specific to the game\nbeing implemented, and the blank methods are overridden there, implementing the game\nrules.\nAt the bare minimum, the programmer needs only to define a custom enumeration\nto represent the game’s players and override four methods of the Game class:\n• validate_move: receives a move and returns a boolean indicating whether the\nmove is valid according to the game rules.\n• game_finished: receives a game state and returns a boolean indicating if,\naccording to the rules, the game state is terminal. This is where the win conditions\nare implemented.\n• get_winner: returns an integer corresponding to the winning player (according\nto the defined enumeration). In case of a draw, it instead returns None.\n• next_player: returns an integer corresponding to the player to make the next\nmove (according to the defined enumeration).\nThese four methods have no preexisting definition, and thus must be overridden\nfor all games (obligatory methods). None of them alter the game state; provided the return\nvalues are of the expected types, the API handles the control flow and state of the game.\n3https://github.com/LabCRAIG/boardwalk/releases/tag/\nrules-phase-no-ai\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nThe other Game class methods all have existing default implementations. Thus,\nthey may suffice for simpler games, given the obligatory methods are implemented.\nHowever, they may also be overridden to implement more complex rules or customize\nthe user interface. The one most often modified will be the perform_move method,\nwhich is responsible for altering the game board according to the informed move once\nvalidated. Programmers may opt to add other modifications to the board as part of this\nmethod to allow for side effects of moves (such as captures and promotions in Checkers,\nfor example).\nThe Game class also has three attributes, which are set during initialization and all\nchild classes must account for: board, which is an instance of the Board class; round,\nan integer which counts the number of turns played, and by default increases by one\nwith every move; and current_player, an integer specifying the programmer-defined\nenumeration of the next player to make a move. These attributes define the game state.\nAll subclasses of the Game class follow the same game loop: for every turn,\nwhile the game is not finished, the current state of the board is printed. The current\nplayer is repeatedly prompted to make a move, until their input is deemed valid by\nvalidate_move. The move is then performed by perform_move, along with its\nside effects. The game then checks if a terminal condition has been met. If yes, the\nwinner is identified and printed, and the game loop ends. If not, the round counter is\nincreased and the next player is assigned, and the loop continues.\n4. Methodology\nThis section describes the methods used to answer our research question, i.e.,\nwhether LLMs can implement digital versions of board games from rules described\nin natural language.\nWe conducted tests on three different, state-of-the-art LLMs:\nDeepSeekV3 [Liu et al. 2025], Claude 3.7 Sonnet [Anthropic 2024], and ChatGPT-4o\n[OpenAI et al. 2024]. These LLMs were accessed via the Poe website4. They will hereby\nbe referred to as DeepSeek, Claude and GPT, respectively.\nWe selected 12 board games.\nWe considered 6 of these (Tic-Tac-Toe, Peg\nSolitaire, Reversi, Nine Men’s Morris, Checkers and Chess) to be widely popular games,\nand the remaining 6 (Ard Ri, Domineering, Tron, Amazons, Kharebga and Unashogi) to\nbe more obscure games5. We considered it likely that the LLMs would be significantly\nmore acquainted with the rules of the well-known games and have implementations of\nthem in code as part of their training data, which would make it easier for them to\nimplement these games. As such, the obscure games might offer a better measure of\nthe models’ capacity for generalization for this task—understanding the game rules and\nimplementing code based purely on natural language descriptions without evoking pre-\nexisting game-specific knowledge.\nAn additional step taken to avoid evoking pre-trained code associated with the\ngames was their anonymization.\nWe removed as much identifying information as\npossible: each game was given a fake name, and pieces received unusual names. For\nexample, Tic-Tac-Toe was named “Peridot” and the pieces were A and V instead of X\nand O. Moreover, all game rules were described in a standardized structure.\n4https://poe.com/\n5Game descriptions can be found in the Ludii database https://ludii.games/library.php\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nEach experiment consists of one model implementing one game.\nFor each\nexperiment, a new chat was initiated, and a series of standardized prompts with\ninformation regarding the game were given to the model, finally asking it to produce\nPython code to implement it. The prompts used are available on GitHub6.\nTo gain a better understanding of LLMs’ capabilities and how much the API\nimpacted their performance, three sets of tests were performed, each presenting the model\nwith different context information (we refer to these as code-generation modes):\n1. the Boardwalk API (Section 3) documentation and the game rules (API\nimplementation);\n2. only the game rules (independent, free-form, implementation);\n3. the API documentation, the game rules, and the independent implementation of\nthe same game by the same model (adapted implementation, where the LLM had\nto adapt the independent code to be API-compliant).\nWe have shown only the documentation of the API to the models, not its code.\nIn total, 108 experiments were run (12 games, 3 models, 3 code-generation modes), with\neach yielding Python code for one game.\nThe outputs were evaluated by manually running the generated code, checking\nthe initial board setup, and manually playing one or more standard rounds of the game to\nverify if the rules were correctly implemented. In cases where the code could not be run\ndue to a syntax error or crash, a manual inspection of the code was made in an attempt to\nidentify the cause. If the issue was easily spotted and fixable, an altered version of the code\nwith that issue corrected was run, to evaluate the game logic despite those errors (though\nthey were still registered). Examples of these fixable issues are given in Section 5.2.\n5. Results\nIn all experiments, all of the models output Python code as requested. All of the results\nwere code for an interactive game, save for one: DeepSeek’s independent implementation\nof Amazons did not take inputs from a player or run a game loop; it simply printed a\nsequence of sample moves in the game, demonstrating how it worked.\nThe results are divided into quantitative analysis (Section 5.1), where we present\nthe success and error rates of each model, and qualitative analysis (Section 5.2), where we\nexamine specific types of errors and their manifestation and severity across models and\ngames.\nThe generated code for all tests is available on GitHub7.\n5.1. Quantitative Analysis\nThe generated codes were evaluated for the presence of the following types of errors:\nPython error (programming errors preventing the game from running); API (e.g. not\nimplementing a mandatory method, or using the input/output API standard incorrectly);\nmove verification (e.g. invalid moves allowed, valid moves not allowed); ending (game\nends improperly, winner incorrectly identified); additional effect (e.g.\ncaptures and\n6https://github.com/LabCRAIG/boardwalk-prompts\n7https://github.com/LabCRAIG/boardwalk-prompts/tree/main/Results\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\npromotions not resolved properly); board error (e.g. incorrect layout or initial setup) and\nturn order (e.g. players are improperly skipped or called to move when they shouldn’t).\nError occurrence was treated as a binary, i.e.\nwe did not count multiple\noccurrences of the same error type, as our focus was to measure the occurrence of different\nerror types. This allows us to gain insight into what aspects are more challenging for the\nLLMs, as well as to compare how the different models performed on the same task. A\nmore detailed analysis of specific errors is given in the Qualitative Analysis subsection.\nTables 1 show the occurrences of each type of error across each experiment group,\nfor Claude, DeepSeek, and GPT, respectively. We remark that each LLM executed a total\nof 36 implementations (12 games, 3 code-generation modes)8. The total number of errors\nmay exceed 36, as the same implementation can have multiple types of errors. Claude\nand DeepSeek showed worse results when using the Boardwalk API compared to the\nindependent implementations, the latter accounting for only 16.7% and 21.9% of the total\nerrors for each model, respectively. For GPT, it is only the adapted implementations that\nshow a more significant increase in error frequency, being responsible for 43.6% of total\nerrors.\nTable 1. Distribution of error types by model and code-generation mode (API-\ncompliant, Indep. for independent/free-form and Adap. for adapting from\nindependent to API). The Total (%) row shows the proportion of errors for\neach code-generation mode within each model, calculated as the count in\neach column divided by the total number of errors for that model.\nClaude\nDeepSeek\nGPT\nError Type\nAPI\nIndep.\nAdap.\nAPI\nIndep.\nAdap.\nAPI\nIndep.\nAdap.\nPython\n0\n1\n1\n0\n3\n4\n0\n2\n6\nAPI\n1\n-\n2\n0\n-\n3\n3\n-\n3\nMove\n4\n2\n4\n6\n1\n4\n4\n4\n5\nEnding\n5\n1\n1\n4\n2\n3\n5\n4\n4\nEffect\n2\n1\n3\n4\n3\n2\n1\n2\n3\nBoard\n3\n1\n1\n1\n2\n2\n1\n2\n2\nTurn order\n1\n0\n2\n3\n0\n1\n1\n2\n1\nTotal\n16\n6\n14\n18\n11\n19\n15\n16\n24\nTotal (%)\n44.4\n16.7\n38.9\n37.5\n22.9\n39.6\n27.3\n29.1\n43.6\nTable 2 shows the number of games each model was able to implement perfectly,\nas well as the number of unplayable games. The first is an important measure of success,\nwhereas the second indicates more serious errors because the games couldn’t even be\nplaytested due to not running or crashing from any input. Overall, 38.9% of all generated\ncode was error-free, while 24.1% was unplayable. However, as shown in the table, those\nrates are highly model-dependent.\nFigure 1 shows the sum total of errors for each game across all implementations.\nWe can glean from it that Nine Men’s Morris, Ard Ri, Kharebga and Unashogi were the\n8As DeepSeek’s independent implementation of Amazons was far outside the expected format, we\nsimply counted it as a Python error, without checking for other error types.\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nTable 2.\nDistribution of perfectly implemented and unplayable games across\nmodels and code-generation modes. Rates are obtained by the sum of\nperfect or unplayable implementations over the total implementations per\nmodel, which is 36 for 12 games and 3 code-generation modes.\nModel\nType\nAPI\nIndep.\nAdap.\nSum/Total\nRate\nClaude\nPerfect\n5\n9\n6\n20/36\n55.6%\nUnplayable\n0\n0\n0\n0/36\n0.0%\nDeepSeek\nPerfect\n1\n5\n4\n10/36\n27.8%\nUnplayable\n2\n5\n5\n12/36\n33.3%\nGPT\nPerfect\n4\n5\n3\n12/36\n33.3%\nUnplayable\n4\n2\n8\n14/36\n38.9%\n1\n5\n2\n29\n8\n8\n20\n5\n13\n15\n22\n17\n0\n10\n20\n30\nTic-Tac-Toe\nPeg Solitaire\nReversi\nNine Men's Morris\nCheckers\nChess\nArd Ri\nDomineering\nTron\nAmazons\nKharebga\nUnashogi\nTurn order\nBoard\nEffect\nEnding\nMove\nAPI\nPython\nFigure 1. Occurrences of each error type per game, pooled from all experiments.\nhardest games for the models to implement, with these 4 games being responsible for\n64% of all errors. In fact, there were no perfect implementations of Nine Men’s Morris\nor Unashogi.\nMeanwhile, Tic-Tac-Toe, Peg Solitaire, Reversi and Domineering were the easiest\nto implement, and in fact account for 57% of all perfect implementations.\nClaude\nimplemented these four games perfectly in all experiments, with the same holding for\nGPT with Tic-Tac-Toe and Reversi.\nThere was no such game for DeepSeek, but it\nprovided the only case of an incorrect independent implementation being perfected in\nadaptation, with Checkers.\nFrom these results, Claude had the best performance overall, with the fewest\nerrors, the largest number of perfect implementations, and no game-breaking bugs. The\nother two models are approximately on par with each other.\nRegarding our belief that the implementations of the popular games benefit from\nthe models’ existing knowledge of them, they account for 38.4% of all errors, 55.8% of\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nall perfect implementations, and 28.0% of all unplayable games. While that does show\nthat the obscure games were harder to implement as a group, we also did not measure the\nintrinsic complexity of each game (how hard its logic is to represent in code), so a fair\ncomparison would require further analysis.\n5.2. Qualitative Analysis\nA common game-breaking error in implementations using the API was the models not\ndefining the next_player method. This is curious, as none of the other mandatory\nmethods were ignored in any implementation.\nFurthermore, many adaptations kept\nreferences to methods or attributes defined only in the original implementation, which\nwould also cause the game to crash.\nSome of these were among the easily fixable\nerrors, as they would simply fail to reference an analogous existing method (e.g. calling\n_check_game_over rather than game_finished), and could be easily identified as\nthe cause of the crash.\nAnother frequent error was separating each character on the board layout string\nwith white spaces, which would conflict with the given board dimensions. These spaces\nwere removed for test playing, with a Board error being noted for those implementations.\nBoth DeepSeek and GPT had a recurring problem in their implementations of\nNine Men’s Morris, Kharebga, and Unashogi: all of these games have piece placement\nin a phase, and piece capture as the goal in a separate phase. However, in many cases,\nthe game would end as soon as the first move was made, as it simply counted that the\nopponent had no pieces without accounting for the first move.\nSome errors occurred in most implementations of certain games by all models:\n• In Checkers, the forced capture rule was usually ignored.\n• In Kharebga, multiple opposing pieces caught between the same pair of the\nplayer’s pieces would incorrectly be captured all at once.\n• In Unashogi, the Kinsh¯o would be incorrectly able to move diagonally backwards.\nAll of these explicitly contradict the rules as described to the model in the prompts given,\nand the fact that at least one correct implementation of each exists proves that it is possible\nfor the models to understand them properly. However, given the frequency of these errors,\nwe acknowledge the possibility of the rule descriptions being insufficiently clear about\nthese points.\nAcross all experiments, the most notably difficult game to implement was Nine\nMen’s Morris. This is likely due to the graph-based nature of the game’s board, which\nLLMs have trouble translating to a rectangular format, even despite efforts to facilitate\nthat in the rules description. The overall trend was the inability to recognize two spaces\nas being adjacent, due to them being separated in the board layout for the sake of visibility.\nWhile the API defines a standard format for inputting moves, it is also projected\nto be customizable in that regard. Therefore, deviations from the standard user interface\nwere not counted as errors, so long as they did not impede usability. However, it is\ninteresting to note that almost all of Claude’s adapted implementations, save for Chess,\nkept the same interface used in the original implementation. The other models did not\nexhibit this behavior, with GPT always conforming the code to the standard input format,\nand DeepSeek doing the same to all but two games (Ard Ri and Amazons).\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\n6. Conclusion\nIn this work, we introduced Boardwalk, a Python API for developing abstract board\ngames, and examined the feasibility of using LLMs to generate board game code from\nnatural language descriptions. We evaluated the implementation of 12 games by three\nLLMs in three code-generation modes: independent (free-form), API-compliant and\nadapting from free-form to API.\nOverall, the answer to our research question (i.e., whether LLMs can implement\ndigital versions of board games from rules described in natural language) is positive:\nClaude 3.7 Sonnet, the best-performing model, generated error-free code 55.6% of the\ntime. Furthermore, most erroneous results could be fixed with less effort than it would\ntake to program the game from scratch, thus still providing a boost to efficiency. We find\nit feasible that, with additional work, this approach could lead to a functional and efficient\nframework for game code generation.\nWhile the tests involving the API generally showed a higher frequency of errors\ncompared to the independent implementations, the usage of the API remains valid as\na means to easily interface with both human and AI players, as well as a means to\nstandardize and simplify the necessary code to develop a board game.\nIt is important to emphasize that these results were obtained from zero-shot\ninference experiments: no examples of code were provided to the models for context.\nIn-context learning (giving several examples of the task at hand as part of the prompt)\n[Brown et al. 2020] as well as few-shot learning (training the LLM for subsequent steps)\ngreatly improve LLM performance. Thus, we intend to explore these approaches in future\nwork, as we expect them to both increase the LLMs’ understanding of the API and to\nbetter understand game descriptions in natural language.\nAnother angle for future work to approach is to expand the API to allow for\na larger breadth of representable games, such as those with hexagonal or graph-based\nboards, or card games. These could be potentially more challenging games and provide\nbetter insight into the LLMs’ capacity for abstraction and reasoning.\nHowever, if a\nsuccessful method to implement these is developed, it could serve as a reliable and\nversatile tool for implementing both existing and novel games, for researchers and game\ndesigners alike.\nFuture work could also investigate how our results could be improved with a few\niterations, i.e., by giving the model feedback on its errors. This would be a step towards\na fully-fledged AI board game creator. However, even at its current stage, the proposed\napproach represents an efficient assistant to programmers willing to create board games,\nas issues in the generated code can be manually fixed.\nAcknowledgments\nThe authors would like to acknowledge Eduardo Dalmas Faé for initial experiments on the\ntopic. This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal\nde Nível Superior - Brasil (CAPES) - Finance Code 001 and the Conselho Nacional de\nDesenvolvimento Científico e Tecnológico (CNPq).\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nReferences\nAnthropic\n(2024).\nThe\nClaude\n3\nModel\nFamily:\nOpus,\nSonnet,\nHaiku.\nhttps://www-cdn.anthropic.com/\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_\nClaude_3.pdf. [Accessed 21-04-2025].\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan,\nA., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,\nHenighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen,\nM., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S.,\nRadford, A., Sutskever, I., and Amodei, D. (2020). Language Models are Few-Shot\nLearners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors,\nAdvances in Neural Information Processing Systems, volume 33, pages 1877–1901.\nCurran Associates, Inc.\nBrowne, C. (2011). Evolutionary Game Design. SpringerBriefs in Computer Science.\nSpringer London, London, UK.\nCoignion, T., Quinton, C., and Rouvoy, R. (2024).\nA Performance Study of LLM-\nGenerated Code on Leetcode. In Proceedings of the 28th International Conference\non Evaluation and Assessment in Software Engineering, pages 79–89.\nGenesereth, M., Love, N., and Pell, B. (2005). General game playing: Overview of the\nAAAI competition. AI magazine, 26(2):62–62.\nHu, C., Zhao, Y., and Liu, J. (2024). Game Generation via Large Language Models. In\n2024 IEEE Conference on Games (CoG), pages 1–4.\nJiang, J., Wang, F., Shen, J., Kim, S., and Kim, S. (2024). A Survey on Large Language\nModels for Code Generation. arXiv preprint arXiv:2406.00515.\nKumaran, V., Rowe, J., Mott, B., and Lester, J. (2023).\nSceneCraft: Automating\nInteractive Narrative Scene Generation in Digital Games with Large Language Models.\nProceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital\nEntertainment, 19(1):86–96.\nLiu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C.,\nRuan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo,\nF., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding,\nH., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang,\nJ., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K.,\nGuan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang,\nL., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N.,\nHuang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin,\nR. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S.,\nLu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S.,\nZhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., Zeng, W., Zhao, W.,\nAn, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Li, X. Q., Jin, X., Wang, X.,\nBi, X., Liu, X., Wang, X., Shen, X., Chen, X., Zhang, X., Chen, X., Nie, X., Sun, X.,\nWang, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Song, X., Shan, X., Zhou, X.,\nYang, X., Li, X., Su, X., Lin, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhu, Y. X., Zhang,\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nY., Xu, Y., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Yu, Y., Zheng,\nY., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Tang, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y.,\nLiu, Y., Guo, Y., Wu, Y., Ou, Y., Zhu, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Zha, Y.,\nXiong, Y., Ma, Y., Yan, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z. Z.,\nRen, Z., Sha, Z., Fu, Z., Xu, Z., Huang, Z., Zhang, Z., Xie, Z., Zhang, Z., Hao, Z.,\nGou, Z., Ma, Z., Yan, Z., Shao, Z., Xu, Z., Wu, Z., Zhang, Z., Li, Z., Gu, Z., Zhu, Z.,\nLiu, Z., Li, Z., Xie, Z., Song, Z., Gao, Z., and Pan, Z. (2025). DeepSeek-V3 Technical\nReport.\nLudii Portal (2020). Ultimate Tic-Tac-Toe. https://ludii.games/details.\nphp?keyword=Ultimate%20Tic-Tac-Toe, accessed in Apr 21, 2025.\nOpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L.,\nAlmeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I.,\nBalaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I.,\nBerdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd,\nM., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T.,\nCampbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C.,\nChantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C.,\nChu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry,\nT., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet,\nA., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J.,\nFulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-\nLopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo,\nY., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey,\nA., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J.,\nJain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B.,\nJun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan,\nT., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, J. H., Kiros, J., Knight,\nM., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K.,\nKrueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li,\nC. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju,\nA., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K.,\nMayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J.,\nMedina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V.,\nMorikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano,\nR., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O’Keefe, C., Pachocki,\nJ., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E.,\nPassos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov,\nM., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V. H., Powell, T.,\nPower, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond,\nC., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M.,\nSanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam,\nD., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E.,\nSimens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N.,\nSuch, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M. B., Tillet,\nP., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C.,\nVallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang,\nXXIV Simpósio Brasileiro de Jogos e Entretenimento Digital (SBGames 2025) - Salvador/BA\nTrilha: Computação\nB., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L.,\nWiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S.,\nWu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R.,\nZhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. (2024).\nGPT-4 Technical Report.\nPiette, E., Soemers, D., Stephenson, M., Sironi, C., Winands, M., and Browne, C. (2020).\nLudii – the ludemic general game system.\nFrontiers in Artificial Intelligence and\nApplications, 325:411–418.\nSudhakaran, S., González-Duque, M., Freiberger, M., Glanois, C., Najarro, E., and\nRisi, S. (2023). MarioGPT: open-ended text2level generation through large language\nmodels. In Proceedings of the 37th International Conference on Neural Information\nProcessing Systems, NIPS ’23, Red Hook, NY, USA. Curran Associates Inc.\nSun, Y., Li, Z., Fang, K., Lee, C. H., and Asadipour, A. (2023). Language as Reality:\nA Co-Creative Storytelling Game Experience in 1001 Nights Using Generative AI.\nProceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital\nEntertainment, 19(1):425–434.\nTanaka, T. and Simo-Serra, E. (2024). Grammar-based Game Description Generation\nusing Large Language Models. IEEE Transactions on Games, pages 1–14.\nTodd, G., Earle, S., Nasir, M. U., Green, M. C., and Togelius, J. (2023). Level Generation\nThrough Large Language Models. In Proceedings of the 18th International Conference\non the Foundations of Digital Games, FDG ’23, New York, NY, USA. Association for\nComputing Machinery.\nTodd, G., Padula, A. G., Stephenson, M., Piette, É., Soemers, D., and Togelius, J. (2024).\nGAVEL: Generating games via evolution and language models. Advances in Neural\nInformation Processing Systems, 37:110723–110745.\n",
    "content": "# Paper Analysis Report\n\n---\n\n## I. Core Content and Major Contributions\n\nThe paper is titled **\"Boardwalk: Towards a Framework for Creating Board Games with LLMs\"**, published at **SBGames 2025**. Its central focus is to explore whether **Large Language Models (LLMs)** can generate digital versions of board games based on natural language rule descriptions, and it proposes a general-purpose game development framework called **Boardwalk**.\n\n### Core Content\n- **Research Question**: Can LLMs implement board games from natural language rules?\n- **Experimental Design**: Use three leading LLMs (Claude 3.7 Sonnet, DeepSeekV3, ChatGPT-4o) to generate code for 12 board games (6 popular + 6 obscure).\n- **Evaluation Method**: Test playability, rule compliance, and analyze error types and success rates.\n- **Innovation**: Introduce the **Boardwalk API**, a Python-based general-purpose framework for game development.\n\n### Major Contributions\n1. **First systematic evaluation of LLM capabilities in board game generation**:\n   - Compare performance of three LLMs under free coding and API-constrained conditions.\n   - Find that **Claude** performs best, achieving a **55.6% error-free implementation rate**.\n2. **Introduction of the Boardwalk API**:\n   - A standardized, easy-to-use Python interface for game development.\n   - Enables rapid implementation of game logic without requiring knowledge of Game Description Languages (GDL).\n3. **Identification of challenges and error patterns in LLM-based game implementation**:\n   - Common issues include missing `next_player` methods, incorrect handling of piece movement and win conditions.\n4. **Open-source release of code and data**:\n   - Includes the Boardwalk API, game rule descriptions, generated code, and evaluation results.\n\n---\n\n## II. Breakthroughs and Innovations\n\n### 1. **Game Code Generation from Natural Language Rules**\n   - Traditional methods rely on Game Description Languages (GDL) like Ludii, which have steep learning curves.\n   - This study is the first to systematically verify that LLMs can directly generate game code from natural language rules, without relying on pre-trained knowledge.\n\n### 2. **Design and Implementation of the Boardwalk API**\n   - A lightweight, Python-based API that lowers the barrier to entry for developers.\n   - Supports standard input/output for easy integration with AI players.\n   - Provides a unified interface that simplifies the implementation of game logic.\n\n### 3. **Game Anonymization and Rule Description Standardization**\n   - To prevent models from relying on pre-trained knowledge, games were anonymized (e.g., Tic-Tac-Toe became Peridot).\n   - Standardized rule descriptions to improve experimental fairness.\n\n### 4. **Systematic Analysis of Error Types and Challenges**\n   - Identified common LLM errors in game implementation (e.g., syntax errors, API violations, move validation errors).\n   - Analyzed performance differences among models across various games.\n\n### 5. **Feasibility Validation under Zero-Shot Inference**\n   - All experiments were conducted without providing example code, testing LLMs' capabilities without in-context learning.\n\n---\n\n## III. Entrepreneurial Project Ideas\n\n### 1. **AI-Powered Game Development Platform**\n   - **Project Name**: GameForge AI  \n   - **Function**: Users input natural language descriptions of game rules, and the system automatically generates executable board game code.  \n   - **Technical Basis**: Boardwalk API + LLM (e.g., Claude or GPT).  \n   - **Business Model**: SaaS model targeting indie game developers, educational institutions, and AI researchers.\n\n### 2. **Educational AI Game Design Assistant**\n   - **Project Name**: BoardGame Tutor  \n   - **Function**: Helps students design game rules through natural language, with AI generating code and providing feedback.  \n   - **Target Market**: K-12 programming education, university AI courses, and game design programs.  \n   - **Additional Features**: Supports multiplayer online gameplay, AI coach hints, and rule visualization.\n\n### 3. **Personalized Board Game Generator**\n   - **Project Name**: MyBoardGame  \n   - **Function**: Users input personalized settings (e.g., characters, background, objectives), and AI generates a custom board game.  \n   - **Technical Implementation**: LLM-generated rules + Boardwalk logic implementation + Unity/Python for UI.  \n   - **Commercial Path**: Custom board game printing services, digital downloads, and community sharing platforms.\n\n### 4. **AI Board Game Testing and Optimization Platform**\n   - **Project Name**: GameCheck AI  \n   - **Function**: Automatically detects rule consistency, playability, and AI controllability in generated games.  \n   - **Use Cases**: Pre-release testing and optimization recommendations for game developers.  \n   - **Technical Highlights**: Integration of LLMs, automated testing, and AI player simulation.\n\n### 5. **AI-Powered Game Design Collaboration Platform**\n   - **Project Name**: GameTogether  \n   - **Function**: Multi-user collaborative game design with AI generating playable prototypes in real time, supporting iterative modifications.  \n   - **Target Users**: Board game design enthusiasts, game development teams, and remote collaborators.  \n   - **Technology Stack**: LLM + real-time collaborative editing + Boardwalk API + cloud execution.\n\n---\n\n## Conclusion\n\nThis paper not only demonstrates the feasibility of using LLMs to generate board games but also introduces a practical development framework, **Boardwalk**, laying the foundation for future AI-driven game development. Its open-source code and systematic evaluation provide a strong starting point for further research and entrepreneurial ventures.\n\nFor future development, integrating **in-context learning (ICL)** and **few-shot learning** could enhance LLM understanding, potentially leading to the creation of a full **AI board game creation system**.",
    "github": "",
    "hf": ""
}