{
    "id": "/linshenkx/prompt-optimizer",
    "issues": "12",
    "watch": "28",
    "fork": "806",
    "star": "6k",
    "topics": [
        "prompt",
        "prompt-toolkit",
        "prompt-tuning",
        "llm",
        "prompt-engineering",
        "prompt-optimization"
    ],
    "license": "MIT License",
    "languages": [
        "TypeScript,59.2%",
        "Vue,28.3%",
        "JavaScript,6.2%",
        "CSS,5.7%",
        "Shell,0.3%",
        "Dockerfile,0.1%"
    ],
    "contributors": [],
    "about": "一款提示词优化器，助力于编写高质量的提示词",
    "is_AI": "y",
    "category": "Development Efficiency Tools",
    "summary": "```markdown\n# Project Summary\n\n## 1. Core Content and Problems Solved\n\nPrompt Optimizer is a tool focused on optimizing AI prompts, aiming to help users write higher-quality prompts to improve the accuracy and relevance of AI-generated content. It addresses the following issues:\n- Poor prompt quality leading to suboptimal AI output.\n- Lack of intuitive tools for comparing and evaluating the effectiveness of different prompts.\n- Integration with multiple mainstream AI models, eliminating the hassle of switching between different platforms for users.\n- Data privacy and security concerns, ensuring user data safety through pure client-side processing and local encrypted storage.\n\n## 2. Breakthroughs and Innovations\n\nThe breakthroughs and innovations of Prompt Optimizer include:\n- **Smart Optimization Functionality**: Provides a one-click prompt optimization feature with support for multi-round iterative improvements, significantly enhancing the accuracy of AI responses.\n- **Real-time Comparative Testing**: Allows users to simultaneously test original prompts and optimized prompts, intuitively showcasing the optimization effects.\n- **Multi-model Integration**: Supports various mainstream AI models (such as OpenAI, Gemini, DeepSeek, etc.), enabling users to flexibly choose based on their needs.\n- **Advanced Parameter Configuration**: Offers independent parameter adjustment options for each model, meeting professional users' fine-tuning requirements.\n- **Secure Architecture and Privacy Protection**: Adopts a pure client-side processing method to ensure that user data does not pass through intermediate servers; local encrypted storage further enhances privacy protection.\n- **Cross-platform Support**: In addition to providing a web application, it also develops a Chrome extension, making it convenient for users to use in different scenarios.\n\n## 3. Inspiration for Startup Projects\n\nBased on the features and characteristics of Prompt Optimizer, here are some great startup ideas:\n\n1. **Personalized AI Writing Assistant**  \n   Develop an AI writing assistant targeted at content creators, leveraging Prompt Optimizer's prompt optimization capabilities to help users generate high-quality articles, blogs, or social media content.\n\n2. **AI Tutoring Tool for Education**  \n   Apply Prompt Optimizer to the education field by creating an AI tutoring platform that helps students better understand complex concepts or complete assignments. For example, optimize prompts to generate detailed step-by-step solutions or learning guides.\n\n3. **Enterprise-level Prompt Management Platform**  \n   Develop a prompt management platform for enterprise users, combining Prompt Optimizer's multi-model support and advanced parameter configuration functions to help businesses optimize their AI-driven business processes (such as customer service, marketing, etc.).\n\n4. **Creative Design Assistance Tool**  \n   Create a creative design assistance tool aimed at designers or artists, using optimized prompts to generate artworks, UI designs, or other visual content.\n\n5. **Language Learning App**  \n   Combine Prompt Optimizer's multilingual support features to develop a language learning application that helps users practice grammar, simulate conversations, or undergo translation training through optimized prompts.\n\n6. **Cross-platform Collaborative Prompt Optimization Tool**  \n   Develop a collaborative prompt optimization tool that supports team collaboration, allowing multiple users to jointly edit and optimize prompts, especially suitable for AI projects or research work requiring multi-person cooperation.\n\n7. **Customizable API Service Middleware Platform**  \n   Build a customizable API service middleware platform based on Prompt Optimizer's API key management and cross-domain solutions, helping businesses quickly integrate and manage multiple AI models.\n\nBy pursuing these directions, the core functionalities of Prompt Optimizer can be fully utilized to expand application scenarios and meet the needs of different industries and user groups.\n```",
    "text": "Prompt Optimizer (提示词优化器) 🚀\nEnglish\n|\n中文\n在线体验\n|\n快速开始\n|\n常见问题\n|\n开发文档\n|\nVercel部署指南\n|\nChrome插件\n📖 项目简介\nPrompt Optimizer是一个强大的AI提示词优化工具，帮助你编写更好的AI提示词，提升AI输出质量。支持Web应用和Chrome插件两种使用方式。\n🎥 功能演示\n✨ 核心特性\n🎯\n智能优化\n：一键优化提示词，支持多轮迭代改进，提升AI回复准确度\n🔄\n对比测试\n：支持原始提示词和优化后提示词的实时对比，直观展示优化效果\n🤖\n多模型集成\n：支持OpenAI、Gemini、DeepSeek、智谱AI、SiliconFlow等主流AI模型\n⚙️\n高级参数配置\n：支持为每个模型单独配置temperature、max_tokens等LLM参数\n🔒\n安全架构\n：纯客户端处理，数据直接与AI服务商交互，不经过中间服务器\n💾\n隐私保护\n：本地加密存储历史记录和API密钥，支持数据导入导出\n📱\n多端支持\n：同时提供Web应用和Chrome插件两种使用方式\n🎨\n用户体验\n：简洁直观的界面设计，响应式布局和流畅交互动效\n🌐\n跨域支持\n：Vercel部署时支持使用Edge Runtime代理解决跨域问题\n🔐\n访问控制\n：支持密码保护功能，保障部署安全\n快速开始\n1. 使用在线版本（推荐）\n直接访问：\nhttps://prompt.always200.com\n项目是纯前端项目，所有数据只存储在浏览器本地，不会上传至任何服务器，因此直接使用在线版本也是安全可靠的\n2. Vercel部署\n方式1：一键部署到自己的Vercel：\n方式2: Fork项目后在Vercel中导入（推荐）：\n先Fork项目到自己的GitHub\n然后在Vercel中导入该项目\n可跟踪源项目更新，便于同步最新功能和修复\n配置环境变量：\nACCESS_PASSWORD\n：设置访问密码，启用访问限制\nVITE_OPENAI_API_KEY\n等：配置各AI服务商的API密钥\n更多详细的部署步骤和注意事项，请查看：\nVercel部署指南\n3. 安装Chrome插件\n从Chrome商店安装（由于审批较慢，可能不是最新的）：\nChrome商店地址\n点击图标即可打开提示词优化器\n4. Docker部署\n#\n运行容器（默认配置）\ndocker run -d -p 80:80 --restart unless-stopped --name prompt-optimizer linshen/prompt-optimizer\n#\n运行容器（配置API密钥和访问密码）\ndocker run -d -p 80:80 \\\n  -e VITE_OPENAI_API_KEY=your_key \\\n  -e ACCESS_USERNAME=your_username\n\\\n#\n可选，默认为\"admin\"\n-e ACCESS_PASSWORD=your_password\n\\\n#\n设置访问密码\n--restart unless-stopped \\\n  --name prompt-optimizer \\\n  linshen/prompt-optimizer\n5. Docker Compose部署\n#\n1. 克隆仓库\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd\nprompt-optimizer\n#\n2. 可选：创建.env文件配置API密钥和访问认证\ncat\n>\n.env\n<<\nEOF\n# API密钥配置\nVITE_OPENAI_API_KEY=your_openai_api_key\nVITE_GEMINI_API_KEY=your_gemini_api_key\nVITE_DEEPSEEK_API_KEY=your_deepseek_api_key\nVITE_ZHIPU_API_KEY=your_zhipu_api_key\nVITE_SILICONFLOW_API_KEY=your_siliconflow_api_key\n# Basic认证配置（密码保护）\nACCESS_USERNAME=your_username  # 可选，默认为\"admin\"\nACCESS_PASSWORD=your_password  # 设置访问密码\nEOF\n#\n3. 启动服务\ndocker compose up -d\n#\n4. 查看日志\ndocker compose logs -f\n你还可以直接编辑docker-compose.yml文件，自定义配置：\nservices\n:\nprompt-optimizer\n:\nimage\n:\nlinshen/prompt-optimizer:latest\ncontainer_name\n:\nprompt-optimizer\nrestart\n:\nunless-stopped\nports\n:\n      -\n\"\n8081:80\n\"\n#\n修改端口映射\nenvironment\n:\n      -\nVITE_OPENAI_API_KEY=your_key_here\n#\n直接在配置中设置密钥\n⚙️ API密钥配置\n方式一：通过界面配置（推荐）\n点击界面右上角的\"⚙️设置\"按钮\n选择\"模型管理\"选项卡\n点击需要配置的模型（如OpenAI、Gemini、DeepSeek等）\n在弹出的配置框中输入对应的API密钥\n点击\"保存\"即可\n支持的模型：\nOpenAI (gpt-3.5-turbo, gpt-4, gpt-4o)\nGemini (gemini-1.5-pro, gemini-2.0-flash)\nDeepSeek (deepseek-chat, deepseek-coder)\nZhipu智谱 (glm-4-flash, glm-4, glm-3-turbo)\nSiliconFlow (Pro/deepseek-ai/DeepSeek-V3)\n自定义API（OpenAI兼容接口）\n除了API密钥，您还可以在模型配置界面为每个模型单独设置高级LLM参数。这些参数通过一个名为\nllmParams\n的字段进行配置，它允许您以键值对的形式指定LLM SDK支持的任何参数，从而更精细地控制模型行为。\n高级LLM参数配置示例：\nOpenAI/兼容API\n:\n{\"temperature\": 0.7, \"max_tokens\": 4096, \"timeout\": 60000}\nGemini\n:\n{\"temperature\": 0.8, \"maxOutputTokens\": 2048, \"topP\": 0.95}\nDeepSeek\n:\n{\"temperature\": 0.5, \"top_p\": 0.9, \"frequency_penalty\": 0.1}\n有关\nllmParams\n的更详细说明和配置指南，请参阅\nLLM参数配置指南\n。\n方式二：通过环境变量配置\nDocker部署时通过\n-e\n参数配置环境变量：\n-e VITE_OPENAI_API_KEY=your_key\n-e VITE_GEMINI_API_KEY=your_key\n-e VITE_DEEPSEEK_API_KEY=your_key\n-e VITE_ZHIPU_API_KEY=your_key\n-e VITE_SILICONFLOW_API_KEY=your_key\n-e VITE_CUSTOM_API_KEY=your_custom_api_key\n-e VITE_CUSTOM_API_BASE_URL=your_custom_api_base_url\n-e VITE_CUSTOM_API_MODEL=your_custom_model_name\n本地开发\n详细文档可查看\n开发文档\n#\n1. 克隆项目\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd\nprompt-optimizer\n#\n2. 安装依赖\npnpm install\n#\n3. 启动开发服务\npnpm dev\n#\n主开发命令：构建core/ui并运行web应用\npnpm dev:web\n#\n仅运行web应用\npnpm dev:fresh\n#\n完整重置并重新启动开发环境\n🗺️ 开发路线\n基础功能开发\nWeb应用发布\nChrome插件发布\n自定义模型支持\n多模型支持优化\n国际化支持\n详细的项目状态可查看\n项目状态文档\n📖 相关文档\n文档索引\n- 所有文档的索引\n技术开发指南\n- 技术栈和开发规范\nLLM参数配置指南\n- 高级LLM参数配置详细说明\n项目结构\n- 详细的项目结构说明\n项目状态\n- 当前进度和计划\n产品需求\n- 产品需求文档\nVercel部署指南\n- Vercel部署详细说明\nStar History\n常见问题\nAPI连接问题\nQ1: 为什么配置好API密钥后仍然无法连接到模型服务？\nA\n: 大多数连接失败是由\n跨域问题\n（CORS）导致的。由于本项目是纯前端应用，浏览器出于安全考虑会阻止直接访问不同源的API服务。模型服务如未正确配置CORS策略，会拒绝来自浏览器的直接请求。\nQ2: 如何解决本地Ollama的连接问题？\nA\n: Ollama完全支持OpenAI标准接口，只需配置正确的跨域策略：\n设置环境变量\nOLLAMA_ORIGINS=*\n允许任意来源的请求\n如仍有问题，设置\nOLLAMA_HOST=0.0.0.0:11434\n监听任意IP地址\nQ3: 如何解决商业API（如Nvidia的DS API、字节跳动的火山API）的跨域问题？\nA\n: 这些平台通常有严格的跨域限制，推荐以下解决方案：\n使用Vercel代理\n（便捷方案）\n使用在线版本：\nprompt.always200.com\n或自行部署到Vercel平台\n在模型设置中勾选\"使用Vercel代理\"选项\n请求流向：浏览器→Vercel→模型服务提供商\n详细步骤请参考\nVercel部署指南\n使用自部署的API中转服务\n（可靠方案）\n部署如OneAPI等开源API聚合/代理工具\n在设置中配置为自定义API端点\n请求流向：浏览器→中转服务→模型服务提供商\nQ4: Vercel代理有什么缺点或风险？\nA\n: 使用Vercel代理可能会触发某些模型服务提供商的风控机制。部分厂商可能会将来自Vercel的请求判定为代理行为，从而限制或拒绝服务。如遇此问题，建议使用自部署的中转服务。\n🤝 参与贡献\nFork 本仓库\n创建特性分支 (\ngit checkout -b feature/AmazingFeature\n)\n提交更改 (\ngit commit -m '添加某个特性'\n)\n推送到分支 (\ngit push origin feature/AmazingFeature\n)\n提交 Pull Request\n提示：使用cursor工具开发时，建议在提交前:\n使用\"code_review\"规则进行代码审查\n按照审查报告格式检查:\n变更的整体一致性\n代码质量和实现方式\n测试覆盖情况\n文档完善程度\n根据审查结果进行优化后再提交\n👏 贡献者名单\n感谢所有为项目做出贡献的开发者！\n📄 开源协议\n本项目采用\nMIT\n协议开源。\n如果这个项目对你有帮助，请考虑给它一个 Star ⭐️\n👥 联系我们\n提交 Issue\n发起 Pull Request\n加入讨论组",
    "readme": "# Prompt Optimizer (提示词优化器) 🚀\n\n<div align=\"center\">\n\n[English](README_EN.md) | [中文](README.md)\n\n[![GitHub stars](https://img.shields.io/github/stars/linshenkx/prompt-optimizer)](https://github.com/linshenkx/prompt-optimizer/stargazers)\n![Chrome Web Store Users](https://img.shields.io/chrome-web-store/users/cakkkhboolfnadechdlgdcnjammejlna?style=flat&label=Chrome%20Users&link=https%3A%2F%2Fchromewebstore.google.com%2Fdetail%2F%25E6%258F%2590%25E7%25A4%25BA%25E8%25AF%258D%25E4%25BC%2598%25E5%258C%2596%25E5%2599%25A8%2Fcakkkhboolfnadechdlgdcnjammejlna)\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Docker Pulls](https://img.shields.io/docker/pulls/linshen/prompt-optimizer)](https://hub.docker.com/r/linshen/prompt-optimizer)\n![GitHub forks](https://img.shields.io/github/forks/linshenkx/prompt-optimizer?style=flat)\n[![Deploy with Vercel](https://img.shields.io/badge/Vercel-indigo?style=flat&logo=vercel)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flinshenkx%2Fprompt-optimizer)\n\n[在线体验](https://prompt.always200.com) | [快速开始](#快速开始) | [常见问题](#常见问题) | [开发文档](dev.md) | [Vercel部署指南](docs/vercel.md) | [Chrome插件](https://chromewebstore.google.com/detail/prompt-optimizer/cakkkhboolfnadechdlgdcnjammejlna)\n\n</div>\n\n## 📖 项目简介\n\nPrompt Optimizer是一个强大的AI提示词优化工具，帮助你编写更好的AI提示词，提升AI输出质量。支持Web应用和Chrome插件两种使用方式。\n\n### 🎥 功能演示\n\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/linshenkx/prompt-optimizer/master/images/contrast.png\" alt=\"功能演示\" width=\"90%\">\n</div>\n\n## ✨ 核心特性\n\n- 🎯 **智能优化**：一键优化提示词，支持多轮迭代改进，提升AI回复准确度\n- 🔄 **对比测试**：支持原始提示词和优化后提示词的实时对比，直观展示优化效果\n- 🤖 **多模型集成**：支持OpenAI、Gemini、DeepSeek、智谱AI、SiliconFlow等主流AI模型\n- ⚙️ **高级参数配置**：支持为每个模型单独配置temperature、max_tokens等LLM参数\n- 🔒 **安全架构**：纯客户端处理，数据直接与AI服务商交互，不经过中间服务器\n- 💾 **隐私保护**：本地加密存储历史记录和API密钥，支持数据导入导出\n- 📱 **多端支持**：同时提供Web应用和Chrome插件两种使用方式\n- 🎨 **用户体验**：简洁直观的界面设计，响应式布局和流畅交互动效\n- 🌐 **跨域支持**：Vercel部署时支持使用Edge Runtime代理解决跨域问题\n- 🔐 **访问控制**：支持密码保护功能，保障部署安全\n\n## 快速开始\n\n### 1. 使用在线版本（推荐）\n\n直接访问：[https://prompt.always200.com](https://prompt.always200.com)\n\n项目是纯前端项目，所有数据只存储在浏览器本地，不会上传至任何服务器，因此直接使用在线版本也是安全可靠的\n\n### 2. Vercel部署\n方式1：一键部署到自己的Vercel：\n   [![部署到 Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flinshenkx%2Fprompt-optimizer)\n\n方式2: Fork项目后在Vercel中导入（推荐）：\n   - 先Fork项目到自己的GitHub\n   - 然后在Vercel中导入该项目\n   - 可跟踪源项目更新，便于同步最新功能和修复\n- 配置环境变量：\n  - `ACCESS_PASSWORD`：设置访问密码，启用访问限制\n  - `VITE_OPENAI_API_KEY`等：配置各AI服务商的API密钥\n\n更多详细的部署步骤和注意事项，请查看：\n- [Vercel部署指南](docs/vercel.md)\n\n### 3. 安装Chrome插件\n1. 从Chrome商店安装（由于审批较慢，可能不是最新的）：[Chrome商店地址](https://chromewebstore.google.com/detail/prompt-optimizer/cakkkhboolfnadechdlgdcnjammejlna)\n2. 点击图标即可打开提示词优化器\n\n### 4. Docker部署\n```bash\n# 运行容器（默认配置）\ndocker run -d -p 80:80 --restart unless-stopped --name prompt-optimizer linshen/prompt-optimizer\n\n# 运行容器（配置API密钥和访问密码）\ndocker run -d -p 80:80 \\\n  -e VITE_OPENAI_API_KEY=your_key \\\n  -e ACCESS_USERNAME=your_username \\  # 可选，默认为\"admin\"\n  -e ACCESS_PASSWORD=your_password \\  # 设置访问密码\n  --restart unless-stopped \\\n  --name prompt-optimizer \\\n  linshen/prompt-optimizer\n  \n```\n\n### 5. Docker Compose部署\n```bash\n# 1. 克隆仓库\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd prompt-optimizer\n\n# 2. 可选：创建.env文件配置API密钥和访问认证\ncat > .env << EOF\n# API密钥配置\nVITE_OPENAI_API_KEY=your_openai_api_key\nVITE_GEMINI_API_KEY=your_gemini_api_key\nVITE_DEEPSEEK_API_KEY=your_deepseek_api_key\nVITE_ZHIPU_API_KEY=your_zhipu_api_key\nVITE_SILICONFLOW_API_KEY=your_siliconflow_api_key\n\n# Basic认证配置（密码保护）\nACCESS_USERNAME=your_username  # 可选，默认为\"admin\"\nACCESS_PASSWORD=your_password  # 设置访问密码\nEOF\n\n# 3. 启动服务\ndocker compose up -d\n\n# 4. 查看日志\ndocker compose logs -f\n```\n\n你还可以直接编辑docker-compose.yml文件，自定义配置：\n```yaml\nservices:\n  prompt-optimizer:\n    image: linshen/prompt-optimizer:latest\n    container_name: prompt-optimizer\n    restart: unless-stopped\n    ports:\n      - \"8081:80\"  # 修改端口映射\n    environment:\n      - VITE_OPENAI_API_KEY=your_key_here  # 直接在配置中设置密钥\n```\n\n## ⚙️ API密钥配置\n\n### 方式一：通过界面配置（推荐）\n1. 点击界面右上角的\"⚙️设置\"按钮\n2. 选择\"模型管理\"选项卡\n3. 点击需要配置的模型（如OpenAI、Gemini、DeepSeek等）\n4. 在弹出的配置框中输入对应的API密钥\n5. 点击\"保存\"即可\n\n支持的模型：\n- OpenAI (gpt-3.5-turbo, gpt-4, gpt-4o)\n- Gemini (gemini-1.5-pro, gemini-2.0-flash)\n- DeepSeek (deepseek-chat, deepseek-coder)\n- Zhipu智谱 (glm-4-flash, glm-4, glm-3-turbo)\n- SiliconFlow (Pro/deepseek-ai/DeepSeek-V3)\n- 自定义API（OpenAI兼容接口）\n\n除了API密钥，您还可以在模型配置界面为每个模型单独设置高级LLM参数。这些参数通过一个名为 `llmParams` 的字段进行配置，它允许您以键值对的形式指定LLM SDK支持的任何参数，从而更精细地控制模型行为。\n\n**高级LLM参数配置示例：**\n- **OpenAI/兼容API**: `{\"temperature\": 0.7, \"max_tokens\": 4096, \"timeout\": 60000}`\n- **Gemini**: `{\"temperature\": 0.8, \"maxOutputTokens\": 2048, \"topP\": 0.95}`\n- **DeepSeek**: `{\"temperature\": 0.5, \"top_p\": 0.9, \"frequency_penalty\": 0.1}`\n\n有关 `llmParams` 的更详细说明和配置指南，请参阅 [LLM参数配置指南](docs/llm-params-guide.md)。\n\n### 方式二：通过环境变量配置\nDocker部署时通过 `-e` 参数配置环境变量：\n```bash\n-e VITE_OPENAI_API_KEY=your_key\n-e VITE_GEMINI_API_KEY=your_key\n-e VITE_DEEPSEEK_API_KEY=your_key\n-e VITE_ZHIPU_API_KEY=your_key\n-e VITE_SILICONFLOW_API_KEY=your_key\n-e VITE_CUSTOM_API_KEY=your_custom_api_key\n-e VITE_CUSTOM_API_BASE_URL=your_custom_api_base_url\n-e VITE_CUSTOM_API_MODEL=your_custom_model_name\n```\n\n## 本地开发\n详细文档可查看 [开发文档](dev.md)\n\n```bash\n# 1. 克隆项目\ngit clone https://github.com/linshenkx/prompt-optimizer.git\ncd prompt-optimizer\n\n# 2. 安装依赖\npnpm install\n\n# 3. 启动开发服务\npnpm dev               # 主开发命令：构建core/ui并运行web应用\npnpm dev:web          # 仅运行web应用\npnpm dev:fresh        # 完整重置并重新启动开发环境\n```\n\n## 🗺️ 开发路线\n\n- [x] 基础功能开发\n- [x] Web应用发布\n- [x] Chrome插件发布\n- [x] 自定义模型支持\n- [x] 多模型支持优化\n- [x] 国际化支持\n\n详细的项目状态可查看 [项目状态文档](docs/project-status.md)\n\n## 📖 相关文档\n\n- [文档索引](docs/README.md) - 所有文档的索引\n- [技术开发指南](docs/technical-development-guide.md) - 技术栈和开发规范\n- [LLM参数配置指南](docs/llm-params-guide.md) - 高级LLM参数配置详细说明\n- [项目结构](docs/project-structure.md) - 详细的项目结构说明\n- [项目状态](docs/project-status.md) - 当前进度和计划\n- [产品需求](docs/prd.md) - 产品需求文档\n- [Vercel部署指南](docs/vercel.md) - Vercel部署详细说明\n\n\n## Star History\n\n<a href=\"https://star-history.com/#linshenkx/prompt-optimizer&Date\">\n <picture>\n   <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://api.star-history.com/svg?repos=linshenkx/prompt-optimizer&type=Date&theme=dark\" />\n   <source media=\"(prefers-color-scheme: light)\" srcset=\"https://api.star-history.com/svg?repos=linshenkx/prompt-optimizer&type=Date\" />\n   <img alt=\"Star History Chart\" src=\"https://api.star-history.com/svg?repos=linshenkx/prompt-optimizer&type=Date\" />\n </picture>\n</a>\n\n## 常见问题\n\n### API连接问题\n\n#### Q1: 为什么配置好API密钥后仍然无法连接到模型服务？\n**A**: 大多数连接失败是由**跨域问题**（CORS）导致的。由于本项目是纯前端应用，浏览器出于安全考虑会阻止直接访问不同源的API服务。模型服务如未正确配置CORS策略，会拒绝来自浏览器的直接请求。\n\n#### Q2: 如何解决本地Ollama的连接问题？\n**A**: Ollama完全支持OpenAI标准接口，只需配置正确的跨域策略：\n1. 设置环境变量 `OLLAMA_ORIGINS=*` 允许任意来源的请求\n2. 如仍有问题，设置 `OLLAMA_HOST=0.0.0.0:11434` 监听任意IP地址\n\n#### Q3: 如何解决商业API（如Nvidia的DS API、字节跳动的火山API）的跨域问题？\n**A**: 这些平台通常有严格的跨域限制，推荐以下解决方案：\n\n1. **使用Vercel代理**（便捷方案）\n   - 使用在线版本：[prompt.always200.com](https://prompt.always200.com)\n   - 或自行部署到Vercel平台\n   - 在模型设置中勾选\"使用Vercel代理\"选项\n   - 请求流向：浏览器→Vercel→模型服务提供商\n   - 详细步骤请参考 [Vercel部署指南](docs/vercel.md)\n\n2. **使用自部署的API中转服务**（可靠方案）\n   - 部署如OneAPI等开源API聚合/代理工具\n   - 在设置中配置为自定义API端点\n   - 请求流向：浏览器→中转服务→模型服务提供商\n\n#### Q4: Vercel代理有什么缺点或风险？\n**A**: 使用Vercel代理可能会触发某些模型服务提供商的风控机制。部分厂商可能会将来自Vercel的请求判定为代理行为，从而限制或拒绝服务。如遇此问题，建议使用自部署的中转服务。\n\n\n## 🤝 参与贡献\n\n1. Fork 本仓库\n2. 创建特性分支 (`git checkout -b feature/AmazingFeature`)\n3. 提交更改 (`git commit -m '添加某个特性'`)\n4. 推送到分支 (`git push origin feature/AmazingFeature`)\n5. 提交 Pull Request\n\n提示：使用cursor工具开发时，建议在提交前:\n1. 使用\"code_review\"规则进行代码审查\n2. 按照审查报告格式检查:\n   - 变更的整体一致性\n   - 代码质量和实现方式\n   - 测试覆盖情况\n   - 文档完善程度\n3. 根据审查结果进行优化后再提交\n\n## 👏 贡献者名单\n\n感谢所有为项目做出贡献的开发者！\n\n<a href=\"https://github.com/linshenkx/prompt-optimizer/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=linshenkx/prompt-optimizer\" alt=\"贡献者\" />\n</a>\n\n## 📄 开源协议\n\n本项目采用 [MIT](LICENSE) 协议开源。\n\n---\n\n如果这个项目对你有帮助，请考虑给它一个 Star ⭐️\n\n## 👥 联系我们\n\n- 提交 Issue\n- 发起 Pull Request\n- 加入讨论组",
    "author": "linshenkx",
    "project": "prompt-optimizer",
    "date": "2025-06-18"
}