{
    "id": "2506.13217",
    "title": "Polyra Swarms: A Shape-Based Approach to Machine Learning",
    "summary": "This paper proposes a new machine learning approach called Polyra Swarms for shape approximation rather than function approximation, and explores its advantages in tasks such as anomaly detection and its automated abstraction mechanisms.",
    "abstract": "We propose Polyra Swarms, a novel machine-learning approach that approximates shapes instead of functions. Our method enables general-purpose learning with very low bias. In particular, we show that depending on the task, Polyra Swarms can be preferable compared to neural networks, especially for tasks like anomaly detection. We further introduce an automated abstraction mechanism that simplifies the complexity of a Polyra Swarm significantly, enhancing both their generalization and transparency. Since Polyra Swarms operate on fundamentally different principles than neural networks, they open up new research directions with distinct strengths and limitations.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Simon Klüttermann,Emmanuel Müller",
    "subjects": [
        "Machine Learning (cs.LG)",
        "Neural and Evolutionary Computing (cs.NE)",
        "Symbolic Computation (cs.SC)"
    ],
    "comments": "Comments:Currently under review",
    "keypoint": "- Polyra Swarms are a novel machine-learning approach that approximates shapes instead of functions, enabling general-purpose learning with very low bias.\n- Depending on the task, Polyra Swarms can be preferable to neural networks, especially for tasks like anomaly detection.\n- An automated abstraction mechanism significantly simplifies the complexity of a Polyra Swarm, enhancing both generalization and transparency.\n- Polyra Swarms operate on fundamentally different principles than neural networks, offering distinct strengths and limitations.\n- Shape approximation has conceptual and empirical benefits compared to traditional function approximation in settings where structure, sparsity, or interpretability are important.\n- Logical underpinnings of Polyra Swarms allow symbolic abstraction techniques that simplify models into minimal logical rules, inherently regularizing models while improving interpretability and generalization.\n- Polyra Swarms complement rather than compete with neural networks; integrating representation learning (e.g., pre-trained embeddings) with shape approximation mitigates high-dimensional challenges while retaining geometric fidelity.\n- Polyra Swarms can approximate complex, high-dimensional structures with low bias and high flexibility.\n- The method constructs regions by combining convex polytopes with logical operations, avoiding gradient-based optimization entirely.\n- Polyra Swarms outperform several recent neural network-based anomaly detection algorithms significantly on benchmark datasets.",
    "date": "2025-06-18",
    "paper": "arXiv:2506.13217v1  [cs.LG]  16 Jun 2025\nPolyra Swarms: A Shape-Based Approach to Machine\nLearning\nSimon Klüttermann\nDepartment of Computer Science\nTU Dortmund Univerity\nDortmund, Germany\nsimon.kluettermann@cs.tu-dortmund.de\nEmmanuel Müller\nDepartment of Computer Science\nTU Dortmund Univerity\nDortmund, Germany\nemmanuel.mueller@cs.tu-dortmund.de\nAbstract\nWe propose Polyra Swarms, a novel machine-learning approach that approximates\nshapes instead of functions. Our method enables general-purpose learning with very\nlow bias. In particular, we show that depending on the task, Polyra Swarms can be\npreferable compared to neural networks, especially for tasks like anomaly detection.\nWe further introduce an automated abstraction mechanism that simplifies the\ncomplexity of a Polyra Swarm significantly, enhancing both their generalization and\ntransparency. Since Polyra Swarms operate on fundamentally different principles\nthan neural networks, they open up new research directions with distinct strengths\nand limitations.\n1\nIntroduction\nTrue negative\nFalse positive\nTrue positive\nFigure 1: Example Polyra Swarm fit to a dataset inspired by the mandelbrot set (purple). To see\nthe area falsely considered normal (blue), we need to zoom in further from left to right, showing\nthe very low bias of our Polyra Swarm fits. Please note that false negatives are impossible here by\nconstruction. Further details about this experiment can be found in Appendix Q.\nTraditional machine learning frames learning as function approximation: inputs are mapped to\noutputs, and model success is evaluated by predictive accuracy. However, many human concepts,\nsuch as \"dog\" or \"chair\", are better characterized as regions within a structured feature space rather\nthan as deterministic functions.\nIn this work, we propose a new learning paradigm: shape approximation. Instead of asking \"What\nis the output corresponding to this input?\", we ask \"What region does this input belong to?\" Our\napproach models such regions using combinations of semi-random polytopes connected via logical\noperations, forming what we call a Polyra Swarm.\nPreprint. Under review.\nPolyra Swarms construct regions by combining convex polytopes (via semi-random initialization)\nwith logical operations (e.g., unions, intersections) to model non-convex structures, avoiding gradient-\nbased optimization entirely and thus can not become stuck in local optima. We demonstrate that\nthey can approximate complex, high-dimensional structures with low bias and high flexibility (see\nFigure 1). This geometric perspective enables general-purpose learning across tasks like classification,\nregression and anomaly detection.\nWe show that shape approximation has conceptual and or empirical benefits compared to traditional\nfunction approximation in several settings, particularly where structure, sparsity, or interpretability are\nimportant. Furthermore, the logical underpinnings of Polyra Swarms allow us to introduce symbolic\nabstraction techniques that simplify models into minimal logical rules, inherently regularizing models\nwhile improving both their interpretability and generalization.\nWhile Polyra Swarms excel at precise, interpretable region modeling in low-to-moderate dimensions,\nthey complement, rather than compete with—neural networks. By integrating representation learning\n(e.g., pre-trained embeddings) with shape approximation, we mitigate high-dimensional challenges\nwhile retaining geometric fidelity.\nLearning through shape approximation offers a promising, logic-grounded alternative to function-\nbased models, with potential implications, especially for shape-like tasks like anomaly detection but\nalso for on-device computing, conceptual generalization, and interpretable machine learning.\nBecause of page limitations, we only briefly describe each experiment in the main paper and include\nfurther information and experiments in the appendix. Additionally, the code to reproduce all our\nexperiments can be found at anonymous.4open.science/r/polyra_exp-2BA7. We also implement\npolyra swarms as a python library, which can be found at anonymous.4open.science/r/polyra_lib-\nD1C0. Unless explicitly mentioned, every experiment conducted here takes either seconds or minutes\non a consumer-grade laptop.\n2\nRelated Work\nMost machine learning methods to date have framed learning as a problem of function approximation,\nmapping inputs to outputs via models optimized for predictive accuracy Rosenblatt [1958], Mitchell\n[1997]. This view aligns with the psychological doctrine of functionalism, which emphasizes\nunderstanding the interaction of mental processes over their internal structure Polger [2012], Wilkes\n[1981]. Functionalism emerged Dewey [1896] in contrast to structuralism Titchener [1896], which\nsought to analyze consciousness by identifying its constituent components.\nInspired by this philosophical distinction, we propose a structuralist perspective on machine learning.\nRather than approximating a function, we aim to model the shape of the data distribution itself.\nSpecifically, we seek to identify the geometric regions in input space that are possible under a given\ndistribution. We term this approach shape approximation.\nSeveral existing approaches partially reflect a structuralist inclination. One-class classification\nmethods Khan and Madden [2010], Ruff et al. [2018b] learn a decision region occupied by a single\nclass and can be seen as modeling structure directly in feature space. Feature engineering Zheng and\nCasari [2018] similarly involves decomposing behavior into meaningful low-level components. In\nsymbolic learning Quinlan [1993], Zelle and Mooney [1996], prototype-based methods Kohonen\n[1990], and topological data analysis (TDA) Carlsson [2009], Hofer et al. [2017], researchers have\nexplored structural descriptions of data. However, these methods typically require manual design or\ndomain-specific tuning and have not achieved the level of automatic, general-purpose learning seen\nin function-oriented models such as deep neural networks.\nOur approach bridges this gap by enabling general-purpose structural modeling: Polyra Swarms\nconstruct interpretable, logic-based shapes that can approximate complex data distributions without\nrelying on gradient descent. This offers a new perspective that complements existing function-based\nparadigms.\n2\n3\nPolyra Swarms\nWe will now describe Polyra Swarms mathematically and prove that they can approximate arbitrary\nshapes. Since our notation is extensive, we provide an overview in Appendix A.\nDefinition 1 (Shape). We consider a shape Q to be a measurable, bounded subset of space Q ⊆Rdim\nwith a finite length border. A shape can be represented through an indicator function q.\nq(x) =\n\u001aTRUE,\nx ∈Q\nFALSE,\nx /∈Q\n(1)\nWe use capital letters to denote shapes, and lowercase letters to refer to indicator functions.\nDefinition 2 (Shape Approximation). Given a set of samples Xtrain, which all lie in a shape Q\n(x ∈Q, ∀x ∈Xtrain), the task of shape approximation is to learn an indicator function f(x) so that\nthe approximation error η =\nR\nRdim ∥f(x) −q(x)∥dx is minimal.\nDefinition 3 (Polytope). A common way of describing convex shapes is through polytopes. These are\ndefined through K constraint vectors MA ∈RK×dim defining directions and connected bA ∈RK\nbounds that represent the maximum value along this direction inside the shape. Thus a polytope A\ncan be described by Equation 2.\nA =\nK\n\\\nj=1\n\b\nx ∈Rn : (MA)j · x ≤(bA)j\t\n(2)\nDefinition 4 (Polyra Swarm). A Polyra Swarm1 P is the conjunction of many base shapes, as shown\nin Equation 3.\np(x) =\nN\n^\ni=0\nfi(x),\nP =\nN\n\\\ni=0\nFi\n(3)\nA sample x is in the shape approximated by a given Polyra Swarm, when it lies in each of the base\nshapes Fi\nDefinition 5 (Polyra base shapes). We use base shapes Fi defined by the logical conjunction of\ntwo polytopes (Equation 4). If a sample lies in the condition polytope Ai, it also has to lie in the\nconsequent polytope Bi.\nfi(x) = (x ∈Ai ⇒x ∈Bi) ⇔fi(x) = (x /∈Ai ∨x ∈Bi) ⇔Fi = A∁\ni ∪Bi\n(4)\nHere A∁represents the complement shape to A (a∁(x) = 1 −a(x)). Effectively, each base shape\nexcludes the shape Ai ∩B∁\ni from the shape approximated by the Polyra Swarm.\nTheorem 1. For every measurable shape Q and every ϵ > 0, there exists a Polyra Swarm P such\nthat\nR\nRd |p(x) −q(x)|dx < ϵ.\nProof. (Sketch) We can select arbitrarily small dim-simplices using the condition polytopes Ai\nand remove the volume outside of Q volume using the consequent polytopes Bi. Full proof :\nAppendix B\nWe now propose an algorithm based on largely random initialization to learn such a swarm of condi-\ntional polytopes to approximate the shape containing a set of observed points Xtrain in Equation 5.\nHere N refers to a normal distribution and U represents a uniform distribution. Additionally, we\nleave (KA)i, (KB)i and N as hyperparameters to be discussed in Appendices E.1, E.2, E.8.\n(MA)µν\ni , (MB)µν\ni\ni.i.d.\n∼N(0, 1)\n(bA)µ\ni\ni.i.d.\n∼U(\nmin\nx∈Xtrain(MA)µ\ni · x,\nmax\nx∈Xtrain(MA)µ\ni · x)\n(5)\nWe reject those base shapes where Ai does not contain any of the training samples (Appendix E.4),\nand select (bB)i from the most extreme values allowed in the condition polytope Ai.\n(bB)µ\ni =\nmax\n(x∈Xtrain) ∪((MA)i·x≤(bA)i)(MB)µ\ni · x\n(6)\n1Polyra is a portmanteau of \"Polytope\" and \"Piranha\". See Appendix G.\n3\nFor symmetry reasons, we also include the second consequent region induced by (MA)i →−(MA)i.\nThis setup can be learned quickly, is easily parallelized, and guarantees that p(x) = True ∀x ∈\nXtrain (Proof : Appendix C).\n4\nUsing Polyra Swarms\nAfter showing that it is possible to approximate any shape using a Polyra Swarm, here we want to\ndemonstrate that universal shape approximation also implies general-purpose learning. Doing so\ngenerally requires rethinking how to approach any machine-learning problem. In this section, we will\ndiscuss some of the most common machine-learning tasks and show how they can be solved using\nPolyra Swarms.\n4.1\nShape Approximation\nFigure 2: Example of how to solve matching using shape approximation. Given a pattern between\nyour and your partner’s attributes, we can describe it as a shape. More details about the experimental\nsetup are found in Appendix R.\nFirst, consider that many machine learning problems are more naturally shape approximation tasks.\nAs an example, consider the toy example in Figure 2 showing how to solve matching problems using\nshape approximation. The task of selecting a well-matching partner depending on your and their\nattributes is a common problem from advertising to job matching. It can be solved through, e.g.,\nsiamese neural networks Koch [2015], learning a representation in which matching samples are close\nto each other. However, instead of learning such a representation, we search for the shape in the space\nof both your and your match attributes of participants who match successfully.\n4.2\nClassification\n(a) Using a neural network\n(b) and a reject option\n(c) Using two Polyra Swarms.\nFigure 3: Classification results on the two moons dataset using various models. The neural network\non the left generally classifies every point in input space, often without basis. For example, the red\nstar is much closer to blue points than to purple ones but is still classified as purple. The Polyra\nswarms on the right side instead only identify the region that matches one class and thus reject\nsamples outside of these regions. More details of our experimental setup can be found in Appendix S.\n4\nA shape approximation model generally answers the question: \"Does this sample belong to a given\nclass\". This is very close to the goal of classification algorithms, and thus we can use one Polyra\nswarm to identify each class. This has some benefits and drawbacks over a function approximation\nalgorithm approximating the decision boundary. First of all, since we are not learning the boundary\nbetween two regions, but describe different classes each, we can add and remove classes without\nneeding to retrain the whole model. A classification setup trained to separate dogs from cats can\nbe partially reused to separate dogs from horses. Additionally, as Figure 3 shows, this inherently\nincludes a very effective reject option Hendrickx et al. [2024]. However, this might not always be\ndesirable. In contrast to the neural network model, which classifies any sample as either class 1 or 0,\na Polyra swarm classifier can also classify a sample as \"neither\" (reject option) or as \"could be both\".\nWhile the last case does not happen in Figure 3, it is prevalent in real applications like the MNIST\nclassification we consider in Appendix I.\n4.3\nAnomaly Detection\n3\n4\n5\n6\nPolyra (0.79) \nNeuTral (0.75) \nDTE (0.71) \nDeepSVDD (0.72) \n DAGMM (0.61)\n NF (0.61)\n VAE (0.61)\n GOAD (0.71)\nAverage Rank\nFigure 4: Anomaly detection critical difference plot comparing Polyra swarms to recent/common deep\nlearning anomaly detection methods (Using a Friedmann Friedman [1940] and Wilcoxon Wilcoxon\n[1945] test and a Bonferroni-Holm correction Holm [1979]). We use 121 common benchmark\ndatasets taken from a recent survey Han et al. [2022] in the semi-supervised setting. As anomaly\ndetection is usually evaluated with continuous anomaly scores (See Appendix H), we calculate the\nfraction of submodels that consider a sample invalid as anomaly score. More about this experiment\ncan be found in Appendix T.\nAnomaly Detection in general, tries to answer whether a sample belongs to the class \"normal\" or\nnot. Thus shape approximation is perfectly suited to anomaly detection. We evaluate this in Figure 4\nusing 7 recent neural network (and thus function approximation-based) anomaly detection algorithms\non the benchmark datasets proposed by a recent survey paper Han et al. [2022]. We find that Polyra\nSwarms outperforms all our competitors and does so significantly for most of them.\n4.4\nRegression and Uncertainty Estimation\nWhile tasks like classification and anomaly detection are well suited to shape approximation, regres-\nsion, as the task of approximating a function is more suited to function approximation. However,\nevery function f : Rd →Rn is still also a shape in Rd+n, which we can approximate. Interestingly,\nwhile doing so we also approximate the uncertainty of the curve (See Figure 5).\n5\nFigure 5: Example of a function (sin(x) + 5% error) approximated by a Polyra swarm. For x = π,\nthe valid range of the model (green region) lies between −0.070 and 0.054, closely following the\nexpectation of 0 ± 0.05. Please note that the generated range is exact, but requires the Rangefinder\nalgorithm as explained in Appendix K and is thus relatively slow (O(0.1s)). More information about\nthis experiment can be found in Appendix U.\n4.5\nPoint generation\nFigure 6: Example of samples generated from a Polyra swarm. We first fit a swarm to represent the\nwhite part of the flag of Türkiye and then generate random samples from this model. These samples\nfit well to the original flag and are visually uniformly distributed over both the half-moon and the\nstar. More information about this experiment can be found in Appendix O. Experiments on higher\ndimensional samples (> 2) can be found in Appendix O.1.\nMost generative models work by considering random noise and learning to transform it into samples\nfollowing the desired distribution Dhariwal and Nichol [2021]. As we can not learn such trans-\nformations, this is impossible with Polyra swarms. However, we can more directly test whether\na sample fulfills p(x) = TRUE. Thus generative polyra swarms are algorithms that efficiently\ngenerate potential candidates inside a given shape. Our example in Figure 6 uses a modification of\nthe HitAndRun algorithm Vempala [2003] to work with non-convex shapes (See Appendix O).\n5\nAbstraction\nOne of the biggest benefits of neural networks and, thus, function approximation is the ability to\nautomatically learn higher-level features during training Kozma et al. [2018]. These tend to generalize\nbetter An et al. [2023] and are a core reason why neural networks work well on high-dimensional\ndata.\nWhile Polyra Swarms can not learn such features during training, they can still do something similar.\nA Polyra Swarm is effectively a tree of logical expressions and we can search for other logical trees\nthat describe approximately the same shape as a given Polyra swarm. Our algorithm for this is rather\n6\ncomplicated and described in Appendices J, K, L and M. In short, our algorithm converts the logical\ntree describing a Polyra swarm into disjunctive normal form while removing mainly redundant terms.\nAn example of what this can do, is shown in Figure 7. The left side shows an effect similar to\noverfitting in neural networks, where the shape contains many holes, which simply did not contain\nany training samples. We call this effect fragmentation. Starting from a quite fragmented shape,\nwhich is described by 2000 submodels, the automatically abstracted version on the right side is\ndescribed by multiple orders less parameters and fits the ground truth significantly better. In fact, this\nlogical tree is simple enough for us to include it in this paper (See Equation 29), in contrast to the\nblack-box problem of neural network approximations Buhrmester et al. [2021].\nvIOU: 0.844\nSize: 72000 floats\nvIOU: 0.994\nSize: 15 floats\nFigure 7: Example of the benefits abstraction can provide. We fit a Polyra swarm to samples uniformly\ndistributed over a simple diamond shape. The learned shape (left plot) does not fit the shape perfectly,\nas shown by the relatively low volumetricIOU (see Appendix D). This could be somewhat manually\nimproved with better hyperparameters (see Appendix E). Instead, we use our abstraction algorithm\n(See Appendix L) to produce the right plot, which fits the ground truth close to perfectly. At the same\ntime, we reduce the complexity of our model by a factor 4800, resulting in a model that is both very\naccurate, fast, and simple enough to be printed into this paper (Equation 7). More information about\nthis experiment can be found in Appendix W.\n\n\n−0.7071\n−0.7072\n−0.7081\n0.7061\n0.7086\n−0.7056\n0.7155\n0.6986\n0.7853\n0.6192\n\n· x ≤\n\n\n−0.5303\n0.1759\n0.1784\n0.8852\n0.8967\n\n\n(7)\nWhile the diamond-shaped toy data in Figure 7 is constructed to be easily described by a few halfspace\nconditions, the same abstraction process also works on real-world datasets. To show this, we consider\nthe classical OldFaithful dataset Azzalini and Bowman [1990] in Figure 8 and the abstracted fit in\nEquation 8.\n\n\n\n\n\n\n\n−0.756\n−0.6546\n0.6207\n−0.784\n0.8259\n−0.5639\n−0.6207\n0.784\n0.7643\n0.6448\n\n· x ≤\n\n\n−0.0522\n0.095\n0.1859\n0.3054\n0.5405\n\n\n\n\n\n\n\n_\n\n\n\n\n\n\n\n\n\n0.4978\n−0.8673\n−0.7498\n0.6617\n−0.1934\n0.9811\n−0.9716\n0.2366\n0.0267\n−0.9996\n−0.9982\n−0.0606\n\n\n· x ≤\n\n\n−0.0259\n0.1568\n0.8063\n−0.2722\n−0.3493\n−0.4175\n\n\n\n\n\n\n\n\n\n(8)\nThe abstracted shape in Equation 8 consists of the union of two polytopes representing the two clusters\nof points in the OldFaithful dataset. We explore further if we can use abstraction as a clustering\nalgorithm in Appendix N.\n7\n2.0\n2.6\n3.4\n4.0\n4.8\nEruption length (min)\n48\n59\n70\n80\n91\nWaiting time (min)\nInitial Fit\nData\nModel\n2.0\n2.6\n3.4\n4.0\n4.8\nEruption length (min)\nAbstracted Model\nFigure 8: Equivalent of Figure 7 on a real-world dataset. Left: Shape approximation of the OldFaithful\ndataset Azzalini and Bowman [1990]. Right: Abstracted version of the same model (See Equation 8).\nMore information about this experiment can be found in Appendix X.\n6\nConclusion, Limitations and Comparison to Neural Networks\n(a) Using a neural network\n(b) Using a Polyra Swarm.\nFigure 9: Comparison of the precision of a Polyra swarm to a neural network. We search for a\nfunction that can decide if a two-dimensional point lies in the graphical representation of the word\n\"Polyra\" or not. While both models theoretically are expressive enough to model such a highly\ncomplicated function, the neural network gets stuck in a local minima and only the polyra swarm\nlearns a shape representing readable text. More information about this experiment can be found in\nAppendix Y.\nWe have shown that polyra swarms can be very useful in certain situations and have nice properties:\nBecause we learn shapes instead of functions, there are tasks like anomaly detection and classification\nthat polyra swarms are more suited to. Similarly, since we do not rely on gradient-based optimization,\nit is impossible to get stuck in local minima, resulting in an often more precise model (See Figure 9).\nAdditionally, using abstraction, we can increase the generalization capabilities, the explainability,\nand the prediction speed of a learned model. However, polyra swarms are not without faults. To help\nmitigate them and provide a fair comparison to neural networks, we will mention them in this section.\n6.1\nHigh Dimensional Data - Precision instead of Scale\nMost of the experiments in this paper have been conducted in two dimensions only. This is partially\nbecause this allows visualizing our results. However, it is also the case that polyra swarms do not\nhandle high-dimensional shapes equally well. Following the proof in Appendix B, we require at\nleast dim + 1 conditions per condition polytope to approximate arbitrary shapes. Additionally,\ngenerative models tend to struggle with high-dimensional data O.1, and so far, the fastest general\nabstraction algorithm has a runtime that is at least cubic in dim (Appendix M). Thus neural networks\nare generally better for high-dimensional data.\nHowever, we find it debatable if it ever should be the goal to replace the very large amount of research\nthat has been put into neural networks already. Instead, we believe there to be tasks where neural\nnetworks and function approximation are best suited, and tasks where shape approximation methods\nlike Polyra Swarms are superior. Furthermore, it is possible to combine the generally highly precise\n8\nPolyra Swarms with existing feature representation algorithms, like PCA Callegari et al. [2011],\nautoencoder Fournier and Aloise [2019] or contrastive approaches Ye et al. [2022]. This reduces the\ndimensionality and cancels out the limitation of Polyra Swarms. We use this in Appendices I and O.1.\n6.2\nBias towards Convex shapes\nSince we represent the base regions (Ai,Bi) in Equation 4 as polytopes, which represent convex\nregions, the resulting fits also have a bias towards learning locally convex shapes (In Figure 9\nthere are more errors inside enclosed regions then next to them. This is even more visible in\nAppendix P). This is a bias since it unreasonably favors one type of shape over another. However,\nsince overfitting/fragmentation represents itself also as holes in a learned shape (Figure 7), it is often\nbeneficial to focus on convex shapes. We include a hyperparameter to control the tradeoff between\nthe convexity and complexity of a learned shape (See Appendix E.4).\n6.3\nLong-tail distributions\nWe generally try to learn a region that contains all possible points. This makes distributions with\nlong tails (like a normal distribution) conceptually challenging. Since the region in which a point\nmight theoretically be observed is often arbitrarily large, we are not able to learn any meaningful\nshape here. In practice, the likelihood of points far away from the center is commonly exponentially\nsuppressed, and thus, we are still able to learn a shape containing all observed points. The limitation\nlies in the fact that this shape will be very fragmented in low-density regions and will not generalize\nwell to other samples observed by the same distribution. However, as Appendix F shows, this can be\nmitigated by a clever choice of hyperparameters.\n7\nFuture Work\nTo propose an algorithm achieving general-purpose learning requires considering many different\nfields. While we believe it to be interesting to focus on the way applying shape approximation\nrequires rethinking existing chains of thought, this also means we could only experiment with most\ntasks qualitatively in the main paper. Thus, we believe this to only be the start of the research into\npolyra-like shape approximation algorithms. Many decades of research time have been spent trying\nto understand and improve neural networks. We believe it is proof of the large potential that Polyra\nSwarms have, that they can already be competitive with neural networks on certain tasks. To further\nhelp with this research, we want to suggest a few research directions.\nWe implement all functionality used in the experiments of this paper into a python library, allowing\nany researcher to use polyra swarms for their own tasks. And while the runtime of polyra swarms\nhas not been a problem so far, we could likely still make this library more efficient by for example\nimplementing GPU support. Similarly, we are interested in different base regions. Polytopes generally\nbecome more complicated in higher dimensions, limiting their applicability without feature extraction.\nInstead, when considering, for example, hyperspheres, this dependency disappears. Additionally,\nsimilar to convolutional layers for neural networks there might be base-regions that are optimal for\ndifferent types of data. Finally, our abstraction algorithms are currently more heuristic than principled,\nwith many design choices significantly influencing their performance. A more systematic study of\nthese choices could lead to substantially more effective abstraction methods.\nReferences\nMichael G. Adam, Martin Piccolrovazzi, Sebastian Eger, and Eckehard Steinbach. Bounding box\ndisparity: 3d metrics for object detection with full degree of freedom. In 2022 IEEE International\nConference on Image Processing (ICIP), page 1491–1495. IEEE, October 2022. doi: 10.1109/\nicip46576.2022.9897588. URL http://dx.doi.org/10.1109/ICIP46576.2022.9897588.\nShengnan An, Zhen Lin, Beichen Chen, Qi Fu, Nian Zheng, and Jianguo Lou. Does deep learning\nlearn to abstract? a systematic probing framework. In Proceedings of the International Confer-\nence on Learning Representations (ICLR), 2023. URL https://openreview.net/forum?id=\nQB1dMPEXau5.\n9\nA. Azzalini and A. W. Bowman. A look at some data on the old faithful geyser. Journal of the Royal\nStatistical Society. Series C (Applied Statistics), 39(3):357–365, 1990. ISSN 00359254, 14679876.\nURL http://www.jstor.org/stable/2347385.\nRichard Bellman. Dynamic programming. Science, 153(3731):34–37, 1966.\nLiron Bergman and Yedid Hoshen. Classification-based anomaly detection for general data. In\nICLR. OpenReview.net, 2020. URL http://dblp.uni-trier.de/db/conf/iclr/iclr2020.\nhtml#BergmanH20.\nAnselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Occam’s razor.\nInformation Processing Letters, 24(6):377–380, 1987. ISSN 0020-0190. doi: https://doi.org/10.\n1016/0020-0190(87)90114-1. URL https://www.sciencedirect.com/science/article/\npii/0020019087901141.\nVanessa Buhrmester, David Münch, and Michael Arens. Analysis of explainers of black box deep\nneural networks for computer vision: A survey. Machine Learning and Knowledge Extraction, 3\n(4):966–989, 2021. ISSN 2504-4990. doi: 10.3390/make3040048. URL https://www.mdpi.\ncom/2504-4990/3/4/48.\nChristian Callegari, Loris Gazzarrini, S. Giordano, Michele Pagano, and Teresa Pepe. A novel\npca-based network anomaly detection. pages 1 – 5, 07 2011. doi: 10.1109/icc.2011.5962595.\nGunnar Carlsson. Topology and data. Bulletin of the American Mathematical Society, 46(2):255–308,\n2009. doi: 10.1090/S0273-0979-09-01249-X.\nYuansi Chen, Raaz Dwivedi, Martin Wainwright, and Bin Yu. Fast mcmc sampling algorithms on\npolytopes. Journal of Machine Learning Research, 19, 10 2017. doi: 10.48550/arXiv.1710.08165.\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal\nProcessing Magazine, 29, 2012.\nJohn Dewey. The Reflex Arc Concept in Psychology, volume 3. The Psychological Review, 1896.\ndoi: 10.1037/h0070601.\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. In Proceedings\nof the 35th International Conference on Neural Information Processing Systems, NIPS ’21, Red\nHook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393.\nBenoit Richard Umbert Dherin, Micheal Munn, and David G. T. Barrett. The geometric occam’s\nrazor implicit in deep learning. ArXiv, abs/2111.15090, 2021.\nQuentin Fournier and Daniel Aloise. Empirical comparison between autoencoders and traditional\ndimensionality reduction methods. In 2019 IEEE Second International Conference on Artificial\nIntelligence and Knowledge Engineering (AIKE), pages 211–214, 2019. doi: 10.1109/AIKE.2019.\n00044.\nMilton Friedman. A comparison of alternative tests of significance for the problem of $m$ rankings.\nAnnals of Mathematical Statistics, 11:86–92, 1940.\nRajib Kumar Halder.\nCardiovascular disease dataset, 2020.\nURL https://dx.doi.org/10.\n21227/7qm5-dz13.\nSongqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. Adbench: Anomaly\ndetection benchmark. In NeurIPS, 2022.\nKilian Hendrickx, Lorenzo Perini, Dries Van der Plas, Wannes Meert, and Jesse Davis. Machine\nlearning with a reject option: A survey, 2024. URL https://arxiv.org/abs/2107.11277.\nChristoph Hofer, Roland Kwitt, Marc Niethammer, and Andreas Uhl. Deep learning with topological\nsignatures. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017.\nSture Holm. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statis-\ntics, 6(2):65–70, 1979. ISSN 03036898, 14679469. URL http://www.jstor.org/stable/\n4615733.\n10\nShehroz S. Khan and Michael G. Madden. A survey of recent trends in one class classification. In\nLorcan Coyle and Jill Freyne, editors, Artificial Intelligence and Cognitive Science, pages 188–197,\nBerlin, Heidelberg, 2010. Springer Berlin Heidelberg. ISBN 978-3-642-17080-5.\nGregory R. Koch. Siamese neural networks for one-shot image recognition. 2015.\nTeuvo Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9):1464–1480, 1990. doi:\n10.1109/5.58325.\nRobert Kozma, Roman Ilin, and Hava T. Siegelmann. Evolution of abstraction across layers in\ndeep learning neural networks. Procedia Computer Science, 144:203–213, 2018. ISSN 1877-\n0509. doi: https://doi.org/10.1016/j.procs.2018.10.520. URL https://www.sciencedirect.\ncom/science/article/pii/S1877050918322294. INNS Conference on Big Data and Deep\nLearning.\nVictor Livernoche, Vineet Jain, Yashar Hezaveh, and Siamak Ravanbakhsh. On diffusion modeling\nfor anomaly detection. In ICLR 2024, 2024. URL https://openreview.net/forum?id=\nlR3rk7ysXz.\nTom M. Mitchell. Machine Learning. McGraw-Hill, New York, 1997. ISBN 9780070428072.\nMadalina Olteanu, Fabrice Rossi, and Florian Yger.\nMeta-survey on outlier and anomaly de-\ntection. Neurocomputing, 555:126634, 2023. ISSN 0925-2312. doi: https://doi.org/10.1016/\nj.neucom.2023.126634. URL https://www.sciencedirect.com/science/article/pii/\nS0925231223007579.\nPanos M. Pardalos. Linear programmingLinear Programming, pages 1276–1278. Springer US,\nBoston, MA, 2001. ISBN 978-0-306-48332-5. doi: 10.1007/0-306-48332-7_261. URL https:\n//doi.org/10.1007/0-306-48332-7_261.\nOliver H E Philcox, Jeremy Goodman, and Zachary Slepian. Kepler’s goat herd: An exact solution\nto kepler’s equation for elliptical orbits. Monthly Notices of the Royal Astronomical Society,\n506(4):6111–6116, May 2021. ISSN 1365-2966. doi: 10.1093/mnras/stab1296. URL http:\n//dx.doi.org/10.1093/mnras/stab1296.\nThomas Polger. Functionalism as a philosophical theory of the cognitive sciences. Wiley Interdisci-\nplinary Reviews: Cognitive Science, 3, 05 2012. doi: 10.1002/wcs.1170.\nImre Pólik and Tamás Terlaky. Interior Point Methods for Nonlinear Optimization, pages 215–\n276. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010. ISBN 978-3-642-11339-0. doi:\n10.1007/978-3-642-11339-0_4. URL https://doi.org/10.1007/978-3-642-11339-0_4.\nChen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt, and Maja Rudolph. Neural transformation\nlearning for deep anomaly detection beyond images, 2022. URL https://arxiv.org/abs/\n2103.16440.\nJ. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993. ISBN 978-1-\n55860-238-0.\nCarl Rasmussen and Zoubin Ghahramani.\nOccam's razor.\nIn T. Leen, T. Dietterich, and\nV. Tresp, editors, Advances in Neural Information Processing Systems, volume 13. MIT\nPress, 2000. URL https://proceedings.neurips.cc/paper_files/paper/2000/file/\n0950ca92a4dcf426067cfd2246bb5ff3-Paper.pdf.\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows.\nIn Proceedings of the 32nd International Conference on International Conference on Machine\nLearning - Volume 37, ICML’15, page 1530–1538. JMLR.org, 2015.\nFrank Rosenblatt. The Perceptron: A Probabilistic Model for Information Storage and Organization\nin the Brain, volume 65. Psychological Review, 1958. doi: 10.1037/h0042519.\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander\nBinder, Emmanuel Müller, and Marius Kloft. Deep one-class classification. In ICML, 2018a.\n11\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander\nBinder, Emmanuel Müller, and Marius Kloft. Deep one-class classification. In Proceedings of the\n35th International Conference on Machine Learning, 2018b.\nLukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gregoire Montavon, Wojciech Samek,\nMarius Kloft, Thomas G. Dietterich, and Klaus-Robert Muller. A unifying review of deep and\nshallow anomaly detection. Proceedings of the IEEE, 109(5):756–795, May 2021. ISSN 1558-\n2256.\nEdward B. Titchener. An Outline of Psychology. Macmillan, 1896.\nSantosh S. Vempala. Hit-and-run is fast and fun. 2003. URL https://api.semanticscholar.\norg/CorpusID:8398546.\nFrank Wilcoxon. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80–83, 1945.\nISSN 00994987.\nK. V. Wilkes. Functionalism, psychology, and the philosophy of mind. Philosophical Topics,\n12(1):147–167, 1981. ISSN 02762080, 2154154X. URL http://www.jstor.org/stable/\n43153849.\nMang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven C. H. Hoi.\nDeep\nLearning for Person Re-Identification: A Survey and Outlook . IEEE Transactions on Pat-\ntern Analysis & Machine Intelligence, 44(06):2872–2893, June 2022. ISSN 1939-3539. doi:\n10.1109/TPAMI.2021.3054775. URL https://doi.ieeecomputersociety.org/10.1109/\nTPAMI.2021.3054775.\nJohn M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic\nprogramming. In Proceedings of the Thirteenth National Conference on Artificial Intelligence\n(AAAI), pages 1050–1055, Portland, OR, USA, 1996. AAAI Press.\nYue Zhao, Zain Nasrullah, and Zheng Li. Pyod: A python toolbox for scalable outlier detection.\nJournal of Machine Learning Research, 20(96):1–7, 2019. URL http://jmlr.org/papers/\nv20/19-011.html.\nAlice Zheng and Amanda Casari. Feature Engineering for Machine Learning: Principles and\nTechniques for Data Scientists. O’Reilly Media, Inc., 1st edition, 2018. ISBN 1491953241.\nArthur Zimek, Matthew Gaudet, Ricardo J.G.B. Campello, and Jörg Sander. Subsampling for efficient\nand effective unsupervised outlier detection ensembles. In Proceedings of the 19th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining, KDD ’13, page 428–436,\nNew York, NY, USA, 2013. Association for Computing Machinery. ISBN 9781450321747. doi:\n10.1145/2487575.2487676. URL https://doi.org/10.1145/2487575.2487676.\nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Dae ki Cho, and Haifeng\nChen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In Interna-\ntional Conference on Learning Representations, 2018. URL https://api.semanticscholar.\norg/CorpusID:51805340.\n12\nA\nNotation Cheat Sheet\nTo help understanding our work, we provide a summary of our used notation in the following table.\nDescription\nSymbol\nShape\nQ (capital letter)\nIndicator function\nq(x) (lowercase\nletter)\nSet of training samples\nXtrain\nPolyra Swarm\nP,p(x)\nBase-shape\nFi, fi(x)\nCondition polytope\nAi\nConsequent polytope\nBi\nApproximation error\nη\nNumber of conditions in a Polytope\nK\nPolytope constraint matrix\nM\nPolytope bounds vector\nb\nNormal distribution\n∼N(mean, std)\nUniform distribution\n∼U(min, max)\nComplement of a shape\nQ∁\nB\nProof of Universal Shape Approximation through Polyra Swarms\nProof. From Definition 1, we know that a shape is a measurable bounded subset of space with a finite\nlength boundary. To prove that a Polyra Swarm can approximate such a set S, we rewrite Equation 3\ninto\nP =\n N\n[\ni=0\nF ∁\ni\n!∁\n=\n N\n[\ni=0\n(Ai \\ Bi)\n!∁\n(9)\nThus, each base shape effectively carves out a region that it considers impossible. We can use 2 · dim\nsuch submodels to reduce every point where one feature h < minh(S) or h > maxh(S). Since S\nis bounded, the resulting shape Papprox is finite and we refer to its volume as V . Any such finite,\nsquare region can be entirely filled with dim-simplices of volume ν. We choose the remaining Ai to\nrepresent these simplices (Thus (KA)i = dim+1) and the matching Bi so that Ai ∩Bi approximates\nAi ∩S best.\nDoing so, for each dim-simplex Ai there are three options.\n1. If Ai ⊆S, we can choose Bi = Ai and incur zero error (η = 0).\n2. Similarly, if Ai ∩S = ∅, we can choose Bi = ∅and also incur zero error (η = 0).\n3. Only if Ai ∩S ̸= ∅and ̸= Ai ⊆S we need to choose a possibly imperfect Bi and might\nincur an error. Importantly this error is still bounded by the volume ν.\nSo the total error of our approximation is bounded by ν · C(ν), where C(ν) is the number of\ndim-simplices of volume ν that intersect the boundary.\nWe want to show now that this term becomes arbitrarily small when we reduce ν: limν→0 ν·C(ν) = 0.\nSince the boundary has finite length, it can be locally well approximated by a (d-1)-dimensional\nhyperplane for sufficiently small ν.\nWe assume each dim-simplex to have approximately equal side-lengths. This means, we expect in\neach dimension there to be\n\u0000 V\nν\n\u0001\n1\ndim dim-simplices, of which every hyperplane only intersects O(1)\nmany and thus the fraction C(ν) ∝O(ν−dim−1\ndim ).\nCombining this together, we find that\nlim\nν→0 ν · C(ν) ∝lim\nν→0 ν · ν−dim−1\ndim\n= 0\n(10)\n13\nThus the erroneous volume η = ∥S \\ P ∪P \\ S∥goes to zero as ν →0 ⇔N →∞, proving our\nconjecture.\nC\nProof that all training samples are included in the shape learned by a\nPolyra Swarm\nProof. Assume that there would be a point x ∈Xtrain, where p(x) = FALSE.\nThis means, there is at least one base shape ∃i, so that fi(x) = FALSE.\nFurther, this implies x ∈Ai and x /∈Bi.\nFocussing on the second part, this means that there ∃j, so that (MB)j\ni · x > (bB)j\ni.\nHowever, following Equation 6, we know that (bB)j\ni = max(x∈Xtrain) ∪(x∈Ai)(MB)j\ni ·x. And since\nx ∈Xtrain and x ∈Ai, we know that (bB)j\ni ≥x and this is contradictory. Thus every sample in the\ntraining set Xtrain lies in the shape P.\nD\nVolumetric IOU\nTo evaluate whether a learned shape (L) fits a given ground truth shape (T), many metrics have been\nsuggested to compare such sets Adam et al. [2022]. Here, we use the Volumetric Intersection Over\nUnion. It is defined as\nvIOU = Volume(L ∩T)\nVolume(L ∪T)\n(11)\nIn case of a perfect fit, L = T and thus vIOU = 1, while in the worst case, there is no overlap\nbetween L and T, and thus vIOU = 0.\nE\nInfluence of various Hyperparameters and Ablation Studies\nPolyra Swarms generally contain many hyperparameters. In each of our experiments, we typically\nshow only the result of a well-parametrized swarm. We keep from optimizing these hyperparameters\nexcessively to show reasonable results possible using Polyra Swarms with limited effort. Still, we\nbelieve it is important to show the effect each hyperparameter has, which is why we study variations\nof each parameter in this section on a toy dataset.\nThis toy dataset is shown in Figure 10 together with a fit using default hyperparameters and another\nfit using a more reasonable choice of hyperparameters (minpoi: 500, extend: 0.1, submodel-count:\n10000). The more reasonable set of hyperparameters produces a better-looking fit, which is also\nconfirmed using the vIOU metric (Appendix D).\nGround Truth\nUsual Hyperp.\nvIOU: 88.18%\nBetter Hyperp.\nvIOU: 95.41%\nFigure 10: From left to right: The ground truth of our toy dataset, the area of a Polyra Swarm with\ndefault hyperparameters fitted to samples following our toy dataset, and the same with better chosen\nhyperparameters.\nWe will now discuss reasonable ranges of each hyperparameter by varying only one parameter in each\nsubsection. For every Polyra Swarm, we state the volume of the learned shape, aswell as the vIOU\n14\nquality compared to the ground truth. We also plot the learned area for one indicative hyperparameter\nvalue.\nEvery other hyperparameter is kept at its default parameter (Table 1). Still, of course, it is often useful\nto vary hyperparameters in tandem (for example one hyperparameter that increases the volume and\none hyperparameter that decreases it). An example of this is studied in Appendix F.\nHyperparameter\nValue\nAdim\n2\nBdim\n2\nExtend\n0\nMinpoi\n0\nQuantile\n0\nSubsample\n0\nSample count\n10000\nModel count\n1000\nTable 1: Default hyperparameter settings used in every experiment unless mentioned differently.\nE.1\nAdim\nThe first hyperparameter we study is the number of conditions in the condition polytope (A in\nEquation 4). To truly approximate every possible shape, we require Adim = dim + 1 (with\nthe dimensionality of the data dim), as seen in Appendix B. Still, most shapes can already be\napproximated well with a lower value of Adim. However, as Figure 11 shows, holes in the learned\nshape become impossible when Adim is chosen too small. Additionally, it can also be useful to\nchoose Adim higher to make the learning algorithm focus more on small features. We do this in the\nthe fit shown in Figure 1.\n2\n4\n6\nAdim\n0.275\n0.300\n0.325\nVolume\nReal Volume\n0.80\n0.85\nvIOU\nAdim=1\nFigure 11: Hyperparameter influence of the Adim hyperparameter.\nE.2\nBdim\nNext, we consider the number of conditions in the consequent polytope in each submodel (B in\nEquation 4) in Figure 12. This parameter is significantly less important than Adim, and serves more\nas a potential speedup. Effectively, it can be seen as a cheaper way of increasing the number of\neffective submodels the swarm uses. Compare for this Figure 12 and Figure 18.\n2\n4\n6\nBdim\n0.30\n0.32\nVolume\nReal Volume\n0.825\n0.850\nvIOU\nBdim=6\nFigure 12: Hyperparameter influence of the Bdim hyperparameter.\n15\nE.3\nExtend\nTo learn the consequent region (Bi in Equation 4), we select the constraint region (bB)i as the most\nextreme values allowed in the condition polytope (See Equation 6). However, it is unlikely that the\nmost extreme value possible in each direction has already been observed in the training set Xtrain.\nThus, we introduce a hyperparameter to extend the range. In total, the range of allowed values is\nevenly extended by a factor 1 + extend. So given values observed between 0 and 1 with an extend of\n0.2, we would allow values between −0.1 and 1.1 in a given submodel.\nAs visible in Figure 13, increasing this parameter can help reduce the fragmentation of the learned\nregion. And while a higher value of extend also increases the volume, there is a sweet spot between\nfalse positives and false negatives.\n0.0\n0.1\n0.2\nextend\n0.32\n0.34\n0.36\nVolume\nReal Volume\n0.850\n0.875\n0.900\nvIOU\nextend=0.04\nFigure 13: Hyperparameter influence of the extend hyperparameter.\nE.4\nMinpoi\nAn alternative to reduce the fragmentation of the learned shape is to demand that a minimum number\nof samples (minpoi) are included in the condition polytope. This removes small condition polytopes,\nwhich tend to lead to highly fragmented shapes. This can be seen as a hyperparameter controlling\nthe convexity of the learned shape. If we demand that every sample is included in each condition\npolytope, it would become trivial and we would only learn the consequent polytopes, resulting in a\nconvex shape. This can be seen in Figure 14, where a high value of minpoi results in the mouth and\neyes not being learned.\n0\n2500\n5000\n7500\nminpoi\n0.31\n0.32\n0.33\nVolume\nReal Volume\n0.845\n0.850\n0.855\nvIOU\nminpoi=7500\nFigure 14: Hyperparameter influence of the minpoi hyperparameter.\nE.5\nQuantile\nThe opposite effect of extend is achieved with the quantile parameter. A quantile > 0 means\nthat only a fraction of 1 −quantile is used to select the most extreme values in Equation 6. This\nhyperparameter breaks the assumption that every training sample is considered inside the learned\nregion (Appendix C). Furthermore, as Figure 15 shows, this parameter increases the fragmentation of\nthe learned shape. This parameter is most useful, when also varying other hyperparameters. See for\nthis Appendix F.\n16\n0.000\n0.005\n0.010\nquantile\n0.1\n0.2\n0.3\nVolume\nReal Volume\n0.25\n0.50\n0.75\nvIOU\nquantile=0.003\nFigure 15: Hyperparameter influence of the quantile hyperparameter.\nE.6\nSubsample\nA classical approach for one-class classification/outlier detection ensemble methods is to use subsam-\npling Zimek et al. [2013] (See Figure 16). Here every submodel is trained only on 1 −subsample of\nthe training samples. This increases fragmentation and also breaks the assumption that every training\nsample is considered inside the learned shape. So far, the only benefit of this parameter we have\nfound is the potential speedup, but we include it here because of potential comparisons to outlier\nensembles.\n0.0\n0.2\n0.4\nsubsample\n0.28\n0.30\n0.32\nVolume\nReal Volume\n0.80\n0.85\nvIOU\nsubsample=0.2\nFigure 16: Hyperparameter influence of the subsample hyperparameter.\nE.7\nDataset Size\nWhile not strictly a hyperparameter, we also study the effect a different training set has. As Figure 17\nshows, a larger data size generally increases both the volume and, at the same time also, the\nperformance. This implies that by reducing the dataset size, we increase primarily the number of false\nnegatives and, thus, the fragmentation. In the extreme, we have so few data points that only these are\nconsidered inside of the shape, and the volume goes to zero from O(100) samples. However, as we\ncan still fit Polyra Swarms to small datasets (e.g. Figure 8 with 272 samples), this is controllable with\nvarious hyperparameters.\n102\n103\n104\n105\ndatasize\n0.0\n0.2\nVolume\nReal Volume\n0.0\n0.5\nvIOU\ndatasize=100000\nFigure 17: Influence of the dataset size/sample count.\nE.8\nEnsemble Size\nThe most important parameter to change for small datasets is the number of submodel equations used.\nSince every equation cuts of parts of the input space, the volume decreases with larger ensembles.\nThis is shown in Figure 17. Thus, to fit Polyra Swarms to smaller datasets, we use fewer submodels,\nand to larger datasets (Figure 1 uses one million submodels), we can and should use more.\n17\n101\n102\n103\n104\ncount\n0.2\n0.4\nVolume\nReal Volume\n0.6\n0.8\nvIOU\ncount=100\nFigure 18: Influence of the number of submodels.\nF\nLinearisation\nIn the previous appendix, we mainly studied the effect singular hyperparameter changes have on a\nlearned model. However, interesting effects also happen when we combine multiple hyperparameters.\nIn Figure 19, we summarize how we can use this to achieve an effect that we call linearisation. This\neffect uses both the quantiles and the extend hyperparameter to, at the same time, cut off extreme\nsamples in long tail distributions and extend the area canceling out this effect.\nFigure 19: Example of a hyperparameter combination that we name linearisation. When approx-\nimating the shape of a distribution with long tails, like the normal distribution shown on the left,\nthe low likelihood of points further away from the mean results in a highly fractured distribution in\nlow-density regions. This can be seen in the fit with default hyperparameters in the middle. Only\n76% of points following the same normal distribution are considered inside of the shape. Instead,\nwe consider a combination of the quantile hyperparameter (quantiles = 0.025) and the extended\nparameter (extend = 0.2). While the quantiles hyperparameter removes noise in the low-density\nregion of the distribution, the extend hyperparameter extends each model to still include the same\nrange. This results in the much better fitting shape on the right side, which considers 98% of normal\nsamples inside the shape. Notice that because of the infinite tails of the gaussian distribution, we can\nnot aim for 100% here.\nG\nPolyra Name Inspiration\nEach submodel of our ensemble cuts away a part of the input space until only the desired shape\nremains. This is very similar to how we imagine piranhas to eat a much larger animal. Thus, we\ndecided to call our method a portmanteau word between \"piranha\" and the \"polytope\"s we use. Thus\nwe call our models \"Polyra Swarms\".\nH\nROC-AUC bias towards Anomaly Scores instead of Binary Decisions\nWhen comparing our Polyra Swarms as anomaly detection algorithms, there is a fundamental\ndifference to most other anomaly detection algorithms. Instead of the binary decisions provided\nby Polyra Swarms, these algorithms usually generate continuous anomaly scores (where a higher\nanomaly score implies a sample to be more anomalous) Olteanu et al. [2023]. As we show in\nFigure 20, this generally increases the ROC-AUC quality such a method achieves, even when the\nmethod used is the exact same.\n18\n−0.10\n−0.05\n0.00\n0.05\n0.10\n0.15\nBinary Threshold\n0.5\n0.6\n0.7\n0.8\n0.9\nROC AUC\nContinuous Score\nBinary Scores\nFigure 20: ROC AUC score on the cardio dataset Halder [2020] with an isolation forest. In red, you\nsee the ROC AUC score when using continuous scores, while the blue curve shows the performance\nwith binary scores (using every possible threshold). The continuous score is at least 6% higher than\nany binary one, showing that we can not fairly compare continuous scores to binary ones.\nI\nMNIST Classification\nWe further extend our classification experiments to higher dimensional MNIST data. Because their\ndistribution is more complicated than the toy dataset in Section 4.2, we generally observe four\noutcomes for a binary classifier:\n• True: The sample is classified as the correct class (and only as the correct class).\n• False: The sample is classified as the wrong class (and only as the wrong class).\n• Outlier: The sample is neither classified as the right or wrong class.\n• Overlap: The sample is classified as both the right and wrong class.\nModel Name\nTrue\nFalse\nOutlier\nOverlap\nDirect Model\n0.8270\n0.0014\n0.0845\n0.0842\nPCA (2 components)\n0.9877\n0.0019\n0.0080\n0.0024\nAnomaly Scores\n0.9943\n0.0024\n-\n0.0033\nTable 2: Performance of three different applications of Polyra Swarm classifiers to the binary MNIST\nclassification task (0 vs 1). We compare four different situations: Either the model correctly classifies\na sample, the model incorrectly classifies a sample, the model considers a sample to be neither class\nor the model could see a sample as either class. Additionally, we consider three different binary\nclassifier. The first one simply trains a Polyra Swarm for each class, the second one does the same,\nbut after applying a pca transformation to reduce the dimensionality. The third model uses not the\nbinary decision of whether a sample is in a class but the anomaly scores as used in Chapter 4.3 to\nconsider where a sample is more likely to match. This makes outliers impossible by construction.\nWe show the fraction of each situation in Table 2. Simply applying polyra classifier to the raw\nimage data is not very effective. While situations where the classifier directly classifies a test sample\nas wrong are very rare, both the situations Outlier and Overlap happen in about 8% of cases.\nWe state our hyperparameters in Table 4, and using various hyperparameters we can change the\ntrade-off between Outlier and Overlap cases (for example using extend). However, using a PCA\nalgorithm to remove every feature except the two most important principle components, we can\nachieve significantly better results, with all error cases much lower than 1%. This is visualized in\nFigure 21.\nWe interpret this effect as indicator that Polyra Swarms don’t work well in high-dimensional data but\nstill have the precision to learn complicated shapes (See Chapter 6.1). We can further improve the\nperformance, by considering anomaly scores (See Chapter 4.3) in the case of outliers. The class that\nconsiders a sample as less anomalous gets assigned to each sample. This roughly halves the overall\nnon-True cases, but also removes the reject option of our classifier.\n19\nFigure 21: Distribution of binary classification task (0 vs 1 on MNIST) in a two-dimensional principal\ncomponent space and learned regions by a Polyra Swarm classifier.\nHyperparameter\nValue\nSample count\n5923 + 6742\nminpoi\n500\nextend\n0.1\nPCA components\n2\nTable 3: Non-default hyperparameter settings used in the mnist classification experiment.\nJ\nAbstraction Procedures\nWe are interested in such algorithms that transform a Polyra Swarm into an approximately similar\nform while drastically reducing its complexity. Since we use a tree-based structure to represent a\nlearned Polyra Swarm, we consider the number of nodes C needed to describe it as a measure of\ncomplexity.\nGenerally, we differentiate between two groups of abstraction procedures. The first one consists\nof simple transformations that slightly reduce the complexity C →C′ ≳C\n2 , while not noticeably\naltering the prediction of the Polyra Swarm. These simplifications are generally extremely fast,\nand will be executed commonly to transform the Polyra Swarm tree in a uniform format. These\nwill be presented in the following subsection J.1. The second, maybe more interesting group of\nabstraction procedures do change the learned prediction of a Polyra Swarm, but reduce the complexity\nsignificantly more C →C′ ≪C. These algorithms are much more complicated and we will\nstudy multiple different algorithms with slightly different approaches and use-cases in the following\nAppendices (K, L, M). All of these algorithms follow the same general structure, which will be\nexplained in Appendix J.2.\n20\nJ.1\nSimplification\nOur simplification procedure generally consists of various transformations that will be applied to\ntransform a Polyra Swarm in tree representation into a uniform form. We will list these transformations\nin this appendix to increase reproducibility, sorted by the type of leaf they are applied to.\nJ.1.1\nAnd-Leaves\nTRUE ∧· · · →· · ·\n(12)\nFALSE ∧· · · →FALSE\n(13)\na ∧(b ∧c) →a ∧b ∧c\n(14)\nJ.1.2\nOr-Leaves\nTRUE ∨· · · →TRUE\n(15)\nFALSE ∨· · · →· · ·\n(16)\na ∨(b ∨c) →a ∨b ∨c\n(17)\nJ.1.3\nNot-Leaves\n¬(A · x ≤b) →−A · x ≤−b −ϵ\n(18)\nThis equation allows us to remove all Not-Leaves from our Polyra Swarm Tree. ϵ represents a small\nconstant and is chosen as ϵ = 10−6.\n¬(a ∨b) →¬a ∧¬b\n(19)\n¬(a ∧b) →¬a ∨¬b\n(20)\n¬¬a →a\n(21)\nJ.2\nGeneral Abstraction\nThe above-described simplification procedure transforms a Polyra Swarm tree into a format that looks\nsimilar to:\n(Halfspace ∨(Halfspace ∧Halfspace)) ∧(Halfspace ∨(Halfspace ∧Halfspace)) ∧· · ·\n(22)\nStill, many of these terms have very similar effects, and it is possible to reduce the amount of these\nterms significantly using the abstraction algorithms we describe in the following appendices. All of\nthe abstraction algorithms that we study here follow the same pattern. We first convert the tree into a\nformat similar to conjunctive normal form (OR Terms below a AND leaf), and transform this form\ninto disjunctive normal form (AND terms below an OR leaf).\nThese transformations rely on well-known logical transformations. Namely\n(a ∨b) ∧(c ∨d) →(a ∧c) ∨(a ∧d) ∨(b ∧c) ∨(a ∧d)\n(23)\nand\n(a ∧b) ∨(c ∧d) →(a ∨c) ∧(a ∨d) ∧(b ∨c) ∧(a ∨d)\n(24)\nThe first transformation is generally fast since the initial, learned tree in Equation 25 is already very\nsimilar to the desired conjunctive normal form and we thus achieve a linear runtime as a function\nof submodels used. However, transforming a large logical tree in conjunctive form (with C1 OR\nleaves containing C2 halfspace conditions each) into the equivalent tree in disjunctive normal form\ngenerally produces a tree containing CC1\n2\ndifferent AND leaves. This is generally infeasible for any\nswarm with more than a few submodels. However, if it would be possible, many of these AND leaves\nare trivial (e.g. a ∧¬a) or duplicates of each other (e.g. a ∨a), possibly resulting in less complexity\nthan the original swarm tree.\nTo circumvent this, we apply our abstraction procedure iteratively.\nInstead of\nq = ABSTRACT(DNF(t1 ∧t2 ∧· · · ∧tN))\n(25)\n21\nWe apply\nqi = ABSTRACT(DNF(qi−1, ti)), q1 = t1, q = qN\n(26)\nThis removes the exponential time cost of Equation 25 when the ABSTRACT procedure is powerful\nenough to guarantee that C(qi) ≤Cmax regardless of the complexity of C(V ti). Instead, we have\nan exponential dependency on (Cmax) since we need to transform an equation containing up to\nCmax terms into a normal disjunctive form for each initial AND leaf. This exponential dependency\ncan be surprisingly useful, since it means that the speed of the algorithm is already an indicator for\nthe resulting complexity reduction of the abstraction. We generally want to build our abstraction\nprocedures and hyperparameters so that C(qi) converges:\nlim\ni→∞C(qi) = C(q) < ∞\n(27)\nWe will now study various ABSTRACT procedures in the following Appendices.\nK\nOne-dimensional Abstraction: The Rangefinder Algorithm\nWe begin by assuming that our input space is one-dimensional. This simplifies creating an abstraction\nalgorithm immensely, as the largest difficulty in creating an abstraction algorithm is to make sure that\nC(qi) ≤Cmax is fulfilled, and more submodels do not require a more complicated description.\nIn one dimension, every halfspace condition can be represented as either x ≥a or x ≤b. Thus, every\npossible shape can be represented as W\ni(ai ≤x ≤bi). Further, we can always find a representation,\nwhere there is no overlap between conditions (ai+1 > bi ∀i), since when two conditions overlap, we\ncan combine them together.\n(a ≤x ≤b) ∨(c ≤x ≤d) ,where (c ≤b ∧a ≤d) = a ≤x ≤d\n(28)\nThus, the resulting maximum complexity is bounded by Cmax = a·(1+Holes) with a small constant\na and the number of holes learned (Holes), guaranteeing effective abstraction.\nIn practice, ABSTRACT1d consists of two steps. We first consider each OR leaf and transform it into\na ≤x ∨x ≤b by considering the most extreme cases (a ≤x ∨b ≤x, a < b →a ≤x). If b < a,\nwe remove the condition and replace it with TRUE. Afterward, we follow the iterative conversion\nprocedure converting to disjunctive normal form and apply the equivalent transformation in each\nAND leaf (a ≤x ∧b ≤x, a < b →b ≤x).\nThis abstraction procedure is significantly faster than the algorithms studied in the following appen-\ndices and, in comparison to the following algorithms, does not need to alter the prediction of a Polyra\nSwarm and gives us a theoretical solution on the abstraction benefits. However, it only works on\none-dimensional swarms. Its current use case in this paper is somewhat hidden. We use it to power\nthe generative model (Appendix O) and for precise range predictions (Sections 4.4 and 4.1).\nL\nN-dimensional Abstraction: Sampling Abstraction\nIn more than one dimension, we are no longer able to reduce every AND or OR leaf into such a simple\nuniform representation. And while there are still many simplifications that can be made, to guarantee\nthat C(qi) is bounded we need to change the predictions of our Model (slightly). As we show in\nthe main paper, this is often beneficial since it counters the fragmentation of our swarm and thus\nreduces overfitting effects. These benefits of reduction are a well-studied effect in machine learning\nand philosophy Rasmussen and Ghahramani [2000], Blumer et al. [1987], Dherin et al. [2021].\nOur procedure ABSTRACTsample consists of three steps.\nL.1\nRedundant ANDs\nThis part considers each AND-like leaf, and thus terms of the form p = h1 ∧h2 ∧h3 ∧· · · , where hi\nare halfspace conditions. We first check if ∃x ∈Rd so that p(x) = TRUE. If not, we can discard the\n22\nleaf (p = FALSE). If we find a single sample that makes this AND-like leaf nontrivial, we consider\nremoving single halfspace conditions and check whether ∃x ∈Rd so that (h2∧h3∧· · · )(x) = TRUE\nbut not h1(x) = FALSE. If none such a sample exists, the condition h1 provides no benefit and is\nremoved. We iterate until there are no more conditions to be removed.\nOur implementation here requires random samples to check whether there exists a sample that is\nfulfilling certain conditions. This dependency on random samples incurs an effectively exponential\ndependency on the dimensionality Bellman [1966], and thus, sample abstraction can only be applied\nto low-dimensional tasks. We consider a better scaling version of this algorithm in Appendix M.\nL.2\nRedundant ORs\nNext, we consider the OR-like leaf, and thus terms of the form s = p1 ∨p2 ∨p3 ∨· · · . Instead of\nrandom samples, we consider the set of points we train on X. We consider each child term and check\nwhether (p2 ∨p3 ∨· · · )(x) = TRUE ∀x ∈X. If this is the case, the term p1 does not provide any\nbenefit and can be removed.\nL.3\nPuzzling\nFinally, we will consider the top-level OR-like leaf again and see if we can merge different terms pi\ntogether. Consider for example:\ns = (p1 ∧p2) ∨(p1 ∧¬p2) = p1\n(29)\nFor this, we consider for each set of two pi = h1\ni ∧h2\ni ∧h3\ni ∧· · · if there is a description that\nmatches both. For this, we exploit that when merging pa and pb, the resulting polytope pm needs to\ncontain only halfspaces that do not contradict either hj\na or hj\nb. Thus, our candidate for the halfspaces\ndescribing pm are those of pa that do not contradict pb and those of pb that do not contradict\npa. This procedure creates a polytope encompassing both initial polytopes and already solves the\nexample in Equation 29. However, it also suggests a hull when the polytopes do not match. So,\nwe need to check whether the resulting volume is larger than the volume of the initial polytopes\nVol(pm) > V ol(pa ∨pb) and reject the combination in this case. To calculate the volume, we again\nuse random samples. We iterate this procedure again until there are no more polytopes to merge.\nSo far, no step in the algorithm has altered the prediction of the Polyra Swarm (if enough samples are\nused for the sampling-based decisions). However, in our experiments, running this algorithm violates\nthe condition that C(qi) is bounded, as the abstraction algorithm requires more and more time in each\nstep. To solve this, we allow the algorithm to approximate slightly by introducing a hyperparameter\nδV . Using this hyperparameter, we allow the merge of two shapes to be slightly bigger than the\ninitial volume\nVol(pm)\nV ol(pa∨pb) ≤1 + δV . This allows the abstraction algorithm to remove small regions\ninside the shape that do not match the expectation, thus preventing fragmentation effects. In our\nexperiments, setting δV = 5% seems to work well.\nM\nAbstraction through Linear Programming\nMany of the decisions of the abstraction algorithm introduced in Appendix L depend on random\nsamples and thus generally might not scale well to high-dimensional tasks. However, we find that\nlarge parts of the abstraction procedure can be replaced by linear programming Pardalos [2001].\nLinear programming is a mathematical method for optimizing a linear objective function, subject to a\nset of linear equality and inequality constraints. Most of the sampling steps in Appendix L can be\nformulated into such a linear program checking whether a set of linear inequality constraints has a\nsolution. We will discuss here quickly how to do so aswell as the limitations of this approach.\nM.1\nTrivial Subterms\nThe first step in Section L.1, checks whether one polytope has a single solution. This can be directly\nexpressed as a linear program, with the halfspace conditions providing the constraints.\n23\nM.2\nRedundant AND-Terms\nSimilarly, our check in Section L.1 for redundant terms iterates over checks that can be expressed as\na linear program. Here h2 · · · hN represent constraints, and by inverting ¬h1 we can also consider\nthis as a further halfspace condition. Notice that the last step requires carefully choosing an ϵ > 0, as\nlinear programs generally assume non-strict constraints. This is less important when using sampling,\nas it is very unlikely that one equation is randomly exactly fulfilled.\nM.3\nRedundant OR-Terms\nOur method in Section L.2 relies not on random samples but on the training data. Since the amount\nof samples does not increase with the dimension, we will keep it as is.\nM.4\nPuzzling\nThe difficult part of our linear programming-based approach is the approximation. While all decisions\nbefore have been of the type ∃x so that · · · , our condition in Section L.3 checks whether there are\ntoo many samples x that are both not in p1 and p2 but are in s. We were not able to solve this using\nlinear programming. Instead, we have to rely on sampling again. Still, we can do better than simply\ndrawing samples from a uniform space. For this, consider that every point of interest should lie in s\nand that we accept a merged description when this polytope contains at most a few samples that are\nnot in p1 and p2. Thus, we can rely on random samples generated from the known polytope s. This is\na well-studied research area Chen et al. [2017] with generally less than exponential time cost. Now\nwe just have to check whether these samples are in p1 or p2.\nM.5\nTime complexity\nWe use sampling-based abstraction in this paper since it is generally more efficient for the two-\ndimensional case, and we cannot visualize higher-dimensional shapes well. However, because\nsampling random points generally might require exponentially many samples in the dimension dim,\nthis approach is less valuable for high-dimensional tasks. In our experiments, the limit seems to be\nroughly dim = 6 on a consumer-grade laptop in a reasonable time.\nTo fix this, we suggest using linear programming. The runtime of this depends on the runtime of\nthe algorithm used to solve linear programs, with some linear programs having time complexity\nO((dim + k)3.5)Pólik and Terlaky [2010], where k is the number of constraints. Additionally, the\nnumber of such checks depends on the hyperparameters chosen but is also generally polynomial in\ndim, resulting in a polynomial runtime.\nStill, even our linear programming abstraction algorithm is very likely also not the best possible\nabstraction algorithm and we look forward to the results of further research.\nN\nClustering through Abstraction\nWhen considering Figure 8 in the main paper, our abstraction process successfully finds the two\nwell-known clusters in the Old-Faithful dataset Azzalini and Bowman [1990]. Here, we want to\nexplore further whether we can use abstraction to find clusters in a dataset. This would be interesting\nsince when we interpret our abstraction algorithm in this way, it generates very efficient, small,\nhuman-readable descriptions for each cluster. One large restriction will be that each abstracted Polyra\nSwarm has to be a set of polytopes connected by OR leaves, limiting the shape of a cluster to be\nconvex.\nWe test this on a toy dataset in Figure 22. We generate data (10k samples) in three uniform\nregions, train a 1000 submodel Polyra Swarm with default parameters, and abstract it with the default\nδV = 5%. This generates the abstract description shown in Equation 30. We show the three polytopes\nfound in different colors in Figure 22, and these match very well with the initial squares (except for\na small deviation in Cluster 3). This proves that there is potential in using polyra abstraction for\nclustering and in a more holistic view of machine learning models and we look forward to considering\nit further in future publications.\n24\nPlease notice that the description of the clusters is still less than optimal. Six conditions describe the\nfirst and second clusters, and seven conditions describe the third cluster. As four conditions would be\noptimal for each cluster, this implies that the description could be abstracted further.\nFigure 22: Toy experiment to show that abstraction can be used for clustering. We generate data\nfollowing one of three square regions and fit a default hyperparameter Polyra Swarm to it. The\nabstraction of this swarm model is the disjunction of three polytopes (Equation 30), which very well\nrepresent the three square regions.\n\n\n\n\n\n\n\n\n\n−0.005\n−1.0\n−1.0\n0.0054\n−0.0159\n0.9999\n−0.9603\n0.2789\n−1.0\n−0.0067\n1.0\n0.0059\n\n\n· x ≤\n\n\n−0.2011\n−0.198\n0.3962\n−0.0839\n−0.2018\n0.4015\n\n\n\n\n\n\n\n\n\n_\n\n\n\n\n\n\n\n\n\n−0.007\n−1.0\n−1.0\n0.0012\n−0.0099\n1.0\n1.0\n−0.0004\n0.0099\n−1.0\n0.8172\n−0.5763\n\n\n· x ≤\n\n\n−0.642\n−0.2493\n0.7968\n0.7497\n−0.6334\n0.2368\n\n\n\n\n\n\n\n\n\n_\n\n\n\n\n\n\n\n\n\n\n\n\n−0.9997\n−0.0236\n−0.8234\n0.5675\n0.0107\n0.9999\n0.998\n0.0624\n−0.0075\n−1.0\n0.0006\n−1.0\n1.0\n0.0067\n0.9631\n−0.2691\n\n\n· x ≤\n\n\n−0.6048\n−0.2704\n0.408\n0.8208\n−0.2051\n−0.1996\n0.802\n0.7124\n\n\n\n\n\n\n\n\n\n\n\n\n(30)\nO\nGenerative Polyra Swarm\nThe Hit-and-Run algorithm Vempala [2003] is a method to generate samples uniformly from a convex\nshape. Starting with a point in the shape, it picks a direction at random. A line through this point\nalong the chosen direction intersects with the border of the shape exactly twice (as the shape is\nconvex), and we pick a random point between these intersections. The process is iterated starting\nfrom this new point until the desired amount of samples is generated.\nTo generalize this behavior to arbitrary shapes, means to consider the case where the number of\nintersections is larger than two. We have already created an algorithm to find the intersection points\nalong a direction in Appendix K. Additionally, we weigh the likelihood of a point in each linear\nsegment by the inverse of its length to achieve somewhat uniformly distributed samples.\n25\nO.1\nGenerating Images\nSimilar to the classification case, we only show the results of our generative model for a toy dataset\nin the main paper. Here, we want to consider the more realistic case by considering MNIST Deng\n[2012] data; namely, we want to generate images of the digit zero.\nWhen training on raw pixel data with 28 = 784 features, we see that every generated image is exactly\nthe same (or very close) as the initial image chosen. This shows a major fault of our generative\nalgorithm. When the learned shape contains many disconnected parts, we need to randomly select\na direction that intersects with one of these disconnected islands. When the volume of the learned\nshape is small and the dimension of the data is high, this is extremely unlikely and thus we never\nleave the initial island, resulting in very similar generated samples.\nTo circumvent this, we again rely on a PCA algorithm to reduce the dimensionality of our feature\nspace. Using this (and the hyperparameters shown in Table 4), we can generate somewhat realistically\nlooking zeros in Figure 23. While these are not optimal, with some versions looking more like\nintersecting rings instead of the digit zero, they are also all new, unique samples. Further it is\nlikely possible to improve this further by using a better feature representation algorithm (like an\nautoencoder Fournier and Aloise [2019]).\nFigure 23: Images of MNIST zeros generated by a Polyra Swarm. Because both Polyra Swarms and\nthe generative algorithm don’t work well on high-dimensional data, we use here a PCA algorithm to\nreduce the dimensionality of the data. The results would likely look even better when using a deep\nlearning-based feature extractor like an autoencoder Fournier and Aloise [2019]. The hyperparameters\nof this experiment can be found in Table 4.\nHyperparameter\nValue\nSample count\n5923\nminpoi\n1000\nPCA components\n3\nTable 4: Non-default hyperparameter settings used in the image generation experiment.\nP\nPolyra-based Optimization\nWhile Polyra Swarms are designed to not require optimization, this does not mean that we can’t\nuse them to find solutions to optimization problems. However, like with most of the applications in\nSection 4, it requires rethinking how we approach these tasks. We show this with Kepler’s equation\n(Equation 31). While this equation is technically simple, finding an inverse solution E(M) is still\npart of active research Philcox et al. [2021] and is most easily done by numerical methods minimizing\nEquation 32.\nM(E) = E −e sin(E)\ne = 1.0\n(31)\nL = ∥M(E) −(E −e sin(E))∥\n(32)\nWe can do something similar using a Polyra Swarm, by searching for the shape created by Equation 32\nbetween L,M and E. By afterward demanding that L = 0, we can find a shape representing the\nrelationship between M and E. This resulting shape (assuming the hyperparameters given in Table 32)\n26\nis shown in Figure 24. While this is by no means the fastest (or most accurate) way of inverting\nEquation 31, it still shows the benefit of having ensembles of small submodels that are easy to\nmanipulate. And instead of finding a singular solution we effectively find almost all solutions at the\nsame time.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nE\n0.00\n0.05\n0.10\n0.15\n0.20\nM(E), e=1.0\nmissed\nmodel\nfound\nFigure 24: Example of how to use a Polyra Swarm to find solutions to Equation 31. We show the\nregion that the swarm believes to be part of the solution in green, and the actual solution in either\nblue or red, depending on whether the swarm believes this equation to be valid.\nHyperparameter\nValue\nSample count\n100000\nminpoi\n1000\nextend\n0.015\nTable 5: Non-default hyperparameter settings used in the optimization experiment.\nQ\nSetup - Mandelbrot fit\nTo achieve the highly accurate fit shown in Figure 1, the largest challenges stem from the very\nlarge dataset we use. We generate a dataset from points that lie in the mandelbrot set2 (don’t violate\n∥Z∥≤2 in 100 steps). We generate 50000 values in each direction (Thus 50, 0002 = 2, 500, 000, 000\nsamples in total) from −2 ≤x ≤1.5 and −2 ≤y ≤2. All points are used as a test set, while only\nthe points that seem to lie in the mandelbrot set are used for training the Polyra Swarm(roughly 10%).\nThis setup makes false negatives impossible since no training sample can lie considered outside of the\nshape(Appendix C). We chose this here to show the low bias of our model and also because repeating\nthis experiment would have been computationally infeasible. In fact, to train this model, we heavily\nparallelize our computation, by noticing that we can combine the output of two Polyra Swarms as\np = min(p1, p2). To focus on the small details of the mandelbrot set, we choose a higher Adim = 6.\nThe full hyperparameters can be found in Table 6. We train 40000 chunks of 25 submodels each.\nEach of our submodels takes about 40min because of the extreme number of training samples (over\n250 million). In total this results in a training time of approximately three years of CPU time.\nHyperparameter\nValue\nSample count\n276147856\nModel count\n1000000\nMinpoi\n10\nAdim\n6\nTable 6: Non-default hyperparameter settings used in the mandelbrot experiment.\n2Technically, the mandelbrot set has a boundary of infinite length and thus does not fulfill our Definition 1 to\nqualify as a shape. Thus we more accurately try to approximate the shape of the set describing a given finite\nresolution of samples following the mandelbrot set.\n27\nR\nSetup - Matching Algorithm\nAs an example of how to use shape approximation for a task that is classically solved by function\napproximation, we consider matching tasks (e.g. a dating app algorithm, matching advertisement to\nthe users who are the most likely to engage with it, ...). As an example we consider a situation where\nboth participants to be matched are described by singular features. This means a successful match is\ndescribed by a point in two-dimensional space. Thus, we search for the shape describing this region\nto see what potential successful matches could look like. Our hyperparameters for this can be found\nin Table 7.\nFurthermore, we can input known information of one participant to reduce the dimensionality of\nthe search space. Using the abstraction procedure described in Appendix J, we can simplify its\ndescription to create a personal description of how a successful match would look like for a given\nparticipant.\nHyperparameter\nValue\nSample count\n10000\nModel count\n5000\nMinpoi\n2500\nExtend\n0.035\nTable 7: Non-default hyperparameter settings used in the matching algorithm experiment.\nS\nSetup - Classification\nWe compare our classification setup qualitatively on a common toy dataset. For this, we use Polyra\nSwarms (which hyperparameters can be found in Table 8) and a neural network (Whose structure\ncan be found in Table 10 and whose hyperparameters can be found in Table 9). We further include a\nreject option for the neural network, where uncertain values with probabilities between 0.05 −0.95\nare rejected.\nTo use Polyra Swarms for classification, we use one Polyra Swarm for each class. This produces a\nfunction that measures association for each class. If there is exactly one class that a sample seems to\nassociate with, we classify the sample as this class. However it is possible that more or fewer classes\nconsider a sample to match this group. This situation and an example of classification for real-world\ndatasets are discussed in Appendix I.\nHyperparameter\nValue\nSample count\n2500 × 2\nModel count\n1000 × 2\nMinpoi\n500\nExtend\n0.1\nTable 8: Non-default hyperparameter settings used for polyra in the classification experiment.\nHyperparameter\nValue\nEpochs\n100\nBatchsize\n32\nPatience\n5\nLearning rate\n0.001\nOptimizer\nAdam\nTable 9: Hyperparameter settings used for the neural network in the classification experiment.\nT\nSetup - Anomaly Detection\nFor our experiment on using Polyra Swarms for anomaly detection, we follow a recent anomaly detec-\ntion benchmarking paper Han et al. [2022]. Following this, we use 121 common anomaly detection\n28\nLayer\nType\nDetails\n1\nInput\nshape = 2\n2-4\nDense (3x)\n64 units, ReLU activation\n5\nDense (Output)\n2 units, Softmax activation\nTable 10: Neural network architecture used in the classification experiment.\nbenchmarking datasets, extracted from various situations. Thus, while every dataset is converted into\na tabular format, some contain medical data and some image or text-based data. We choose as our\ncompetitors recent deep learning anomaly detection algorithms. These include NeuTral Qiu et al.\n[2022], Diffusion Time Estimation (DTE) Livernoche et al. [2024], GOAD Bergman and Hoshen\n[2020], DAGMM Zong et al. [2018], DeepSVDD Ruff et al. [2018a] and Normalizing Flow Rezende\nand Mohamed [2015]. We use the implementation and hyperparameters of either pyod Zhao et al.\n[2019] or the original papers. We further consider each algorithm in the one-class setting, where only\naccess to normal samples is given.\nAnomaly detection is usually evaluated using continuous anomaly scores and not using the binary\ndecisions (normal or not normal) Polyra Swarms generate. Because continuous scores generally\nresult in a higher anomaly score (See Appendix H), we need to propose a way of generating\nanomaly scores from a Polyra Swarm. These are less than optimal since, e.g., abstraction generally\nchanges these scores. However, it is also rare that anomaly detection requires continuous scores\nin practice. Generally, anomaly detection algorithms also propose continuous scores to allow for\nvarying thresholds, but it is also possible to control these thresholds through hyperparameters.\nWe propose here two ways of generating continuous anomaly scores:\nscore1(x) = meani(fi(x))\n(33)\nscore2(x) = meani(x ∈(Ai ∩Bi))\nmeani(x ∈Ai)\n(34)\nAdditionally, since we now only calculate fractions instead of the most extreme decision by every\nmodel, the dependency of the volume on the number of submodels (as shown in Appendix E.8)\ndoes no longer apply. Instead using larger ensembles only reduces random noise and thus increases\naverage performance. This is shown in Table 11. In fact, when using even more submodels than the\n400000 used here, the anomaly detection performance would likely still improve. However, since\nthese these experiments take around 1h of average computation time per dataset (for a total time of\nroughly five days), we did not test this further.\nIt seems like the second anomaly score (Equation 34), which calculates the number of submodels\nwhere the submodel is fulfilled divided by the number of submodels where the sample is included in\nthe condition polytope, performs better. We use the largest ensemble with the second anomaly score\nin Figure 4. However, the difference to the simpler to calculate first anomaly score (Equation 33) is\nsmall for large ensemble sizes. Also, it is likely that by varying hyperparameters, we could increase\nthe performance further (Table 12).\nSize\nPerformance (1)\nPerformance (2)\n1000\n0.7253\n0.7395\n10000\n0.7641\n0.7683\n100000\n0.7814\n0.7839\n400000\n0.7842\n0.7887\nTable 11: Average anomaly detection performance with different ensemble sizes and anomaly score\nfunctions.\nHyperparameter\nValue\nSample count\nvariable\nModel count\nup to 400000\nTable 12: Non-default hyperparameter settings used in the anomaly detection experiment.\n29\nU\nSetup - Regression\nWe approximate the shape spanned by the function sin(x) and an uncertainty of 5% using the\nhyperparameters shown in Table 13. Using this shape, we find the region inside the shape for a given\ninput value (here x = π). The ground truth is given by sin(π) = 0. The learned region ranging\nfrom −0.070 and 0.054 also represents the 5% uncertainty well. We find these (exact) limits with the\none-dimensional abstraction described in Appendix K.\nHyperparameter\nValue\nSample count\n10000\nMinpoi\n1000\nExtend\n0.0125\nTable 13: Non-default hyperparameter settings used in the regression experiment.\nV\nSetup - Turkish Flag\nWe describe the algorithm generating random points in Appendix O. Here, we test the distribution of\npoints by first learning a shape representing the white part of the Turkish flag (Using the hyperparam-\neters in Table 14) and then generating points in this shape. As Figure 6 shows, these points match\nquite well to the original flag.\nHyperparameter\nValue\nSample count\n27052\nModel count\n5000\nextend\n0.025\nminpoi\n1000\nGenerated Samples\n500\nTable 14: Non-default hyperparameter settings used in the Turkish flag experiment.\nW\nSetup - Diamond Abstraction\nTo show the effect of abstraction, we generate samples uniformly distributed in a 45 rotated rectangle.\nWe fit a Polyra Swarm with the hyperparameters in Table 15 to this data and abstract it with sample\nabstraction (Appendix L) using default parameters.\nHyperparameter\nValue\nSample count\n10000\nModel count\n2000\nTable 15: Non-default hyperparameter settings used in the diamond abstraction experiment.\nX\nSetup - Old Faithful Abstraction\nTo further show the benefits of abstraction, we use the OldFaithful dataset Azzalini and Bowman\n[1990]. This dataset contains a link between the time between eruptions and the length of the following\neruption for the Old Faithful Geyser in Wyoming. We fit a Polyra Swarm with the hyperparameters in\nTable 16 to this data and abstract it with sample abstraction (Appendix L) using default parameters.\nY\nSetup - Text Approximation\nFinally, to show the precision of a Polyra Swarm, we generate data following the word \"Polyra\". We\nwant to decide if a two-dimensional point is inside the region covered by this text (There would be\nink at this point when writing the word \"Polyra\" on Paper). We do this first using only samples inside\n30\nHyperparameter\nValue\nSample count\n272\nModel count\n100\nextend\n0.1\nminpoi\n100\nTable 16: Non-default hyperparameter settings used in the old faithful experiment.\nthe text and a Polyra Swarm with the hyperparameters in Table 17. Additionally, we do the same\nwith both samples inside and outside of the text using a neural network with the setup described in\nTable 19 and the hyperparameters described in Table 18. For the neural network fit, we try to simplify\nthe task as much as possible: First, we use both negative and positive samples to not have to rely on\none-class classification algorithms, which tend to perform worse than algorithms with more explicit\nsupervision Ruff et al. [2021]. Additionally, we use the same data for both training and testing in\nboth cases, which makes overfitting impossible to detect. However still, we observe that the neural\nnetwork regression understands the structure of the text significantly worse compared to the Polyra\nSwarm. We interpret this as the neural network optimization reaching a local minimum and a benefit\nof the fact that Polyra Swarms do not require optimization and thus can not get stuck in local minima.\nHyperparameter\nValue\nSample count\n59240\nModel count\n10000\nextend\n0.01\nminpoi\n500\nTable 17: Non-default hyperparameter settings used for polyra in the text approximation experiment.\nHyperparameter\nValue\nSample count\n280053\nEpochs\n100\nBatchsize\n32\nPatience\n5\nLearning rate\n0.001\nOptimizer\nAdam\nTable 18: Hyperparameter settings used for the neural network in the text approximation experiment.\nLayer\nType\nDetails\n1\nInput\nshape = 2\n2-4\nDense (3x)\n64 units, ReLU activation\n5\nDense (Output)\n2 units, Softmax activation\nTable 19: Neural network architecture used in the text approximation experiment.\nZ\nHardware setup\nFor every experiment, except the mandelbrot fit (Figure 1) and the anomaly detection performance\n(Figure 4), we use only a consumer-grade laptop to conduct our experiments. We use a Thinkpad T14\nwith an Intel i7-1260p (16 cores) and 32gb RAM and every experiment takes less than 10min. No\nGPU is used during any experiments.\nThe anomaly detection experiment (Figure 4) is conducted on a server with an Intel Xeon w9-3495X\nCPU with 112 cores and 512gb of RAM. The mandelbrot fit is conducted on a cluster with varying\nCPU’s and 40gb of RAM per process. These experiments also do not use any GPU and more details\nabout their runtime can be found in Appendix T and Appendix Q.\n31\n",
    "content": "### 1. What is the core content and main contribution of this paper?\n\nThis paper introduces a completely new machine learning method called **Polyra Swarms (Multi-body Swarms)**, which achieves learning tasks by approximating shapes rather than functions. Polyra Swarms can perform universal learning with very low bias and may outperform neural networks in certain tasks, such as anomaly detection. Additionally, the paper proposes an **automatic abstraction mechanism** that significantly simplifies the complexity of Polyra Swarms, thereby enhancing its generalization ability and transparency.\n\n**Main Contributions:**\n- A new machine learning paradigm based on shape approximation: **Polyra Swarms**.\n- Demonstrates the application potential of Polyra Swarms in tasks such as classification, regression, and anomaly detection.\n- Introduces an automatic abstraction mechanism to simplify Polyra Swarm models, improving their interpretability and generalization capabilities.\n\n---\n\n### 2. What are the breakthroughs or innovations of this paper?\n\n**Breakthroughs and Innovations:**\n- **New Learning Paradigm:** Polyra Swarms provide a learning approach different from traditional function approximation—shape approximation. This method is particularly suitable for tasks requiring regional modeling, such as anomaly detection and classification.\n- **Non-gradient Optimization:** Polyra Swarms do not rely on gradient descent optimization, avoiding local optima, which makes them superior in some complex tasks.\n- **Automatic Abstraction:** Provides a logical abstraction mechanism that simplifies complex Polyra Swarm models into forms that are easy to understand and compute while retaining predictive power.\n- **Cross-domain Applicability:** The method is not only applicable to low-dimensional data but also capable of handling high-dimensional data when combined with feature extraction techniques, showcasing its broad applicability across various tasks.\n\n---\n\n### 3. Based on the content of this paper, what are some good ideas for startup projects?\n\nGiven the characteristics of Polyra Swarms, here are some potential startup project directions:\n\n#### **1. Anomaly Detection Service**\nPolyra Swarms excel in anomaly detection and can be applied in areas such as industrial monitoring, cybersecurity, and medical diagnostics. For example:\n- Develop a real-time anomaly detection tool for monitoring equipment failures on production lines.\n- Create a cloud-based service to help businesses detect cyberattacks or fraudulent behavior.\n\n#### **2. Explainable AI Platform**\nSince Polyra Swarms can generate easily understandable rule sets through symbolic abstraction, they are well-suited for building explainable AI systems. For example:\n- Build credit scoring models for the financial industry that provide clear decision-making rationale.\n- Develop diagnostic assistance tools for the medical field to help doctors understand the logic behind model predictions.\n\n#### **3. Data Generation Tool**\nPolyra Swarms can efficiently generate data points that conform to specific shape distributions, which can be used for data augmentation or simulation scenario generation. For example:\n- Develop a data augmentation tool to expand training datasets, especially in image and text generation.\n- Create a virtual environment generator for autonomous driving or robot training.\n\n#### **4. Few-shot Learning Solution**\nPolyra Swarms have advantages in few-shot learning and can be used for rapid model deployment in resource-constrained environments. For example:\n- Develop lightweight machine learning models for edge devices that support local inference without relying on the cloud.\n- Provide customized few-shot learning solutions to address cold-start problems or personalized recommendation systems.\n\n#### **5. Shape Modeling and Analysis Tool**\nLeveraging Polyra Swarms' strong shape modeling capabilities, specialized tools can be designed to analyze complex geometric structures. For example:\n- Develop a space utilization optimization tool for architectural design.\n- Create microstructure modeling and analysis software for materials science.\n\nThese projects not only fully utilize the technical advantages of Polyra Swarms but also meet practical application needs, offering high commercial value and development potential.",
    "github": "",
    "hf": ""
}