{
    "id": "2510.26790",
    "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
    "summary": "This paper introduces a new task called Gistify, which requires language models to generate a minimized code file to reproduce a specific functionality in a code repository. It discovers that the current state-of-the-art models perform poorly on the Gistify task, especially when it comes to tasks with longer execution trajectories.",
    "abstract": "As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.",
    "category1": "Theoretical Foundations",
    "category2": "",
    "category3": "Multi-Agent",
    "authors": "Hyunji Lee,Minseon Kim,Chinmay Singh,Matheus Pereira,Atharv Sonwane,Isadora White,Elias Stengel-Eskin,Mohit Bansal,Zhengyan Shi,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan,Lucas Caccia",
    "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "",
    "keypoint": "Current state-of-the-art models struggle to reliably solve Gistify tasks, especially with long execution traces.\nFaithfully reproducing the test function in the generated file is a strong indicator of gistified performance.\nEnabling execution tools yields small but consistent performance gains.\nProviding global code context and runtime information further boosts performance.\nAgentic models benefit from dynamically deciding what to read and refining reasoning through multi-step trajectories.",
    "date": "2025-11-02",
    "paper": "Gistify! Codebase-Level Understanding via\nRuntime Execution\nHyunji Lee∗1, Minseon Kim2, Chinmay Singh2, Matheus Pereira2,\nAtharv Sonwane3, Isadora White4, Elias Stengel-Eskin5, Mohit Bansal1, Zhengyan Shi2,\nAlessandro Sordoni2, Marc-Alexandre Côté2, Xingdi Yuan2, Lucas Caccia∗2\n∗Equal contribution\n1University of North Carolina at Chapel Hill\n2Microsoft Research\n3Cornell University\n4University of California San Diego\n5University of Texas at Austin\nhyunjil@cs.unc.edu\ndebug-gym@microsoft.com\nhttps://microsoft.github.io/debug-gym\nAs coding agents are increasingly deployed in large codebases, the need to automatically design challenging,\ncodebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create\na single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The\ncoding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command),\nand the generated file must replicate the output of the same command ran under the full codebase,\nwhile containing only the essential components necessary to execute the provided command. Success\non Gistify requires both structural understanding of the codebase, accurate modeling of its execution\nflow as well as the ability to produce potentially large code patches. Our findings show that current\nstate-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.\n1\nIntroduction\nLarge language models (LLMs) are increasingly being used in code-related tasks, powering applications in\ndebugging (Yuan et al., 2025) and agentic code generation (Yang et al., 2024; Liang et al., 2025). Thus,\nthe ability to handle isolated snippets and reasoning across entire codebases, including complex file and\nmodule relationships, is becoming increasingly essential. Yet, the evaluation toolkit for assessing such\ncapabilities has lagged behind. Recent evidence shows that widely-adopted repository-level benchmarks such\nas SWE-bench (Jimenez et al., 2024) and RepoBench (Liu et al., 2023b) still do not require full reasoning over\nthe whole execution and could be solved through heuristic shortcuts or retrieval of localized patches (Aleithan\net al., 2024; Liang et al., 2025). Moreover, because many of these datasets rely on GitHub issues or pull\nrequests for construction, they are not easily generalizable to arbitrary repositories. At the same time,\ncoding agents are increasingly deployed in large, real-world codebases, highlighting the need for automatically\nconstructed, broadly applicable, and more challenging repository-level evaluation.\nTo fill this gap, we introduce the Gistify task, which is deliberately inspired by a common practice of how\ndevelopers navigate and understand unfamiliar repositories. Rather than reading files in isolation, they start\nfrom a concrete execution point such as test command or entry script often mentioned in READMEs. Then,\nthey iteratively reason over the runtime behavior such as identifying dependencies, following control paths to\nuncover the codebase’s structure and functionality. Gistify formalizes this practice by requiring an (agentic)\ncoding model to extract the gist of a given command, i.e. to generate a single, self-contained, minimal,\nand executable gistified file that faithfully reproduces the runtime behavior of a given command as when\nusing the original full codebase (Figure 1). In addition to serving as a challenging coding task, such gistified\nrepositories might give human coders a better understanding of a specific functionality of a given codebase,\nor even a way to export the single functionality of interest without inheriting heavy dependencies.\nTo perform well in Gistify, an agent should generate a single gistified file that satisfies four key requirements:\nit should be self-contained, including all necessary components from the codebase so that it can be executed\nindependently; it should ensure execution fidelity, producing the same outputs as the original codebase\nunder the given command; it should satisfy minimality, retaining only the essential code required for\nexecution without redundant or extraneous lines; and it should guarantee faithful preservation, avoiding\nhallucinated or fabricated code and relying solely on content from the original codebase. To assess model\nperformance, we introduce evaluation metrics that align with these requirements, providing a systematic\narXiv:2510.26790v1  [cs.CL]  30 Oct 2025\nGistify! Codebase-Level Understanding via Runtime Execution\nfrom requests.compact import Morsel\nfrom adapters import HTTPAdapter\nclass TestMorsel:\n    morsel = Morsel()\n...\ndef test_cookie():\n    s = TestMorsel()\n    s.mount(HTTPAdapter(0, 0))\ntest_requests.py\nfrom http.cookies import Morsel\n...\ncompact.py\ndef _basic_auth(username):\n    ...\nauth.py\nfrom auth import _basic_auth\nclass BaseAdapter:\ndef auth(self):\n    _basic_auth(self.name)\nclass HTTPAdapter(BaseAdapter):\n    def __init__(self):\n        ...\n        self.auth()\nadapters.py\nCodebase\npytest test_requests.py::test_cookie\nCommand\nfrom http.cookies import Morsel\ndef _basic_auth(username):\n    ...\nclass BaseAdapter:\n    ...\nclass HTTPAdapter(BaseAdapter):\n    ...\nclass TestMorsel: \n    morsel = Morsel()\n    ...\ndef test_cookie():\n    s = TestMorsel()\n    s.mount(\"http://\", HTTPAdapter(0, 0))\ngistified_file.py\nGistify\nFigure 1: The Gistify task: given a codebase and a command of entrypoint, the goal is to generate a\nminimal, self-contained gistified code file that faithfully reproduces the original runtime behavior using code\nfrom the given codebase.\nway to measure codebase-level understanding. Gistify requires agents to follow the execution path through\nthe codebase without bypassing modules, i.e., understanding how relevant objects are modified along the\nway, and identifying which classes or functions can be simplified or removed. Since even moderately sized\ncodebases exceed the context window of current LLMs, success also requires effective search capabilities.\nThe advantages that Gistify brings are multiple: first, it provides direct insight into the ability of models\nto reason at the codebase level with an understanding of runtime execution, rather than on isolated code\nsnippets. Second, it is lightweight and broadly applicable: it requires only the repository and an entrypoint,\nwithout issued logs or pull requests. This allows automatic construction of challenging tasks for any arbitrary\nrepositories, including private ones. Finally, gistified files themselves are valuable outputs: by compressing\na specific feature of a large codebase into a minimal file, they can be applied to various downstream tasks,\nincluding automated debugging or error localization.\nWe conduct experiments across a variety of frameworks (mini-SWE-agent, SWE-agent, and Copilot) and\nmodels (GPT-5-mini, GPT-5, Claude-3.7-Sonnet, and Claude-Sonnet-4) and uncover several interesting\nfindings. First, even widely used, high-performing frameworks and models struggle to create a successful\ngistified file, especially when execution traces are long and have high coverage on the repositories. Second,\nfaithfully reproducing the test function in the generated file is a strong indicator of gistified performance, as it\nserves as the starting step for reasoning about execution traces. Third, enabling execution tools yields small\nbut consistent performance gains, and additionally providing global code context and runtime information\nfurther boosts performance. Finally, agentic models benefit from dynamically deciding what to read and\nrefine their reasoning through multi-step trajectories, outperforming static approaches.\n2\nGistify\n2.1\nTask Definition\nAs shown in Figure 1, when given a codebase and a command as input, the coding agent must generate a\nsingle gistified file that reproduces the runtime behavior of the original codebase under the given command.\nSpecifically, the gistified file must satisfy the following requirements.\nSelf-Contained: All necessary components from the given codebase must be included so that the gistified\nfile can be executed standalone, i.e. without relying on the codebase. The model must identify all relevant\nmodules and dependencies, demonstrating understanding of inter-file relationships.\nExecution Fidelity: Executing the gistified file must replicate the original codebase’s runtime behavior,\nensuring the model captures the dynamic execution, not just static code patterns.\n2\nGistify! Codebase-Level Understanding via Runtime Execution\nMinimalism: Only the code essential to reproducing the runtime behavior should be preserved, with unused\nfunctions and objects pruned. This requires fine-grained understanding of the code to identify which lines are\nactually executed and essential for the task.\nGrounded Preservation: No hallucinated code may be introduced. All content must be derived directly from\nthe original codebase. This ensures the task evaluates the model’s understanding of the codebase, rather\nthan its ability to generate arbitrary code that happens to satisfy the command.\n2.2\nEvaluation Protocol\nThere are two inputs to a Gistify task: i) a docker image containing the target codebase, for consistent\nevaluation; ii) an entrypoint, such as a pytest command on one of the tests in the codebase. Test cases are\nexisting entrypoints one can easily leverage, but broadly, any command that the user would want to use to\nrun a functionality of the existing codebase is allowed.\nAll models are prompted to generate a gistified file for the entrypoint. We can programmatically verify\nwhether the expected behavior is preserved when the ground-truth test is run within this setup. Here, we\nfocus on comparing outputs of test commands. Once the model generates the gistified file, to ensure that\nexecution for evaluation is based on the original test, we integrate the test code from the original codebase to\nthe gistified file and execute it. This ensures that the model does not cheat by modifying the test.\n2.3\nMetrics\nOnce a gistified file is generated, we evaluate it using the given execution command. The evaluation considers\nthree dimensions, aligned with the task requirements, to provide a comprehensive measure of a model’s ability\nto reason over an entire codebase and understand its execution behavior. See Appendix A.1 for more details.\nExecution Fidelity is a binary metric where 1 means the gistified file runs successfully and produces the same\noutput as the original codebase when executed under the given command; otherwise, it is 0. Failures include\ncases where the file is not runnable or yields different outputs. The comparison checks for tests pass/fail\nconsistency and stdout/stderr matching.\nFormally, let c denote the given command, C a given codebase, and G a gistified file. Define runs(c, C) as\nan indicator of whether c executes without crashing when running over C, and out(c, C) returns the set of\noutputs and error traces from running c with C. Then, execution fidelity is defined as\n1\n\u0002\nruns(c, G) ∧(out(c, G) = out(c, C))\n\u0003\n,\n(1)\nwhere 1[·] is the indicator function.\nLine Execution Rate measures minimality by calculating the fraction of lines in the gistified file that are\nactually executed under the given command. A 100% execution rate means all lines are essential, indicating\na focused and concise file. This metric is only computed for files that run successfully, since the execution\ntrace is required to determine which lines are run.\nFormally, let Lexec(G) be a list of executable lines (i.e., no comments) in G. Then, the Line Execution rate is\ndefined as\n1\n|Lexec(G)|\nX\nℓ∈Lexec(G)\n1[ℓis executed].\n(2)\nLine Existence Rate measures the proportion of code in the gistified file that is directly preserved from the\noriginal codebase. Specifically, lines of code are grouped into blocks (classes, functions, or top-level units),\nand matches are computed block by block while respecting the code hierarchy. This helps avoiding false\nmatches from common lines appearing in unrelated parts of the codebase. To ensure robustness, we normalize\nacross common variations such as indentation, multi-line statements, and imports. A 100% existence rate\nindicates full fidelity to the original codebase without hallucination.\n3\nGistify! Codebase-Level Understanding via Runtime Execution\nFormally, let BG and BC be the sets of blocks in the gistified file and the original codebase, respectively. For\na block b, let L(b) represent its set of lines. Then, the existence rate is defined as\n1\nP\nb∈BG |L(b)|\nX\nb∈BG\nX\nℓ∈L(b)\n1{ℓ∈LC(b)} ,\n(3)\nwhere 1{ℓ∈LC(b)} = 0, if no matching block exists in BC.\n3\nExperiments\n3.1\nSetting\nWe conduct experiments using three widely adopted open-sourced frameworks. SWE-Agent (Yang et al.,\n2024) and GitHub Copilot (Microsoft, 2025) provide a rich scaffolding to LLM-based agents, enabling them\nto autonomously perform software engineering tasks. This includes a set of tools for creating and editing\ncode files, navigating repositories, and executing tests. These frameworks also offer the LLM controllable\ncache management, and LLMs follow the standard tool-calling format. We also experiment with Mini-SWE-\nAgent (Yang et al., 2024), a lightweight framework where LLMs only have access to a bash terminal to solve\nthe task. Commands are parsed from the agent output and executed directly. As the task objective is for the\nmodel to use reasoning over the execution flow rather than the ability of tool usage, for the agentic models,\nwe exclude the execution tools (“python”, “pytest”) in the default setting where execution is disabled.\nOur evaluation spans four leading LLM variants: GPT-5 (OpenAI, 2025a), GPT-5-mini (OpenAI, 2025b),\nClaude-3.7-Sonnet (Anthropic, 2025a), and Claude-Sonnet-4 (Anthropic, 2025b), offering different cost /\nperformance tradeoffs. For ease or reading, we will refer to the last two models as Claude-3.7 and Claude-4.\nWe use a 128K token limit for all models. All experiments run are capped at 50 steps, after which whatever\nis generated at this moment in the gistifed file is submitted for evaluation.\nOn the data side, we experiment with widely used GitHub repositories which are present in SWE-\nBench (requests, pylint, flask, scikit-learn, seaborn).\nWe also explore an additional repository,\ndebug-gym (Yuan et al., 2025)1. This library is relatively new and importantly does not overlap with\nSWE-Bench. We extract and filter test sets for each repository. Namely, we remove tests whose execution is\ndependent on the test’s file location. For the main experiment, we evaluate over 25 tests for each of the 5\nrepositories. More details regarding the evaluation setup and prompt can be found in the Appendix A.\n3.2\nResults\nWe begin by giving an overview of the main results presented in Table 1. We report results for our main\nevaluation protocol, where the model does not have access to execution tools (e.g. “python” and “pytest”\ncommands), as well as the alternative. Examples of gistified files are in Appendix B.1.\nStrong models and frameworks still struggle with Gistify task. Across models and execution frameworks,\nperformance remains limited: even the strongest model with strong framework (Copilot with Claude-4)\nachieves 58.7% average Execution Fidelity, a binary success/fail metric, indicating that reliably producing a\ncorrect gistified file is still challenging. Among the models evaluated, Claude-4 tends to perform best; however,\nperformance drops sharply on the hard subsets (Section 4.2), suggesting that the benchmark can scale in\ndifficulty and will remain a meaningful target as future models strengthen and require more challenging\nevaluations.\nDifferent model families exhibit distinct strengths. Claude-4 achieves the highest Line Existence scores,\nindicating that it most faithfully extracts relevant code from the original codebase. In contrast, GPT-5\nproduces the most concise outputs, with a substantially higher Line Execution rate than other models. We\n1We provide a link to all the GitHub repositories used in this work in Table 4.\n4\nGistify! Codebase-Level Understanding via Runtime Execution\nTable 1: Average Performance over three agentic frameworks with four models. We evaluated over 25 tests\nover 5 repositories. Execution Fidelity is shown as w/o exec, and w execution tools. Line Existence and\nExecution are average across the two settings for clarity.\nFramework\nModel\nExecution Fidelity\nLine Existence\nLine Execution\n(wo exec / w. exec)\nmini-SWE-agent\nGPT-5-mini\n17.1 / 24.0\n44.9\n61.2\nGPT-5\n51.0 / 54.0\n56.8\n83.1\nClaude-3.7\n38.7 / 43.3\n66.0\n69.2\nClaude-4\n54.0 / 55.3\n67.0\n75.7\nSWE-agent\nGPT-5-mini\n30.9 / 45.3\n47.9\n74.8\nGPT-5\n30.7 / 46.0\n48.3\n81.7\nClaude-3.7\n40.7 / 46.0\n66.8\n69.9\nClaude-4\n56.7 / 57.3\n66.3\n72.9\nCopilot\nGPT-5-mini\n58.0 / 55.3\n62.4\n77.8\nGPT-5\n58.7 / 60.7\n66.9\n81.4\nClaude-3.7\n43.3 / 56.0\n63.0\n74.4\nClaude-4\n58.7 / 61.3\n69.6\n80.3\nobserve a similar trend for GPT-5-mini and Claude-3.7: in general, GPT models achieve higher Line Existence,\nwhereas Claude models achieve higher Line Execution.\nSmall(er) models perform well with scaffolding. We note that GPT-5-mini’s performance varies significantly\nacross different evaluation settings, from 17% in a bash-only setup to 58% when provided with a large inventory\nof tools from the Copilot framework (see Appendix B.3 for a full list). We note that this performance increase\nis also reflected in the quality of the generated gist, where we see a notable increase in line existence and line\nexecution.\nFrontier models (GPT-5 / Claude-4) are strong bash users. When looking at performance on mini-swe-agent,\nwhere the models only have access to a bash terminal to solve the task, both models perform relatively well,\nsolving over half of the tasks. Importantly, this is not the case for smaller and previous-generation models.\nExecution tools are not a silver bullet. Overall, when comparing performance with and without execution in\nTable 1, we note that in most cases we observe only a small performance gain. We expected that current\ncoding LLMs could better leverage execution tools: indeed, using tools specifically for runtime execution\nanalysis, such as a debugger, could significantly help solving a gistify task. However, we are not seeing this\nbehavior emerge, even from frontier models. We observed a sharp decrease in performance for the GPT-5\nmodel when evaluated on SWE-Agent without execution tools. We performed a visual inspection and noticed\nformatting issues when rewriting the input test function. A detailled discussion can be found in Appendix B.2.\n3.3\nError Analysis Over Execution Failure\nWe proceed with an analysis of the underlying failure causes, in order to understand which aspect of the\nGistify task different models struggle with. Table 2 shows that each model tends to fail for different reasons.\nSee Appendix B.4 for detailed examples of each error case.\nImport Error occurs when the model incorrectly imports the original codebase (e.g., import requests)\ninstead of inlining the required modules into the gistified file. We note that this error occurs even as coding\nLLMs are explicitly prompted not to import the specific packages in question. Perhaps surprisingly, the best\nperforming model, Claude-4, commits this seemingly innocuous error the most out of all four models.\nFile Creation Failure errors arise when the model fails to generate the gistified file. This can happen in two\nways: the model exceeds the maximum step limit, or the model terminates the task without any file being\ngenerated.\n5\nGistify! Codebase-Level Understanding via Runtime Execution\nTable 2: Average error rates (%) of different failure reasons when running SWE-agent across models. Error\ncases are categorized into four groups. The numbers in parentheses indicate the number of errors for each\ncategory.\nModels\nImport Error\nFile Creation Failure\nMissing Test Function\nPytest Runtime Error\nGPT-5-mini\n2.1 (2)\n11.3 (11)\n76.3 (72)\n10.3 (10)\nGPT-5\n5.2 (4)\n10.4 (8)\n77.9 (60)\n6.5 (5)\nClaude-Sonnet-3.7\n20.0 (10)\n20.0 (10)\n2.0 (1)\n58.0 (29)\nClaude-Sonnet-4\n32.5 (13)\n10.0 (4)\n7.5 (3)\n50.0 (20)\nMissing Test Function errors occur when the generated gistified file does not contain the function implemen-\ntation for the test specified in the given command, or implements the test in a different structure. This can\nhappen when the model strips out the content of the test and executes it outside of the pytest wrapper,\nunder e.g. if __name__ == __main__:. Claude models tend to avoid this mistake, while this is the main\nsource of error for GPT-5 models, specifically under the SWE-agent framework. Importantly, we observe that\nthis error does not happen at random, but rather alongside other execution errors; we attempted to add the\nmissing test function, and it in most cases the test fails to run, i.e. it results in a runtime error. This aligns\nwith the analysis in the next section, showing a strong correlation between the task’s success and the fidelity\nbetween the original and the generated tests.\nPytest Runtime Error occurs when the execution of the generated file fails, either due to a runtime error or\nbecause the gistified output does not match the output from the original codebase. The results indicate this\nis the most common cause of error for the best performing model, Claude-4.\n3.4\nImportance of Faithfully Preserving the Test Function\nWe observe that models frequently modify the test function, despite being provided with explicit instructions\nto copy without modification, except for unavoidable adjustments (e.g., removing imports). Again, to\nensure consistent evaluation, we replace the test function in the gistified file with the original version before\nevaluation.\nTo measure such modifications, we define the Test F1 Score as the line-level overlap between the test code\nof the original file and the gistified version. High Test F1 Score indicates that the model has successfully\nidentified and copied the correct test function to the gistified file. We observe a strong correlation between\nTest F1 Score and execution fidelity (correlation=0.76, p=0.01); test instances with higher F1 scores are\nsubstantially more likely to produce a successful gistified file. We hypothesize that this arises because in the\nGistify task, models often reason backwards from the test file, thereby if the model fails from identifying or\ncopying the test function, the subsequent reasoning process is highly likely to fail.\nTo better understand the impact of the first step—searching, viewing, and copying the test function—we\nconduct an ablation study where we remove potential failure at this stage. Specifically, we explicitly provide\nthe correct test function body and signature in the prompt, so the model no longer needs to locate or copy\nit. This isolates the effect of errors in identifying the test function. In this setting, we observe that Test\nF1 Score improves highly from the base Gistify 68.4 to 85.3, along with execution fidelity (from 42.0% to\n60.0%). This suggests that accurately handling the test function is a critical first step to do the Gistify task\nsuccessfully. Detailed results are in Appendix B.5.\n4\nAnalysis\nIn this section, we analyze how different strategies and tools affect performance on the Gistify task, identify\nfactors that contribute to its difficulty, and experiment with the use of a static coding LLM to gain a\ndeeper understanding of the task. For all experiments, we evaluate 50 test instances drawn from the pylint\ncodebase, a setting where the model generally exhibited modest performance. We use SWE-Agent paired\nwith Claude-Sonnet-4.\n6\nGistify! Codebase-Level Understanding via Runtime Execution\nTable 3: Analysis of the effect of different strategies and tools (global information, execution) on the Gistify\ntask. We evaluate SWE-Agent with Claude 4 using 50 test instances from the pylint codebase. Max Steps\nReached (%) indicates the percentage of runs that terminated because the maximum step limit was reached.\nAblation\nType\nExecution Fidelity\nLine Existence\nLine Execution\nMax Steps Reached (%)\nBase Gistify\n42.0\n65.0\n58.3\n14.6\nPrompted Strategies\nTracing\n48.0\n75.4\n62.8\n0.0\nReading\n50.0\n77.6\n62.6\n3.9\nGlobal Info (Tool)\nRepoGraph\n52.0\n76.1\n60.1\n6.0\nTracing\n56.0\n75.1\n65.1\n0.0\nExecution (Tool)\nBash\n52.0\n73.1\n64.2\n16.0\nEdit And Execute\n56.0\n74.3\n64.2\n10.0\n4.1\nEffect of Various Strategies and Tools\nIn this section, we analyze how different strategies and sources of information affect model performance.\nWe begin with the simplest approach, modifying the prompt to guide the model (Prompt-Based Guidance),\nand then move to more explicit approaches that rely on additional tools: providing global context (Global\nInformation via Tools) or feedback from code execution (Execution-Based Tools). Detailed descriptions of\nprompts and tools, along with examples, are provided in the Appendix C.1.\nPrompt-Based Guidance\nWe first begin with the simplest approach: modifying the prompt to provide explicit\ntask guidance. We experiment in two settings. In the former, we prompt the model to perform step-by-step\nreasoning, by first predicting the execution traces and then going over them, adding relevant code snippets\nalong the way (tracing). In the latter, a similar approach is used, with explicit instructions on how to\nrecursively determine the execution traces: starting from the test, identify the relevant components and\nread the files where they are defined, and repeat until the end (reading). As shown in Table 3, we observe\nthat adding such strategies tends to enhance overall metrics, giving both better execution fidelity and more\nfaithful code extractions, as measured by line existence.\nGlobal Information via Tools\nBuilding on the above observation, we next assess the effect of explicitly\nproviding global context through external tools, rather than predicting it. We examine two tools: (1)\nRepoGraph (Ouyang et al., 2024), which constructs a graph of the codebase where each node represents a line\nof code and edges capture connections between lines, enabling graph-based search over the entire codebase;\nand (2) a Tracing tool that exposes gold execution traces obtained from running the given test command.\nResults in Table 3 show that both tools improve performance, with the Tracing tool yielding the largest gains.\nThis finding suggests that access to the global context, especially the gold tracing information, substantially\nstrengthens the model’s ability to perform runtime reasoning, as it can easily identify which file to look at.\nExecution-Based Tools\nIn Section 3.2, we saw that enabling execution tools resulted in small but consistent\ngains overall. In this section, we examine whether having unrestricted access to a bash terminal is really\nnecessary to observe these gains, or whether simply having access to execution logs of the generated file is\nenough. For this experiment we compare Bash access with a simple method that executes and prints the\noutput of the gistified file whenever it is edited (Edit And Execute). No other execution tools are available to\nthe agent, including runtime information about the ground truth test. The results are surprising: having\naccess to fewer tools actually increases performance. Indeed, we note that when give access to a full set of\nbash commands, the coding LLM tends to explore more tools, increasing the overall trajectory length, and\npotentially reaching the maximum step limit.\n4.2\nTests with High Coverage are Harder to Gistify\nIn this section, we investigate what properties makes a given test hard to Gistify. We hypothesize that tests\ngenerating a longer and more complex execution trace would entail a harder task for the coding LLM. To this\nend, we investigate how two axes to measure a runtime execution’s difficulty affect performance: the length\nof trace, as measured by the number of function calls executed, and the number of unique files touched by\n7\nGistify! Codebase-Level Understanding via Runtime Execution\n[0.0, 0.2]\n[0.2, 0.4]\n[0.4, 0.6]\n[0.6, 0.8]\n[0.8, 1.0]\nBinned Test Quantiles according to difficulty metric\n20\n40\n60\n80\n100\nExecution Fidelity (%)\nPerformance according to Exec. Trace Difficulty\nTrace Length\nNumber of Files Covered\n(a) Difficulty of the Gistify task is measured as a function\nof the execution trace difficulty of the underlying test.\nExecution\nFidelity\nLine\nExistence\nLine\nExecution\n0\n20\n40\n60\n80\nScores\nstatic coding LLM\nmini-SWE-Agent\nSWE-Agent\nCopilot\n(b) Performance of a static coding LLM and various agentic\ncoding LLMs (mini-SWE-Agent, SWE-Agnet, Copilot).\nthe tracing procedure. While these metrics correlate with one another, they will differ when, for example, a\nfunction is looped over many times or when the location of the relevant functions is in a single file versus\nacross multiple files.\nFor this experiment, we use again the same configuration as prior analysis, namely Claude-4 with 50 tests\nsampled from the pylint codebase. In Figure 2a, we see a clear correlation between the difficulty of a given\nGistify task, and how complex the execution traces are, according to both metrics considered. We leverage\nthis insight to create a Gistify-hard subset, where we select the 30 most difficult examples according to each.\nWe end up with 57 unique datapoints (30 from pylint, 28 from sklearn, 6 from seaborn). On this subset,\nperformance drops to 21%, as compared to 43%, the baseline weighted performance average following the\nsame distribution over repositories. Overall, this selection criteria offers a promising direction for designing\nchallenging evaluation scenarios with Gistify.\n4.3\nStatic Coding LLM\nIn this section, we experiment over how models perform in a static setup, where they have no access to tools\nand cannot iterate on the generated solution. As such static coding LLMs do not have tools, they cannot\nsearch or view files dynamically. Thereby, to measure a possible upper bound for non-agentic approaches, we\nprovide as input all files that were accessed during the original program execution (gold files). Also, as they\ncannot iterate over multiple steps, they have to output everything at once and are therefore restricted by the\ncontext window of the LLM. Since solving the Gistify task involves touching multiple files, we observe in\nmany cases that the inputs exceed the model’s maximum sequence length. Thus, we sample a subset of test\nexamples where the combined content fits within the 128K token limit of the LLM. As shown in Figure 2b,\nagentic models outperform static ones even when the latter receive all relevant files. This suggests that\nselecting files dynamically over multiple iterations is more effective than providing everything at once, which\ncan overwhelm the model2. However, interestingly, the static coding LLM setup achieves the highest Line\nExistence score. This is likely because the model can copy lines directly from input, yet it performs worse\non Line Execution and Execution Fidelity, suggesting that models do not have a good understanding of the\ncodebase, often copying lines that are incomplete or incorrect.\n2See Appendix C.2 for detailed statistics on the usage of various tools.\n8\nGistify! Codebase-Level Understanding via Runtime Execution\n5\nRelated Works\n5.1\nCodebase-level Understanding Benchmark\nPrevious work has introduced a variety of benchmarks to evaluate LLMs on codebase-level code understanding.\nThese generally fall into three categories: question answering, code synthesis, and mapping natural language\nspecifications to the entire codebase. Several benchmarks introduce codebase-level question-answering (Strich\net al., 2024; Li et al., 2024b; Sahu et al., 2024; Chen et al., 2025; Hu et al., 2024; Fu et al., 2025). In these\nsettings, the model must correctly answer questions that require an understanding of the codebase. The\nquestions are drawn from various sources, including real-world GitHub issues and queries resembling those\nasked of tools like Copilot. Another line of work evaluates whether models can synthesize code by leveraging\ninformation distributed across multiple files in the codebase (Zhang et al., 2023; Liu et al., 2023b; Ding\net al., 2023; Li et al., 2024a; Yu et al., 2024). These benchmarks include tasks such as retrieval-augmented\ncompletion, cross-file refactoring, and more specialized settings such as sketch-based coding or codebase\nevolution. Moreover, there is a line of benchmark that maps natural language specifications to entire code\nrepositories, leveraging hierarchical or multi-stage representations to capture inter-file relationships and\nmaintain consistency across a codebase (Tang et al., 2023; Zan et al., 2024; Ni et al., 2025). Our work tackles\na more complex setting, where models must reason over full execution traces and examine multiple files,\nmaking the task challenging, and even widely used agentic models struggle alongside static ones.\nThere are also works that isolates (or sandboxes) functionalities from the codebase, to simplify dependencies\nwhile preserving executability (Xie et al., 2025b; Jain et al., 2024). This sandboxing step is similar to Gistify\nin that it tries to construct a simplified file that has the feature extracted. However, the sandboxing step is\ndone programmatically: relevant code snippets are extracted by leveraging the (static) call graph. In contrast,\nour work focuses on generating a simplified file using an LLM, thereby evaluating the model’s ability to\nreason about both code dependencies and runtime behavior. Notably, while prior work acknowledges cases in\nwhich static programmatic sandboxing fails (e.g., when functions have large dependency slices) and discards\nthose examples, we consider them informative because they require reasoning about more complex runtime\nbehavior. We further observe that these instances also present challenging examples for the Gistify task.\n5.2\nMethods for Codebase-Level Understanding\nRecent work on autonomous agents for codebase-level code understanding has focused on improving code\nnavigation, reasoning, and generation through structured representations and planning. Approaches leverage\nstructural information of code for function-call graphs, module-dependency graphs, and hierarchical code\nstructures to provide models with core components of repositories (Wang et al., 2025; Liu et al., 2024).\nAnother line of work integrate multi-step reasoning and state update policies to enable more effective planning\nover complex tasks (Bairi et al., 2024; Gautam et al., 2025). Additional methods combine various agents with\nmultiple tools to streamline codebase-level exploration and task solving (Luo et al., 2024; Zhang et al., 2023;\nShrivastava et al., 2023; Wang et al., 2024; Yang et al., 2024; Tang et al., 2023; aider, 2025; Microsoft, 2025;\ncursor, 2025).\n5.3\nRuntime Execution\nVarious works have introduced benchmarks to evaluate LLMs’ ability to reason over code execution at\nruntime (Gu et al., 2024; Chen et al., 2024; Xie et al., 2025a; Beger & Dutta, 2025; Hu et al., 2025). These\nbenchmarks typically test whether models can predict execution traces or intermediate states such as variable\nvalues, control flow, or data dependencies—given code and inputs, or alternatively, infer inputs from code\nand outputs. Some benchmarks further extend this paradigm by leveraging execution traces to construct new\nproblems through program composition, thereby varying complexity in a principled way. Beyond evaluation,\nexecution traces have also been incorporated into training pipelines to strengthen models’ runtime reasoning\nabilities (Liu et al., 2023a; Ding et al., 2024). By augmenting pre-training and fine-tuning with execution\nstates, paths, and coverage signals, these methods help models capture program dynamics and generalize to\nexecution-aware tasks. At inference time, several frameworks leverage runtime feedback to iteratively guide\nmodels in debugging or completing partial programs, thereby improving performance on execution-driven\n9\nGistify! Codebase-Level Understanding via Runtime Execution\ntasks (Zhong et al., 2024; Xue et al., 2024). In this work, we extend prior approaches by going beyond\nreasoning over execution traces to also reformulate programs; the model not only tracks execution but also\nidentifies how to compress and organize code into a concise, coherent file. We further show that this capability\nserves as a useful tool at inference time, helping models better structure and complete execution-driven tasks.\n6\nDiscussion and Conclusion\nIn this paper, we introduced the Gistify task in which a coding LLM extracts a specific funtionality\nof a codebase into a single, self-contained file. Beyond serving as a standalone evaluation task that is\neasily applicable to arbitrary repositories and execution commands, the gistified file itself also opens several\npromising directions for research and practical applications. Large codebases often overwhelm automated\nagents due to their complex dependencies, and they especially struggle when tasked with fixing bugs that\nspan multiple files (Ganhotra, 2025). In such scenarios, a gistified file would greatly reduce this challenge,\nand enable a more efficient reasoning about the codebase without navigating through unrelated code. In\nother words, this file could be leveraged in other downstream tasks, such as code refactoring or debugging, or\neven as a way to extract and share a minimal implementation of a specific codebase functionality.\nIn summary, with coding LLMs increasingly being deployed in real-world software development, the need\nfor automatically constructing evaluation setups that require codebase-level understanding of arbitrary\nrepositories is growing. Through extensive experiments across a range of models and frameworks, we found\nthat state-of-the-art LLMs still face challenges on the Gistify task, especially when faced with long, complex\nexecution traces. Our analysis shows that incorporating global code context or execution-aware tools improves\nperformance, and agentic coding LLM tend to handle the task more effectively by reasoning about which\nfiles to inspect using various tools. Beyond serving as a benchmark, the gistified files themselves are valuable\nartifacts. They distill the essential functionality of complex systems into a compact, executable form, making\nthem easier to inspect and understand. Such files could support a range of practical applications, including\ndebugging, refactoring, and code review, which we leave for future work.\n10\nGistify! Codebase-Level Understanding via Runtime Execution\nReferences\naider. Ai pair programming in your terminal. 2025. URL https://github.com/Aider-AI/aider?tab=\nreadme-ov-file.\nReem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang.\nSwe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024.\nAnthropic. Claude sonnet 3.7. https://www.anthropic.com/news/claude-3-7-sonnet, 2025a. Hybrid\nreasoning model; accessed: 2025-09-25.\nAnthropic. Claude sonnet 4. https://www.anthropic.com/claude/sonnet, 2025b. Improved version over\nSonnet 3.7; accessed: 2025-09-25.\nRamakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram\nRajamani, Balasubramanyan Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms\nand planning. Proceedings of the ACM on Software Engineering, 1(FSE):675–698, 2024.\nClaas Beger and Saikat Dutta. Coconut: Structural code understanding does not fall out of a tree. In 2025\nIEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pp. 128–136. IEEE,\n2025.\nJialiang Chen, Kaifa Zhao, Jie Liu, Chao Peng, Jierui Liu, Hang Zhu, Pengfei Gao, Ping Yang, and Shuiguang\nDeng. Coreqa: uncovering potentials of language models in code repository question answering. arXiv\npreprint arXiv:2501.03447, 2025.\nJunkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. Reasoning runtime behavior of a\nprogram with llm: How far are we? arXiv preprint arXiv:2403.16437, 2024.\ncursor. cursor. 2025. URL https://cursor.com/.\nYangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan,\nRamesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: A diverse and multilingual benchmark\nfor cross-file code completion. Advances in Neural Information Processing Systems, 36:46701–46723, 2023.\nYangruibo Ding, Benjamin Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. Traced: Execution-\naware pre-training for source code. In Proceedings of the 46th IEEE/ACM International Conference on\nSoftware Engineering, pp. 1–12, 2024.\nLingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu, Zongyu Wang, Lin Qiu, Xunliang\nCai, Xuezhi Cao, et al. Corecodebench: A configurable multi-scenario repository-level benchmark. arXiv\npreprint arXiv:2507.05281, 2025.\nJatin Ganhotra.\nDo swe-agents solve multi-file issues like humans?\na deep dive into swe-\nbench verified, January 2025.\nURL https://jatinganhotra.dev/blog/swe-agents/2025/01/05/\nswe-bench-mutliple-files/. Blog post.\nDhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, and Roshanak Zilouchian Moghaddam. Refac-\ntorbench: Evaluating stateful reasoning in language agents through code. arXiv preprint arXiv:2503.07832,\n2025.\nAlex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang.\nCRUXEval: A benchmark for code reasoning, understanding and execution. In Ruslan Salakhutdinov,\nZico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp\n(eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of\nMachine Learning Research, pp. 16568–16621. PMLR, 21–27 Jul 2024.\nRuida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, and\nCuiyun Gao. Coderepoqa: A large-scale benchmark for software engineering question answering. arXiv\npreprint arXiv:2412.14764, 2024.\n11\nGistify! Codebase-Level Understanding via Runtime Execution\nWenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, and Kaidi Xu. Dynacode: A dynamic\ncomplexity-aware code benchmark for evaluating large language models in code generation. arXiv preprint\narXiv:2503.10452, 2025.\nNaman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2e: Turning any\ngithub repository into a programming agent environment. In ICML, 2024.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan.\nSwe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R\nNarasimhan.\nSWE-bench: Can language models resolve real-world github issues?\nIn The Twelfth\nInternational Conference on Learning Representations, 2024.\nJia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. Evocodebench: An evolving code generation\nbenchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599, 2024a.\nLinyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and\nHongxia Yang. Infibench: Evaluating the question-answering capabilities of code large language models.\nAdvances in Neural Information Processing Systems, 37:128668–128698, 2024b.\nShanchao Liang, Spandan Garg, and Roshanak Zilouchian Moghaddam. The swe-bench illusion: When\nstate-of-the-art llms remember instead of reason. arXiv preprint arXiv:2506.12286, 2025.\nChenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan,\nand Nan Duan.\nCode execution with pre-trained language models.\nIn Anna Rogers, Jordan Boyd-\nGraber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL\n2023, pp. 4984–4999, Toronto, Canada, July 2023a. Association for Computational Linguistics.\ndoi:\n10.18653/v1/2023.findings-acl.308. URL https://aclanthology.org/2023.findings-acl.308/.\nTianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-\ncompletion systems. arXiv preprint arXiv:2306.03091, 2023b.\nXiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, and Wenmeng\nZhou. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv\npreprint arXiv:2408.03910, 2024.\nQinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai\nLin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repository-level code\ndocumentation generation. arXiv preprint arXiv:2402.16667, 2024.\nMicrosoft.\nGithub copilot in vs code.\n2025.\nURL https://code.visualstudio.com/docs/copilot/\noverview.\nZiyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun,\nHongzhang Liu, et al. Gittaskbench: A benchmark for code agents solving real-world tasks through code\nrepository leveraging. arXiv preprint arXiv:2508.18993, 2025.\nOpenAI. Gpt-5 technical overview. https://platform.openai.com/docs, 2025a. Accessed: 2025-09-25.\nOpenAI. Gpt-5 mini. https://platform.openai.com/docs/models/gpt-5-mini, 2025b. Compact variant\nof GPT-5; accessed: 2025-09-25.\nSiru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming\nZhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph.\narXiv preprint arXiv:2410.14684, 2024.\nSurya Prakash Sahu, Madhurima Mandal, Shikhar Bharadwaj, Aditya Kanade, Petros Maniatis, and Shirish\nShevade. Codequeries: A dataset of semantic queries over code. In Proceedings of the 17th Innovations in\nSoftware Engineering Conference, pp. 1–11, 2024.\n12\nGistify! Codebase-Level Understanding via Runtime Execution\nDisha Shrivastava, Denis Kocetkov, Harm De Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion:\nTraining code models to understand your repository. arXiv preprint arXiv:2306.10998, 2023.\nJan Strich, Florian Schneider, Irina Nikishina, and Chris Biemann. On improving repository-level code QA\nfor large language models. In Xiyan Fu and Eve Fleisig (eds.), Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 209–244,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. ISBN 979-8-89176-097-4. doi:\n10.18653/v1/2024.acl-srw.28.\nXiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai\nAn, Ruijun Huang, et al. Ml-bench: Evaluating large language models and agents for machine learning\ntasks on repository-level code. arXiv preprint arXiv:2311.09835, 2023.\nHuacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du,\net al. Repomaster: Autonomous exploration and understanding of github repositories for complex task\nsolving. arXiv preprint arXiv:2505.21577, 2025.\nXingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song,\nBowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist\nagents. arXiv preprint arXiv:2407.16741, 2024.\nDanning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, and Xiangyu Zhang.\nCore: Benchmarking llms code reasoning capabilities through static analysis tasks.\narXiv preprint\narXiv:2507.05269, 2025a.\nYiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. Repost: Scalable\nrepository-level coding environment construction with sandbox testing. Conference on Language Modeling,\n2025b.\nZhipeng Xue, Zhipeng Gao, Shaohua Wang, Xing Hu, Xin Xia, and Shanping Li. Selfpico: Self-guided partial\ncode execution with llms. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software\nTesting and Analysis, pp. 1389–1401, 2024.\nJohn Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik R Narasimhan, and Ofir\nPress. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth\nAnnual Conference on Neural Information Processing Systems, 2024.\nHao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang,\nand Tao Xie. Codereval: A benchmark of pragmatic code generation with generative pre-trained models.\nIn Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1–12, 2024.\nXingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee,\nLucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, et al. debug-gym: A text-based\nenvironment for interactive debugging. arXiv preprint arXiv:2503.21557, 2025.\nDaoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin\nChen, Bei Guan, et al. Codes: Natural language to code repository via multi-layer sketch. arXiv preprint\narXiv:2403.16443, 2024.\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and\nWeizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation.\narXiv preprint arXiv:2303.12570, 2023.\nLi Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger via\nverifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906, 2024.\n13\nGistify! Codebase-Level Understanding via Runtime Execution\nA\nExperimental Setting\nA.1\nMetrics\nExecution Fidelity\nExecution fidelity measures whether the generated gistified file reproduces the same\nfunctional behavior as the original codebase under the given command. This includes producing the same\nnumber of test passes or failures, as well as consistent outputs and error handling. If the file’s behavior\nmatches the original codebase, it is assigned 100%; otherwise, it receives 0%.\nLine Execution Rate\nThe line execution rate measures the proportion of lines in the gistified file that are\nactually executed when running it under the given command. We first analyze the gistified file to identify\nwhich lines are executable (e.g., imports, function or class definitions) versus not-executable (e.g., comments).\nUsing a tracing function, we then determine which of the executable lines are touched during execution.\nThe line execution rate is computed as the fraction of executable lines that are executed. A rate of 100%\nindicates that the gistified file is concise and contains primarily necessary lines that are executed, while 0%\nindicates that non of the executable lines were touched. When calculating line execution rate, we exclude the\ntests where the self-containment is 0% as the goal of line execution rate is to evaluate the model’s ability to\nconstruct a concise, executable file, not to penalize failures in generating runnable code.\nWe classify each line of code into three categories: executable, potentially executable, and non-executable.\nExecutable lines include imports and functional code that can be directly run. Potentially executable lines\nare those that may or may not be executed during a run, such as the except block of a try-except statement\nor placeholders for classes and function definitions. Non-executable lines, such as comments, are those that\nhave no effect on execution. To calculate the line execution rate, we first classify each line in the gistified file\nand then consider only the executable lines. Non-executable lines are ignored since their presence or absence\ndoes not affect execution outcomes, and potentially executable lines are excluded because they are often\nambiguous (e.g., placeholders) and cannot be reliably judged as necessary or removable.\nLine Existence Rate\nThe line existence rate measures the proportion of lines in the gistified file that are\ndirectly preserved from the original codebase. We first parse both the gistified file and the original codebase\ninto blocks, where each block corresponds to a class or function. Within classes, functions are nested under\ntheir parent class, forming a hierarchy. Lines outside of any block (e.g., top-level statements) are treated as\nstandalone units.\nFor each block in the gistified file, we locate the corresponding block in the original codebase using its name\nand hierarchical position. If a matching block exists, we compare the two line by line to determine which\nlines are preserved; whether the lines in the gistified block appear in the corresponding original block. If no\nmatch is found, all lines in that block are treated as non-existent. For lines outside any block, existence is\ndetermined by direct comparison with top-level lines in the original codebase.\nAn existence rate of 100% indicates perfect preservation of the original code without hallucinated content.\nNormalization for Line-wise Code Matching\nWhen checking the existence of a code line within a file, as our\nobjective is to determine semantic equivalence rather than strict syntactical identity, we do normalization;\ncode that is functionally identical may differ in formatting, such as multiline statements, indentations, or\nspace, which can hinder direct line-wise comparison. To address this, we normalize each code block before\nperforming line-wise matching. Specifically, we parse the code into an Abstract Syntax Tree (AST) and ignore\ncomments; split combined import statements into individual imports; merge statements that span multiple\nlines into a single line; remove inline comments (e.g., for i in range(5):\n# comment); and eliminate\nindentation and redundant spaces. These normalizations ensure robustness by making the comparison focus\non the code’s underlying structure and functionality rather than superficial formatting differences.\nA.2\nFramework\nWe evaluate experiments with three agentic frameworks: mini-SWE-Agent (Yang et al., 2024), SWE-\nAgent (Yang et al., 2024), and Copilot (Microsoft, 2025). Unless otherwise noted, all experiments are run in\nthe default Gistify setup, where the model is restricted from executing any commands (e.g., python, pytest).\n14\nGistify! Codebase-Level Understanding via Runtime Execution\nSWE-Agent and Copilot Agent enable LLMs to interact with a codebase through a suite of tools, including\nbash commands. These tools support capabilities such as viewing, searching, editing, and creating files or\ndirectories. In addition, Copilot Agent extends this functionality with browser integration, explicit reasoning,\nand API usage. mini-SWE-agent is a simplified variant of SWE-Agent that only supports bash commands.\nDespite its minimal design, it achieves strong performance on the SWE-Bench Verified benchmark (Jimenez\net al., 2023). For both mini-SWE-Agent and SWE-Agent, we set the maximum number of steps to 50 and\nrun them in the same Docker environment, using the current version of the repositories.\nA.3\nExperimental Test Set Construction\nTable 4: Details of the GitHub repositories used as the test set.\nRepository\nURL\nLicense\nflask\nhttps://github.com/pallets/flask\nBSD 3-Clause\nrequests\nhttps://github.com/psf/requests\nApache-2.0\npylint\nhttps://github.com/pylint-dev/pylint\nGPL 2.0\nscikit-learn\nhttps://github.com/scikit-learn/scikit-learn\nBSD 3-Clause\nseaborn\nhttps://github.com/mwaskom/seaborn\nBSD 3-Clause\ndebug-gym\nhttps://github.com/microsoft/debug-gym\nMIT\nTable 4 summarizes the repositories used in our evaluation. For each repository, we begin by extracting\nall available test cases, including parameterized ones. For experimental test runs, we group tests3 that\nshare the same base structure but differ only in parameterization, treating them as a single test. During\nevaluation, however, we execute all parameterized instances and measure how many are passed, thereby\nassessing execution fidelity. Finally, we filter out environment-dependent tests, such as those requiring relative\nfile paths or fixed module locations. In the main experiments, we used 25 test instances for each of the five\ncodebases, and the analysis was conducted using 50 test instances from the pylint codebase.\nA.4\nPrompt for Gistify\nFigure 3 shows the prompt used in the main experiments.\nA.5\nProviding specific parameters to commands tends to make models generate parameter-specific\ngistified files\nWe observe that when specific command-line parameters are provided, models often adapt the generated\ngistified file to those parameters rather than producing a fully general solution. Examples of this parameter-\nspecific behavior are shown in Figures 4 and 5. Accordingly, in our experiments, we group test cases based on\nthe parameters provided to the command.\nB\nResults\nB.1\nExample of gistified file\nFigure 6 and Figure 7 show two gistified files on the same test case with different models; each model succeed\nor fail in generating a gistified file with execution fidelity of 100% and 0%, respectively. In the successful case\n(Figure 6), the generated file handles both parameters correctly, achieving a 100% line existence rate, a 65.5%\nexecution rate, and a test F1 score of 100. In contrast, the failed case (Figure 7) cannot execute due to a\nmissing import pytest statement. Moreover, the hallucinated test function yields a test F1 score of 0, and\nthe file shows a much lower line existence rate of 28%.\n3We adopt this grouping design as we observe that models often overfit to specific values when parameters are\nprovided. See Appendix A.5 for more details.\n15\nGistify! Codebase-Level Understanding via Runtime Execution\nPrompt for Gistify\nI’ve uploaded a python code repository in the directory {working dir}.\nThere is an original test invocation (the command that reproduces behavior we want to preserve):\nproblem statement Your job: create a single file named ‘concise.py’ saved at {working dir} that is\n**self-contained**, **minimal**, and **executable**, and when used in place of the original test run\nreproduces the same runtime behavior and outputs. Follow the instructions below when creating the\nfile.\nOUTPUT\n- Produce one file only: ‘{working dir}/concise.py’.\n- The assistant must return only the contents of ‘concise.py’ (no extra files, no analysis, no\ncommentary).\nHIGH-LEVEL RULES for creating ‘concise.py’\n1. Inline internal dependencies\n* Copy into ‘concise.py’ every function, class, or top-level code from the files inside {working dir}\nthat is executed when running {problem statement}.\n* Do not use ‘import’ statements for modules defined in {working dir}.\n2. Remove unexecuted lines\n* When copying lines in ‘concise.py’, keep only the lines that is actually executed when running\n{problem statement}.\n* Delete unused functions, classes, variables, if-else, imports, and unreachable branches.\n* Ensure the file remains syntactically correct and minimal after removal.\n3. Preserve original source lines\n* Do not rewrite or reformat lines unless necessary to keep the files valid.\n* Do not arbitrary generate new lines that do not exist in the original {working dir} files.\n* You may adjust indentation, remove empty ‘else’‘ blocks, or adapt ‘try-except’ structures only\nwhen required to preserve correctness.\n4. Keep external imports\n* Leave imports to external libraries, frameworks, or standard runtime libraries unchanged. * Only\nremove or inline dependencies that come from {working dir}.\n5. No shortcuts or cheating\n* Do not stub, fake, or monkey-patch external modules.\n* Do not reimplement or newly add third-party libraries.\n* Do not hard-code outputs\n* Do not replace test logic with simplified equivalents\n6. Preserve test behavior\n* The test function much remain unchanged, except for import adjustments needed to reference\ninlined code.\n* The output, exceptions, or exit codes must match the original run of {problem statement}.\n7. Do not execute the code\n* Do not run or simulate the program (e.g., with ‘pytest’, ‘python’, or any other tools)\nFigure 3: Base Prompt Template for Gistify Task.\nB.2\nError analysis over execution failure\nWe categorize errors into four types:\nImport Error\nFigure 8 shows an example of Import Error. This occurs when the model incorrectly imports\nthe original repository (e.g., import requests) instead of inlining the required modules into the gistified file.\nFile Creation Failure\nThis error arises when the model fails to generate the gistified file. This can happen in\ntwo ways: (1) the model exceeds the maximum step limit or (2) the model completes within the time limit\nbut still fails to generate the new file using the tool.\n16\nGistify! Codebase-Level Understanding via Runtime Execution\n@pytest.mark.parametrize(\n\"value ,␣expected\",\n(\n(\"application/xml\", (\"application/xml\", {})),\n(\n\"application/json␣;␣charset=utf -8\",\n(\"application/json\", {\"charset\": \"utf -8\"}),\n),\n(\"text/plain\", (\"text/plain\", {})),\n...\n)\ndef\ntest__parse_content_type_header (value , expected):\nassert\n_parse_content_type_header (value) == expected\n(a) Original Test Case\ndef\ntest__parse_content_type_header ():\n\"\"\"Test␣for␣the␣_parse_content_type_header ␣function␣with␣application/json␣and␣charset=utf -8\"\"\"\nvalue = \"application/json␣;␣charset=utf -8\"\nexpected = (\"application/json\", {\"charset\": \"utf -8\"})\nassert\n_parse_content_type_header (value) == expected\n(b) Gistified File\nFigure 4: Example of a model generating a parameter-specific gistified file when given a command that\nincludes a parameter.\n@pytest.mark.parametrize(\n\"url ,␣expected\",\n(\n(\"http ://192.168.0.1:5000/ \", True),\n...\n(\"http :// google.com :5000/ v1.0/\", False),\n),\n)\ndef\ntest_should_bypass_proxies_no_proxy (url , expected , monkeypatch):\n\"\"\"Tests␣for␣function␣should_bypass_proxies ␣to␣check␣if␣proxy\n␣␣␣␣can␣be␣bypassed␣or␣not␣using␣the␣’no_proxy ’␣argument\n␣␣␣␣\"\"\"\nno_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\"\n# Test ’no_proxy ’ argument\nassert\nshould_bypass_proxies (url , no_proxy=no_proxy) == expected\n(a) Original Test Case\ndef\ntest_should_bypass_proxies_no_proxy (url , expected , monkeypatch):\n\"\"\"Tests␣for␣function␣should_bypass_proxies ␣to␣check␣if␣proxy\n␣␣␣␣can␣be␣bypassed␣or␣not␣using␣the␣’no_proxy ’␣argument\n␣␣␣␣\"\"\"\nno_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\"\n# Test ’no_proxy ’ argument\nassert\nshould_bypass_proxies (url , no_proxy=no_proxy) == expected\n(b) Gistified File\nFigure 5: Example of a model generating a parameter-specific gistified file when given a command that\nincludes a parameter.\nMissing Test Function\nThis occurs when the generated gistified file does not contain the modules for specified\ntest in the given command. It typically arises when the model fails to locate or copy the modules necessary\nfor the test into the gistified file. Conceptually, this corresponds to a 0% line existence rate for the test\nfunction. Since the presence of the modules for the given test case is essential for validation, we classify this\nas an error.\nWe also observe an interesting behavior of GPT-5 where it tends to insert __name__ == \"__main__\" even\nthough it is not provided in the original codebase and even though it is explicitly mentioned that we will test\non the provided command and expect the same output. They often remove the test function but move the\nlines in the test function under the \"__main__\" guard (e.g., Figure 10). We hypothesize that this may be\nbecause they are more familiar with codebases following this pattern. We also observe cases where the model\n17\nGistify! Codebase-Level Understanding via Runtime Execution\n# Licensed\nunder the GPL: https :// www.gnu.org/licenses/old -licenses/gpl -2.0. html\n# For\ndetails: https :// github.com/pylint -dev/pylint/blob/main/LICENSE\n# Copyright (c) https :// github.com/pylint -dev/pylint/blob/main/ CONTRIBUTORS .txt\nfrom\n__future__\nimport\nannotations\nimport os\nfrom\ncollections.abc import\nSequence\nfrom\ntyping\nimport Any\nimport\npytest\ndef\ndiscover_package_path (modulepath: str , source_roots: Sequence[str]) -> str:\n\"\"\"Discover␣package␣path␣from␣one␣its␣modules␣and␣source␣roots.\"\"\"\ndirname = os.path.realpath(os.path.expanduser(modulepath))\nif not os.path.isdir(dirname):\ndirname = os.path.dirname(dirname)\n# Look for a source\nroot that\ncontains\nthe module\ndirectory\nfor\nsource_root in source_roots:\nsource_root = os.path.realpath(os.path.expanduser(source_root))\nif os.path.commonpath ([ source_root , dirname ]) in [dirname , source_root ]:\nreturn\nsource_root\n# Fall back to legacy\ndiscovery by looking\nfor\n__init__.py upwards as\n# it’s the only way given\nthat\nsource\nroot was not found or was not\nprovided\nwhile\nTrue:\nif not os.path.exists(os.path.join(dirname , \"__init__.py\")):\nreturn\ndirname\nold_dirname = dirname\ndirname = os.path.dirname(dirname)\nif old_dirname == dirname:\nreturn os.getcwd ()\n@pytest.mark.parametrize(\n\" py_mod_base_name \",\n(\"__init__\", \"impl\"),\nids=(\"explicit -namespace\", \"implicit -namespace\"),\n)\ndef\ntest_discover_package_path_source_root_as_parent (\npy_mod_base_name : str ,\ntmp_path: Any ,\n) -> None:\n\"\"\"Test␣discover_package_path ␣when␣source␣root␣is␣a␣parent␣of␣the␣module.\"\"\"\n# Create\nthis\ntemporary\nstructure:\n# /tmp_path/\n#\nproject/\n#\nmy -package/\n#\n__init__.py\nproject_dir = tmp_path / \"project\"\npackage_dir = project_dir / \"mypackage\"\npackage_dir.mkdir(parents=True)\n(package_dir / f\"{ py_mod_base_name }.py\").touch ()\n# Test with\nproject_dir as source\nroot (parent of package)\nresult = discover_package_path (str(package_dir), [str(project_dir)])\nassert\nresult == str(project_dir)\nFigure 6: Example of a successful gistified file: the code correctly handles all parameters, achieving 100%\nline–existence rate, a test F1 score of 100, and an execution rate of 65.5%.\nattempts to “cheat” the task by injecting a mock, in-memory version of the original codebase package to\nsatisfy import dependencies, rather than copying the necessary code inline (e.g., Figure 12).\nPytest Runtime Error\nThis error refers to failures that occur during pytest execution, such as syntax errors or fixture-related issues\n(e.g., Figure 9). Although the absence of test functions is also one of pytest failures, we explicitly separate\nthose cases by first verifying the presence of the required test functions and running pytest only when they\nexist.\n18\nGistify! Codebase-Level Understanding via Runtime Execution\nimport os\nimport sys\nfrom\ncontextlib\nimport\ncontextmanager\nfrom\npathlib\nimport\nPath\ndef\ndiscover_package_path (path , _search_paths ):\n\"\"\"\n␣␣␣␣Minimal␣version:␣accept␣a␣filesystem␣path␣(string␣or␣Path)␣and␣return␣a␣Path\n␣␣␣␣to␣that␣directory.␣This␣matches␣the␣usage␣in␣the␣specific␣test␣where␣a\n␣␣␣␣path␣to␣the␣tests␣data␣directory␣is␣passed.\n␣␣␣␣\"\"\"\nif isinstance(path , Path):\np = path\nelse:\np = Path(path)\nif p.exists () and p.is_dir ():\nreturn p\nraise\nValueError(\"Cannot␣determine␣package␣path\")\n@contextmanager\ndef\naugmented_sys_path (paths):\n\"\"\"\n␣␣␣␣Minimal␣context␣manager:␣set␣sys.path␣to␣the␣parent␣directories␣of␣the\n␣␣␣␣provided␣paths␣(in␣order),␣restoring␣the␣original␣sys.path␣on␣exit.\n␣␣␣␣\"\"\"\noriginal = list(sys.path)\ntry:\nnew = []\nfor p in paths:\npth = Path(p)\n# Ensure we use the parent\ndirectory (the \"source\nroot\" in the test)\nparent = pth.parent\nnew.append(str(parent))\nsys.path [:] = new\nyield\nfinally:\nsys.path [:] = original\n@pytest.mark.parametrize(\n\" py_mod_base_name \",\n(\"__init__\"),\nids=(\"explicit -namespace\"),\n)\ndef\ntest_discover_package_path_source_root_as_parent ():\n# This test ’s behavior is preserved: it uses\ndiscover_package_path\non the\n# tests\ndata\ndirectory\nand\nensures\naugmented_sys_path\nsets sys.path to its\n# parent (the\nproject/tests\ndirectory).\nTEST_DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"tests\", \"data\"))\nPROJECT_ROOT_DIR = os.path.abspath(os.path.join(TEST_DATA_DIR , \"..\"))\nwith\naugmented_sys_path ([ discover_package_path (TEST_DATA_DIR , [])]):\nassert sys.path == [ PROJECT_ROOT_DIR ]\nFigure 7: Example of failed gistified file: the code fails to import pytest. The model hallucinates the\nfunction test_discover_package_path_source_root_as_parent(), resulting in a test F1 score of 0 and a\nlow line–existence rate of 28.0%\nB.3\nTools Available in GitHub Copilot\nTable 5 shows the list of available tools in Github Copilot.\nB.4\nChange Test\neven high performing models and frameworks (especially GPT-5 and GPT-5-mini) seems to modify test codes\neven though explicitly mentioned not to. We observed three common modification: (1) removing the test\nfunction but move the lines in the test function under the \"__main__\" guard (e.g., Figure 10), (2) adding\nthe \"__main__\" guard even though unnecessary (e.g., Figure 11), and (3) mocking a minimal in-memory\npackage to bypass missing dependencies and force the test to run (e.g., Figure 12).\n19\nGistify! Codebase-Level Understanding via Runtime Execution\n@click.option(\"--all -methods\", is_flag=True , help=\"Show␣HEAD␣and␣OPTIONS␣methods.\")\n@with_appcontext\ndef\nroutes_command (sort , all_methods):\n\"\"\"Show␣all␣registered␣routes␣with␣endpoints␣and␣methods.\"\"\"\nfrom\nflask\nimport\ncurrent_app\nrules = list(current_app.url_map.iter_rules ())\nif not rules:\nclick.echo(\"No␣routes␣were␣registered.\")\nreturn\nFigure 8: Example of an Import Error: the gistified file imports from the original repository (e.g., from\nflask import current_app).\nT = t.TypeVar(\"T\")\nclass\nConfigAttribute (t.Generic[T]):\n\"\"\"Makes␣an␣attribute␣forward␣to␣the␣config\"\"\"\ndef\n__init__(\nself , name: str , get_converter: t.Callable [[t.Any], T] | None = None\n) -> None:\nself.__name__ = name\nself.get_converter = get_converter\n(a) Original Test Case\nclass\nConfigAttribute :\ndef\n__init__(\nself , name: str , get_converter: t.Callable [[t.Any], T] | None = None\n) -> None:\nself.__name__ = name\nself.get_converter = get_converter\n(b) Gistified File\nFigure 9: Example of an Pytest Runtime Error: gistified file fails with error message E TypeError:\ntype\n’ConfigAttribute’ is not subscriptable\nB.5\nAdditional Metrics\nTable 6 shows the result of additional evaluation metrics, including the Average Pytest Pass Rate, which\nis defined as the average test pass rate over cases with at least one successful run, and the Test F1 Score,\nwhich quantifies the line-wise F1 existence between the test functions in the original codebase and those in\nthe gistified fie.\nGPT-5 shows a notably higher Average Pytest Pass Rate, indicating that among the ones they successfully\ngenerate, they tend to pass all pytest. For the Test F1 Score, Claude-4 shows the highest performance, aliging\nwith the trend discussed in Section 3.4.\nC\nAnalysis\nC.1\nEffect of various strategies and tools\nPrompt-Based Guidance\nWe experiment with two variants of the prompt, Reading and Tracing, where, on top\nof the base prompt (Figure 3), we add specific instructions of How to Operate to encourage reasoning using\na particular strategy. The addition prompt detail of Reading is in Figure 14, and for Tracing is in Figure 15.\nGlobal Information via Tools\nWe experiment with two tools that provide global information: RepoGraph and\nTracing. Details of the information provided to the model about each tool are shown in Figure 16.\nRepoGraph (Ouyang et al., 2024) is a plug-in module designed to help LLMs leverage the codebase-level\nstructure. It parses code at the line level, extracts relationships, and constructs a graph where each node\nrepresents a line of code and each edge encodes dependencies between code definitions and their references.\n20\nGistify! Codebase-Level Understanding via Runtime Execution\nTool\nDescription\ncopilot_getNotebookSummary\nReturns the list of Notebook cells with id, types, line ranges, language, execution\ninfo, and output mime types. Useful for getting cell IDs, execution order, and\noutputs.\nedit_notebook_file\nEdit an existing Notebook file in the workspace. Supports inserting, deleting,\nor editing cells while preserving whitespace and indentation.\napply_patch\nEdit text files using a special diff/patch format.\nDo not use for Jupyter\nnotebooks.\nsemantic_search\nRun a natural language search for relevant code or documentation comments in\nthe workspace.\ncreate_directory\nCreate a new directory structure in the workspace (like mkdir -p).\ncreate_file\nCreate a new file with specified content. Automatically creates directories if\nthey do not exist.\nfile_search\nSearch for files in the workspace by glob pattern (e.g., **/*.js). Returns\nmatching paths only.\ntest_search\nFor a source file, find the corresponding test file, and vice versa.\ngrep_search\nFast text or regex search in the workspace. Useful for exact string or regex\nqueries.\nrun_notebook_cell\nRun a code cell in a notebook file and return the output.\nAvoid running\nMarkdown cells.\nread_notebook_cell_output\nRetrieve the latest output for a notebook cell, even if not run in the current\nsession.\nget_search_view_results\nReturns results from the search view.\ngithub_repo\nSearch a GitHub repository for relevant code snippets. Use only for external\nrepos, not local workspaces.\ninsert_edit_into_file\nInsert or edit code in an existing file using minimal hints, avoiding duplication\nof unchanged code.\ninstall_extension\nInstall an extension in VS Code. Used only during workspace creation.\nlist_dir\nList the contents of a directory (folders and files).\ncreate_new_jupyter_notebook\nGenerate a new Jupyter Notebook (.ipynb) in VS Code.\ncreate_new_workspace\nSet up a complete new project (scaffolding, dependencies, config, boilerplate).\nget_project_setup_info\nProvides project setup information for a VS Code workspace after workspace\ncreation.\nread_file\nRead the contents of a file. Supports offsets and limits for large files.\nopen_simple_browser\nPreview or open a URL in VS Code’s Simple Browser.\ntest_failure\nInclude test failure information in the prompt.\nthink\nThink deeply about a request and log structured reasoning (no execution).\nUseful for planning, debugging, and brainstorming.\nget_vscode_api\nRetrieve comprehensive VS Code API documentation and references for exten-\nsion development.\nrun_vscode_command\nRun a VS Code command by ID with arguments. Used mainly in workspace\ncreation.\nfetch_webpage\nFetch main content from a webpage for summarization or analysis.\nTable 5: Available tools and their descriptions. We note that many tools available to the agent are never used.\nTable 6: Average Pytest Pass Rate and Test F1 Score of different models using SWE-Agent on the main table\n(Table 1) test dataset.\nModels\nExecution Fidelity\nAverage Pytest Pass Rate\nTest F1 Score\nGPT-5-mini\n30.9\n49.2\n47.9\nGPT-5\n30.7\n88.8\n45.0\nClaude-3.7\n40.7\n61.9\n55.9\nClaude-4\n56.7\n72.2\n60.0\n21\nGistify! Codebase-Level Understanding via Runtime Execution\nclass\nTestGetNetrcAuth :\ndef\ntest_works(self , tmp_path , monkeypatch):\nnetrc_path = tmp_path / \".netrc\"\nmonkeypatch.setenv(\"NETRC\", str(netrc_path))\nwith open(netrc_path , \"w\") as f:\nf.write(\"machine␣example.com␣login␣aaaa␣password␣bbbb\\n\")\nauth = get_netrc_auth (\"http :// example.com/thing\")\nassert\nauth == (\"aaaa\", \"bbbb\")\n(a) Original Test Case\nif __name__ == \"__main__\":\n# Reproduce\ntests/test_utils.py:: TestGetNetrcAuth :: test_works\nwith\ntempfile. TemporaryDirectory () as tmpdir:\nnetrc_path = os.path.join(tmpdir , \".netrc\")\nos.environ[\"NETRC\"] = netrc_path\nwith open(netrc_path , \"w\") as f:\nf.write(\"machine␣example.com␣login␣aaaa␣password␣bbbb\\n\")\nauth = get_netrc_auth (\"http :// example.com/thing\")\nassert\nauth == (\"aaaa\", \"bbbb\")\n(b) Gistified File\nFigure 10: Test Modification Case 1: The test TestGetNetrcAuth.test_works is converted from a pytest\nunit test into a standalone script.\n# Test\nclass and method - preserved\nunchanged\nclass\nTestArgparseOptionsProviderMixin :\n\"\"\"Tests␣for␣the␣argparse␣implementation ␣of␣OptionsProviderMixIn .\n␣␣␣␣The␣logger␣checker␣is␣used␣as␣an␣example␣checker␣for␣this␣implementation .\n␣␣␣␣\"\"\"\n@staticmethod\ndef\ntest_logger_without_options () -> None:\n\"\"\"Check␣that␣we␣raise␣messages␣when␣we␣do␣not␣supply␣any␣options.\"\"\"\nwith\npytest.raises(SystemExit) as ex:\nRun([ LOGGING_TEST ])\nassert ex.value.code == 2\n# Main\nexecution\nfor pytest\nif __name__ == \"__main__\":\ntest = TestArgparseOptionsProviderMixin ()\ntest. test_logger_without_options ()\nFigure 11: Test Modification Case 2: Adding unnecessary \"__main__\" guard\nThereby, when given a specific module, it returns the relationship with other modules as represented within\nthe constructed graph.\nTracing is a tool that uses the tracer provided from the sys module to execute a command and track which\ncomponents of the codebase are accessed. When the model uses the tool with a specific command, the tool\nprovides the model with the files and functions touched when running the command, in the order in which\nthey are encountered.\nExecution-Based Tools\nWe experiment with two execution-based tools: the Bash tool and the Edit and\nExecute tool.\nThe Bash tool is a basic utility that allows the model to invoke any necessary Bash commands. In contrast,\nthe Edit and Execute tool is designed specifically for working with the gistified file: it enables the model to\ncreate or modify the gistified file and optionally execute it to verify changes.\nThe primary difference between the two tools is their scope of execution. The Bash tool can run commands on\nboth the original codebase and the gistified file, whereas the Edit and Execute tool is restricted to executing\nonly the gistified file.\nWe include an example of the behavior observed when adding the execution tool in Figure 17. Common\npatterns we observe are: (1) the model first runs the provided command to identify which files are accessed\n22\nGistify! Codebase-Level Understanding via Runtime Execution\n# Create a minimal in -memory ’requests ’ package\nwith\nrequired\nsubmodules.\nrequests_mod = types.ModuleType(’requests ’)\nrequests_mod .__path__ = []\ncompat_mod = types.ModuleType(’requests.compat ’)\nstructures_mod = types.ModuleType(’requests.structures ’)\n# Populate\ncompat\nwith only what ’s needed by this test\nsuite\nimport\npaths.\ncompat_mod.Mapping = Mapping\ncompat_mod. MutableMapping = MutableMapping\ncompat_mod.urljoin = urljoin\n# Populate\nstructures\nwith the\nclasses.\nstructures_mod . CaseInsensitiveDict = CaseInsensitiveDict\nstructures_mod .LookupDict = LookupDict\n# Wire the\npackage\nhierarchy\nand\nregister in sys.modules.\nrequests_mod.compat = compat_mod\nrequests_mod.structures = structures_mod\nsys.modules[’requests ’] = requests_mod\nsys.modules[’requests.compat ’] = compat_mod\nsys.modules[’requests.structures ’] = structures_mod\nif __name__ == ’__main__ ’:\nimport\npytest\nraise\nSystemExit(pytest.main ([’-q’, ’tests/ test_structures .py:: TestCaseInsensitiveDict :: test_list ’]))\nFigure 12: Test Modification Case 3: Manually mocking a minimal in-memory package to bypass missing\ndependencies and force the test to run.\n@pytest.mark.parametrize(\n\"value ,␣expected\",\n(\n(’foo=\"is␣a␣fish\",␣bar=\"as␣well\"’, {\"foo\": \"is␣a␣fish\", \"bar\": \"as␣well\"}),\n(\" key_without_value \", {\" key_without_value \": None }),\n),\n)\ndef\ntest_parse_dict_header (value , expected):\nassert\nparse_dict_header (value) == expected\n(a) Original Test Case\nassert\nparse_dict_header (’foo=\"is␣a␣fish\",␣bar=\"as␣well\"’) == {\"foo\": \"is␣a␣fish\", \"bar\": \"as␣well\"}\nassert\nparse_dict_header (\" key_without_value \") == {\" key_without_value \": None}\n(b) Gistified File\nFigure 13:\nThe test function test_parse_dict_header is simplified:\nin the original,\nit used\n@pytest.mark.parametrize to feed multiple input/expected pairs into one function; in the gistified version,\nthis is replaced with two direct assert statements, one per case.\nTable 7: Analysis of tool usage during the Gistify task\nModels\nAvg. tool usage\nview\nsearch\nexecute\nother\nGPT-5-mini\n10.8\n71.9\n9.8\n1.7\n16.6\nGPT-5\n18.5\n72.4\n8.3\n3.3\n16.1\nClaude-Sonnet-3.7\n17.3\n67.5\n10.1\n4.5\n17.9\nClaude-Sonnet-4\n19.3\n74.6\n2.1\n11.8\n11.5\nand to gather execution feedback; (2) after creating a file, it iteratively executes it to verify that the generated\ngistified file behaves as expected; and (3) it repeatedly compares the outputs of the gistified file and the\noriginal codebase under the given command. We also observe that, due to this iterative checking process,\nenabling the execution tool often leads the model to terminate because it reaches the maximum step limit.\n23\nGistify! Codebase-Level Understanding via Runtime Execution\nBehavior Reading\nHow to Operate:\n1. Examine the test file and the test function used for {problem statement}\n2. Identify which module used by these functions are defined in {working dir}\n3. Copy and inline the code from those modules into ‘concise.py’\n4. Check these modules for any internal functions or classes and inline them as needed.\n5. Repeat this process recursively until all internal dependencies are inlined.\n6. Do not forget to copy and paste external imports.\nFigure 14: Prompt for Reading strategy.\nTrace Reasoning\nHow to Operate:\n1. Predict the execution traces.\n2. Follow the traces and inline (copy) only the necessary executed lines into ‘concise.py’\n3. Repeat until all traces are fully handled.\nFigure 15: Prompt for Tracing strategy.\nC.2\nTool Usage Rates\nTable 7 shows the statistics on tool usage across models using SWE-bench. We group various tools into four\ncategories: view, search, execute, and other, which includes all remaining tools. For all models, we compute\nusage rates both with and without execution enabled, and then average across the two settings.\nAmong all models, Claude-4 exhibits the highest average tool usage for each test cases, followed by GPT-5,\nClaude-3.7, and GPT-5-mini. In terms of specific functionality, Claude-4 shows the highest rate of both\nview and execute tool usage, while Claude-3.7 shows the highest usage of the search tool. To generate a\nhigh-quality gistified file, a model must effectively view relevant files and copy only the necessary content.\nThe strong performance of Claude-4 on line existence may be related to its high usage of the view tool. Also,\nthe execution tool tends to support correctness verification of the generated file, which would lead to high\nexecution fidelity.\nTool Information\n* search repo (repograph): searches in the mentioned repository with a specific function or class, and\nreturns the def and ref relations for the search term.\n* trace info (tracing): returns the trace log when running the ‘pytest’ command for the test currently\nbeing debugged in the given repo\nFigure 16: Details of the information provided to the model about each tool.\n24\nGistify! Codebase-Level Understanding via Runtime Execution\n{\n\"role\": \"assistant\",\n\"tool_calls\": {\n\"arguments\": \"{\\\" command \\\":\\\" cd␣/app␣&&␣python␣-m␣pytest␣\nconcise.py:: TestDeprecationOptions :: test_new_names ␣-v\\\"}\",\n\"name\": \"bash\"\n}\n},\n{\n\"role\": \"tool\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"OBSERVATION:␣test␣session␣starts␣... AssertionError \\n... pylint:␣error:␣unrecognized ␣\narguments:␣/app/regrtest_data /empty.py\\n␣FAILED␣\nconcise.py:: TestDeprecationOptions :: test_new_names ␣...\"\n}\n},\n...\n(fix\nconcise.py based on observation)\n...\n{\n\"role\": \"assistant\",\n\"tool_calls\": {\n\"arguments\": \"{\\\" command \\\":\\\" cd␣/app␣&&␣python␣-m␣pytest␣\ntests/config/ test_argparse_config .py:: TestDeprecationOptions :: test_new_names ␣-v␣-s\\\"}\",\n\"name\": \"bash\"\n}\n},\n...\n{\n\"role\": \"assistant\",\n\"content\": \"Excellent!␣The␣messages␣are␣identical.␣Let␣me␣verify␣my␣implementation ␣removes␣all␣\nunused␣imports␣and␣only␣includes␣what ’s␣needed:\"\n}\nFigure 17: Trajectory when including execution tool. Models often iteratively execute the given codebase or\ngenerated gistified file to ensure that it operates expectedly.\n25\n",
    "content": "# Gistify! Codebase-Level Understanding via Runtime Execution – Paper Summary\n\n## 1. Core Content and Key Contributions\n\n**Core Idea:**  \nThis paper introduces a novel task called **Gistify**, designed to evaluate large language models’ (LLMs) ability to understand code at the **codebase level**. The task requires models to extract the \"essence\" of a specific functionality from an entire codebase—producing a single, self-contained, minimal, and executable file (a *gistified file*) that behaves identically to the original codebase when run with a given command.\n\nSpecifically, the model must:\n- Start analysis from a specified entry point (e.g., `pytest test_requests.py::test_cookie`)\n- Trace runtime execution paths and identify all dependencies\n- Inline necessary code fragments into one file without importing from the original project\n- Remove redundant, unexecuted code to ensure minimality\n- Use only content from the original codebase, avoiding hallucinated code\n\n**Main Contributions:**\n1. **Introducing the Gistify Task Framework**: First formalization of developers’ real-world practice of understanding codebases through runtime debugging into a quantifiable benchmark.\n2. **Defining a Multi-Dimensional Evaluation Metric System**:\n   - **Execution Fidelity**: Whether the generated file reproduces the original behavior (binary metric)\n   - **Line Execution Rate**: Measures how concise the file is—the higher, the better\n   - **Line Existence Rate**: Evaluates faithfulness to source code, preventing hallucinations\n3. **Revealing Limitations of Current SOTA Models**: Experiments show even top-tier models (e.g., GPT-5, Claude-4) perform poorly (~58% average success rate), especially on long execution traces.\n4. **Providing a Generalizable & Scalable Evaluation Method**: Only requires a codebase and an entry command to build test cases—applicable to any public or private repository without manual issue/PR labeling.\n5. **Open-Sourcing Supporting Resources**: Includes experimental framework, tool integration examples, and dataset construction methods.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### (1) Paradigm Shift: From Static Comprehension to Dynamic Execution Reasoning\nTraditional codebase understanding tasks focus on static problems like question answering, completion, or bug fixing (e.g., SWE-bench). In contrast, Gistify emphasizes **modeling runtime execution flow**, requiring accurate tracing of function calls, object state changes, and inter-module interactions to correctly extract components—mirroring real developer debugging workflows.\n\n> 🔍 **Innovation Essence**: Upgrades “code understanding” from “reading code” to “running code and reverse-engineering it.”\n\n### (2) Challenging Design: Countering Heuristic Shortcuts\nExisting benchmarks are often vulnerable to shortcuts like retrieving local patches or memorizing historical PRs (“SWE-bench illusion”). Gistify prevents such exploits via:\n- Requiring full executable files—not just diffs\n- Enforcing inlining instead of imports, eliminating external dependencies\n- Mandating minimality to exclude irrelevant code\n- Preserving test structure to prevent logic simplification\n\nThese constraints force models to truly comprehend execution logic rather than “guess” or “copy.”\n\n### (3) Practical Task Output**\nUnlike pure evaluation tasks, Gistify’s output has direct practical value:\n- Serves as a **minimal reproducible example** for documentation or bug reporting\n- Enables automated **functional sandboxing**, facilitating isolated testing, migration, or sharing of modules\n- Provides lightweight inputs for downstream tasks like debugging, refactoring, or code review\n\nThis “evaluation-as-application” philosophy enhances the real-world impact of the research.\n\n### (4) Fine-Grained Diagnostic Capabilities**\nThe paper goes beyond overall performance metrics by conducting detailed error classification (import errors, missing tests, runtime failures, etc.) and analyzing tool usage patterns across models (e.g., Claude excels at file inspection; GPT prefers execution validation), offering clear directions for improvement.\n\n---\n\n## 3. Startup Ideas Inspired by This Paper\n\n### 🚀 Startup Idea 1: **CodeSandboxer — Automated Feature Extraction & Microservice Platform**\n\n#### Concept  \nAn AI-powered tool for enterprise codebases that automatically extracts independent functional modules from monolithic applications into lightweight, deployable microservice prototypes (gistified services), accelerating modernization efforts.\n\n#### Key Features\n- Select a test case or API endpoint → automatically generate a standalone Python package with all required dependencies\n- One-click export to Docker images or FastAPI microservice templates\n- Built-in dependency analysis and conflict detection with warnings about coupling risks\n- Visualize extracted scope and execution path\n\n#### Business Value\n- **Reduces microservice migration cost**: Cuts weeks of manual analysis down to minutes for initial decomposition\n- **Accelerates legacy system modernization**: Helps finance, telecom, and other industries migrate aging systems incrementally\n- **Improves onboarding efficiency**: New engineers quickly access simplified versions of core business logic\n\n#### Technical Extension  \nIntegrate Gistify + RepoGraph + Tracing tools, enhanced with RAG for contextual awareness, supporting multiple languages (Java, Go, etc.).\n\n---\n\n### 🚀 Startup Idea 2: **BugMini — Minimal Bug Reproduction Generator (DevTool SaaS)**\n\n#### Concept  \nA debugging assistant for developers: when facing hard-to-reproduce production bugs, users upload logs or stack traces, and the system automatically generates a minimal runnable script that perfectly reproduces the error.\n\n#### Workflow\n1. Input: Error log + trigger conditions (e.g., URL, parameters)\n2. Backend invokes Gistify engine to reverse-engineer the triggering path\n3. Output: A `.py` file that reproduces the error upon execution, along with an execution fidelity score\n4. Compare environment variable differences between original and gist environments\n\n#### Key Advantages\n- Dramatically shortens the “identify → reproduce → fix” cycle\n- Automatically generates issue templates with minimal reproduction scenarios, improving team collaboration\n- Integrates into CI/CD pipelines for automated regression test generation\n\n#### Target Customers  \nEngineering teams in mid-to-large tech companies, open-source maintainers, and technical support teams at SaaS platforms.\n\n---\n\n### 🚀 Startup Idea 3: **FeatureShare — Open-Source Function Marketplace**\n\n#### Concept  \nCreate an “npm for features” platform where developers can upload verified “functional units” processed by Gistify. Others can directly download and integrate them, eliminating redundant development.\n\n#### Example Use Cases\n- “Extract cookie handling module from Requests library” → publish as `@mini/requests-cookie`\n- “Isolate random forest training snippet from Scikit-learn” → publish as `@mini/sklearn-rf-train`\n\nEach submission undergoes:\n- Execution fidelity verification\n- Security scanning (no malicious code)\n- License compliance checks\n\n#### Revenue Model\n- Premium review services for advanced features\n- Enterprise deployment of private instances\n- Revenue sharing based on popularity and recommendation\n\n#### Social Impact  \nPaves the way toward an era of “atomic-function reuse,” significantly reducing global duplicate coding effort.\n\n---\n\n> 💡 **Summary**: Gistify is more than just an evaluation task—it opens the door to **programmatic deconstructionism**. Any scenario involving **understanding, simplifying, migrating, or reusing existing code functionality** is a potential application. Entrepreneurs should focus on transforming this capability into productivity tools that enhance software engineering efficiency.",
    "github": "",
    "hf": ""
}