{
    "id": "2510.16724",
    "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications",
    "summary": "This paper summarizes the agent search methods based on reinforcement learning, explores their functional roles, optimization strategies, and application scope, and proposes future research directions.",
    "abstract": "The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \\emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available atthis https URL.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Multi-Agent",
    "authors": "Minhua Lin,Zongyu Wu,Zhichao Xu,Hui Liu,Xianfeng Tang,Qi He,Charu Aggarwal,Hui Liu,Xiang Zhang,Suhang Wang",
    "subjects": [
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)"
    ],
    "comments": "Comments:38 pages, 4 figures, 7 tables",
    "keypoint": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search provides the first systematic overview of RL-based agentic search by organizing the field into three dimensions: functional roles, optimization strategies, and application scope.\n\nRL enables LLMs to act as autonomous agents that dynamically plan, retrieve, reason, and reflect through multi-step interactions with search environments.\n\nTraditional Retrieval-Augmented Generation (RAG) systems are limited by being single-turn and heuristic-driven, lacking adaptive control over retrieval and reasoning processes.\n\nReinforcement learning addresses the limitations of traditional RAG by enabling self-improving, adaptive search behaviors through trial-and-error learning.\n\nThe survey categorizes RL’s role in agentic search into five functional roles: retrieval control, query optimization, reasoning-retrieval integration, multi-agent collaboration, and tool and knowledge integration.\n\nRetrieval control involves deciding when, how often, and how persistently to retrieve information, balancing between internal knowledge use and external retrieval.\n\nSearch intensity refers to optimizing the depth and persistence of retrieval, especially for complex or ambiguous queries.\n\nSearch efficiency focuses on minimizing redundancy, API costs, and latency while maintaining task performance.\n\nQuery optimization includes conversational reformulation and retriever-aware adaptation to improve query quality and alignment with retrieval systems.\n\nReasoning–retrieval integration emphasizes interleaving reasoning with retrieval steps and managing context and memory effectively during long-horizon tasks.\n\nMulti-agent collaboration frameworks use RL to coordinate specialized agents such as planners and executors or cooperative systems sharing global rewards.\n\nTool and knowledge integration extends agentic search beyond text retrieval to include multi-modal tools, code execution, and structured knowledge sources like knowledge graphs.\n\nRL is applied through various optimization strategies including on-policy methods like PPO and GRPO, off-policy methods like DPO, and hybrid training regimes.\n\nCold-start initialization via supervised fine-tuning (SFT) is commonly used before RL fine-tuning to stabilize early learning.\n\nSimulation-based training environments reduce cost and increase reproducibility during RL training.\n\nCurriculum learning and horizon scaling are used to manage sparse rewards and improve convergence in long-horizon search tasks.\n\nIterative and self-evolving frameworks use RL-generated trajectories to refine supervision data, creating a feedback loop for continuous improvement.\n\nReward design includes outcome-level rewards (e.g., answer correctness) and process-level rewards (e.g., information gain, faithfulness, redundancy penalties).\n\nOutcome-level rewards focus on final task success but increasingly incorporate multiple objectives like accuracy, efficiency, and format compliance.\n\nProcess-level rewards provide dense feedback during intermediate steps, enabling fine-grained optimization of search behavior.\n\nThe scope of RL application spans agent-level (single or multi-agent), module-level, step-level, and system-level optimization.\n\nAgent-level optimization trains end-to-end policies for autonomous search agents.\n\nModule-level optimization targets specific components (e.g., query rewriter) while keeping other parts frozen.\n\nStep-level optimization applies RL to individual actions within a search trajectory, such as query generation or document selection.\n\nSystem-level frameworks like AgentGym-RL and Chain-of-Agents provide unified platforms for training and evaluating RL-based agentic search systems.\n\nEvaluation metrics include answer quality (EM, F1, BERTScore), search effectiveness (Recall, MRR, NDCG), and search efficiency (query count, API cost, response time).\n\nSpecialized process metrics assess intermediate behaviors such as query quality, evidence utilization rate, and information gain per step.\n\nDatasets for evaluation span knowledge-intensive QA, web-based search, multi-modal tasks, conversational search, and domain-specific applications.\n\nApplications of RL-based agentic search include deep research, multi-modal search, code agents, AI assistants, and enterprise search.\n\nChallenges include developing robust multi-modal search, improving long-horizon memory management, ensuring trustworthy behavior under adversarial conditions, achieving cross-domain generalization, and enabling human-AI co-search.\n\nFuture directions emphasize trustworthy agentic search, memory-augmented architectures, cross-domain adaptability, and collaborative human-AI search paradigms.",
    "date": "2025-10-22",
    "paper": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search:\nFoundations, Roles, Optimizations, Evaluations, and Applications\nMINHUA LIN and ZONGYU WU, The Pennsylvania State University, USA\nZHICHAO XU, The University of Utah, USA\nHUI LIU and XIANFENG TANG, Amazon, USA\nQI HE, Microsoft, USA\nCHARU AGGARWAL, IBM T.J. Watson Research Center, USA\nHUI LIU, Michigan State University, USA\nXIANG ZHANG, The Pennsylvania State University, USA\nSUHANG WANG∗, The Pennsylvania State University, USA\nThe advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language\ninteraction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or\ndomain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external\nevidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning.\nRecent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step\ninteraction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive\nand self-improving search behavior. This survey provides the first comprehensive overview of RL-based agentic search, organizing\nthe emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization\nstrategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and\napplications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search\nsystems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at\nhttps://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.\nCCS Concepts: • Computing methodologies →Artificial intelligence.\nAdditional Key Words and Phrases: Large Language Models, Reinforcement Learning, Agentic Search\nACM Reference Format:\nMinhua Lin, Zongyu Wu, Zhichao Xu, Hui Liu, Xianfeng Tang, Qi He, Charu Aggarwal, Hui Liu, Xiang Zhang, and Suhang Wang.\n2025. A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and\nApplications. J. ACM 37, 4, Article 111 (October 2025), 38 pages. https://doi.org/XXXXXXX.XXXXXXX\n∗Corresponding Author\nAuthors’ Contact Information: Minhua Lin, mfl5681@psu.edu; Zongyu Wu, The Pennsylvania State University, University Park, USA; Zhichao Xu, The\nUniversity of Utah, Salt Lake City, USA; Hui Liu; Xianfeng Tang, Amazon, USA; Qi He, Microsoft, USA; Charu Aggarwal, IBM T.J. Watson Research\nCenter, USA; Hui Liu, Michigan State University, USA; Xiang Zhang, The Pennsylvania State University, University Park, USA; Suhang Wang, The\nPennsylvania State University, University Park, USA, szw494@psu.edu.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nManuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2510.16724v1  [cs.AI]  19 Oct 2025\n2\nMinhua Lin et al.\n1\nIntroduction\nLarge Language Models (LLMs) [137, 189, 242] have shown unprecedented capabilities in natural language understanding,\nreasoning, and generation, fundamentally reshaping how users access and interact with information. Despite these\nadvantages, LLMs still suffer from several limitations: they are constrained by static knowledge cutoffs [32], prone to\nfactual hallucinations [157], and unable to access real-time or domain-specific information. To address these challenges,\nthe paradigm of Retrieval-Augmented Generation (RAG) [57, 92] has emerged as a popular solution. RAG combines the\nreasoning power of LLMs with the precision of classical information retrieval (IR) techniques such as TF–IDF [2, 172],\nBM25 [154, 155], and link-analysis models like PageRank [13, 18, 138]. By retrieving evidence from external knowledge\nbases and conditioning responses on this context, RAG enables LLMs to generate more accurate and factually grounded\noutputs, particularly in knowledge-intensive tasks [9, 16, 49].\nHowever, traditional RAG systems [23] are typically single-turn and heuristic-driven: they retrieve once and generate\nonce, lacking the ability to iteratively refine queries or adapt retrieval strategies based on intermediate feedback.\nRetrieved documents may be irrelevant or noisy, hindering downstream reasoning [20, 82–84]. Moreover, LLMs often\nstruggle to fully utilize retrieved evidence, limiting the overall effectiveness of the pipeline. These limitations motivates\nthe development of more agentic search systems, where LLMs act as autonomous decision-makers that dynamically\nplan, retrieve, reason, and reflect over multiple steps.\nTo this end, researchers have proposed search agents i.e., LLM-based systems capable of multi-step interaction with\nsearch environments [78, 247]. Unlike traditional RAG, search agents can iteratively issue and refine queries, assess\nthe quality of retrieved results, and dynamically adapt their strategies to solve complex, multi-hop tasks. This shift\nfrom passive retrieval to active agency represents a paradigm change in information-seeking. However, early search\nagents often heavily rely on handcrafted prompting [105] or supervised fine-tuning [8, 148], limiting their ability to\nautonomously discover optimal strategies.\nRecently, reinforcement learning (RL) [178] has emerged as a promising paradigm for developing adaptive and\nautonomous search agents [84, 202]. We define RL-based agentic search as training an LLM as a decision-making agent\nthat interacts with a search environment, receives external feedback, and iteratively improves its strategy to maximize\nrewards. This formulation highlights three key aspects: (i) autonomy, where the agent determines its search actions; (ii)\nlearning, where strategies are acquired through reinforcement rather than manual design; and (iii) interaction, where\nthe agent engages in multi-turn exchanges with search environments to refine reasoning and retrieval.\nDespite rapid progress, a systematic understanding of RL–based agentic search remains limited. As summarized in\nTable 1, recent surveys [58, 102, 220] have examined agentic search from various perspectives. However, they either\npay less attention to RL [220] or focus on specific sub-domains such as Deep Research [102] and RAG [58]. The role of\nRL in enabling adaptive and autonomous search behaviors remains underexplored. In contrast, this paper presents\nthe first comprehensive survey dedicated to RL-based agentic search, aiming to clarify how RL benefits agentic search\nacross three complementary dimensions: (i) What RL is for, describing its functional roles in guiding retrieval, reasoning,\nand decision making; (ii) How RL is used, covering optimization strategies such as reward design, policy learning, and\nadvanced training methods; and (iii) Where RL is applied, examining the scope of RL intervention from the agent level\nto the step and module levels. For each dimension, we review representative methods and summarize emerging trends.\nThe overview structure of our paper is shown in Figure 1.\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n3\nTable 1. Comparison of representative surveys and this work. ✓indicates the topic is a primary focus; ✗indicates limited or no\ncoverage. Unlike prior surveys that focus on non-RL agentic RAG or general search agents, or on RL methods limited to building\ndeep-research systems, our work uniquely unifies RL foundations with agentic search behavior, analyzing how RL benefits\nagentic search, how it optimizes search agents, and how such systems can be effectively evaluated.\nSurvey\nAnalytical Focus\nRL Foun-\ndations\nSearch\nBehavior\nReasoning\nIntegration\nEvaluation\nScope\nApplication Scope\nSingh et al. [169]\nAgentic RAG\n✗\n✗\n✓\n✓\n✓\nLiang et al. [108]\nReasoning in RAG\n✗\n✗\n✓\n✗\n✗\nGao et al. [58]\nReasoning in RAG\n✗\n✗\n✓\n✓\n✓\nXi et al. [220]\nGeneral search agents\n✗\n✓\n✗\n✓\n✓\nLi et al. [102]\nRL-based deep research\n✓\n✗\n✗\n✓\n✓(Deep Research)\nOurs\nRL-based agentic search\n✓\n✓\n✓\n✓\n✓\nThis paper is organized as follows: Section 2 introduces the foundations of agentic search and RL. From Sections 3\nto 5, we examine RL for agentic search from the three perspectives outlined above. Section 6 reviews evaluation metrics\nand representative applications, and Sec. 7 concludes with open challenges and future directions.\n2\nBackground and Preliminary\n2.1\nLarge Language Models as Agents\nLLMs [114, 137, 189, 194, 229, 239] have demonstrated remarkable capabilities in text understanding, reasoning, and\ngeneration, fundamentally reshaping how humans access and interact with information. Their success has enabled\nnatural language interfaces to diverse knowledge resources. However, these models remain limited by static training\ncorpora, hallucinations, and their inability to access real-time or domain-specific knowledge directly [75]. To over-\ncome these, researchers have increasingly augmented LLMs with external information sources and decision-making\ncapabilities. A prominent direction is Retrieval-Augmented Generation (RAG) [57, 92, 117], where LLMs query external\nknowledge bases to ground responses in retrieved evidence. Building on this paradigm, recent advances [148, 247]\nfurther position LLMs as agentic systems, capable of invoking external tools such as search engines, code interpreters,\nknowledge-base query APIs, and web browsers to interact with dynamic environments and perform multi-step reasoning.\n2.2\nFrom Traditional IR to Agentic Search\n2.2.1\nTraditional IR. In classical information retrieval (IR), the primary objective is to return a ranked list of documents\nthat best match a user query, relying on statistical models such as TF–IDF [172] and BM25 [155], as well as link analysis\nmethods like PageRank [18, 138] that incorporates metadata beyond pure texts. Retrieval itself is the endpoint of the\nprocess, leaving users to interpret and synthesize the results. In addition, while effective for many tasks, traditional IR\nmethods are fundamentally limited in their ability to capture complex user intent or perform multi-step reasoning [161].\n2.2.2\nRAG. Retrieval-Augmented Generation (RAG) [92] integrates retrieval into the generation process by conditioning\nLLM responses on retrieved documents. In its standard pipeline, the model issues a query, retrieves relevant evidence,\nand generates an answer based on this input. While this retrieve–then–read architecture improves factual grounding,\nit remains limited: RAG is typically single-turn, lacks mechanisms for adaptive query refinement, and is vulnerable\nto irrelevant or noisy retrievals [82, 83]. Iterative extensions [8, 190] allow multiple rounds of retrieval, but these\napproaches still position the LLM as a largely passive consumer of evidence rather than an active search agent.\nManuscript submitted to ACM\n4\nMinhua Lin et al.\nTotal Name\nIntroduction\n(§1)\nBackground\n(§2)\nLLMs as Agents (§2.1)\nFrom IR to Agentic Search (§2.2)\nTraditional IR (§2.2.1)\nTF-IDF [2]; BM25 [154]; PageRank [13]\nRAG (§2.2.2)\nNaive RAG [92]; Iterative RAG [8, 190]\nAgentic Search (§2.2.3)\nRL Basic (§2.3)\nOn-policy Optimization (§2.2.1)\nPPO [160]; GRPO [162]; DAPO [235]\nOff-policy Optimization (§2.2.2)\nDPO [150]; ReMix [109]\nRL-based Agentic Search (§2.4)\nWhat RL is for\n(§3)\nRetrieval Control (§3.1)\nAdaptive Search Decisions\n(§3.1.1)\nSearch-R1 [84]; ReSearch [26]; R1-Searcher [170]; Deep-\nRAG [63]; UR2 [104]; SSRL [50]; VERITAS [228]\nSearch Intensity (§3.1.2)\nPangu DeepDiver [165]; ReZero [42]; StepSearch [202];\nReasonRAG [241]; WebSailor-V2 [98]\nSearch Efficiency (§3.1.3)\nIKEA [73]; R1-Searcher++ [171]; DeepRAG [63]; Search\nWisely [215]; StepSearch [202]; ZeroSearch [176]; Paral-\nlelSearch [245]; RAG-R1 [182]; WebThinker [106]...\nQuery Optimization (§3.2)\nConversational Reformulation\n(§3.2.1)\nConvSearch-R1 [254]; MaskSearch [216]; RAG-R1 [182];\nParallelSearch [245]\nRetriever-aware Optimization\n(§3.2.2)\nDeepRetrieval [79]; ZeroSearch [176]; s3 [80]; Web-\nThinker [106]\nReasoning-Retrieval Integration\n(§3.3)\nReasoning–Search Interleaving\n(§3.3.1)\nR-Search [244]; AutoRefine [166]; EvolveSearch [238];\nReasonRAG [241]; O2-Searcher [130] ...\nContext and Memory Manage-\nment (§3.3.2)\nReSum [217]; SFR-DeepResearch [135]\nMulti-Agent Collaboration (§3.4)\nPlanner–Executor Architectures\n(§3.4.1)\nMAO-ARAG [30]; OPERA [119]; AI-SearchPlanner [131]\nCooperative Multi-Agent Systems\n(§3.4.2)\nSIRAG [195]; MMOA-RAG [29]; AgentGym-RL [222];\nChain-of-Agents [103]\nMulti-Agent Collaboration (§3.5)\nMulti-tool and Multi-modality\nReasoning (§3.5.1)\nTool-Star [45]; VerlTool [76]; WebWatcher [59]; AI-\nSearchPlanner [131]; WebSailor-V2 [98]; Visual-\nARFT [122]; VRAG-RL [198]; MMSearch-R1 [211]\nStructured Knowledge Navigation\n(§3.5.2)\nGRAIL [21]; DynaSearcher [64]\nHow RL is\nUsed (§4)\nTraining Regime (§4.1)\nReward Design (§4.2))\nOutcome-level Rewards (§4.2.1)\nAnswer Correctness [84, 170, 177]; Format Reward [26];\nSearch Efficiency [63, 104, 171]; Search Effectiveness [42,\n80, 131, 165]; Tool Effectiveness [43]\nProcess-level Rewards (§4.2.2)\nTrajectory Quality [61, 167, 241]; Intermediate Action\nQuality [122, 195, 202, 245, 254]; Retrieval Quality [70,\n202, 234, 244]\nWhere RL is\nApplied (§5)\nAgent-level (§5.1)\nSingle-agent Optimization (§5.1.1)\nSearch-R1[84]; ReSearch[26]; R1-Searcher++[171]; Auto-\nCoA [242]; DeepRAG[63]; WebSailor[100] ...\nMulti-agent Coordination (§5.1.2)\nHARIS [70]; SIRAG[195]; MAO-ARAG[30]; MMOA-\nRAG[29]; OPERA [119]\nModule-level & Step-level (§5.2))\nModule-level Optimization\n(§5.2.1)\ns3[80]; AI-SearchPlanner[131]; DeepResearcher [247]\nStep-level Optimization (§5.2.2)\nStepSearch[202]; AutoRefine[166]; Search Wisely[215];\nConvSearch-R1 [254]; Atom-Searcher [44];\nReasonRAG[241]; SWiRL [61]\nSystem-level (§5.3))\nUnified RL-based Framework\n(§5.3.1)\nAgentGym-RL[222]; Verl [163]; VerlTool[76]; RAG-\nGym[224]; Chain-of-Agents[103]\nEvaluation and\nApplication\n(§6)\nMetrics (§6.2))\nAnswer Quality (§6.2.1)\nEM [84]; F1 score [26]; LLM Judge [64] ...\nSearch Effectiveness (§6.2.2)\nRecall; MRR; NDCG [79, 254] ...\nSearch Efficiency (§6.2.3)\nQuery Number [165]; API Call Cost [30]; Response\nTime [115]; Search Redundancy [171]\nSpecialized Process Metric (§6.2.4)\nQuery Quality [195]; Evidence Utilization Rate [244] ...\nApplications (§6.3))\nDataset (§6.1)\nKnowledge-Intensive QA (§6.1.1)\nNQ [90]; TriviaQA [86]; HotpotQA [231]; 2WikiMulti-\nHopQA [68]; MuSiQue [191]; PopQA [127]; CAG [140];\nC-SimpleQA [203]; SuperGPQA [48]; FEVER [188] ...\nWeb-based Search (§6.1.2)\nMind2Web [62]; WebArena [251]; WebWalkerQA [214];\nAgentBench [118]; BrowseComp-en [204] ...\nKnowledge Sources (§6.1.3)\nwiki-dump [210]; Common Crawl [40]; KILT [141];\nPubMed [134]; Arxiv [6]\nMulti-modal (§6.1.4)\nInfoSeek [28]; MMSearch [77]; MMSearch-Plus [185]\nSimpleVQA [33]; LiveVQA [54]; MM-BrowseComp [101];\nMAT-Search [122]; Mocheg [232]; MFC-Bench [200]\nConversational and Multi-turn\nSearch (§6.1.5)\nCoQA [152]; QuAC [35]; MSMarco [10]; TopiOCQA [1];\nQReCC [5]; OR-QuAC [149]; NarrativeQA [88] ...\nDomain-specific Search (§6.1.6)\nMATH [67]; MedQA [85]; OlympiadBench [65];\nHLE [143]; MIRAGE [46]; HERB [36]; SciQ [208] ...\nChallenges\nand Future\nDirection (§7)\nFig. 1. Overview of RL-based Agentic Search.\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n5\n2.2.3\nAgentic Search. Agentic search moves beyond RAG by framing the LLM as an autonomous decision-making\nagent. Rather than passively consuming retrieved documents, the model determines when, where, and how to search,\nand integrates retrieved evidence into its ongoing reasoning and actions. This paradigm, often instantiated as deep\nresearch agents [226], represents a shift from retrieval as static evidence injection to retrieval as dynamic tool use for\nproblem solving. Formally, deep research agents are LLM-powered systems that integrate dynamic reasoning, adaptive\nplanning, multi-turn data retrieval, tool use, and evidence synthesis to support complex informational research tasks.\n2.3\nBasics of Reinforcement Learning\nAgent\nEnvironment\naction\nreward\nstate\n𝑎𝑡\n𝑠𝑡\n𝑟𝑡\nFig. 2. Overview of RL components\nReinforcement Learning (RL) is a fundamental paradigm in machine learning that\nstudies how an agent interacts with its environment to maximize cumulative rewards\nthrough trial and error [178]. As illustrated in Figure 2, the agent observes a state 𝑠𝑡\nfrom the environment at each time step 𝑡, selects an action 𝑎𝑡according to a policy\n𝜋(𝑎𝑡|𝑠𝑡), and then receives a reward 𝑟𝑡as the environment transitions to a new\nstate 𝑠𝑡+1. The agent continuously updates its policy 𝜋to maximize the cumulative\nreward over time. Formally, such an optimization problem is modeled as a Markov\nDecision Process (MDP), represented by a tuple (S, A, T, R), where S is the set of possible states, A is the action\nspace, T : S × A × S →[0, 1] denotes the state transition probability function, and R : S × A × S →R defines the\nreward function. The optimization objective is to learn a policy 𝜋that maximizes the expected discounted cumulative\nreward Í∞\n𝑘=0 𝛾𝑘𝑟𝑡+𝑘+1, where 𝛾∈(0, 1] is the discount factor.\nPolicy gradient methods [51, 120, 160] are widely used in RL-based agentic search, as they directly optimize stochastic\npolicies over large discrete action spaces. Generally, they can be grouped into (i) on-policy optimization, which updates\nthe policy from fresh rollouts (e.g., PPO [160] and GRPO [162]); and (ii) off-policy or preference-based optimization, which\nleverages offline trajectories or preference data without requiring online sampling (e.g., DPO [150] and ReMix [109]).\n2.3.1\nOn-policy Optimization. On-policy algorithms interact with the environment using the current policy to collect\nrollouts, estimate advantages, and update the same policy that generated those samples. They are favored in large-scale\nLLM and agentic search training due to their ability to directly optimize behavioral policies under accurate reward\nsignals. Within this family, two subgroups can be distinguished:\n• Critic-based algorithms. These methods rely on an explicit value function or critic model to estimate the expected\nreturn for each state or token. The critic provides token-level feedback that reduces the variance of policy gradients\nand stabilizes training, but it also introduces additional computational cost and memory overhead. PPO [160] is the\nmost widely used example of this paradigm.\n• Critic-free algorithms. In contrast, critic-free approaches remove the value network entirely and estimate the\nadvantage directly from relative reward statistics. Instead of relying on learned value predictions, these algorithms\nsample multiple responses for each input and compute a group-based advantage by normalizing rewards within the\ngroup. This strategy significantly reduces training complexity and GPU memory consumption while maintaining\nstable optimization. Representative examples include GRPO [162], Dr.GRPO [120], DAPO [235], and GiGPO [51].\nProximal Policy Optimization (PPO). PPO [160] is one of the most widely used methods for training RL agents. It\naims to maximize the following objective function:\nJ𝑃𝑃𝑂(𝜃) = E𝑥∼D,𝑦∼𝜋𝑜𝑙𝑑(·|𝑥)\n\u0014\nmin\n\u0012 𝜋𝜃(𝑦|𝑥)\n𝜋𝑜𝑙𝑑(𝑦|𝑥) 𝐴,\n𝑐𝑙𝑖𝑝𝜖\n\u0012 𝜋𝜃(𝑦|𝑥)\n𝜋𝑜𝑙𝑑(𝑦|𝑥)\n\u0013\n𝐴\n\u0013\n−𝛽D𝐾𝐿(𝜋𝜃||𝜋𝑟𝑒𝑓)\n\u0015\n,\n(1)\nManuscript submitted to ACM\n6\nMinhua Lin et al.\nwhere 𝜋𝜃and 𝜋𝑜𝑙𝑑denote the current and previous policy models, respectively. 𝜋𝑟𝑒𝑓is the reference model that\nregularizes the policy update via a KL-divergence penalty, measured and weighted by D𝐾𝐿and 𝛽, respectively. 𝑥denotes\nthe input samples drawn from the distribution 𝐷. 𝑐𝑙𝑖𝑝𝜖is the clipping function with hyperparameter 𝜖for stabilizing\ntraining. The advantage estimate 𝐴is computed using Generalized Advantage Estimation (GAE) [159], based on the\nreward 𝑟and a learned value function 𝑉𝜓.\nGroup Relative Policy Optimization (GRPO). GRPO [162] extends PPO by eliminating the need for a separate value\nfunction model, which often doubles memory usage. Instead, it estimates relative advantages within groups of sampled\nresponses from the same input, leading to improved training efficiency. Specifically, for each input 𝑥∈𝐷, GRPO samples\na group of outputs {𝑦1,𝑦2, · · · ,𝑦𝐺} from the old policy 𝜋𝑜𝑙𝑑and optimizes the new policy 𝜋𝜃by maximizing:\nJ𝐺𝑅𝑃𝑂(𝜃) = E𝑥∼D,{𝑦𝑖}𝐺\n𝑖=1∼𝜋𝑜𝑙𝑑(·|𝑥)\n1\n𝐺\n𝐺\n∑︁\n𝑖=1\n\u0014\nmin\n\u0012 𝜋𝜃(𝑦|𝑥)\n𝜋𝑜𝑙𝑑(𝑦|𝑥) 𝐴𝑖,𝑐𝑙𝑖𝑝𝜖\n\u0012 𝜋𝜃(𝑦|𝑥)\n𝜋𝑜𝑙𝑑(𝑦|𝑥)\n\u0013\n𝐴𝑖\n\u0013\n−𝛽D𝐾𝐿(𝜋𝜃||𝜋𝑟𝑒𝑓)\n\u0015\n,\n(2)\nwhere 𝐴𝑖is the advantage computed using rewards {𝑟1,𝑟2, . . . ,𝑟𝐺} corresponding to the outputs within each group:\n𝐴𝑖= 𝑟𝑖−mean({𝑟1,𝑟2, . . . ,𝑟𝐺})\nstd({𝑟1,𝑟2, . . . ,𝑟𝐺})\n.\n(3)\nDecoupled Clip and Dynamic Sampling Policy Optimization (DAPO). DAPO [235] is an emerging RL approach\nfor training long chain-of-thought (CoT) reasoning models. Specifically, DAPO addresses several limitations of GRPO,\nincluding entropy collapse, reward noise, and training instability. It introduces four key techniques to improve RL\nperformance in long CoT scenarios: clip-higher, dynamic sampling, token-level policy gradient loss, and overlong\nreward shaping. Formally, the objective function for DAPO aims to maximize the following:\nJ𝐷𝐴𝑃𝑂(𝜃) =E𝑥∼D,{𝑦𝑖}𝐺\n𝑖=1∼𝜋𝑜𝑙𝑑(·|𝑥)\n1\n𝐺\n𝐺\n∑︁\n𝑖=1\n\u0014\nmin\n\u0012 𝜋𝜃(𝑦|𝑥)\n𝜋𝑜𝑙𝑑(𝑦|𝑥) 𝐴𝑖,𝑐𝑙𝑖𝑝\n\u0012 𝜋𝜃(𝑦|𝑥)\n𝜋𝑜𝑙𝑑(𝑦|𝑥) , 1 −𝜖𝑙𝑜𝑤, 1 + 𝜖ℎ𝑖𝑔ℎ\n\u0013\n𝐴𝑖\n\u0013\u0015\n𝑠.𝑡., 0 < |{𝑦𝑖| 𝑖𝑡_𝑒𝑞𝑢𝑖𝑣𝑎𝑙𝑒𝑛𝑡(𝑥, 𝑦𝑖)}| < 𝐺,\n(4)\nwhere 𝐴𝑖is the advantage estimate defined in Eq. (3). 𝜖ℎ𝑖𝑔ℎis typically larger than 𝜖𝑙𝑜𝑤to provide more flexibility for\nincreasing low-probability tokens, and 𝑖𝑡_𝑒𝑞𝑢𝑖𝑣𝑎𝑙𝑒𝑛𝑡is the dynamic sampling function that over-samples and filters\nout prompts with accuracy equal to 1 or 0. Note that the KL term is excluded in DAPO because the model distribution\ncan diverge significantly from the initial model during the training of long CoT models.\n2.3.2\nOff-policy Optimization. Off-policy and preference-based algorithms, in contrast, do not require new rollouts\nfrom the current policy. Instead, they learn from previously collected trajectories or explicit preference annotations,\nwhich greatly improves data efficiency and stability. These methods are particularly useful in large-scale LLM alignment\nand agentic search scenarios, where collecting online feedback is costly or impractical.\nDirect Preference Optimization (DPO). DPO [150] is a representative RL-free approach for aligning LLMs with\nhuman preferences. Unlike conventional Reinforcement Learning from Human Feedback (RLHF) [37, 137, 174, 243],\nwhich trains a separate reward model and performs iterative policy optimization (e.g., via PPO), DPO formulates\nalignment as a direct probabilistic classification problem. It bypasses the explicit reward modeling and RL loop by\nlearning directly from preference-labeled response pairs. Formally, Given a dataset D containing triplets (𝑥,𝑦𝑤,𝑦𝑙),\nwhere 𝑥is a prompt, and 𝑦𝑤and 𝑦𝑙denote the preferred (winning) and dispreferred (losing) responses respectively, the\npreferences are assumed to be generated by an underlying latent reward function 𝑟∗(𝑦,𝑥) such that 𝑟∗(𝑦𝑤,𝑥) > 𝑟∗(𝑦𝑙,𝑥).\nDPO optimizes the policy 𝜋𝜃to increase the relative likelihood of 𝑦𝑤over 𝑦𝑙with respect to a reference model 𝜋ref as:\nJ𝐷𝑃𝑂(𝜃) = E(𝑥,𝑦𝑤,𝑦𝑙)∼D\n\u0014\nlog𝜎\n\u0012\n𝛽𝜋𝜃(𝑦𝑤|𝑥)\n𝜋𝑟𝑒𝑓(𝑦𝑤|𝑥) −𝛽𝜋𝜃(𝑦𝑙|𝑥)\n𝜋𝑟𝑒𝑓(𝑦𝑙|𝑥)\n\u0013\u0015\n,\n(5)\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n7\nQuery\nSingle Agent\nMulti Agents\nLLM Policy\nQuery Rewriter\nRetriever\nAnswer Agent\n…\nRetrieval Control\nLLM \nPolicy\nSearch \nEngine\nReasoning\nSearch \nThink\nTool & knowledge Integration\nLLM \nPolicy\nSearch \nEngine\nCode \nExecuter\n…\nQuery Optimization\nLLM \nPolicy\nQuery\nRefined \nQuery\nReasoning-Retrieval \nIntegration\nLLM \nPolicy\nRefined \nReasoning\nRetrieved \nEvidence\nMemory\nFinal \nAnswer\nTrainable\nFreeze\nFig. 3. Overview of RL-based agentic search. RL intervenes at multiple decision points—controlling when to retrieve (retrieval control),\nhow to formulate queries (query optimization), how to integrate evidence into reasoning (reasoning-retrieval integration), and which\ntools or knowledge sources to use (tool and knowledge integration).\nwhere 𝜋𝑟𝑒𝑓is the reference model, and 𝛽is a hyperparameter that controls the strength of this regularization. The 𝜎\nfunction is the sigmoid, which helps to optimize the relative probability of the two responses. By using this objective,\nDPO directly optimizes the policy to reflect human preferences without needing an intermediate reward model.\n2.4\nRL-based Agentic Search\nIn agentic search, retrieval and reasoning are embedded in a sequential decision process rather than executed as fixed,\none-shot steps. The agent must decide when to search, how to formulate or refine queries, and how to incorporate\nretrieved evidence into multi-step reasoning. Figure 4 sketches this pipeline and highlights the decision points where\nRL can intervene: (i) search control (whether/when to retrieve), (ii) query optimization (how to retrieve), and (iii)\nreasoning integration (how to use retrieved information).\n2.4.1\nComparison with Pre-RL Agentic Search. Before the introduction of RL into agentic search, most systems relied\non either structured prompting [31, 91, 196, 225, 252] or supervised fine-tuning (SFT) [3, 7, 158, 246] to guide retrieval\nand reasoning behaviors.\nPrompting-based Methods. These methods primarily depend on human-designed heuristics and pre-defined reasoning\nworkflows. For instance, PlanRAG [91] and MetaRAG [252] employ an iterative loop in which the agent alternates\nbetween searching, generating an answer, and reflecting on its quality before deciding whether to conduct further\nsearches. This process repeats until a satisfactory response is achieved. Similarly, Knowledge-driven CoT [196] follows\na reflection chain that encourages the model to re-evaluate intermediate reasoning and adjust its strategy dynamically\nbased on retrieved evidence. While effective, these prompting-based systems rely on fixed symbolic templates or\nhandcrafted prompt structures that cannot adapt to unseen task distributions or dynamic retrieval environments.\nSFT-based Methods. These methods train models on datasets of high-quality trajectories that include search, re-\nflection, and generation actions, allowing the model to internalize these behaviors into its parameters. For example,\nToolformer [158] fine-tunes an LM on self-labeled data where API calls are automatically inserted into text generation.\nIt learns to decide when and how to use external tools such as calculators or Wikipedia search engines, improving\nfactuality without additional human supervision. Similarly, SelfRAG [7] introduces self-reflective retrieval-augmented\ngeneration, where the model is supervised to generate both normal tokens and special reflection tokens (e.g., <Retrieve>,\n<Relevant>, <Supported>) that indicate when to retrieve new evidence and how well each generation is supported\nManuscript submitted to ACM\n8\nMinhua Lin et al.\nby retrieved passages. Despite these advances, SFT-based approaches remain fundamentally imitation-driven. They\ncan capture correlations between context and actions but lack mechanisms for long-horizon credit assignment or\noutcome-driven optimization.\nLimitations and Why RL. Despite their progress, both prompting- and SFT-based agents face inherent limitations:\n• Poor adaptivity: Their behaviors are largely predefined or imitated from static datasets. They cannot dynamically\nadjust retrieval frequency or reformulate queries when facing unseen tasks or API behaviors.\n• Supervision bottleneck: High-quality reasoning and search trajectories are costly to collect and difficult to scale across\ntasks, which constrains generalization and makes further improvement beyond demonstrations challenging.\nRL provides a principled way to overcome these issues by optimizing the agent as a policy 𝜋𝜃that interacts with an\nenvironment, receives feedback, and adapts through trial and error. Unlike SFT-based imitation, RL directly optimizes\ntask-level rewards that integrate correctness, cost, and latency, enabling the discovery of adaptive and efficient retrieval\npolicies. This paradigm allows the agent to reason about the long-term consequences of each search decision, moving\nbeyond static imitation toward outcome-driven learning.\n2.4.2\nFormalization. Formally, RL-based agentic search can be modeled as a MDP. The goal is to train a policy 𝜋𝜃that\nmaximizes cumulative reward by taking a sequence of actions in an environment. The key components are: (i) Agent:\nThe LLM policy 𝜋𝜃, parameterized by 𝜃, which generates actions conditioned on the current state; (ii) Environment:\nExternal resources the agent can interact with, such as search engine APIs, retrievers, knowledge graphs, or tool\ninterfaces; (iii) State (𝑠𝑡): The current context, including the original query, intermediate reasoning traces, retrieved\nevidence, and action history; (iv) Action (𝑎𝑡): A discrete decision, such as issuing a query, reformulating an existing\nquery, selecting documents, invoking tools (e.g., search APIs, retrievers), or terminating with a final answer; (v) Action\n(𝑎𝑡): A discrete decision, such as issuing a query, reformulating an existing query, selecting documents, invoking tools\n(e.g., search APIs, retrievers), or terminating with a final answer; (vi) Reward (𝑟𝑡): A scalar feedback signal capturing\ntask success (e.g., answer correctness, factual consistency), process quality (e.g., query efficiency, reasoning coherence),\nor resource costs (e.g., API calls, latency); and (vii) Transition (T): The dynamics induced by both the environment\n(e.g., a search engine returning documents) and the agent’s internal updates.\n3\nWhat RL is for: Functional Roles in Agentic Search\nRL plays a wide range of functional roles within agentic search, extending well beyond basic retrieval. In this section,\nwe categorize these roles into five major dimensions to illustrate how RL enables agents to decide not only when to\nsearch, but also how to formulate queries, how to interleave reasoning with evidence, and how to coordinate across\nmultiple agents and tools. Table 2 summarizes representative works of each RL’s role.\n3.1\nRetrieval Control\nA core role of RL in agentic search is to control whether, when, and how an agent retrieves external information. Rather\nthan being a fixed design principle, this perspective synthesizes recent trends observed across RL-based retrieval\nsystems [73, 84, 202, 215], where retrieval control emerges as a central optimization target. Effective retrieval control\nis crucial, since excessive or unnecessary queries increase cost and latency, while insufficient retrieval risks missing\ncritical evidence. RL enables agents to balance this trade-off by learning adaptive retrieval policies that respond to task\ncontext and uncertainty. Methods in this category address three key aspects: (i) adaptive search decisions—whether to\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n9\nTable 2. The categorization of RL-based search agents from functional roles’ perspective.\nCategory\nFunctional Roles\nMethods\nRetrieval Control\nAdaptive search decisions\nSearch-R1 [84]; ReSearch [26]; DeepRAG [63]; UR2 [104];\nSSRL [50]; R1-Searcher [170]; AutoCoA [242];DeepNote [199];\nSWiRL [61];DeepResearcher [247]; MedResearcher-R1 [234]\nSearch intensity and persistence\nPangu DeepDiver [165]; ReZero [42]; StepSearch [202]; Reason-\nRAG [241]; WebSailor-V2 [98]\nSearch efficiency\nIKEA [73]; R1-Searcher++ [171]; DeepRAG [63]; Search\nWisely [215]; StepSearch [202]; ZeroSearch [176]; Paral-\nlelSearch [245]; RAG-R1 [182]; ReasonRAG [241]; Web-\nThinker [106];DeepResearcher [247]\nQuery Optimization\nConversational reformulation\nConvSearch-R1 [254]; MaskSearch [216]; RAG-R1 [182]; Paral-\nlelSearch [245]; OPERA [119]; WebExplorer [116]; DeepNote [199]\nRetriever-aware optimization\nDeepRetrieval [79]; ZeroSearch [176]; s3 [80]; WebThinker [106];\nMMOA-RAG [29]\nReasoning–Retrieval\nIntegration\nReasoning–search interleaving\nSWiRL [61] R-Search [244]; AutoRefine [166]; EvolveSearch [238];\nReasonRAG [241]; O2-Searcher [130];Atom-Searcher [44]\nContext and memory management\nReSum [217]; SFR-DeepResearch [135]; DeepResearcher [247];\nRECON [227]; WebSailor [100]; WebSailor-V2 [98]; ASearcher [56]\nMulti-Agent\nCollaboration\nPlanner–executor orchestration\nMAO-ARAG [30]; OPERA [119]; AI-SearchPlanner [131]\nCooperative multi-agent systems\nSIRAG [195]; MMOA-RAG [29]; AgentGym-RL [222]; Chain-of-\nAgents [103]; WebExplorer [116]\nTool and Knowledge\nIntegration\nMulti-tool\nTool-Star [45]; VerlTool [76]; WebWatcher [59]; AI-\nSearchPlanner [131]; WebSailor-V2 [98]; WebResearcher [147];\nMedResearcher-R1 [234]\nMulti-modality\nVisual-ARFT [122]; VRAG-RL [198]; MMSearch-R1 [211]; Web-\nWatcher [59]\nStructured knowledge navigation\nGRAIL [21]; DynaSearcher [64]\nretrieve or rely on parametric knowledge, (ii) search intensity and persistence—how often and how deeply to retrieve,\nand (iii) search efficiency—minimizing redundancy, cost, and latency while preserving task performance.\n3.1.1\nAdaptive Search Decisions. RL enables agents to decide whether a question can be answered using internal\nparametric knowledge or requires external retrieval. Search-R1 [84], ReSearch [26], and R1-Searcher [170] are early\nexamples that teach LLMs to invoke search engines only when necessary. Specifically, as shown in Table 3, these methods\nencourage LLMs to call a search engine to access external information when the internal knowledge is insufficient to\nproduce an accurate answer. Building on this idea, DeepRAG [63] formulates RAG as a MDP, where complex queries\nare iteratively decomposed into atomic subqueries, each representing a focused information need. At each reasoning step,\nthe agent decides whether to answer the subquery using its parametric knowledge or to retrieve external evidence,\nguided by a reward that jointly optimizes answer correctness and retrieval cost.\n3.1.2\nSearch Intensity. For complex or ambiguous queries, a single retrieval attempt may be insufficient. RL has been\nused to optimize the depth and persistence of the search process. Pangu DeepDiver [165] introduces Search Intensity\nManuscript submitted to ACM\n10\nMinhua Lin et al.\nScaling, rewarding agents for intensifying retrieval when ambiguity is detected. ReZero [42] rewards retry attempts\nafter failed searches, encouraging persistence and robustness. StepSearch [202] introduces step-wise rewards based on\ninformation gain and redundancy penalties to guide retrieval step by step.\n3.1.3\nSearch Efficiency. Efficiency concerns both the cost of retrieval (e.g., number of API calls, training rollouts)\nand the time required to complete searches. R1-Searcher++ [171] extends R1-Searcher by introducing a group reward\nthat measures retrieval thriftiness through the variance of retrieval counts across responses, rewarding the correct\nanswer that requires the fewest retrieval calls while penalizing redundant searches. IKEA [73] introduces knowledge-\nboundary–aware rewards that favor internal reasoning unless external retrieval is necessary. Search Wisely [215]\nimproves cost efficiency by filtering low-confidence queries that are likely to yield poor results. StepSearch [202]\npenalizes redundant queries with step-wise rewards, encouraging more concise retrieval strategies. ZeroSearch [176]\nreduces API overhead by simulating retrieval in latent space, enabling curriculum-style training without reliance on\nreal search engines. Beyond reducing retrieval calls, ParallelSearch [245] decomposes complex questions into parallel\nsub-queries to maintain coverage while significantly lowering response time, and RAG-R1 [182] similarly incentivizes\nmulti-query parallelism to enhance inference efficiency. In addition, WebThinker [106] extends the notion of efficiency\nfrom search cost to reasoning behavior, applying preference optimization to align query strategies with long-horizon\nreasoning objectives such as correctness, tool efficiency, and thinking conciseness, thereby refining retrieval decisions\nthrough reasoning-driven feedback rather than retrieval accuracy alone.\n3.2\nQuery Optimization\nEven when retrieval is triggered, the quality of queries strongly influences outcomes. Poorly posed queries yield\nirrelevant or noisy results. RL is then used to refine query generation based on feedback, moving beyond static heuristics.\nExisting works can be categorized into (i) conversational reformulation and (ii) retriever-aware optimization.\n3.2.1\nConversational Reformulation. In interactive settings, user queries are often ambiguous or context-dependent,\nmaking direct retrieval unreliable. RL enables agents to reformulate such inputs into self-contained queries by framing\nreformulation as a sequential decision-making process. ConvSearch-R1 [254] optimizes a rewriter policy with retrieval-\nbased rewards, where higher rewards are assigned when reformulated queries retrieve gold passages at higher ranks. Its\nrewriter is first fine-tuned through SFT on data generated via retrieval-guided self-distillation, and then refined through\nRL using a Rank-Incentive Reward Shaping function that encourages ranking gold passages higher while mitigating\nreward sparsity. This two-stage design aligns the query rewriter with retriever preferences and improves retrieval\nprecision in multi-turn search. MaskSearch [216] extends this paradigm by incorporating a Rewriter Agent to refine\nsearch queries for more comprehensive retrieval, whose outputs are further used in the reasoning traces for the SFT\nof the LLM. Instead of optimizing a separate rewriter policy, RAG-R1 [182] encourages the LLM itself to generate\nmultiple parallel queries within a single prompt to improve inference efficiency and retrieval diversity. Similarly,\nParallelSearch [245] trains LLMs to decompose complex or multi-hop questions into parallel sub-queries within a\nsingle reasoning turn. During RL fine-tuning, a decomposition reward encourages effective query breakdown, while a\nsearch-count reward penalizes excessive search actions, balancing reformulation granularity and retrieval efficiency.\n3.2.2\nRetriever-Aware Optimization. While conversational reformulation focuses on resolving user-side ambiguity,\nretriever-aware optimization instead targets the system side of query generation. It trains agents to adapt their queries\nto the characteristics, biases, and feedback signals of specific retrievers. The objective is to bridge the semantic gap\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n11\nbetween LLM-generated queries and the retriever’s actual ranking behavior, thereby improving retrieval accuracy and\nrobustness across different search infrastructures. DeepRetrieval [79] exemplifies this idea by training LLMs to produce\nqueries that align with the biases of black-box search engines, effectively exploiting retriever behavior to maximize recall.\nWebThinker [106] applies preference optimization to align query strategies with long-horizon reasoning objectives\nsuch as correctness, tool efficiency, and thinking conciseness, enabling the agent to refine its search behavior using\nreasoning-driven feedback instead of retrieval accuracy alone. ZeroSearch [176] further extends this approach by\nsimulating retrieval environments, allowing agents to learn robust query behaviors that generalize across different\nretrievers while avoiding the cost and instability of real API calls. Similarly, s3 [80] introduces a lightweight RL-based\nsearcher module decoupled from the LLM generator, enabling scalable and model-agnostic query optimization. Together,\nthese approaches highlight the broader goal of designing retriever-aware query policies that remain effective across\nheterogeneous search environments.\n3.3\nReasoning–Retrieval Integration\nBeyond deciding when and how to search effectively, knowledge-intensive tasks often require tight coupling between\nreasoning and retrieval. Evidence is only valuable if it improves reasoning, and reasoning should guide what to retrieve\nnext. RL optimizes how LLMs interleave these processes, manage context, and refine reasoning based on feedback.\n3.3.1\nReasoning–Search Interleaving. Beyond simply allowing retrieval during reasoning [26, 84], RL optimizes retrieval\nto enhance reasoning quality. R-Search [244] introduces an evidence reward to encourage high-quality query generation\nyielding more informative evidences. AutoRefine [166] extends the standard “search-and-think” paradigm to “search-and-\nrefine-during-think,” rewarding intermediate refinement steps to reinforce faithful and targeted knowledge extraction.\nEvolveSearch [238] further strengthens reasoning–retrieval interplay through iterative cycles of SFT and RL to enhance\nthe data efficiency during training, enabling agents to progressively refine both their reasoning paths and retrieval\nstrategies. In contrast, MaskSearch [216] focuses on enhancing the model’s retrieval-aware reasoning ability before\nRL optimization. It introduces a Retrieval-Augmented Mask Prediction (RAMP) pretraining task, which teaches the\nmodel to leverage external search tools to fill masked spans with retrieved knowledge in the SFT stage. This pre-RL\nobjective establishes a retrieval-aware prior that aligns reasoning and retrieval behaviors, enhancing the universal\nsearch capabilities across various downstream tasks.\n3.3.2\nContext and Memory Management. While existing agentic search systems [84, 202, 244] are effective for short-\nhorizon tasks such as single-turn retrieval or step-level reasoning, they often struggle in long-horizon or multi-session\nsettings, where agents must manage extended interaction histories within limited context windows. To operate efficiently\nunder such constraints, agents need to actively manage memory—deciding what information to retain, summarize, or\ndiscard as a search episode unfolds. Recent studies [24, 56, 98, 100, 217, 227] apply RL to optimize this process, framing\nmemory control as a sequential decision problem balancing information fidelity and context efficiency. Specifically, two\ncomplementary strategies have emerged:\n• Internal management: The agent itself performs memory operations such as summarizing, refreshing, or pruning\nits working context under RL guidance. For instance, ReSum [217] trains agents with RL to generate concise\nsummaries of past reasoning and interactions, enabling long-context reasoning without exceeding token limits.\nSFR-DeepResearch [135] further introduces explicit memory actions (e.g., clean_memory, store_snippet), using RL\nsignals to decide when to retain or discard past information, thus preventing memory overflow and redundancy.\nManuscript submitted to ACM\n12\nMinhua Lin et al.\nQuery\nRewriter\nAnswer\nAgent\nRetriever\n…\nPlanner\nMulti-turn\nTrajectory\nFinal \nAnswer\nQuery\nSearch \nEngine\nWeb \nBrower\n…\nTool \nBox\nCode \nExecuter\nQuery\nQuery\nRewriter\nSub-\nquestions\nRetriever\nDocuments\nSelector\nSelected\nDocuments\nAnswer \nAgent\nFinal \nAnswer\nGloden\nAnswer\nGlobal \nReward\n(a) Planner-executor architecture\n(b) Cooperative multi-agent system\nFig. 4. Overview of RL for multi-agent collaboration. (a) Planner–executor architecture: a central planner coordinates specialized\nexecutor agents for task decomposition and dynamic subtask allocation. (b) Cooperative multi-agent system: multiple agents jointly\noptimize shared objectives through communication, coordination, and reward sharing.\n• External management: Other frameworks use auxiliary summarization modules to compress historical context\nbefore reinjection into the agent’s reasoning stream. In such cases, RL or policy learning is used to determine when\nand how to invoke these summarizers. For example, WebSailor [100] employs an external summarizer to condense\nbrowsing traces for multi-page search; ASearcher [56] dynamically summarizes multi-turn research sessions to\npreserve key findings; and RECON [227] integrates a frozen, pretrained summarizer into an RL-based search agent\n(e.g., Search-R1); the summarizer, trained via supervised relevance pretraining and multi-aspect distillation, enables\nthe agent to reason over concise, factual evidence while substantially reducing context length and cost.\n3.4\nMulti-Agent Collaboration\nBeyond relying on a single LLM to handle both reasoning and retrieval, advanced agentic search systems [30, 195]\ndecompose the process into multiple specialized modules, such as query rewriting [125], document selection [87],\nand reasoning control. RL is then used to align the objectives of distinct agents, ensuring that local decisions, such as\nwhen to reformulate, which evidence to retain, and how to schedule retrieval steps, contribute to globally coherent\nand efficient search. Existing approaches can be broadly categorized into (i) planner–executor architectures and (ii)\ncooperative multi-agent systems.\n3.4.1\nPlanner–Executor Architectures. A representative paradigm is the planner–executor architecture, where a high-\nlevel planner orchestrates specialized executors responsible for distinct retrieval or reasoning operations. As shown in\nFigure 4(a), the planner acts as a meta-policy that decides which executor to invoke, when to switch subtasks, and how\nto allocate search or computational budgets, thus achieving adaptive orchestration across heterogeneous RAG modules.\nMAO-ARAG [30] exemplifies this design. It models multi-agent RAG as a multi-agent semi-Markov decision process\n(MSMDP), where the planner coordinates executors such as query rewriters, document selectors, retrievers, and\ngenerators. Specifically, a planner agent intelligently se- lects and integrates the appropriate agents from these execu-\ntors into a suitable workflow tailored for each query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using PPO, optimizing by the following reward:\n𝑟𝑡= 𝑟F1 −𝛼· 𝑟CP −𝑟FP,\n(6)\nwhere 𝑟F1 is the outcome-based reward based on F1 score, and 𝑟CP and 𝑟FP are the cost penalty and format penalty,\nrespectively. These rewards together improve answer quality while keeping costs within a reasonable range.\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n13\nOPERA [119] extends this idea to multi-hop retrieval and reasoning. It adopts a hierarchical RL framework composed\nof a high-level planning module and low-level execution agents. Three role-specific agents, including Plan, Analy-\nsis–Answer, and Rewrite, are optimized with Multi-Agents Progressive GRPO (MAPGRPO), a GRPO-based algorithm\nthat provides fine-grained, role-specific credit assignment. Each agent is trained with tailored reward signals: the Plan\nAgent for decomposition validity, the Analysis–Answer Agent for reasoning and factual correctness, and the Rewrite\nAgent for retrieval relevance and formatting. This hierarchical optimization yields stable convergence and interpretable\nreasoning trajectories, enabling OPERA to learn cost-efficient, verifiable retrieval–reasoning workflows.\n3.4.2\nCooperative Multi-Agent Systems. Another workflow models the agentic search as a cooperative muti-agent\ngames, where each module is treated as an RL agent whose actions influence retrieval outcomes, with a shared global\nreward aligning their behaviors toward better performance. The overall framework is illustrated in Figure 4(b). For\nexample, SIRAG [195] trains a Decision-Maker to decide when to retrieve and a Knowledge-Selector to filter which\ndocuments should be passed downstream, with RL rewards aligning their decisions toward high-quality evidence\nintegration. MMOA-RAG [29] generalizes this setting to larger agent pools, where RL optimizes how agents share\nresponsibilities for query reformulation, evidence selection, and verification. In addition, some works such as AgentGym-\nRL [222] and Chain-of-Agents [103] provide general infrastructures for training multi-agent systems, where agentic\nsearch is a core evaluation setting.\n3.5\nTool and Knowledge Integration\nFinally, rather than relying solely on text retrieval, agentic search increasingly requires integration with heterogeneous\nexternal resources, including APIs [76], multi-modal tools [59, 198], and structured knowledge bases [21, 64], to extend\nthe scope of tasks agents can solve, where RL is a natural solution to enable them. Research in this category can be\ngrouped into two directions: (i) multi-tool and multi-modality reasoning, where agents learn to coordinate across diverse\ntoolkits such as search engines, code interpreters, and vision models, and (ii) structured knowledge exploration, where\nRL trains agents to navigate symbolic environments like knowledge graphs or tables in a goal-directed way.\n3.5.1\nMulti-tool and Multi-modality Reasoning. Many tasks require more than text-based retrieval, demanding agents\nto combine computation, web search, and multimodal understanding. RL has been used to optimize tool selection and\nsequencing by providing feedback on whether tool calls lead to accurate reasoning or task completion. Tool-Star [45]\nintegrates six tools, including search engines and code generators, using a self-critic RL setup that rewards correct\nintermediate outputs. VerlTool [76] generalizes this with a unified RL framework that manages heterogeneous APIs and\nmulti-modal LLMs (MLLMs). In multi-modal contexts, MMSearch-R1 [211], Visual-ARFT [122], and VRAG-RL [198]\nextend Search-R1 paradigms to visual question answering by rewarding policies that align retrieved text and visual\nevidence. WebWatcher [59] further trains agents with RL to coordinate multiple tools simultaneously, handling both\ntextual and visual inputs.\n3.5.2\nStructured Knowledge Navigation. In many domains, critical information is stored in structured resources such as\nknowledge graphs (KG) or databases [14, 112, 113, 248]. RL is applied by defining traversal as a sequential decision-\nmaking process: each step selects which entity or relation to follow, with rewards reflecting correctness, coverage, or\nefficiency. For instance, GRAIL [21] applies RL to learn KG traversal policies that reach correct answers efficiently. Dy-\nnaSearcher [64] extends this with multi-reward RL, jointly optimizing for accuracy, efficiency, and balanced exploration\nof KG.\nManuscript submitted to ACM\n14\nMinhua Lin et al.\nTakeaways:\n• Retrieval Control: RL improves when to retrieve, how persistently to search, and how to minimize cost\nand latency. Current limitations include the reliance on narrow reward signals (often correctness-only) and\nevaluations confined to controlled settings, which limit robustness in real, noisy retrieval environments.\n• Query Optimization: RL enables both query reformulation and retriever-aware adaptation, improving\nretrieval precision. A key gap is generalization beyond static datasets, simulators, or single-retriever setups.\n• Reasoning–Retrieval Integration: RL also extends beyond retrieval control to jointly optimize reasoning\nand evidence use, while also empowering active memory management such as summarizing, refreshing, and\npruning; yet most memory mechanisms remain heuristic and struggle with long-term continuity.\n• Multi-Agent Collaboration: RL aligns planner–executor and cooperative agents so local actions (re-\nformulate, select, verify) serve global objectives, improving division of labor and consistency in complex\npipelines.\n• Tool & Knowledge Integration: RL allows agents to coordinate heterogeneous tools and structured\nknowledge sources beyond text-only retrieval, although current systems remain at an early stage and face\nchallenges in maintaining coherent reasoning across modalities and asynchronous feedback.\nOverall, these roles form a continuum that spans from when to retrieve through how to query and how to\nthink with evidence, to who coordinates and which tools or knowledge bases to use, revealing RL as the unifying\nmechanism that grounds, scales, and organizes agentic search behaviors.\n4\nHow RL is Used: Optimization Strategies\nThis section examines how RL is applied in agentic search systems, covering training pipelines, algorithmic design, and\nreward mechanisms. Table 7 summarizes representative works with corresponding optimization strategies.\n4.1\nTraining Regime\nThe training regime defines how RL is integrated into agentic search, encompassing initialization strategies, environment\ndesign, and optimization workflows. It determines how agents acquire, refine, and stabilize their decision-making\npolicies throughout interaction-based learning.\n4.1.1\nStandard Agentic Search Pipeline. A typical RL training pipeline for agentic search, exemplified by Search-R1 [84],\ncomprises two stages: a cold-start initialization and subsequent RL fine-tuning. The cold-start phase ensures interface\ncompliance (e.g., API calls, tool schemas) and stabilizes early rollouts. During RL training, the policy LLM receives\ncomplex queries and generates interleaved reasoning and tool-use actions within simulated or real search environments.\nThe overall training pipeline and prompt template are summarized in Table 3.\n4.1.2\nCold Start. A dominant paradigm initializes agents via supervised fine-tuning (SFT) before RL optimization [45,\n100, 171, 212]. This stage equips models with baseline task competence and mitigates early instability caused by\nsparse rewards in long-horizon environments. For instance, Webagent-R1 [207] shows that SFT provides crucial web-\ninteraction knowledge for downstream RL, while WebSailor [100] finds that SFT accelerates convergence and stabilizes\nmulti-step tool use. EvolveSearch [238] further introduces a self-improving SFT–RL loop, where RL-refined policies\ngenerate new demonstrations for iterative SFT retraining. Conversely, several works [176, 222] question the necessity\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n15\nTable 3. Standard agentic search prompt template. We use the prompt template of Search-R1 [84] as an example.\nSearch-R1 Promp Template\nAnswer the given question. You must conduct reasoning inside <think> and </think> first every time you get new\ninformation. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query\n</search>, and it will return the top searched results between <information> and </information>. You can search\nas many times as you want. If you find no further external knowledge needed, you can directly provide the answer\ninside <answer> answer </answer> without detailed illustrations. For example, <think> xxx </think>. Question:\nquestion.\nof SFT. ZeroSearch [176] replaces it with latent-space retrieval simulation, enabling pure RL training without external\nsupervision, while AgentGym-RL [222] employs curriculum-based horizon scaling to stabilize RL-only training.\n4.1.3\nSimulation-Based Training. Training RL agents in real-world search environments can be prohibitively expensive,\nslow, and non-reproducible. Simulation environments provide a controlled, accelerated, and cost-effective alternative.\nFor example, ZeroSearch [176] proposes a novel RL framework that simulates search by transforming an LLM into\na retrieval module, avoiding the cost and noise of real search engines during training. It employs a curriculum that\nincrementally degrades the quality of simulated documents, forcing the agent to become more robust. O2-Searcher [130]\nalso leverages an efficient, locally simulated search environment for training, focusing on open-domain open-ended\nquestion answering scenarios. WebSailor-V2 [98] proposes a dual-environment RL framework, utilizing a high-fidelity\nsimulator for rapid algorithm iteration and a robust, managed real-world environment for stable final policy training.\nThis hybrid approach addresses the challenges of both scalability and realism.\n4.1.4\nRL Algorithms. Most RL-based search agents employ policy-gradient algorithms, particularly PPO [160], GRPO [162],\nand Reinforce++ [69]. Recent variants adapt these methods to the search context: Search Wisely [215] introduces\n𝛽-GRPO for uncertainty-aware calibration, StepSearch [202] implements step-wise PPO aligned with information gain,\nand ReinforceRAG [237] augments policy gradients with retrieval-aware baselines to mitigate variance under sparse\nrewards. The details of the RL algorithms applied in RL-based search agents are in Table 7.\n4.1.5\nCurriculum Learning and Horizon Scaling. RL training for long-horizon search tasks remains challenging due to\nsparse rewards and unstable credit assignment. Curriculum learning alleviates these issues by gradually expanding\ntask complexity or interaction length. AgentGym-RL [222] proposes ScalingInter-RL, which progressively extends\nthe interaction horizon—starting from short, focused tasks and gradually scaling to multi-step reasoning—balancing\nexploration and exploitation. ZeroSearch [176] employs a curriculum that systematically increases retrieval noise,\ncompelling agents to develop more resilient strategies. InfoSeek [223] similarly generates progressively harder research\ntasks to facilitate structured capability growth. These strategies jointly improve convergence stability and support\ncontinual capability scaling.\n4.1.6\nIterative and Self-Evolving Frameworks. Beyond static curricula, some frameworks close the loop between data\ngeneration and policy learning. EvolveSearch [238] epitomizes this approach: RL-trained models generate higher-quality\nsearch trajectories that are distilled back into SFT data, creating a self-reinforcing cycle of improvement. Such iterative\nframeworks demonstrate how RL can act not only as a training objective but as a data generator, continuously refining\nboth model behavior and supervision quality.\nManuscript submitted to ACM\n16\nMinhua Lin et al.\nTable 4. Comparison of representative reward functions in RL-based agentic search. 𝑎pred and 𝑎gt denote the predicted and ground-\ntruth answers, respectively. 𝑟ans is the answer-level reward; 𝑅𝑇is the number of retrieval steps; 𝑅𝑇max is the maximum retrieval budget;\n𝑟kb+ and 𝑟kb−denote the maximal knowledge-boundary reward and a small penalty, respectively. I(·) is the indicator function, 𝛾the\ndiscount factor, 𝑣(·) the rollout value, and 𝛼a decay coefficient. 𝑟sim(·, ·) is the reward function based on the semantic similarity\nbetween the model-generated search query and the ground-truth query using a Sentence Transformer.\nReward Type\nMethod\nRL Role\nReward Name\nReward Definition\nOutcome\nSearch-R1 [84]\nAdapt-Search\nAnswer EM\n𝑟= 𝐸𝑀(𝑎pred,𝑎gt)\nReSearcher [84]\nAdapt-Search\nAnswer F1\n𝑟= 𝐹1(𝑎pred,𝑎gt)\nReZero [42]\nSearch Intensity\nRetry Reward\n𝑟=\n\n\nÍ𝑁retry\n𝑘=1\n𝛾𝑘−1,\nif format valid,\n0,\notherwise.\nPangu Deep-\nDiver [202]\nSearch Intensity\nExtra Search Call\nReward\n𝑟=\n\n\n1,\nif uses search and answer is correct,\n0,\notherwise.\nIKEA [73]\nSearch Efficiency\nKnowledge-\nBoundary-Aware\nReward\n𝑟=\n\n\n𝑟kb+\n\u0010\n1 −\n𝑅𝑇\n𝑅𝑇max\n\u0011\n,\n𝑟ans = 1,\n0,\n𝑟ans = 0 ∧𝑅𝑇= 0,\n𝑟kb−,\n𝑟ans = 0 ∧𝑅𝑇> 0.\nProcess\nAutorefine [84]\nRetrieval–Search\nInteraction\nRetrieval-Specific\nReward\n𝑟= I\u0000𝑎gt ∩𝑎refine = 𝑎gt\n\u0001, where\n𝑎refine =\nØ\n𝑡\n{ 𝑐𝑡| (𝑠𝑡,𝑐𝑡) ∈𝑎, 𝑠𝑡= ⟨refine⟩}.\nR-Search [244]\nRetrieval–Search\nInteraction\nEvidence Quality\nReward\n𝑟= 𝐹1(𝛼cf, 𝛼gold),\n𝛼cf ∼𝜋cf(· | 𝑞,𝑒)\nReasonRAG [241]\nSearch Efficiency\nShortest-Path Re-\nward Estimation\n𝑟= 1\nℎ\nℎ\n∑︁\n𝑖=1\n𝑣(rollout𝑖) · 𝛼step(rollout𝑖)\nVisual-\nARFT [122]\nMulti-Tool /\nMulti-Modal\nAdapt-Search\nSemantic Similar-\nity Reward\n𝑟= 𝑟sim(𝑎search,𝑠)\n4.2\nReward Design\nReward design is paramount in RL training for agentic search, determining which behaviors are reinforced and\nhow credit is allocated across complex trajectories. Modern agentic search employs multi-faceted, multi-turn reward\nmechanisms that optimize not only accuracy of final outcomes and intermediate reasoning, but also diverse desiderata\nsuch as clarity, truthfulness, conciseness, efficiency, and reduced hallucination tendencies. These sophisticated reward\nstructures can be categorized along two complementary dimensions: temporal scope (outcome vs. process-level) and\nobjective diversity (single vs. multi-faceted optimization). Table 4 summarizes representative reward functions adopted\nin recent RL-based agentic search frameworks [26, 42, 84, 122, 165, 166, 241, 244], illustrating how different designs\nbalance final-answer accuracy, intermediate reasoning quality, and resource-efficient retrieval.\n4.2.1\nOutcome-level Rewards. Outcome-level rewards evaluate final task completion but increasingly incorporate\nmultiple quality dimensions beyond simple correctness. Early approaches like Search-R1 [84] and ReSearch [26]\nrely on basic exact match (EM) and format reward for correctness and style consistency. Subsequent multi-faceted\nextensions enhance these metrics: R-Search [244] introduces cross-model evidence utility, rewarding evidence quality\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n17\nand interpretability alongside correctness. IKEA [73] designs knowledge-boundary shaping to optimize both accuracy\nand efficiency by discouraging redundant retrieval. R1-Searcher++ [171] measure group-relative efficiency through\nretriever call variance, balancing task success with resource conservation. O2-Searcher [130] introduces a diversity\nreward to encourage query diversity to mitigate duplication under budget constraints.\n4.2.2\nProcess-level Rewards. While outcome signals are simple and effective for general tasks, they often prove too\nsparse to guide learning in long-horizon, multi-step search settings [44]. Process-level rewards address this limitation by\nproviding dense, fine-grained feedback throughout the reasoning–retrieval trajectory, enabling multi-turn, multi-faceted\noptimization of intermediate behaviors, such as faithfulness [166] and efficiency [202]. ReasonRAG [241] introduces\nshortest-path reward estimation (SPRE), which simultaneously optimizes reasoning quality and conciseness by simulating\nits possible outcomes and penalizing unnecessarily long trajectories. StepSearch [202] evaluates the utility of each\nretrieval step across multiple dimensions, including information gain and redundancy penalties. AutoRefine [166]\nreinforces faithful and targeted knowledge extraction through iterative step-level rewards. In addition to these verifiable\nrule-based rewards, some works [44, 195] also sample rewards from LLMs for providing step-level rewards to address\nthe sparse reward and training stability or enable faithful search [228].\nTakeaways:\n• Rewards have shifted from single-objective outcomes to multi-faceted objectives. Outcome-level\nsignals now combine correctness with efficiency, interpretability, and diversity; process-level signals provide\ndense guidance (e.g., info-gain, redundancy penalties, shortest-path/length control, faithfulness).\n• Unifying outcomes and processes is key. Effective agents balance final accuracy with intermediate\nbehavior quality; shaping should align step-wise improvements with end goals to avoid myopic optimization.\n• Open challenges. Credit assignment over long horizons, reward hacking/overfitting, objective balancing\n(accuracy–efficiency–faithfulness), and stable scaling (cost/latency) remain active problems; self-evolving\nloops (RL↔SFT) are promising but need careful control and evaluation.\n5\nWhere RL is Applied: The Scope of Optimization\nThe application of RL in agentic search can be categorized by the architectural level at which optimization occurs.\nThis perspective clarifies whether RL refines specific sub-skills, optimizes the policy of a single agent, or orchestrates\nbehavior across multi-agent or system-wide search infrastructures. We summarize representative works across these\nthree levels of scope in Table 5.\n5.1\nAgent-level Scpoe\nAt the agent level, RL optimizes end-to-end search policies, either for single autonomous search agents or coordinated\nmulti-agent search systems. This scope captures how RL shapes the core search decision-making processes that define\neffective information-seeking behavior.\n5.1.1\nSingle-agent Optimization. This is the most prevalent paradigm, where RL directly optimizes a unified policy\ngoverning the agent’s entire search workflow. The agent learns when to retrieve, how to formulate queries, how to\ninterpret evidence, and when to terminate its search. Search-R1 [84] exemplifies this approach, training an LLM to\nautonomously decide when and how to invoke external search engines during reasoning. R1-Searcher++ [171] extends\nManuscript submitted to ACM\n18\nMinhua Lin et al.\nTable 5. The categorization of RL-based search agents from the optimization scope’s perspective.\nCategory\nOptimization Scope\nMethods\nAgentic-level\nSingle-agent Optimization\nSearch-R1[84]; ReSearch[26]; R1-Searcher++[171]; AutoCoA [242];\nDeepRAG[63]; WebSailor[100]; WebSailor-V2 [98]; WebDancer[212];\nWebThinker [106]; WebWatcher [59]; ExSearch [167]; GRAIL [21]; Dy-\nnaSearcher [64]; SimpleDeepSearcher [177]; DeepResearcher[247];\nReSum[217]; R-Search [244]; ParallelSearch [245]; EvolveSearch [238]; O2-\nSearcher [130]; Pangu DeepDiver[165]; IKEA [73]; UR2 [104]; SSRL [50];\nZeroSearch [176]; MaskSearch [216]; ReZero [42]; Tool-Star [45]; Web-\nExplorer [116]; SFR-DeepResearch [135]; WebResearcher [147]; Visual-\nARFT [122]; MMSearch-R1 [211]; VRAG-RL [198]; Lucy [43]; MedResearcher-\nR1 [234]; DeepRetrieval[79]; Webthinker [106]\nMulti-agent Coordination\nHARIS [70]; SIRAG[195]; MAO-ARAG[30]; MMOA-RAG[29]; OPERA [119]\nModule-Level\n& Step-level\nModule-level Optimization\ns3[80]; AI-SearchPlanner[131]; DeepResearcher [247]\nStep-level Optimization\nStepSearch[202]; AutoRefine[166]; Search Wisely[215]; ConvSearch-R1 [254];\nAtom-Searcher [44]; ReasonRAG[241]; SWiRL [61]; Atom-Searcher [44];\nSystem-level\nUnified RL-based Agentic\nFramework\nAgentGym-RL[222]; Verl [163]; VerlTool[76]; RAG-Gym[224]; Chain-of-\nAgents[103]\nthis by balancing internal knowledge use with external search reliance. Web-based agents such as WebSailor [100] and\nWebDancer [212] demonstrate RL’s potential to train robust, long-horizon search policies for complex web environments.\n5.1.2\nMulti-agent Coordination. For more complex search pipelines, distinct agents specialize in search-related functions\nsuch as query reformulation, document selection, and evidence synthesis. RL coordinates these specialized search agents\nto achieve coherent information-seeking behavior. SIRAG [195] jointly trains a Decision Maker to control search timing\nand a Knowledge Selector to filter retrieved documents under a shared reward function. MAO-ARAG [30] orchestrates\nmultiple search specialists (e.g., query reformulators, document selectors, answer generators) using RL to optimize their\ncollaborative search performance.\n5.2\nModule-Level & Step-level Scope\nThis scope focuses on optimizing specific search components or decision steps within broader agentic search workflows.\nInstead of training the entire agent policy end-to-end, RL refines localized behaviors, making it valuable for improving\nspecific aspects of the search pipeline.\n5.2.1\nModule-level Optimization. RL can enhance specialized modules that operate alongside frozen LLMs. This modular\napproach isolates search-specific capabilities for targeted improvement without full-model retraining. The s3 [80]\nexemplifies this strategy by training a lightweight searcher module while keeping the generator frozen, ensuring\nefficiency and model-agnostic adaptability. AI-SearchPlanner [131] follows a similar design, training a retrieval-planning\nmodule to decide when and how to query while leveraging a frozen QA model for final answer generation.\n5.2.2\nStep-level Optimization. RL can also provide fine-grained feedback on individual search actions, such as query\ngeneration, document selection, or refinement. StepSearch [202] provides step-wise rewards based on information gain\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n19\nand redundancy penalties to encourage concise, effective search. AutoRefine [166] reinforces iterative “search-and-\nrefine” behaviors, encouraging agents to iteratively improve their information gathering. Search Wisely [215] applies\nRL to control retrieval confidence, discouraging low-confidence searches that waste resources.\n5.3\nSystem-level Scope\nAt the system level, RL orchestrates comprehensive search infrastructures and multi-agent search ecosystems. Rather\nthan optimizing individual search agents, this scope addresses how RL can improve entire search system architectures,\nresource allocation, and search workflow management across complex information-seeking platforms.\n5.3.1\nUnified RL-based Framework for Search. Several recent works build general-purpose platforms for developing,\ntraining, and evaluating RL-based search agents. AgentGym-RL [222] provides a modular benchmark suite that supports\ndiverse RL algorithms across multiple information environments. RAG-Gym [224] offers structured environments for\noptimizing retrieval-augmented agents and systematically comparing reward and policy designs. VerlTool [76] extends\nthis trend to tool-augmented systems, offering unified APIs and environments for training agents that operate over\nheterogeneous information sources and modalities.\nTakeaways:\n• Agent-level RL establishes the foundation for end-to-end search intelligence. Single-agent optimization\nyields coherent policies for when and how to search, while multi-agent coordination introduces modular spe-\ncialization and interpretability. The trade-off lies between unified autonomy and orchestrated collaboration.\n• Module- and step-level RL provide fine-grained control for improving local behaviors without full-model\nretraining. Module-level tuning enhances efficiency via lightweight plug-ins, and step-level rewards supply\ndense supervision for precise search decisions. However, effective credit assignment remains an open challenge\nfor connecting local improvements to global task success.\n• System-level RL extends beyond individual agents to entire infrastructures. Frameworks such as AgentGym-\nRL and RAG-Gym foster reproducibility, standardized evaluation, and scalable experimentation—marking a\nshift from isolated prototypes to deployable, ecosystem-level optimization.\n• Across levels, the scope of RL optimization reflects a continuum: from micro-level behavioral refinement,\nthrough agent-level policy learning, to macro-level system orchestration. Future progress will hinge on\nunifying these layers—developing hierarchical or multi-scale RL frameworks that integrate step-wise feedback,\nagent collaboration, and system-wide efficiency under shared reward principles.\n6\nEvaluation and Application\nEvaluating RL-based agentic search systems requires multi-dimensional assessment across search effectiveness, rea-\nsoning quality, efficiency, and generalization. This section reviews the datasets, evaluation metrics, and application\ndomains that currently define the landscape of RL-based agentic search evaluation and deployment.\nManuscript submitted to ACM\n20\nMinhua Lin et al.\nTable 6. The categorization of commonly used datasets in RL-based agentic search.\nCategory\nDataset\nKnowledge Source\nwiki-dump [210]; Common Crawl [40]; KILT [141]; PubMed [134]; Arxiv [6];\nKnowledge-Intensive QA\nNQ [90]; TriviaQA [86]; HotpotQA [231]; 2WikiMultiHopQA [68]; MuSiQue [191]; PopQA [127];\nCAG [140]; C-SimpleQA [203]; SuperGPQA [48]; BRIGHT [175]; SealQA [142]; BLUR [19];\nNaturalReasoning [236] FEVER [188]; EX-FEVER [124]; FEVEROUS [4]; FactBench [11]; Real-\nFactBench [230]; LongFact [206]; FRAMES [89] RAG-Bench [53]; BEIR [187]; AmbigQA [133];\nMetaQA [145];WebQuestions [12]; CWQ [179]; CheckWhy [168]; BeerQA [146]\nWeb-based Search\nWebQA [22]; Bamboogle [144]; Mind2Web [62]; WebArena [251]; WebWalkerQA [214]; Agent-\nBench [118]; BrowseComp-en [204]; BrowseComp-zh [249]; GAIA [132]; GAIA-2 [156];\nXbenchDeepSearch [219]; WebPuzzle [165]; InfoDeepSeek [221]; ORION [71]; WebShaperQA [186]\nMulti-modal\nInfoSeek [28]; MMSearch [77]; MMSearch-Plus [185] SimpleVQA [33]; LiveVQA [54]; MM-\nBrowseComp [101]; MAT-Search [121]; Mocheg [232]; MFC-Bench [200]; ViDoSeek [197]; Slide-\nVQA [183]; MMLongBench [126]\nConversational\nCoQA [152]; QuAC [35]; MSMarco [10]; TopiOCQA [1]; QReCC [5]; OR-QuAC [149]; Narra-\ntiveQA [88]; Doc2Dial [52]\nDomain-specific\nMATH [67]; MATH500 [110] AIME24 [74]; AIME25 [129]; GSM8K [39]; Minerva [93]; MMLU [66];\nMMLU-Pro [201]; NuminaMath [95]; MedQA [85]; MedMCQA [139]; MedBrowseComp [27]\nOlympiadBench [65]; USACO [164]; HLE [143] FinSearchBench-24 [96]; FinAgentBench [34]\nxbench [25]; MIRAGE [46]; SolutionBench [107]; DQA [91]; AirQA [72]; HERB [36]; SciQ [208];\nSciFact [193];ARC [38]; ScIRGen-Geo [111]; DeepShop [123]; NFCorpus [17]; OpenThoughts [136];\n6.1\nDatasets\nRL-based agentic search is evaluated across diverse benchmarks that test retrieval effectiveness and reasoning ability\nin open-domain, web-based, and domain-specific settings. Table 6 summarizes these representative datasets and the\ncorresponding studies that adopt them. Next, we give the details.\n6.1.1\nKnowledge-Intensive QA Benchmarks. A primary evaluation setting for agentic search is knowledge-intensive\nquestion answering (QA), where answering a question requires retrieving external evidence beyond the model’s para-\nmetric knowledge. These benchmarks jointly evaluate the agent’s ability to (i) retrieve relevant information and (ii)\nsynthesize evidence into correct, verifiable answers. Natural Questions (NQ) [90] and TriviaQA [86] serve as founda-\ntional single-hop QA datasets, widely used in works such as Search-R1 [84] and R-Search [244], to test when and how\nagents invoke retrieval. For multi-hop reasoning, HotpotQA [231] is employed in ReSearch [26] and AutoRefine [166],\nrequiring iterative retrieval and reasoning over multiple evidence chains. Fact-checking tasks such as FEVER [188]\nfurther test retrieval faithfulness and evidence verification. HARIS [70], for instance, uses FEVER to train agents that\nassess the credibility of retrieved claims under RL signals.\n6.1.2\nWeb-based Search Benchmarks. Web environments provide more realistic and dynamic evaluation settings.\nWebQA [22] offers large-scale web-based QA tasks used in WebThinker [106]. GAIA (General AI Assistant) defines\nmulti-step, interactive web tasks requiring reasoning and tool coordination, serving as a key benchmark for AgentGym-\nRL [222] and WebSailor-V2 [98]. Mind2Web [62] and related web navigation datasets evaluate the ability of web agents\nsuch as WebDancer [212] to handle multi-hop web browsing and action planning.\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n21\n6.1.3\nKnowledge Sources. Most open-domain and web-based agents rely on large-scale text corpora as retrieval\nbackends. Common choices include the English Wikipedia dump [210], widely used in benchmarks such as NQ,\nTriviaQA, and HotpotQA; web-scale resources such as Common Crawl [40] and KILT [141]; and domain-specific\nknowledge bases such as PubMed [134] and arXiv [6], which support research-oriented agents [233, 247]. Some systems,\nincluding DeepResearcher [247] and WebThinker [106], further augment these static corpora with dynamic web-search\nAPIs to access up-to-date or domain-targeted information.\n6.1.4\nMulti-modal Search. Recent advances in agentic search [122, 211] extend beyond text-only retrieval to incorporate\nvisual and structured modalities, motivating new benchmarks for multi-modal search. Early datasets, e.g., InfoSeek [28]\nand SlideVQA [183], established vision–language question answering over slides and figures, bridging perception\nand reasoning. Building on this foundation, Liu et al. [122] introduce MAT-Search and MAT-Coding to evaluate agentic\nretrieval and tool-use abilities under verifiable reward signals. MFC-Bench [200] benchmarks multimodal fact-checking\nwith 35𝑘image–text samples across manipulation, out-of-context, and veracity subtasks, providing a large-scale\ntestbed for factual grounding. Meanwhile, MMLongBench-Doc [126] focuses on long-context multimodal document\nunderstanding, covering 135 lengthy documents that combine text, layout, tables, and charts. Together, these benchmarks\nadvance RL-based agentic search toward unified, perception-grounded multi-modal retrieval and reasoning.\n6.1.5\nConversational and Multi-turn Search. CoQA [152] and QuAC [35] benchmark the ability of agents to main-\ntain context across multi-turn interactions, as explored in ConvSearch-R1 [254]. MSMarco [10] evaluates large-scale\npassage retrieval and ranking, assessing an agent’s ability to locate relevant information efficiently, as applied in\nDeepRetrieval [79] and RAG-Gym [224].\n6.1.6\nDomain-specific Search Tasks. Some specialized datasets [38, 66, 180, 208] target specific reasoning domains.\nFor instance, SciQ [208] and ARC [38] focus on scientific reasoning, relevant to agents like DeepResearcher [247].\nCommonsenseQA [180] tests the integration of factual retrieval and commonsense reasoning, used in IKEA [73].\nMMLU [66] evaluates general knowledge breadth, serving as a multi-domain benchmark for tool-augmented systems\nsuch as Tool-Star [45].\n6.2\nMetrics\nEvaluating RL-based agentic search requires metrics that capture multiple dimensions of performance, including answer\nquality, retrieval effectiveness, efficiency, and process-level behavior.\n6.2.1\nAnswer Quality. Exact Match (EM) and F1 score are two of the most commonly used metrics, which provide direct\nmeasures of task success, serving as primary evaluation metrics in many works [42, 84]. To evaluate the generated\nanswer quality against reference responses, ROUGE and BLEU scores evaluate generated answer quality against\nreference responses. To handle the case that answers may be correct but phrased differently from gold standards,\nBERTScore [240] is applied in RAG-Gym [224].\n6.2.2\nSearch Effectiveness. To measure the quality of the retrieved information, several traditional information retrieval\nmetrics remain fundamental. Specifically, Precision, Recall, and F1 measure the quality of retrieved information. Mean\nReciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) evaluate ranking quality when systems\nneed to prioritize multiple search results. For example, DeepRetrieval [79] trains LLMs to generate queries that maximize\nthe retrieval performance of black-box search engines in terms of retrieval metrics like Recall and NDCG.\nManuscript submitted to ACM\n22\nMinhua Lin et al.\n6.2.3\nSearch Efficiency. It aims to measure search agents’ efficiency from both resource and latency cost perspectives.\nNumber of Search Queries [165] measures how many queries an agent issues, while API Call Cost [30] quantifies the\nexpense of invoking external services. Response Time assesses end-to-end latency, important for interactive settings.\nSearch Redundancy [171] captures repeated or semantically similar queries that waste resources.\n6.2.4\nProcess Metrics. Beyond end-task accuracy, several works assess intermediate behaviors. StepSearch [202] defines\nInformation Gain per retrieval step to quantify the utility of each search action. SIRAG [195] measures Query Quality\nScore via LLM-as-Judge to evaluate whether generated queries are likely to yield relevant evidence. R-Search [244]\nintroduces Evidence Utilization Rate to measure how effectively agents leverage retrieved information in final reasoning.\n6.3\nApplications\nThe progress in RL-based agentic search has led to broad practical applications spanning scientific research, software\ndevelopment, multi-modal reasoning, and conversational AI.\n6.3.1\nDeep Research. Scientific and academic research represents a major application domain for RL-based search agents.\nDeepResearcher [247] demonstrates automated literature review and hypothesis generation through RL-optimized\nsearch strategies across academic databases. MedResearcher-R1 [233] specializes in medical research, using RL to\nnavigate complex biomedical knowledge bases and synthesize clinical evidence. WebResearcher [147] extends research\ncapabilities to general web-based investigation with unbounded reasoning horizons. SFR-DeepResearch [135] focuses on\nautonomous reasoning for research tasks, while Atom-Searcher [44] enhances deep research through fine-grained atomic\nthought rewards. WebThinker [106] is a deep research agent empowered with comprehensive research capabilities\nacross diverse domains through iterative online DPO.\n6.3.2\nMulti-modal Search. In addition to text-only search, there are several recent efforts [198, 211] exploring multi-\nmodality search agents, combining both text and visual information. VRAG-RL [198] enables vision-perception-based\nRAG for visually rich information understanding, using RL to iteratively reason across both textual and visual content.\nVisual-ARFT [122] demonstrates visual agentic reinforcement fine-tuning for tasks requiring integrated visual and\ntextual search. WebWatcher [59] breaks new ground in vision-language deep research agents, combining web search\nwith visual analysis capabilities. These applications are particularly valuable in domains like e-commerce, where product\nsearch requires understanding both descriptions and images, and in scientific research involving visual data analysis.\n6.3.3\nCode Agents. Beyond typical search-related applications, RL-powered search agents are being integrated into\nprogramming and software development workflows. Tool-Star [45] demonstrates multi-tool reasoning capabilities that\ninclude code execution and debugging, using RL to coordinate between search engines, code interpreters, and other\ndevelopment tools. VerlTool [76] provides a unified framework for agentic RL with tool use that specifically supports\ncode interpreters alongside other APIs, enabling agents to search for code solutions, execute them, and iteratively refine\nimplementations. These systems learn to balance web search for coding solutions with direct code experimentation,\noptimizing both information gathering and implementation efficiency.\n6.3.4\nAI Assistants. Conversational AI is a growing deployment area for RL-based search agents, which is far beyond a\nnaive chatbot but like a personal assistant with the capability to handle various realistic tasks. For instance, ConvSearch-\nR1 [254] specifically addresses conversational search scenarios, using RL to enhance query reformulation and maintain\ncontext across multi-turn interactions. Lucy [43] demonstrates edge-running agentic web search on mobile devices\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n23\nwith machine-generated task vectors, showcasing practical deployment in resource-constrained environments. MAO-\nARAG [30] provides adaptive retrieval-augmented generation through multi-agent orchestration, suitable for intelligent\nassistant applications that need to balance response quality with computational efficiency. These systems use RL to\nlearn to understand user intent, search for relevant information, and provide contextually appropriate responses while\nmaintaining conversation flow.\n6.3.5\nDomain-specific Applications. In addition to the aforementioned general applications, RL-based search agents are\nalso applied in specialized domains tailored to specific knowledge areas and user needs. For instance, HierSearch [181]\npresents enterprise search frameworks that integrate local knowledge bases with web search, addressing corporate\ninformation management needs. KunLunBaizeRAG [94] focuses on inference performance optimization for large\nlanguage models in domain-specific RAG scenarios. DynaSearcher [64] demonstrates dynamic knowledge graph (KG)\naugmented search for structured information retrieval, particularly valuable in domains with rich relational data.\nGRAIL [21] enables interactive KG exploration for retrieval-augmented reasoning through RL.\n6.3.6\nTakeaways. The diversity of applications demonstrates the broad applicability and practical value of RL-based\nagentic search systems. From code development [45] to scientific research [247], multi-modal understanding [211],\nconversational AI [254], and specialized domains [181], these systems address real-world information-seeking challenges\nacross multiple sectors. The success of these applications highlights the importance of domain-specific adaptation,\nmulti-modal capabilities, and efficient resource management in practical deployments. Future applications will likely\nsee increased integration across modalities and domains, with RL enabling agents to adapt their search strategies\ndynamically based on task requirements and user contexts.\n7\nChallenges and Future Directions\nDespite the remarkable strides of RL-based agentic search, many fundamental challenges and opportunities lie ahead.\nIn this section, we discuss key future directions that will shape the evolution of intelligent search agents, addressing\nboth technical limitations and emerging requirements for real-world deployment.\nMulti-modal Agentic Search. Real-world information exists across multiple modalities, including text, images, videos,\naudio, and structured data. Current RL-based search agents primarily focus on textual information, limiting their\napplicability to complex, multi-modal information-seeking tasks that require understanding and reasoning across\ndiverse content types. While initial efforts [59, 198, 211] enable search engines to facilitate reasoning in vision-language\nmodels [15, 55, 218], several fundamental limitations persist: (i) how to ensure consistency between textual descriptions\nand visual content during search-integrated reasoning; (ii) how to determine which modality contributes most to\nsuccessful outcomes in multi-modal search tasks; and (iii) how to design reward functions that jointly capture relevance,\ncoherence, and cross-modal alignment. Addressing these challenges is essential for moving toward robust multi-modal\nagentic search, where agents can adaptively select, integrate, and reason over heterogeneous sources to solve open-ended\nreal-world queries.\nMemory-augmented and Long-horizon Search. Real-world information-seeking often spans multiple sessions,\nwhere agents must remember past queries, retrieved evidence, or user feedback. Current RL-based search agents [84, 244]\ntypically operate within limited context windows and lack sophisticated memory mechanisms for long-term information\nretention and retrieval. While some initial efforts [135, 217] consider simple memory management techniques such as\nsummarization and cleanup operations, they still struggle with more complex tasks requiring long-term interactions and\ncross-session continuity. To advance agentic search in long-horizon scenarios, future research should explore developing\nManuscript submitted to ACM\n24\nMinhua Lin et al.\nsophisticated memory architectures that can selectively store, organize, and retrieve search-related knowledge over\ntime. Promising directions include: (i) hierarchical memory systems that differentiate between short-term working\nmemory, episodic memory across sessions, and long-term semantic knowledge; (ii) selective memory mechanisms that\nuse RL signals to decide what retrieved information to retain, compress, or discard based on long-term utility; and\n(iii) temporal reasoning integration that allows agents to model information decay, relevance shifts, and evolving user\nintents;\nTrustworthy Agentic Search. Search agents operating in open environments face pressing security, ethical, and\nreliability challenges that directly affect user trust. These agents may encounter adversarial content, misinformation,\nor malicious actors attempting to manipulate their behavior for harmful purposes. Existing studies have revealed\nsignificant vulnerabilities in search-augmented systems. For instance, PoisonedRAG [255] demonstrates that RAG can\nbe misled by injected malicious knowledge, resulting in incorrect or unsafe outputs. While Search Wisely [215] explores\nuncertainty-aware search to mitigate overconfidence, it remains unclear how search agents perform under adversarial\nconditions and how to guarantee robustness in real-world deployments. Moreover, these agents frequently interact with\nsensitive information, raising concerns about privacy protection, ethical information use, and compliance with data\ngovernance regulations. Future research should investigate how to develop reliable, privacy-preserving and ethically\naligned search agents. Promising directions include: (i) adversarially robust RL training, where agents are exposed\nto poisoned or noisy retrieval environments to learn resilient policies; (ii) privacy-preserving agentic search, such as\nfederated or encrypted search agents, to safeguard sensitive user information; (iv) value-aligned reward design, ensuring\nthat optimization objectives incorporate fairness, transparency, and safety constraints; and (v) auditing and verification\ntools that allow both developers and end users to interpret, monitor, and evaluate agent behavior. In conclusion, these\napproaches would move RL-based agentic search toward systems that are not only effective but also secure, ethical, and\ntrustworthy for real-world applications.\nCross-domain Generalization. Current RL-based search agents are often trained for specific domains or tasks, limiting\ntheir generalizability. Real-world deployment requires agents that can adapt their search strategies across diverse\ndomains and contexts. To solve this challenge and expand agentic search to broader applications, future works can\nfocus on learning generalizable search principles that can be applied across diverse contexts. For example, one potential\nsolution is to develop meta learning approach to to create universal search strategies that can transfer across different\ninformation spaces, or to build agents that can automatically identify and adapt to domain-specific search requirements.\nHuman–AI Co-search. Traditional IR systems were designed for humans as the primary end users [128, 209]. The\nintegration of retrieval into large-scale AI systems has reshaped this paradigm, particularly with the rise of LLMs.\nRetrieval is no longer performed solely for human consumption but increasingly serves to enhance models’ reasoning and\ngeneration capabilities [226]. This shift raises fundamental questions about how humans and AI agents will collaboratively\nengage in exploratory search. RL–based agentic search systems provide a natural foundation for this shift. Through\ninteraction and feedback, RL enables agents to learn adaptive retrieval policies that align with evolving user intents and\ncontextual cues, fostering human–AI co-search where agents act as copilots that assist users in locating, interpreting,\nand synthesizing information. Future research may explore: (i) Adaptive interaction modeling, where RL agents learn\nuser preferences and search behaviors to personalize strategies and result presentation; (ii) Explainable search reasoning,\nallowing agents to justify retrieval choices and promote transparency; (iii) Collaborative query refinement, enabling\niterative reformulation of search goals through natural-language interaction.\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n25\n8\nConclusion\nThe integration of RL into agentic search marks a fundamental shift in how LLMs interact with external knowledge.\nUnlike naive RAG, RL enables agents to dynamically decide when, what, and how to search, transforming search into\nan adaptive and interactive process. This survey provides the first systematic overview of RL-based agentic search,\nsynthesizing research across three perspectives: (i) What RL is for; (ii) How RL is used; and (iii) Where RL is applied. We\nfurther examine evaluation metrics, system benchmarks, and representative applications, offering a comparative view\nof current progress. Looking ahead, RL-based agentic search holds the potential to redefine information retrieval and\nreasoning. We hope this survey provides a foundation for advancing research in this emerging field and inspires new\ndirections toward practical, robust, and intelligent agentic search systems.\nTable 7. Overview of RL-based agentic search from the perspective of reinforcement learning optimization strategies. ORM and PRM\ndenote the Outcome Reward Model and the Process Reward Model, respectively. “Rule-based” indicates that the reward function is\nentirely computed from predefined rules; otherwise, an LLM is involved as a reward judge.\nMethod\nRL Func. Role\nCold\nStart?\nTraining\nEnv.\nRL Alg.\nReward\nType\nReward Func.\nOpt. Scope\nDataset\nSearch-R1 [84]\nAdapt-Search\n✗\nReal-world\nPPO\nGRPO\nRule-based\nORM\nAnswer EM\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nReSearch [26]\nAdapt-Search\n✗\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer F1\nSingle-agent\n[68, 144, 191, 231]\nAutoCoA [242]\nAdapt-Search\n✓\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nSimpleDeep-\nSearcher [177]\nAdapt-Search\n✓\nReal-world\nDPO\nReinforce++\nRule-based\nORM\nFormat\nAnswer F1\nSingle-agent\n[68, 89, 132, 144,\n184, 191, 205, 231,\n250]\nExSearch [167]\nAdapt-Search\n✗\nReal-world\nGEM\nPRM\nTrajectory Quality\nSingle-agent\n[90, 191, 231]\nIKEA [73]\nSearch Efficiency\n✗\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nKnowledge-boundary\nStep-level\n[68, 90, 127, 231]\nR1-Searcher [170]\nAdapt-Search\n✓\nReal-world\nGRPO\nReinforce++\nRule-based\nORM\nFormat\nAnswer F1\nSingle-agent\n[68, 144, 191, 231]\nR1-\nSearcher++ [171]\nSearch Efficiency\n✓\nReal-world\nGRPO\nReinforce++\nRule-based\nORM\nFormat\nAnswer EM\nStd of Search Calls\nSingle-agent\n[68, 144, 191, 231]\nDeepRAG [63]\nAdapt-Search\nSearch Efficiency\n✓\nReal-world\nGRPO\nRule-based\nORM\nAnswer EM\nRetrieval Cost\nSingle-agent\n[12, 68, 127, 140,\n191, 231]\nUR2 [104]\nAdapt-Search\n✓\nReal-world\nCurriculum\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nFallback Penalty\nSingle-agent\n[67, 68, 85, 93, 144,\n191, 201, 231]\nSSRL [50]\nAdapt-Search\n✓\nSimulated\nSelf-Search\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nSingle-agent\n[68, 86, 90, 144,\n191, 231]\nPangu Deep-\nDiver [165]\nAdapt-Search\nSearch Intensity\n✓\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nExtra Search\nSingle-agent\n[89, 144, 165, 203]\nReZero [42]\nSearch Intensity\n✗\nReal-world\nGRPO\nORM+PRM\nFormat\nAnswer LLM-Judge\nRetry\nStep-level\n[41]\nManuscript submitted to ACM\n26\nMinhua Lin et al.\nMethod\nRL Func. Role\nCold\nStart?\nTraining\nEnv.\nRL Alg.\nReward\nType\nReward Func.\nOpt. Scope\nDataset\nStepSearch [202]\nAdapt-Search\nSearch Intensity\n✗\nReal-world\nPPO\nRule-based\nORM+PRM\nFormat\nAnswer F1\nSearch Key\nInformation Gain\nRedundancy Penalty\nStep-level\n[68, 144, 191, 231]\nVERITAS [228]\nAdapt-Search\nR-Aware Opt.\n✗\nReal-world\nPPO\nORM+PRM\nAnswer EM\nEnhancing Faithfulness Step-level\n[68, 86, 90, 127,\n144, 191, 231]\nReasonRAG [241]\nSearch Efficiency\nR-S Inter.\n✗\nReal-world\nMCTS\nDPO\nPRM\nShortest Path\nStep-level\n[68, 127, 144, 191,\n231]\nWeb-Sailor [100]\nAdapt-Search\nCtx-Mem.\n✓\nReal-world\nDUPO\nORM\nFormat\nAnswer F1\nSingle-agent\n[99, 132, 205, 219,\n250]\nWebSailor-V2 [98]\nMulti-tool\nCtx-Mem.\n✓\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer F1\nSingle-agent\n[47, 97, 132, 143,\n205, 219, 250]\nSearch\nWisely [215]\nSearch Efficiency\n✗\nReal-world\n𝛽-GRPO\nRule-based\nORM\nConfidence-based\nAnswer EM\nSingle-agent\n[68, 86, 90, 144,\n191, 231]\nZeroSearch [176]\nSearch Efficiency\n✓\nSimulated\nCurriculum\nPPO\nGRPO\nReinforce\nRule-based\nORM\nAnswer F1\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nParallelSearch [245]\nSearch Efficiency\n✓\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nQuery Decomopse\nSearch count\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nRAG-R1 [182]\nSearch Efficiency\nConv-Reform.\n✓\nReal-world\nPPO\nORM\nAnswer EM\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nConvSearch-\nR1 [254]\nConv-Reform.\n✓\nReal-world\nGRPO\nORM\nFormat\nRank-Incentive\nStep-level\n[1, 5]\nMaskSearch [216]\nConver. Reform.\nR–S Inter.\n✓\nReal-world\nCurriculum\nRAMP\nDAPO\nRule-based\nORM\nFormat\nAnswer Recall\nLength penalty\nSingle-agent\n[68, 144, 191, 192,\n231, 253]\nDeepRetrieval [79]\nR-Aware Opt.\n✓\nSimulated\nPPO\nORM\nFormat\nAnswer Recall\nSingle-level\n[86, 90, 151, 188,\n193]\nWebThinker [106]\nSearch Efficiency\n✗\nReal-world\nDPO\nPRM\nAnswer EM\nTool Calls\nLength penalty\nSingle-agent\n[48, 95, 132, 136,\n143, 153, 214, 236]\ns3 [80]\nR-Aware Opt.\n✓\nSimulated\nPPO\nRule-based\nORM\nGain Beyond RAG\nModule-level\n[68, 86, 90, 127,\n191, 231]\nR-Search [244]\nR–S Inter.\n✗\nReal-world\nPPO\nGRPO\nRule-based\nORM+PRM\nFormat\nAnswer F1\nEvidence Quality\nSingle-agent\n[68, 144, 191, 231]\nAutoRefine [166]\nR–S Inter.\n✗\nReal-world\nGRPO\nORM plus\nPRM\nAnswer F1\nRetrieval Reward\nStep-level\n[68, 86, 90, 127,\n144, 191, 231]\nEvolveSearch [238]\nR–S Inter.\n✓\nReal-world\nSelf-evolving\nGRPO\nORM\nFormat\nAnswer LLM-Judge\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nO2-Searcher [130]\nR–S Inter.\n✓\nSimulated\nGRPO\nRule-based\nORM\nFormat\nDiversity reward\nFactual reward\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n27\nMethod\nRL Func. Role\nCold\nStart?\nTraining\nEnv.\nRL Alg.\nReward\nType\nReward Func.\nOpt. Scope\nDataset\nAtom-\nSearcher [44]\nR–S Inter.\n✓\nReal-world\nCurriculum\nGRPO\nPRM+Rule-\nbased ORM\nFormat\nAnswer F1\nAtomic thought reward\nStep-level\n[68, 86, 90, 127,\n144, 191, 231]\nReSum [217]\nCtx-Mem.\n✗\nReal-world\nResume-\nGRPO\nORM\nAnswer LLM-Judge\nSingle-agent\n[132, 203, 205, 214,\n219, 250]\nSFR-\nDeepResearch [135]\nCtx-Mem.\nMulti-tool\n✗\nReal-world\nREINFORCE\nORM\nAnswer LLM-Judge\nSingle-agent\n[89, 132, 143]\nMAO-ARAG [30]\nP–E Orches.\n✓\nReal-world\nPPO\nORM\nFormat\nCost Penalty\nAnswer F1\nMulti-agent\n[68, 90, 127, 133,\n144, 191, 231]\nOPERA [119]\nP–E Orches.\n✓\nReal-world\nMAPGRPO\nPRM+ORM\nAnswerer Reward\nPlaner Reward\nRewriter Reward\nMulti-agent\n[68, 90, 184, 191,\n231]\nAI-\nSearchPlanner [131]\nP–E Orches.\n✗\nReal-world\nPPO\nORM\nAnswer LLM-Judge\nTrajectory Rationality\nModule-level\n[68, 86, 90, 127,\n144, 191, 231]\nSIRAG [195]\nCooperative\n✗\nReal-world\nPPO\nPRM\nProcess LLM-Judge\nMulti-agent\n[68, 90, 127, 231]\nMMOA-RAG [29]\nCooperative\nR-aware Opt.\n✗\nReal-world\nMA-PPO\nRule-based\nORM\nAnswer F1\nEfficiency penalty\nMulti-agent\n[68, 133, 231]\nTool-Star [45]\nMulti tool\n✓\nReal-world\nREINFORCE++\nGRPO\nDPO\nRule-based\nORM\nFormat\nAnswer EM\nSingle-agent\n[39, 67, 68, 110,\n144, 191, 214, 231]\nWebWatcher [59]\nMulti tool\nMulti-modal\n✓\nReal-world\nGRPO\nORM\nFormat\nAnswer LLM-Judge\nSingle-agent\n[33, 54, 77, 101,\n143]\nVisual-ARFT [122]\nMulti-modal\nMulti-tool\nAdapt-Search\n✓\nReal-world\nGRPO\nRule-based\nORM+PRM\nFormat\nAnswer F1\nQuery Semantic Sim.\nSingle-agent\n[121]\nVRAG-RL [198]\nMulti-modal\nSearch Efficiency\n✓\nSimulated\nGRPO\nORM\nFormat\nAnswer LLM-Judge\nRetrieval Efficiency\nSingle-agent\n[126, 183, 197]\nMMSearch-\nR1 [211]\nMulti-modal\nSearch Efficiency\n✗\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nSearch Penalty\nSingle-agent\n[28, 33, 54, 77,\n211]\nGRAIL [21]\nAdapt-Search\nStruct-Nav.\n✓\nReal-world\nGraph Env.\nGRPO\nPRM\nProcess LLM-Judge\nSingle-agent\n[12, 145, 179]\nDynaSearcher [64]\nStruct-Nav.\n✗\nReal-world\nGraph Env.\nKG+Doc Search\nGRPO\nRule-based\nORM\nFormat\nAnswer F1\nInformation Gain\nRetrieval Penalty\nSingle-agent\n[68, 89, 144, 191,\n231]\nHARIS [70]\nR-S Inter.\n✗\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer Accuracy\nDecision Accuracy\nMulti-agent\n[81, 124, 168]\nDeepNote [199]\nAdapt-Search\nConv-Reform.\n✗\nReal-world\nDPO\n-\n-\nSingle-agent\n[60, 68, 173, 191,\n231]\nDeepResearcher [247] Adapt-Search\nSearch Efficiency\nCtx-Mem.\n✗\nReal-world\nGRPO\nRule-based\nORM\nFormat\nAnswer F1\nModule-level\n[68, 86, 90, 231]\nManuscript submitted to ACM\n28\nMinhua Lin et al.\nMethod\nRL Func. Role\nCold\nStart?\nTraining\nEnv.\nRL Alg.\nReward\nType\nReward Func.\nOpt. Scope\nDataset\nSWiRL [61]\nAdapt-Search\nR-S Inter.\n✓\nReal-world\nPPO\nPRM\nStep LLM-Judge\nStep-level\n[39, 146, 191, 213,\n231]\nWebDancer [212]\nMulti tool\n✓\nReal-world\nDAPO\nORM\nAnswer EM\nSingle-agent\n[132, 205, 214,\n250]\nMedResearcher-\nR1 [234]\nAdpt-Search\nMulti-Tool\n✓\nReal-world\nMedical Tool\nGRPO\nORM\nAnswer Acc\nResponse Quality\nEfficiency penalty\nSingle-agent\n[27, 132, 219]\nLucy [43]\nSearch Efficiency\nR–S Inter.\n✓\nReal-world\nSLMs\nDAPO\nRule-based\nORM\nFormat/XML validity\nAnswer EM\nTool exec. success\nVisit/Search ratio\nEfficient thinking\nSingle-agent\n[203]\nASearcher [56]\nR-S Inter.\nCtx-Mem.\nMulti-tool\n✓\nReal-world\nBrowser Env.\nAsynchronous\nGRPO\nORM\nAnswer LLM-Judge\nSingle-agent\n[68, 86, 89, 90, 127,\n132, 144, 191, 219,\n231]\nWebExplorer [116]\nCtx-Mem.\nConv-Reform.\n✓\nReal-world\nCurriculum\nGRPO\nRule-based\nORM\nFormat\nAnswer EM\nSingle-agent\n[89, 132, 143, 205,\n214, 219, 250]\nWebResearcher [147] Multi-tool\n✓\nReal-world\nCurriculum\nGSPO\nRule-based\nORM\nAnswer EM\nSingle-agent\n[89, 132, 143, 205,\n219, 250]\nRECON [227]\nCtx-Mem.\n✓\nReal-world\nPPO\nRule-based\nORM\nAnswer EM\nSingle-agent\n[68, 86, 90, 127,\n144, 191, 231]\nAgentGym-\nRL [222]\nCooperative\nMulti tool\n-\n-\n-\n-\n-\nUnified RL Agentic\nFramework\n-\nChain-of-\nAgents [103]\nCooperative\nMulti tool\n-\n-\n-\n-\n-\nUnified RL Agentic\nFramework\n-\nVerl [163]\nMulti tool\n-\n-\n-\n-\n-\nUnified RL Agentic\nFramework\n-\nVerlTool [76]\nMulti tool\n-\n-\n-\n-\n-\nUnified RL Agentic\nFramework\n-\nReferences\n[1] Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Suleman, Harm de Vries, and Siva Reddy. 2022. Topiocqa: Open-domain conversational question\nanswering with topic switching. Transactions of the Association for Computational Linguistics 10 (2022), 468–483.\n[2] Akiko Aizawa. 2003. An information-theoretic perspective of tf–idf measures. Information Processing & Management 39, 1 (2003), 45–65.\n[3] Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh\nSrinivasan, et al. 2023. Rest meets react: Self-improvement for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003 (2023).\n[4] Rami Aly, Zhijiang Guo, Michael Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal.\n2021. Feverous: Fact extraction and verification over unstructured and structured information. arXiv preprint arXiv:2106.05707 (2021).\n[5] Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-Domain Question\nAnswering Goes Conversational via Question Rewriting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies.\n[6] arXiv. 2025. About arXiv. https://info.arxiv.org/about/index.html. Accessed 2025-10-08.\n[7] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through\nself-reflection. In The Twelfth International Conference on Learning Representations.\n[8] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-rag: Learning to retrieve, generate, and critique through\nself-reflection. (2024).\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n29\n[9] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau Yih. 2024. Reliable, adaptable, and\nattributable language models with retrieval. arXiv preprint arXiv:2403.03187 (2024).\n[10] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri\nNguyen, et al. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 (2016).\n[11] Farima Fatahi Bayat, Lechen Zhang, Sheza Munir, and Lu Wang. 2024. FactBench: A Dynamic Benchmark for In-the-Wild Language Model\nFactuality Evaluation. arXiv preprint arXiv:2410.22257 (2024).\n[12] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the\n2013 conference on empirical methods in natural language processing. 1533–1544.\n[13] Monica Bianchini, Marco Gori, and Franco Scarselli. 2005. Inside pagerank. ACM Transactions on Internet Technology (TOIT) 5, 1 (2005), 92–128.\n[14] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring\nhuman knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. 1247–1250.\n[15] Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud,\nBargav Jayaraman, et al. 2024. An introduction to vision-language modeling. arXiv preprint arXiv:2405.17247 (2024).\n[16] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore,\nChris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen,\nand Laurent Sifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In Proceedings of the 39th International Conference on\nMachine Learning, Vol. 162. PMLR, 2206–2240.\n[17] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A Full-Text Learning to Rank Dataset for Medical Information Retrieval.\nProceedings of the 38th European Conference on Information Retrieval. http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ECIR2016.pdf\n[18] Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks 30 (1998), 107–117.\nhttps://snap.stanford.edu/class/cs224w-readings/Brin98Anatomy.pdf\n[19] Sky CH-Wang, Darshan Deshpande, Smaranda Muresan, Anand Kannappan, and Rebecca Qian. 2025. Browsing Lost Unformed Recollections: A\nBenchmark for Tip-of-the-Tongue Search and Reasoning. arXiv preprint arXiv:2503.19193 (2025).\n[20] Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh, Menghai Pan, Chin-Chia Michael Yeh, Guanchu Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng,\nMahashweta Das, and Na Zou. 2025. MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation. In Proceedings of the 63rd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher\nPilehvar (Eds.). 2607–2622.\n[21] Ge Chang, Jinbo Su, Jiacheng Liu, Pengfei Yang, Yuhao Shang, Huiwen Zheng, Hongli Ma, Yan Liang, Yuanchun Li, and Yunxin Liu. 2025. GRAIL:\nLearning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning. arXiv preprint arXiv:2508.05498 (2025).\n[22] Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal qa. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. 16495–16504.\n[23] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1870–1879.\n[24] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. 2025. Sft or rl? an early investigation into\ntraining r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468 (2025).\n[25] Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. 2025.\nxbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations. arXiv preprint arXiv:2506.13651 (2025).\n[26] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. 2025.\nLearning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470 (2025).\n[27] Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, and Danielle S Bitterman.\n2025. MedBrowseComp: Benchmarking Medical Deep Research and Computer Use. arXiv preprint arXiv:2505.14963 (2025).\n[28] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang. 2023. Can pre-trained vision and language\nmodels answer visual information-seeking questions? arXiv preprint arXiv:2302.11713 (2023).\n[29] Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang Wang, Dawei Yin, Yiming Yang, and Jiaxin Mao. 2025. Improving\nRetrieval-Augmented Generation through Multi-Agent Reinforcement Learning. arXiv preprint arXiv:2501.15228 (2025).\n[30] Yiqun Chen, Erhan Zhang, Lingyong Yan, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, and Jiaxin Mao. 2025. MAO-ARAG: Multi-Agent Orchestration\nfor Adaptive Retrieval-Augmented Generation. arXiv preprint arXiv:2508.01005 (2025).\n[31] Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. 2024. Mindsearch: Mimicking human minds\nelicits deep ai searcher. arXiv preprint arXiv:2407.20183 (2024).\n[32] Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, and Benjamin Van Durme. 2024. Dated data: Tracing knowledge cutoffs\nin large language models. arXiv preprint arXiv:2403.12958 (2024).\n[33] Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. 2025.\nSimplevqa: Multimodal factuality evaluation for multimodal large language models. arXiv preprint arXiv:2502.13059 (2025).\n[34] Chanyeol Choi, Jihoon Kwon, Alejandro Lopez-Lira, Chaewoon Kim, Minjae Kim, Juneha Hwang, Jaeseon Ha, Hojun Choi, Suyeol Yun, Yongjin\nKim, et al. 2025. FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering. arXiv preprint arXiv:2508.14052\nManuscript submitted to ACM\n30\nMinhua Lin et al.\n(2025).\n[35] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question answering in\ncontext. arXiv preprint arXiv:1808.07036 (2018).\n[36] Prafulla Kumar Choubey, Xiangyu Peng, Shilpa Bhagavath, Kung-Hsiang Huang, Caiming Xiong, and Chien-Sheng Wu. 2025. Benchmarking Deep\nSearch over Heterogeneous Enterprise Data. arXiv preprint arXiv:2506.23139 (2025).\n[37] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.\nAdvances in neural information processing systems 30 (2017).\n[38] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018).\n[39] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton,\nReiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021).\n[40] Common Crawl. 2025. Common Crawl: Open Repository of Web Crawl Data. https://commoncrawl.org/overview. Accessed 2025-10-08.\n[41] Alan Dao and Thinh Le. 2025. Apollo 3 Mission Dataset. https://github.com/menloresearch/ReZero. Used in the ReZero paper: Dao, A. and Le, T.\n(2025). \"ReZero: Enhancing LLM Search Ability by Trying One-More-Time.\" arXiv:2504.11001.\n[42] Alan Dao and Thinh Le. 2025. ReZero: Enhancing LLM search ability by trying one-more-time. arXiv preprint arXiv:2504.11001 (2025).\n[43] Alan Dao, Dinh Bach Vu, Alex Nguyen, and Norapat Buppodom. 2025. Lucy: edgerunning agentic web search on mobile with machine generated\ntask vectors. arXiv preprint arXiv:2508.00360 (2025).\n[44] Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, et al.\n2025. Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward. arXiv preprint arXiv:2508.12800 (2025).\n[45] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. 2025.\nTool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning. arXiv preprint arXiv:2505.16410 (2025).\n[46] Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-Tür, and Vikram S Adve. 2025. MIRAGE: A Benchmark\nfor Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations. arXiv preprint arXiv:2506.20100 (2025).\n[47] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. 2025. DeepResearch Bench: A Comprehensive Benchmark for Deep\nResearch Agents. arXiv preprint arXiv:2506.11763 (2025).\n[48] Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. 2025.\nSupergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739 (2025).\n[49] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A survey on rag meeting llms:\nTowards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining.\n6491–6501.\n[50] Yuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai Zhu, Che Jiang, Yuchen Zhang, et al. 2025. SSRL:\nSelf-Search Reinforcement Learning. arXiv preprint arXiv:2508.10874 (2025).\n[51] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. 2025. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978\n(2025).\n[52] Song Feng, Hui Wan, Chulaka Gunasekara, Siva Patel, Sachindra Joshi, and Luis Lastras. 2020. doc2dial: A Goal-Oriented Document-Grounded\nDialogue Dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 8118–8128.\n[53] Robert Friel, Masha Belyi, and Atindriyo Sanyal. 2024. RagBench: Explainable Benchmark for Retrieval-Augmented Generation Systems. arXiv\npreprint arXiv:2407.11005 (2024).\n[54] Mingyang Fu, Yuyang Peng, Benlin Liu, Yao Wan, and Dongping Chen. 2025. LiveVQA: Live Visual Knowledge Seeking. arXiv preprint\narXiv:2504.05288 (2025).\n[55] Hongcheng Gao, Zihao Huang, Lin Xu, Jingyi Tang, Xinhao Li, Yue Liu, Haoyang Li, Taihang Hu, Minhua Lin, Xinlong Yang, et al. 2025. Pixels,\nPatterns, but No Poetry: To See The World like Humans. arXiv preprint arXiv:2507.16863 (2025).\n[56] Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. 2025. Beyond ten turns: Unlocking long-horizon\nagentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976 (2025).\n[57] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 2, 1 (2023).\n[58] Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. 2025. Synergizing rag and reasoning: A systematic review. arXiv\npreprint arXiv:2504.15909 (2025).\n[59] Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. 2025.\nWebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent. arXiv preprint arXiv:2508.05748 (2025).\n[60] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering\nBenchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics 9 (2021), 87–100. doi:10.1162/tacl_a_\n00370\n[61] Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher D Manning. 2025. Synthetic data generation & multi-step rl for reasoning &\ntool use. arXiv preprint arXiv:2504.04736 (2025).\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n31\n[62] Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, et al.\n2025. Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge. arXiv preprint arXiv:2506.21506 (2025).\n[63] Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. 2025. DeepRAG: Thinking to\nRetrieval Step by Step for Large Language Models. arXiv preprint arXiv:2502.01142 (2025).\n[64] Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, and Hao Wang. 2025. DynaSearcher: Dynamic Knowledge Graph Augmented Search Agent via\nMulti-Reward Reinforcement Learning. arXiv preprint arXiv:2507.17365 (2025).\n[65] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al.\n2024. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint\narXiv:2402.14008 (2024).\n[66] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask\nlanguage understanding. arXiv preprint arXiv:2009.03300 (2020).\n[67] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring\nmathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 (2021).\n[68] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop qa dataset for comprehensive evaluation\nof reasoning steps. arXiv preprint arXiv:2011.01060 (2020).\n[69] Jian Hu. 2025. Reinforce++: A simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262 (2025).\n[70] Qisheng Hu, Quanyu Long, and Wenya Wang. 2025. Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification.\narXiv preprint arXiv:2506.07528 (2025).\n[71] Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, and Wayne Xin Zhao. 2025. ManuSearch: Democratizing Deep\nSearch in Large Language Models with a Transparent and Open Multi-Agent Framework. arXiv preprint arXiv:2505.18105 (2025).\n[72] Tianyang Huang, Rui Cao, Yujia Zhang, Ziyu Kang, Zixuan Wang, Chen Wang, Yizhu Luo, Haonan Zheng, Lingfeng Qian, Lin Chen, and Kai Yu.\n2025. AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation. arXiv preprint arXiv:2509.16952 (2025).\n[73] Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, and Kang Liu. 2025. Reinforced Internal-External Knowledge Synergistic Reasoning for\nEfficient Adaptive Search Agent. arXiv preprint arXiv:2505.07596 (2025).\n[74] HuggingFaceH4 Team. 2024. AIME 2024 Dataset. https://huggingface.co/datasets/HuggingFaceH4/aime_2024. Hugging Face, Accessed: 2025-10-11.\n[75] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of\nhallucination in natural language generation. ACM computing surveys 55, 12 (2023), 1–38.\n[76] Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. 2025. VerlTool: Towards\nHolistic Agentic Reinforcement Learning with Tool Use. arXiv preprint arXiv:2509.01055 (2025).\n[77] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, et al. 2024.\nMmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959 (2024).\n[78] Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. 2024. RAG-Star: Enhancing Deliberative\nReasoning with Retrieval Augmented Verification and Refinement. arXiv preprint arXiv:2412.12881 (2024).\n[79] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. Deepretrieval: Hacking\nreal search engines and retrievers with large language models via reinforcement learning. arXiv preprint arXiv:2503.00223 (2025).\n[80] Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. s3: You Don’t Need That Much Data to\nTrain a Search Agent via RL. arXiv preprint arXiv:2505.14146 (2025).\n[81] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction\nand claim verification. arXiv preprint arXiv:2011.03088 (2020).\n[82] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active\nRetrieval Augmented Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for\nComputational Linguistics, 7969–7992. https://aclanthology.org/2023.emnlp-main.495/\n[83] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O Arik. 2025. Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG. In\nThe Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=oU3tpaR8fm\n[84] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to\nreason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516 (2025).\n[85] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? a large-scale\nopen domain question answering dataset from medical exams. Applied Sciences 11, 14 (2021), 6421.\n[86] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading\ncomprehension. arXiv preprint arXiv:1705.03551 (2017).\n[87] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Bridging the preference gap between retrievers\nand llms. arXiv preprint arXiv:2401.06954 (2024).\n[88] Tomáš Kočisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa\nreading comprehension challenge. Transactions of the Association for Computational Linguistics 6 (2018), 317–328.\n[89] Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2024. Fact, Fetch,\nand Reason: A Unified Evaluation of Retrieval-Augmented Generation. Proceedings of NAACL 2025 (2024). https://aclanthology.org/2025.naacl-\nManuscript submitted to ACM\n32\nMinhua Lin et al.\nlong.243/\n[90] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob\nDevlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational\nLinguistics 7 (2019), 453–466.\n[91] Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A plan-then-retrieval augmented generation for generative large language models\nas decision makers. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers). 6537–6555.\n[92] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems 33\n(2020), 9459–9474.\n[93] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol\nSchlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in neural information processing\nsystems 35 (2022), 3843–3857.\n[94] Cheng Li, Jiexiong Liu, Yixuan Chen, Qihang Zhou, and KunLun Meta. 2025. KunLunBaizeRAG: Reinforcement Learning Driven Inference\nPerformance Leap for Large Language Models. arXiv preprint arXiv:2506.19466 (2025).\n[95] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, et al.\n2024. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository\n13, 9 (2024), 9.\n[96] Jinzheng Li, Jingshu Zhang, Hongguang Li, and Yiqing Shen. 2024. An Agent Framework for Real-Time Financial Information Searching with\nLarge Language Models. arXiv preprint arXiv:2502.15684 (2024).\n[97] Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, et al. 2025.\narXiv:2509.13305 [cs.CL] https://arxiv.org/abs/2509.13305\n[98] Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, et al. 2025. WebSailor-V2:\nBridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning. arXiv preprint arXiv:2509.13305 (2025).\n[99] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou\nShen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. arXiv:2507.02592 [cs.CL]\nhttps://arxiv.org/abs/2507.02592\n[100] Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou\nShen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebSailor: Navigating\nSuper-human Reasoning for Web Agent. arXiv:2507.02592 [cs.CL] https://arxiv.org/abs/2507.02592\n[101] Shilong Li, Xingyuan Bu, Wenjie Wang, Jiaheng Liu, Jun Dong, Haoyang He, Hao Lu, Haozhe Zhang, Chenchen Jing, Zhen Li, et al. 2025.\nMM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents. arXiv preprint arXiv:2508.13186 (2025).\n[102] Wenjun Li, Zhi Chen, Jingru Lin, Hannan Cao, Wei Han, Sheng Liang, Zhi Zhang, Kuicai Dong, Dexun Li, Chen Zhang, et al. 2025. Reinforcement\nLearning Foundations for Deep Research Systems: A Survey. arXiv preprint arXiv:2509.06733 (2025).\n[103] Weizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang, Qianben Chen, Weichen Sun, Qiexiang Wang,\net al. 2025. Chain-of-agents: End-to-end agent foundation models via multi-agent distillation and agentic rl. arXiv preprint arXiv:2508.13167 (2025).\n[104] Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, and Yang Liu. 2025. UR2: Unify RAG and Reasoning through Reinforcement\nLearning. arXiv preprint arXiv:2508.06165 (2025).\n[105] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-\nenhanced large reasoning models. arXiv preprint arXiv:2501.05366 (2025).\n[106] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025. Webthinker: Empowering\nlarge reasoning models with deep research capability. arXiv preprint arXiv:2504.21776 (2025).\n[107] Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, and Le Sun. 2025. Deepsolution: Boosting\ncomplex engineering solution design via tree-based exploration and bi-point thinking. arXiv preprint arXiv:2502.20730 (2025).\n[108] Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, and Ziyue Li. 2025. Reasoning RAG via System 1 or System 2: A Survey on Reasoning\nAgentic Retrieval-Augmented Generation for Industry Challenges. arXiv preprint arXiv:2506.10408 (2025).\n[109] Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, and Jianye Hao. 2025. Squeeze the soaked sponge: Efficient off-policy\nreinforcement finetuning for large language model. arXiv preprint arXiv:2507.06892 (2025).\n[110] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl\nCobbe. 2023. Let’s verify step by step. In The Twelfth International Conference on Learning Representations.\n[111] Junyong Lin, Lu Dai, Ruiqian Han, Yijie Sui, Ruilin Wang, Xingliang Sun, Qinglin Wu, Min Feng, Hao Liu, and Hui Xiong. 2025. ScIRGen: Synthesize\nRealistic and Large-Scale RAG Dataset for Scientific Research. arXiv preprint arXiv:2506.11117 (2025).\n[112] Minhua Lin, Zhengzhang Chen, Yanchi Liu, Xujiang Zhao, Zongyu Wu, Junxiang Wang, Xiang Zhang, Suhang Wang, and Haifeng Chen. 2024.\nDecoding time series with llms: A multi-agent framework for cross-domain annotation. arXiv preprint arXiv:2410.17462 (2024).\n[113] Minhua Lin, Enyan Dai, Junjie Xu, Jinyuan Jia, Xiang Zhang, and Suhang Wang. 2025. Stealing training graphs from graph neural networks. In\nProceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1. 777–788.\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n33\n[114] Minhua Lin, Hui Liu, Xianfeng Tang, Jingying Zeng, Zhenwei Dai, Chen Luo, Zheng Li, Xiang Zhang, Qi He, and Suhang Wang. 2025. How Far are\nLLMs from Real Search? A Comprehensive Study on Efficiency, Completeness, and Inherent Capabilities. arXiv preprint arXiv:2502.18387 (2025).\n[115] Hanwen Liu, Qihan Zhang, Ryan Marcus, and Ibrahim Sabek. 2025. SEFRQO: A Self-Evolving Fine-Tuned RAG-Based Query Optimizer. arXiv\npreprint arXiv:2508.17556 (2025).\n[116] Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, et al. 2025. Webexplorer: Explore\nand evolve for training long-horizon web agents. arXiv preprint arXiv:2509.06501 (2025).\n[117] Jiale Liu, Jiahao Zhang, and Suhang Wang. 2025. Exposing Privacy Risks in Graph Retrieval-Augmented Generation. arXiv preprint arXiv:2508.17222\n(2025).\n[118] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang\nDeng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2024.\nAgentBench: Evaluating LLMs as Agents. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=\nzAdUB0aCTQ\n[119] Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, and Zhiyuan Ma. 2025. OPERA: A Reinforcement\nLearning–Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval. arXiv preprint arXiv:2508.16438\n(2025).\n[120] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training:\nA critical perspective. arXiv preprint arXiv:2503.20783 (2025).\n[121] Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025. MAT-Search.\nhttps://arxiv.org/abs/2505.14246\n[122] Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. 2025. Visual Agentic\nReinforcement Fine-Tuning. arXiv preprint arXiv:2505.14246 (2025).\n[123] Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, Maarten de Rijke, Zhaochun Ren, and Xiuying Chen. 2025. DeepShop: A Benchmark for Deep Research\nShopping Agents. arXiv preprint arXiv:2506.02839 (2025).\n[124] Huanhuan Ma, Weizhi Xu, Yifan Wei, Liuji Chen, Liang Wang, Qiang Liu, Shu Wu, and Liang Wang. 2024. EX-FEVER: A Dataset for Multi-hop\nExplainable Fact Verification. In Findings of the Association for Computational Linguistics: ACL 2024. 9340–9353.\n[125] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in retrieval-augmented large language models. In\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 5303–5315.\n[126] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. 2024. Mmlongbench-\ndoc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems 37 (2024),\n95963–96010.\n[127] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. When not to trust language models:\nInvestigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511 (2022).\n[128] Gary Marchionini. 2006. Exploratory search: from finding to understanding. Commun. ACM (April 2006), 41–46.\n[129] Math-AI Team. 2025. AIME 2025 Dataset. https://huggingface.co/datasets/math-ai/aime25. Hugging Face, Accessed: 2025-10-11.\n[130] Jianbiao Mei, Tao Hu, Daocheng Fu, Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xinyu Cai, Xing Gao, Yu Yang, et al. 2025. O2-Searcher:\nA Searching-based Agent Model for Open-Domain Open-Ended Question Answering. arXiv preprint arXiv:2505.16582 (2025).\n[131] Lang Mei, Zhihan Yang, and Chong Chen. 2025. AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement\nLearning. arXiv preprint arXiv:2508.20368 (2025).\n[132] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: a benchmark for general ai assistants. In The\nTwelfth International Conference on Learning Representations.\n[133] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. arXiv\npreprint arXiv:2004.10645 (2020).\n[134] National Library of Medicine, NIH. 2025. About PubMed. https://pubmed.ncbi.nlm.nih.gov/about/. Accessed 2025-10-08.\n[135] Xuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, and Shafiq Joty. 2025. SFR-DeepResearch:\nTowards Effective Reinforcement Learning for Autonomously Reasoning Single Agents. arXiv preprint arXiv:2509.06283 (2025).\n[136] OpenThoughts Team. 2025. Open Thoughts. https://open-thoughts.ai. Accessed: January 2025.\n[137] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35\n(2022), 27730–27744.\n[138] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: Bringing order to the web. Technical\nReport. Stanford infolab.\n[139] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: A large-scale multi-subject multi-choice dataset for medical\ndomain question answering. In Conference on health, inference, and learning. PMLR, 248–260.\n[140] Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, and Le Sun. 2024. Not All Contexts Are Equal: Teaching\nLLMs Credibility-aware Generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing.\nManuscript submitted to ACM\n34\nMinhua Lin et al.\n[141] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,\nJean Maillard, et al. 2020. KILT: a benchmark for knowledge intensive language tasks. arXiv preprint arXiv:2009.02252 (2020).\n[142] Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. 2025. SealQA: Raising the Bar for Reasoning in\nSearch-Augmented Language Models. arXiv preprint arXiv:2506.01062 (2025).\n[143] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi,\net al. 2025. Humanity’s last exam. arXiv preprint arXiv:2501.14249 (2025).\n[144] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in\nlanguage models. arXiv preprint arXiv:2210.03350 (2022).\n[145] Haritz Puerto, Gözde Gül Şahin, and Iryna Gurevych. 2021. Metaqa: Combining expert agents for multi-skill question answering. arXiv preprint\narXiv:2112.01922 (2021).\n[146] Peng Qi, Haejun Lee, Tg Sido, and Christopher D Manning. 2021. Answering Open-Domain Questions of Varying Reasoning Steps from Text. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 3599–3614.\n[147] Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, et al. 2025.\nWebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents. arXiv preprint arXiv:2509.13309 (2025).\n[148] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating\nlarge language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 (2023).\n[149] Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft, and Mohit Iyyer. 2020. Open-Retrieval Conversational Question Answering. In\nProceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 539–548.\n[150] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization:\nYour language model is secretly a reward model. Advances in Neural Information Processing Systems 36 (2023), 53728–53741.\n[151] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv\npreprint arXiv:1606.05250 (2016).\n[152] Siva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversational question answering challenge. Transactions of the Association\nfor Computational Linguistics 7 (2019), 249–266.\n[153] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. 2024.\nGpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling.\n[154] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information\nRetrieval 3, 4 (2009), 333–389.\n[155] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. British Library\nResearch and Development Department.\n[156] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. 2025. Gaia-2: A Controllable\nMulti-View Generative World Model for Autonomous Driving. arXiv preprint arXiv:2503.20523 (2025).\n[157] Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, and Aman Chadha. 2024. A Comprehensive Survey of Hallucination in\nLarge Language, Image, Video and Audio Foundation Models. In Findings of the Association for Computational Linguistics: EMNLP 2024. Association\nfor Computational Linguistics, Miami, Florida, USA, 11709–11724.\n[158] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas\nScialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2023),\n68539–68551.\n[159] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. High-dimensional continuous control using generalized\nadvantage estimation. arXiv preprint arXiv:1506.02438 (2015).\n[160] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint\narXiv:1707.06347 (2017).\n[161] Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih,\nPang Wei Koh, et al. 2025. ReasonIR: Training Retrievers for Reasoning Tasks. arXiv preprint arXiv:2504.20595 (2025).\n[162] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. 2024. Deepseekmath:\nPushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 (2024).\n[163] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. HybridFlow: A\nFlexible and Efficient RLHF Framework. arXiv preprint arXiv: 2409.19256 (2024).\n[164] Quan Shi, Michael Tang, Karthik Narasimhan, and Shunyu Yao. 2024. Can Language Models Solve Olympiad Programming? arXiv preprint\narXiv:2404.10952 (2024).\n[165] Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Xiaozhe Ren, Chen Zhang, Hanting Chen, Yasheng Wang, Lifeng Shang, Fisher Yu,\net al. 2025. Pangu deepdiver: Adaptive search intensity scaling via open-web reinforcement learning. arXiv preprint arXiv:2505.24332 (2025).\n[166] Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, and Xiang Wang. 2025. Search and Refine During Think:\nAutonomous Retrieval-Augmented Reasoning of LLMs. arXiv preprint arXiv:2505.11277 (2025).\n[167] Zhengliang Shi, Lingyong Yan, Dawei Yin, Suzan Verberne, Maarten de Rijke, and Zhaochun Ren. 2025. Iterative self-incentivization empowers\nlarge language models as agentic searchers. arXiv preprint arXiv:2505.20128 (2025).\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n35\n[168] Jiasheng Si, Yibo Zhao, Yingjie Zhu, Haiyang Zhu, Wenpeng Lu, and Deyu Zhou. 2024. CHECKWHY: Causal Fact Verification via Argument\nStructure. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 15636–15659.\ndoi:10.18653/v1/2024.acl-long.835\n[169] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. 2025. Agentic retrieval-augmented generation: A survey on agentic rag. arXiv\npreprint arXiv:2501.09136 (2025).\n[170] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-searcher: Incentivizing\nthe search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592 (2025).\n[171] Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen.\n2025. R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning. arXiv preprint arXiv:2505.17005\n(2025).\n[172] Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation 28, 1 (1972),\n11–21.\n[173] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing. 8273–8288.\n[174] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020.\nLearning to summarize with human feedback. Advances in neural information processing systems 33 (2020), 3008–3021.\n[175] Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary S Siegel, Michael Tang,\net al. 2024. Bright: A realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883 (2024).\n[176] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. 2025. Zerosearch:\nIncentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588 (2025).\n[177] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, et al. 2025.\nSimpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834 (2025).\n[178] Richard S Sutton, Andrew G Barto, et al. 1998. Reinforcement learning: An introduction. Vol. 1.\n[179] Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. arXiv preprint arXiv:1803.06643 (2018).\n[180] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering challenge targeting common-\nsense knowledge. arXiv preprint arXiv:1811.00937 (2018).\n[181] Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, and Ji-Rong Wen. 2025. HierSearch: A Hierarchical Enterprise Deep Search\nFramework Integrating Local and Web Searches. arXiv preprint arXiv:2508.08088 (2025).\n[182] Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, and Jinjie Gu. 2025. RAG-R1 : Incentivize the Search and Reasoning\nCapabilities of LLMs through Multi-query Parallelism. arXiv:2507.02962 [cs.CL] https://arxiv.org/abs/2507.02962\n[183] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. Slidevqa: A dataset for document visual\nquestion answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 13636–13645.\n[184] Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. arXiv preprint arXiv:2401.15391\n(2024).\n[185] Xijia Tao, Yihua Teng, Xinxing Su, Xinyu Fu, Jihao Wu, Chaofan Tao, Ziru Liu, Haoli Bai, Rui Liu, and Lingpeng Kong. 2025. MMSearch-Plus: A\nSimple Yet Challenging Benchmark for Multimodal Browsing Agents. arXiv preprint arXiv:2508.21475 (2025).\n[186] Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. 2025.\nWebshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061 (2025).\n[187] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot\nevaluation of information retrieval models. arXiv preprint arXiv:2104.08663 (2021).\n[188] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and\nVERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers). 809–819.\n[189] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[190] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Interleaving retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509 (2022).\n[191] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question\nComposition. Transactions of the Association for Computational Linguistics 10 (2022), 539–554.\n[192] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2024.\nFreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. In Findings of the Association for Computational Linguistics:\nACL 2024. 13697–13720.\n[193] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction:\nVerifying scientific claims. arXiv preprint arXiv:2004.14974 (2020).\n[194] Fali Wang, Minhua Lin, Yao Ma, Hui Liu, Qi He, Xianfeng Tang, Jiliang Tang, Jian Pei, and Suhang Wang. 2025. A Survey on Small Language\nModels in the Era of Large Language Models: Architecture, Capabilities, and Trustworthiness. In Proceedings of the 31st ACM SIGKDD Conference\nManuscript submitted to ACM\n36\nMinhua Lin et al.\non Knowledge Discovery and Data Mining V. 2. 6173–6183.\n[195] Junlin Wang, Zehao Wu, Shaowei Lu, Yanlan Li, and Xinghao Huang. 2025. SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised\nMulti-Agent Framework. arXiv preprint arXiv:2509.18167 (2025).\n[196] Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong, and Zhang Xiong. 2023. Knowledge-driven cot:\nExploring faithful reasoning in llms for knowledge-intensive question answering. arXiv preprint arXiv:2308.13259 (2023).\n[197] Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, and Feng Zhao. 2025. Vidorag: Visual document retrieval-\naugmented generation via dynamic iterative reasoning agents. arXiv preprint arXiv:2502.18017 (2025).\n[198] Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao. 2025. VRAG-RL: Empower\nVision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning. arXiv preprint\narXiv:2505.22019 (2025).\n[199] Ruobing Wang, Daren Zha, Shi Yu, Qingfei Zhao, Yuxuan Chen, Yixuan Wang, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, et al. 2024.\nRetriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation. arXiv preprint arXiv:2410.08821 (2024).\n[200] Shengkang Wang, Hongzhan Lin, Ziyang Luo, Zhen Ye, Guang Chen, and Jing Ma. 2024. Mfc-bench: Benchmarking multimodal fact-checking with\nlarge vision-language models. arXiv preprint arXiv:2406.11288 (2024).\n[201] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al.\n2024. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems\n37 (2024), 95266–95290.\n[202] Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. 2025. StepSearch: Igniting LLMs Search Ability via\nStep-Wise Proximal Policy Optimization. arXiv preprint arXiv:2505.15107 (2025).\n[203] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring\nshort-form factuality in large language models. arXiv preprint arXiv:2411.04368 (2024).\n[204] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and\nAmelia Glaese. 2025. Browsecomp: A simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516 (2025).\n[205] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and\nAmelia Glaese. 2025. Browsecomp: A simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516 (2025).\n[206] Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, et al. 2024. Long-form\nfactuality in large language models. arXiv preprint arXiv:2403.18802 (2024).\n[207] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. 2025. Webagent-r1:\nTraining web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421 (2025).\n[208] Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209 (2017).\n[209] Ryen W. White and Resa A. Roth. 2009. Exploratory Search: Beyond the Query-Response Paradigm. Synthesis Lectures on Information Concepts,\nRetrieval, and Services 1 (2009), 98. https://api.semanticscholar.org/CorpusID:35584347\n[210] Wikimedia Foundation. 2025. Wikimedia Database Dumps. https://dumps.wikimedia.org/backup-index.html. Accessed 2025-10-08.\n[211] Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. 2025. MMSearch-R1: Incentivizing LMMs to Search. arXiv\npreprint arXiv:2506.20670 (2025).\n[212] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, et al. 2025.\nWebDancer: Towards Autonomous Information Seeking Agency. arXiv preprint arXiv:2505.22648 (2025).\n[213] Jian Wu, Linyi Yang, Zhen Wang, Manabu Okumura, and Yue Zhang. 2024. Cofca: A Step-Wise Counterfactual Multi-hop QA benchmark. arXiv\npreprint arXiv:2402.11924 (2024).\n[214] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. 2025.\nWebwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572 (2025).\n[215] Peilin Wu, Mian Zhang, Xinlu Zhang, Xinya Du, and Zhiyu Zoey Chen. 2025. Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing\nUncertainty. arXiv preprint arXiv:2505.17281 (2025).\n[216] Weiqi Wu, Xin Guan, Shen Huang, Yong Jiang, Pengjun Xie, Fei Huang, Jiuxin Cao, Hai Zhao, and Jingren Zhou. 2025. MASKSEARCH: A Universal\nPre-Training Framework to Enhance Agentic Search Capability. arXiv preprint arXiv:2505.20285 (2025).\n[217] Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, et al. 2025. ReSum:\nUnlocking Long-Horizon Search Intelligence via Context Summarization. arXiv preprint arXiv:2509.13313 (2025).\n[218] Zongyu Wu, Minhua Lin, Zhiwei Zhang, Fali Wang, Xianren Zhang, Xiang Zhang, and Suhang Wang. 2025. Image Corruption-Inspired Membership\nInference Attacks against Large Vision-Language Models. arXiv preprint arXiv:2506.12340 (2025).\n[219] Xbench-Team. 2025. Xbench-DeepSearch. https://xbench.org/agi/aisearch\n[220] Yunjia Xi, Jianghao Lin, Yongzhao Xiao, Zheli Zhou, Rong Shan, Te Gao, Jiachen Zhu, Weiwen Liu, Yong Yu, and Weinan Zhang. 2025. A survey of\nllm-based deep search agents: Paradigm, optimization, evaluation, and challenges. arXiv preprint arXiv:2508.05668 (2025).\n[221] Yunjia Xi, Jianghao Lin, Menghui Zhu, Yongzhao Xiao, Zhuoying Ou, Jiaqi Liu, Tong Wan, Bo Chen, Weiwen Liu, Yasheng Wang, et al. 2025.\nInfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation. arXiv preprint arXiv:2505.15872 (2025).\n[222] Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, et al.\n2025. AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning. arXiv preprint\nManuscript submitted to ACM\nA Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations,\nEvaluations, and Applications\n37\narXiv:2509.08755 (2025).\n[223] Ziyi Xia, Kun Luo, Hongjin Qian, and Zheng Liu. 2025. Open Data Synthesis For Deep Research. arXiv preprint arXiv:2509.00375 (2025).\n[224] Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, et al. 2025.\nRag-gym: Optimizing reasoning and search agents with process supervision. arXiv preprint arXiv:2502.13957 (2025).\n[225] Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. 2024. Search-in-the-chain: Interactively enhancing large language\nmodels with search for knowledge-intensive tasks. In Proceedings of the ACM Web Conference 2024. 1362–1373.\n[226] Zhichao Xu, Fengran Mo, Zhiqi Huang, Crystina Zhang, Puxuan Yu, Bei Wang, Jimmy Lin, and Vivek Srikumar. 2025. A survey of model\narchitectures in information retrieval. arXiv preprint arXiv:2502.14822 (2025).\n[227] Zhichao Xu, Minheng Wang, Yawei Wang, Wenqian Ye, Yuntao Du, Yunpu Ma, and Yijun Tian. 2025. RECON: Reasoning with Condensation for\nEfficient Retrieval-Augmented Generation. arXiv:2510.10448 [cs.CL] https://arxiv.org/abs/2510.10448\n[228] Zhichao Xu, Zongyu Wu, Yun Zhou, Aosong Feng, Kang Zhou, Sangmin Woo, Kiran Ramnath, Yijun Tian, Xuan Qi, Weikang Qiu, Lin Lee Cheong,\nand Haibo Ding. 2025. Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation. arXiv preprint arXiv:2508.17556\n(2025).\n[229] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025.\nQwen3 technical report. arXiv preprint arXiv:2505.09388 (2025).\n[230] Shuo Yang, Yuqin Dai, Guoqing Wang, Xinran Zheng, Jinfeng Xu, Jinze Li, Zhenzhe Ying, Weiqiang Wang, and Edith CH Ngai. 2025. RealFactBench:\nA Benchmark for Evaluating Large Language Models in Real-World Fact-Checking. arXiv preprint arXiv:2506.12538 (2025).\n[231] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600 (2018).\n[232] Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2023. End-to-end multimodal fact-checking and explanation\ngeneration: A challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval. 2733–2743.\n[233] Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, et al. 2025. Medreseacher-r1: Expert-level\nmedical deep researcher via a knowledge-informed trajectory synthesis framework. arXiv preprint arXiv:2508.14880 (2025).\n[234] Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, et al. 2025. MedResearcher-R1:\nExpert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework. arXiv preprint arXiv:2508.14880 (2025).\n[235] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. 2025. Dapo:\nAn open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 (2025).\n[236] Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Ilia Kulikov, Kyunghyun Cho, Dong Wang, Yuandong Tian, Jason E Weston, et al. 2025.\nNaturalreasoning: Reasoning in the wild with 2.8 m challenging questions. arXiv preprint arXiv:2502.13124 (2025).\n[237] Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong. 2025. Reinforcing Multi-Turn Reasoning in LLM\nAgents via Turn-Level Credit Assignment. arXiv preprint arXiv:2505.11821 (2025).\n[238] Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, et al. 2025.\nEvolveSearch: An Iterative Self-Evolving Search Agent. arXiv preprint arXiv:2505.22501 (2025).\n[239] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. 2024. Offline training of language model agents\nwith functions as learnable weights. In Forty-first International Conference on Machine Learning.\n[240] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint\narXiv:1904.09675 (2019).\n[241] Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, et al.\n2025. Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning. arXiv preprint arXiv:2505.14069 (2025).\n[242] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. 2025. Agent models: Internalizing chain-of-action generation into\nreasoning models. arXiv preprint arXiv:2503.06580 (2025).\n[243] Zhiwei Zhang, Hui Liu, Xiaomin Li, Zhenwei Dai, Jingying Zeng, Fali Wang, Minhua Lin, Ramraj Chandradevan, Zhen Li, Chen Luo, et al. 2025.\nBradley-Terry and Multi-Objective Reward Modeling Are Complementary. arXiv preprint arXiv:2507.07375 (2025).\n[244] Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, and Limin Liu. 2025. R-Search: Empowering LLM Reasoning with Search via Multi-Reward\nReinforcement Learning. arXiv preprint arXiv:2506.04185 (2025).\n[245] Shu Zhao, Tan Yu, Anbang Xu, Japinder Singh, Aaditya Shukla, and Rama Akkiraju. 2025. ParallelSearch: Train your LLMs to Decompose Query\nand Search Sub-queries in Parallel with Reinforcement Learning. arXiv preprint arXiv:2508.09303 (2025).\n[246] Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, and Dawei Yin. 2025. TURA:\nTool-Augmented Unified Retrieval Agent for AI Search. arXiv preprint arXiv:2508.04604 (2025).\n[247] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research\nvia reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160 (2025).\n[248] Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement\nlearning. arXiv preprint arXiv:1709.00103 (2017).\n[249] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. 2025.\nBrowsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314 (2025).\nManuscript submitted to ACM\n38\nMinhua Lin et al.\n[250] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. 2025.\nBrowsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314 (2025).\n[251] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, et al. 2023. WebArena: A Realistic Web Environment\nfor Building Autonomous Agents. arXiv preprint arXiv:2307.13854 (2023).\n[252] Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. 2024. Metacognitive retrieval-augmented large language models. In Proceedings\nof the ACM Web Conference 2024. 1453–1463.\n[253] Andrew Zhu, Alyssa Hwang, Liam Dugan, and Chris Callison-Burch. 2024. FanOutQA: A Multi-Hop, Multi-Document Question Answering\nBenchmark for Large Language Models. In Proceedings of ACL 2024 (Short Papers). https://aclanthology.org/2024.acl-short.2.pdf\n[254] Changtai Zhu, Siyin Wang, Ruijun Feng, Kai Song, and Xipeng Qiu. 2025. ConvSearch-R1: Enhancing Query Reformulation for Conversational\nSearch with Reasoning via Reinforcement Learning. arXiv preprint arXiv:2505.15776 (2025).\n[255] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2025. PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation\nof Large Language Models. In 34th USENIX Security Symposium (USENIX Security 25). 3827–3844.\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\nManuscript submitted to ACM\n",
    "content": "# Interpretation of \"A Survey on Reinforcement Learning-based Agentic Search\"\n\n## 1. Core Content and Key Contributions\n\nThis paper presents a comprehensive survey on **Reinforcement Learning-based Agentic Search**, systematically organizing research advancements in this emerging field. Its central theme is to explore how reinforcement learning (RL) techniques can transform large language models (LLMs) from passive information retrievers into active \"agents\" capable of planning, retrieving, reflecting, and making decisions.\n\n### Main Contributions:\n\n- **First Systematic Definition and Taxonomy**: The paper offers the first clear definition of \"Reinforcement Learning-based Agentic Search\" and proposes a three-dimensional analytical framework:\n  1. **What RL is for**: The functional role of RL in agentic search.\n  2. **How RL is used**: Optimization strategies such as reward design and algorithm selection.\n  3. **Where RL is applied**: The granularity level of optimization—from single agents to multi-agent systems.\n\n- **Comprehensive Literature Review**: Through detailed tables and case studies, it summarizes representative methods, evaluation metrics, and application scenarios, providing researchers with a clear knowledge map.\n\n- **Identification of a Key Paradigm Shift**: It highlights the transition from traditional, heuristic, single-step retrieval-augmented generation (RAG) to dynamic, adaptive, multi-turn agentic search, emphasizing the pivotal role of reinforcement learning in enabling this shift.\n\n- **Future Research Directions**: It thoroughly discusses current challenges—such as multimodal search, long-term memory, and trustworthiness—and outlines promising directions for future work.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nWhile the paper itself is a survey, the field it reviews demonstrates significant breakthroughs and innovations:\n\n- **From “Static Knowledge Base” to “Dynamic Tool User”**: Traditional RAG treats retrieval as a one-time injection of information. In contrast, agentic search views LLMs as autonomous decision-makers that dynamically determine *whether* to search, *how* to search, and *how* to integrate results during reasoning—mimicking human behavior. This greatly enhances the model's ability to solve complex, multi-hop problems.\n\n- **Self-Evolution via Reinforcement Learning**: This is the most transformative innovation. Instead of being driven by predefined rules, agents use RL to interact with environments (e.g., search engines, databases) and learn optimal search strategies through feedback (rewards). This trial-and-error learning mechanism enables agents to discover highly efficient pathways that are difficult to encode manually, achieving true adaptability and continuous improvement.\n\n- **Fine-Grained Control and Optimization**: RL is applied across multiple stages of the search process, going far beyond simple \"search-or-not\" decisions, including:\n  - **Retrieval Control**: Dynamically managing search intensity, frequency, and efficiency to minimize unnecessary API calls.\n  - **Query Optimization**: Refining ambiguous user queries in real time to better match retrieval system requirements.\n  - **Reasoning-Retrieval Integration**: Precisely interleaving reasoning and retrieval steps to ensure steady progress toward solving the problem.\n  - **Multi-Agent Collaboration**: Using RL to coordinate specialized agents (e.g., planner, executor, rewriter) into an efficient collaborative network.\n\n- **Diverse Reward Mechanisms**: Rewards extend beyond final answer accuracy to include multi-level signals such as:\n  - *Process quality* (information gain),\n  - *Efficiency* (number of API calls),\n  - *Faithfulness* (evidence utilization),\n  guiding holistic agent development.\n\n---\n\n## 3. Promising Startup Ideas Based on This Paper\n\nThe technological landscape outlined in this paper inspires several high-potential entrepreneurial opportunities. Below is one concrete and compelling idea:\n\n### Startup Concept: **\"IntelliResearch Assistant\" — An AI Research Assistant Platform for Enterprise R&D Teams**\n\n#### Project Overview  \nDevelop a SaaS platform powered by reinforcement learning-based agentic search, designed specifically for enterprise R&D departments (e.g., pharmaceuticals, biotechnology, new materials, AI) to automate deep literature review and knowledge discovery. Beyond answering questions, it helps scientists uncover cross-domain connections, accelerating innovation.\n\n#### Core Features and Technical Implementation (Based on the Paper)\n1. **Integration with Heterogeneous Data Sources**:\n   - Unify access to diverse sources such as PubMed, arXiv, patent databases, internal corporate repositories, and the open web.\n\n2. **RL-Driven Intelligent Search Agent**:\n   - Implement architectures inspired by `DeepResearcher` or `MedResearcher-R1`, training a domain-specific \"research agent\".\n   - Apply **module-level optimization** (e.g., `s3` method), adding a lightweight RL controller without modifying the base LLM.\n   - Design a **multi-objective reward function**:\n     - *Accuracy Reward*: Based on expert validation or authoritative database alignment.\n     - *Efficiency Reward*: Minimize API cost and latency.\n     - *Novelty Reward*: Encourage discovery of under-cited but promising papers.\n     - *Explainability Reward*: Require transparent reasoning chains and citation trails.\n\n3. **Long-Term Memory and Collaboration**:\n   - Incorporate ideas from `ReSum` or `SFR-DeepResearch` to manage long-term research sessions, automatically summarizing and storing key findings.\n   - Adopt **planner-executor architectures** like `MAO-ARAG` or `OPERA`, where a \"planning agent\" decomposes complex tasks and delegates subtasks (literature search, data analysis, summary generation) to specialized \"executor agents\", coordinated via RL.\n\n4. **Trustworthiness and Security**:\n   - Implement adversarial robustness mechanisms discussed in `PoisonedRAG` to guard against malicious or misleading information.\n   - Provide full audit logs showing every decision step and its rationale, enhancing user trust and transparency.\n\n#### Target Customers and Market Value\n- **Customers**: Pharmaceutical companies, biotech startups, university labs, consulting firms—any organization requiring extensive literature review.\n- **Value Proposition**:\n  - **Massive Time Savings**: Reduce weeks of manual research to hours.\n  - **Lower Labor Costs**: Free junior researchers from repetitive data gathering.\n  - **Increased Innovation Potential**: Surface hidden interdisciplinary insights to spark new research directions.\n  - **Improved Research Quality**: Deliver traceable, verifiable evidence chains that enhance credibility.\n\n#### Business Model\n- **Subscription Pricing**: Tiered plans based on team size and usage (e.g., monthly API call volume).\n- **Custom Solutions**: Offer premium services to train and deploy private, domain-specific agents integrated with proprietary knowledge bases.\n\nThis startup idea aligns perfectly with the paper’s emphasis on **domain-specific applications** and **deep research** scenarios, transforming cutting-edge academic concepts into practical solutions for real-world challenges—with immense commercial potential.",
    "github": "https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers",
    "hf": ""
}