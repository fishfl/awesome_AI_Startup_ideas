{
    "id": "2509.15292",
    "title": "An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature",
    "summary": "Sure, please provide the text you would like translated into English.",
    "abstract": "We propose an automated pipeline for performing literature reviews using semantic similarity. Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity. By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input. Three embedding models were evaluated. A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline. Despite the absence of heuristic feedback or ground truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Abhiyan Dhakal(1),Kausik Paudel(1),Sanjog Sigdel(1) ((1) Kathmandu University, Dhulikhel, Nepal)",
    "subjects": [
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:8 pages, 6 figures, 1 table, National Conference on Computer Innovations",
    "keypoint": "- The proposed automated pipeline uses semantic similarity for rapid literature review with minimal input (title and abstract).\n- It generates keywords using the Gemini-2.0-flash LLM and retrieves relevant papers from arXiv via keyword search.\n- Three embedding models were evaluated: TF-IDF, all-MiniLM-L6-v2, and Specter2.\n- TF-IDF showed poor semantic understanding, with low similarity scores and inability to capture conceptual matches.\n- all-MiniLM-L6-v2 provided balanced performance with broader semantic coverage and effective separation between relevant and irrelevant papers.\n- Specter2 achieved high similarity scores but suffered from score saturation, leading to clustering of scores and need for strict thresholding.\n- A statistical thresholding method based on IQR (Q3 + 0.5×IQR) was used to filter relevant papers, ensuring adaptability across distributions.\n- The pipeline includes text extraction from PDFs using PyMuPDF and regex-based section identification.\n- Summarization of paper sections was performed using Gemini-2.0-flash, structured into problem statement, methodology, key findings, and conclusions.\n- Citation intent tagging and contribution classification were implemented using Gemini LLM based on predefined taxonomies.\n- BibTeX entries were generated automatically from arXiv metadata for proper citation formatting.\n- The system synthesized a literature review paragraph using structured summaries, tags, and metadata.\n- Evaluation was conducted on 178 papers fetched from arXiv.\n- TF-IDF retained 19 papers with a threshold of 0.204, showing high lexical precision but limited semantic recall.\n- all-MiniLM-L6-v2 retrieved 20 papers with a threshold of 0.659, demonstrating moderate score distribution and better semantic retrieval.\n- Specter2 retrieved only 11 papers at a high threshold of 0.924 due to narrow score range [0.756, 0.945] and negative skewness.\n- The system lacks ground-truth relevance labels, limiting direct assessment of retrieval accuracy.\n- General-purpose embedding models may underperform in specialized domains due to lack of domain-specific tuning.\n- The heuristic thresholding method may exclude semantically relevant but lexically distant papers.\n- No qualitative evaluation was conducted; reliance is on quantitative metrics like cosine similarity distributions.\n- Future work includes adaptive thresholding, human-in-the-loop evaluation, re-ranking, domain adaptation, and integration with citation networks.",
    "date": "2025-09-23",
    "paper": "An Artificial Intelligence Driven Semantic\nSimilarity-Based Pipeline for Rapid Literature\nReview\nAbhiyan Dhakal\nKathmandu University\nDhulikhel, Nepal\nitsabhiyandhakal@gmail.com\nKausik Paudel\nKathmandu University\nDhulikhel, Nepal\nkausikpaudel@gmail.com\n*Sanjog Sigdel\nKathmandu University\nDhulikhel, Nepal\nsanjog.sigdel@ku.edu.np\nAbstract—We propose an automated pipeline for performing\nliterature reviews using semantic similarity. Unlike traditional\nsystematic review systems or optimization-based methods, this\nwork emphasizes minimal overhead and high relevance by\nusing transformer-based embeddings and cosine similarity. By\nproviding a paper’s title and abstract, it generates relevant\nkeywords, fetches relevant papers from open-access repositories\n(e.g., ArXiv), and ranks them based on their semantic closeness\nto the input. Three embedding models: TF-IDF, all-MiniLM-L6-\nv2, and Specter2 were evaluated. While TF-IDF struggled with\ncapturing deeper semantic meaning, all-MiniLM-L6-v2 provided\nbroader conceptual coverage. Specter2, specifically fine-tuned for\nscientific texts, exhibited score saturation in similarity scores. A\nstatistical thresholding approach is then applied to filter relevant\npapers, enabling an effective literature review pipeline. Despite\nthe absence of heuristic feedback or ground-truth relevance\nlabels, the proposed system shows promise as a scalable and\npractical tool for preliminary research and exploratory analysis.\nI. INTRODUCTION\nConducting a literature review is an important step in any\nresearch project. By examining existing studies, researchers\nunderstand the progress that has been made in their field.\nLiterature reviews summarizes the current state of research,\nand identify gaps providing insights to the direction of future\nstudies.\nTraditional systematic literature review (SLR) methods are\nmostly time consuming and require extensive manual efforts to\nselect papers, filter them, and organize relevant papers. Recent\nautomated approaches, such as Swarm SLR [1] has attempted\nto address these challenges by introducing structured workflow\nand visualization for systematic reviews, however, it requires\nextensive configuration, user’s expertise for proper parameter\ntuning and possesses challenges to perform quick, exploratory\nliterature reviews on the provided context.\nConsidering the research gap, we propose AutoLit, an\nautomated pipeline designed to perform literature reviews\nusing semantic similarity. AutoLit operates on minimal input.\nIt requires the title and abstract of the research paper, and\ngenerates a list of relevant papers based on transformer-based\nembeddings and cosine similarity. The pipeline automatically\n*Corresponding author: Sanjog Sigdel (sanjog.sigdel@ku.edu.np).\ngenerates keywords from the input, retrieves papers from open-\naccess repositories like arXiv [2], and filters them based on\nsemantic similarity using a customizable threshold.\nThis approach is useful for early-stage research and ex-\nploratory analysis, focusing on quick and relevant results\nrather than complete coverage. It minimizes manual interven-\ntion, and thus provides an efficient alternative to existing SLR\nsystems for efficient literature discovery and reviews.\nII. RELATED WORKS\nWith the development and innovation of Large Language\nModels (LLMs), semantic embeddings and workflow op-\ntimization techniques, automating the process of literature\nreview has seen significant progress in the recent years. In re-\nplacement of traditional SLR, Wittenborg et al. [1] introduced\nSWARM-SLR that has effectively structured the systematic\nliterature review process by defining clear requirements, es-\ntablishing a modular workflow and creating a framework to\nevaluate the supporting tools. This has had impact in the early\nstages of research, such as searching other relevant papers\nand initial data processing. However, SWARM-SLR lacks the\nactual process of writing literature review.\nIn contrast to SWARM-SLR, Tang et al. [3] has explored\nthe feasibility of LLMs in automating various processes of\nliterature reviews such as reference generation, abstraction\ncomposition and complete review writing. Their evaluation\nshowed that while LLMs showed impressive results, they often\nsuffer from hallucination in references, favoring the tasks most\ncited. Additionally, generating incomplete author lists, and\nvariable performance across various disciplines were major\nlimitations. This is partly addressed by Li et al. [4] through\ntheir system ChatCite, which combines human workflow guid-\nance within the LLM agent. It is a semi-automated system\nthat generates comparative summaries between studies, which\nwas not addressed by prior LLM based approaches. However,\nit relies on curated datasets and thus may not generalize\nsummaries without further adaptation.\nReimers and Gurevych [5] introduced Sentence Bert\n(SBERT) for semantic similarity, which has become a baseline\nin many automated pipelines largely due to its efficient and\narXiv:2509.15292v1  [cs.AI]  18 Sep 2025\naccurate generation of sentence embeddings. SBERT enables\nprecise comparison using cosine similarity. This significantly\nimproves tasks such as relevant literature retrieval, ranking,\nand clustering which is useful for understanding the closeness\nof different papers.\nKasanishi et al. [6] proposed SciReviewGen, a large-scale\ndataset and framework designed for automatic generation of\nliterature reviews by summarizing multiple scientific papers\nusing transformer-based models like Fusion-in-Decoder [7] ex-\ntended for literature review generation. It focuses on producing\nsummaries but still faces challenges like hallucinations and\nmissing detailed information.\nAdditionally, Wang et al. [8] evaluate the capability of Chat-\nGPT in generating Boolean queries for systematic literature\nsearches. Their findings showed that query refinement using\nChatGPT can improve the recall and F1 scores. The refined\nquery after single prompting resulted an F1 score of 0.0772\nwhile guided prompting increased the F1 score to 0.5171\nwhich is a significant rise. Also the highest recall they were\nable to achieve was 0.9128. This indicates that combining\nbase queries with ChatGPT refinement can generate effective\nBoolen queries suitable for rapid literature reviews where time\nconstraints matter. Although LLMs can approximate human\nlike query construction, they fall short in encoding complex\nBoolean logic effectively.\nDespite these advances, many existing systems are based\non multi-stage workflows, which can significantly hamper the\nperformance of the system. In contrast, semantic pipelines that\nprioritize minimal input and provide relevant results would\nbe beneficial for streamlining the literature review process.\nThis approach addresses the current gap by focusing on\nsemantic similarity-based classification and filtering of open-\naccess repositories such as arXiv. It emphasizes transformer-\nbased embeddings and cosine similarity for semantic search\nthat enables the extraction of a highly relevant set of papers\nwhile performing relevant literature review of the given paper.\nIII. METHODOLOGY\nThis section outlines the pipeline designed to automate\nthe literature review process. An overview of the pipeline is\nillustrated in Figure 1. The pipeline consisted of seven key\nstages, each designed to efficiently and systematically process\nscholarly articles relevant to the research topic.\nA. Keyword Generation\nThe initial stage involved generating a set of relevant key-\nwords to guide the literature search. Given the title and abstract\nof the target paper, 5 to 10 keywords were generated using a\nLLM, gemini-2.0-flash [9], which recent studies have\nshown to be effective for zero-shot keyphrase extraction [10],\nto capture both explicit and semantically related terms, as\nillustrated in Figure 2.\nB. Fetch Papers\nFor each keyword generated, a corpus of a maximum of 20\nrelevant papers was fetched from arXiv via keyword-based\nsearch using the arXiv API. Since the papers were being\nfetched using different keywords, there were some duplicates.\nThose duplicate papers were filtered out on the basis of their\nmetadata ensuring that only unique papers remained in the\ncorpus.\nC. Retrieve Relevant Papers\nTo refine the selection of the papers obtained, a semantic\nsimilarity assessment was performed. The sentence trans-\nformer model all-MiniLM-L6-v2 [11] was used to gen-\nerate 384-dimensional dense vector embeddings for the input\nquery (derived from keywords) and the title and abstract\nof the papers obtained. The cosine similarities between the\nembeddings of the fetched papers and that of the input query\nwere then calculated.\nA statistical thresholding method based on the interquartile\nrange (IQR) of the similarity scores distribution was applied to\nfilter the papers. The IQR method is a robust, distribution-free\napproach for outlier detection that does not assume normality\nof the data [12, 13]. In line with recent NLP work [14], we\nadopt a conservative upper bound of Q3 + 0.5 × IQR to\nselect only the highest-scoring papers, ensuring high precision\nwhile retaining adaptability across different similarity score\ndistributions.\nThis adaptive thresholding approach accounted for the nat-\nural variance and skew in the similarity scores, resulting in\na more reliable filtering of semantically relevant papers, as\nillustrated in Figure 3.\nFor comparison, we also evaluated other embedding mod-\nels, including Specter2 [15], which is specifically trained on\nscientific citation data, and a traditional TF-IDF with cosine\nsimilarity baseline. Specter2 produced tightly clustered high\nsimilarity scores that required stricter thresholding to avoid\nfalse positives, while TF-IDF offered greater precision but\nlacked semantic coverage, often missing conceptually related\npapers. Based on this evaluation, all-MiniLM-L6-v2 was\nselected as a balanced choice for semantic retrieval in this\npipeline.\nD. Text Extraction from PDF\nFor each filtered paper, either the abstract or the full PDF\nwas processed to extract structured textual content. We used\nPyMuPDF [16] library for full text extraction when PDFs\nwere available. To structure the content of each paper, regular\nexpression (regex) patterns were defined to identify and de-\nlineate different sections: Introduction, Methodology, Results,\nand Conclusion. The defined patterns were as follows:\n• Abstract: The regex pattern (?i)\\babstract\\b was\nused to locate the abstract section of the paper.\n• Introduction: The pattern (?i)\\bintroduction\\b\nwas applied to identify the introduction.\n• Methods: To capture the methodology section, the pattern\n(?i)\\b(methodology|methods|approach)\\b\nwas used, accounting for various terminologies that\nmight describe this section.\nFig. 1. Pipeline Overview\nFig. 2. Keyword Generation Processs\n• Results: To capture the results section, the pattern\n(?i)\\b(results|findings|experiments)\\b\nwas employed, which might also be referred to as\nfindings or experiments.\n• Conclusion:\nThe\nregex\npattern\n(?i)\\b(conclusion|discussion|summary)\\b\nwas defined to capture the conclusion, which might also\nappear under the names ”discussion” or ”summary” in\nsome papers.\nE. Summarization\nThe extracted text from each section of the relevant papers\nwas\nthen\nsummarized\nusing\nthe\ngemini-2.0-flash\nLLM.\nTo\nensure\nconsistent\ninformation\nextraction,\nthe\nLLM\nwas\ninstructed\nto\nstructure\nthe\nsummaries\nunder the summary key, which contained four distinct\ncategories:\nproblem_statement,\nmethodology,\nkey_findings, and conclusion_recommendations\n(see Listing 1). Each of these categories was designed to hold\nFig. 3. Relevant Papers Retrieval Processs\na list of bullet points summarizing the respective information\nextracted from the paper.\nListing 1. Example of structured summary JSON\n{\n\"summary\": {\n\"problem_statement\": [\n\"Bullet point 1\",\n\"Bullet point 2\"\n],\n\"methodology\": [\n\"Bullet point 1\",\n\"Bullet point 2\"\n],\n\"key_findings\": [\n\"Bullet point 1\",\n\"Bullet point 2\"\n],\n\"conclusion_recommendations\": [\n\"Bullet point 1\",\n\"Bullet point 2\"\n]\n}\n}\nSpecifically, under the problem_statement category,\nthe LLM would list the identified research problems or\ngaps addressed in the paper as individual bullet points.\nThe methodology category would contain bullet points\ndescribing the research approach, methods, or techniques\nemployed by the authors. The key_findings category\nwould present the most significant results or discoveries\nreported in the paper in a bulleted format. Finally, the\nconclusion_recommendations category would list the\nmain conclusions drawn by the authors, along with any recom-\nmendations for future work or applications of their findings.\nF. Citation Intent and Contribution Tagging\nTo further analyze the nature and contributions of each\nselected paper, the Gemini LLM was employed to perform\ntwo key tasks:\n(i) Citation intent tagging, following a predefined tax-\nonomy: Background, Comparison, Extension, Criticism,\nApplication, Future Work, and Other.\n(ii) Contribution classification, assigning each paper to one\nof: Dataset, Algorithm, Framework, Review, Benchmark,\nSurvey, System, Theoretical Analysis, or Other.\nThis approach follows prior research on citation function\nclassification and contribution categorization in scientific lit-\nerature [17, 18, 19] and recent advances employing LLMs for\nautomated citation intent and contribution tagging.\nG. BibTex Entry and Literature Review Generation\nThe overall pipeline for generating the literature review is\nshown in Figure 4. It begins with metadata retrieval from\nthe arXiv API, followed by BibTeX entry generation, paper\nsummarization and tagging using the Gemini LLM, and finally\nsynthesis of the literature review paragraph based on these\nstructured inputs.\nThe metadata of the relevant papers, retrieved from the\narXiv API in earlier stages, was utilized to generate BibTeX\nentries for each paper. This ensured proper citation formatting\nfor any inclusion of these sources in the final paper.\nFig. 4. Litereature Review Synthesis\nFor the generation of the literature review itself, the sum-\nmaries, citation intent tags, and contribution type tags, previ-\nously obtained using the gemini-2.0-flash LLM, served\nas the primary input. The Gemini model was instructed to\nsynthesize this information into a comprehensive overview of\nthe existing literature related to the research topic. This step\naimed to provide a high-level understanding of the current\nstate-of-the-art and key contributions in the field, leveraging\nthe automated analysis performed in the preceding stages of\nthe pipeline.\nIV. EVALUATION\nTo evaluate the effectiveness of embedding models in filter-\ning relevant scientific papers for automated literature review,\nwe experimented with three distinct approaches for research\npaper similarity:\n• TF-IDF: Classical lexical vectorization.\n• all-MiniLM-L6-v2: General-purpose transformer embed-\ndings.\n• Specter2: Scientifically tuned transformer embeddings.\nA total of 178 papers were fetched from arXiv API. Each\ninput paper’s vector was compared to fetched candidates using\ncosine similarity. Table I summarizes the distributions.\nA. TF-IDF\nTF-IDF is a purely syntactic model, it lacks semantic\nencoding capabilities. With a calculated threshold of 0.204,\nit retained 19 papers. Although 19 articles were filtered, the\ncosine similarity distribution was highly positive and skewed\ntoward the range [0.010, 0.294], indicating generally low\nsimilarity scores across the corpus, which is illustrated in\nFigure 5 and Figure 6. Also, the model failed to capture the\nTABLE I\nCOMPARATIVE SUMMARY OF EMBEDDING MODELS AND THEIR PERFORMANCE\nEmbedding Model\nThreshold\nSkewness\nValue Range\n# Retrieved Papers\nTF-IDF\n0.204\n0.622\n[0.010, 0.294]\n19\nall-MiniLM-L6-v2\n0.659\n0.390\n[0.070, 0.804]\n20\nSpecter2\n0.924\n−0.963\n[0.756, 0.945]\n11\nFig. 5. Line plot showing cosine similarity scores across models\nsemantic meaning of the input paper even among the selected\nrelevant papers above the threshold. Thus, TF-IDF excels in\nhigh lexical overlap but misses conceptual matches.\nB. all-MiniLM-L6-v2\nThe all-MiniLM-L6-v2 model is a general-purpose semantic\nembedding model. With a calculated threshold set at 0.659, it\nretrieved 20 papers. The cosine similarity scores was slightly\npositively skewed in the range [0.070, 0.804] with similarity\nscore of 0.390. Performance reflects a moderately spread\ndistribution of similarity scores. Figure 5 and Figure 6 shows\nthat this model provided convincing separations between rel-\nevant and less relevant documents compared to both TF-IDF\nand specter2. Unlike TF-IDF, this model captured semantic\nrelations, enabling retrieval of relevant papers even when exact\nterminology differed.\nC. Specter2\nSpecter2 is a fine-tuned model explicitly for scientific\ndocument embeddings. As a result, it achieved the highest\nabsolute similarity scores. A threshold of 0.924 resulted in\nthe retrieval of 11 papers, with the score distribution skewed\nin the range [0.756, 0.945] with a negative skewness value\nof 0.963. Specter2 effectively captures both semantics and\ndomain-specific knowledge. However, with our threshold set\nat 0.924, the distribution exhibited higher density near 1.0. All\nthe values were clustered at a range of 0.75 to 0.95 requiring\nan extremely precise threshold, which might not always be the\ncase for varied input papers. This can result in false positive\nvalues.\nV. LIMITATIONS\nA. Lack of Ground-Truth Relevance Labels\nThe current system operates without any human-annotated\nor expert-verified relevance labels. This makes it challenging\nto assess whether the papers retrieved by the semantic simi-\nlarity filter are truly relevant to the input query. In the absence\nof proper benchmarks, only indirect metrics such as similarity\nscore distributions can be used to approximate relevance. This\nmay not always reflect actual correctness.\nFig. 6. Similarity distribution visualization of models using box plots\nB. Generalization Constraints of Embedding Models\nWhile models like Specter2 are trained on scientific text,\ngeneral-purpose transformers such as all-MiniLM-L6-v2\nare optimized for broad coverage across diverse domains rather\nthan the scholarly literature specifically. Prior studies have\nshown that such models often underperform in specialized\ndomains, as they may not capture domain-specific terminology,\ncitation context, or methodological nuances with the same\nfidelity as models trained on in-domain corpora [20, 21, 22].\nThis mismatch can lead to embeddings that overlook subtle but\nimportant conceptual relationships, reducing retrieval precision\nin fields with specialized jargon or structured discourse, such\nas medicine, chemistry, or legal research.\nC. Heuristic Thresholding Method\nThe thresholding method employed, using the third quartile\nplus half the interquartile range (Q3+0.5·IQR) is a statistical\nheuristic designed to filter out low-similarity papers. However,\nthis approach does not guarantee semantic relevance. It may\nexclude important but lexically distant papers and include\nirrelevant ones that happen to lie within the upper tail of\nthe similarity distribution. The threshold’s effectiveness is also\ndependent on the characteristics of the embedding model and\nthe corpus.\nD. Lack of Qualitative Evaluation\nEvaluation of the retrieved papers depends entirely on\nquantitative metrics such as cosine similarity distributions,\nmean scores, and standard deviations. There is no qualitative\nassessment of the relevance of the chosen papers in terms of\ntheir content or research methods. Conducting such studies\nwould require curated gold-standard datasets or involvement\nof subject matter experts, which was not feasible within the\ncurrent resource. Without involvement of human reviewers or\ncase studies, it remains uncertain how well the system supports\nreal-world tasks like literature reviews or knowledge discovery.\nVI. CONCLUSION\nThis work presents an automated pipeline for conducting\ntargeted literature reviews using semantic similarity. By using\ntransformer-based embeddings and large language models, it\nefficiently retrieves, filters, and summarizes academic literature\nwith minimal human intervention. By integrating TF-IDF, all-\nMiniLM-L6-v2, and Specter2 embeddings, the system evalu-\nated and filtered 178 papers based on their relevance to a given\nquery derived from title and abstract prompts.\nTF-IDF filters documents based on exact text matches,\nresulting in loss of semantic meaning. all-MiniLM-L6-v2\nprovides a balanced performance but lacks domain-specific\ntuning. Specter2 is best aligned with scientific language, how-\never suffered from similarity saturation, due to rigid threshold\ncalibration.\nIn future work, we plan to focus on implementing adaptive\nthresholding to dynamically adjust based on score distribu-\ntion characteristics. Incorporating human-in-the-loop evalua-\ntion will provide qualitative insights and a ground-truth basis\nfor refining automated decisions. Re-ranking techniques will\nbe explored to help improve the precision and contextual\nrelevance of the retrieved papers.\nBeyond thresholding and ranking improvements, domain\nadaptation and model fine-tuning (e.g., on specialized corpora)\nwill be investigated to better capture field-specific seman-\ntics. Integration with citation network data sources such as\nSemantic Scholar [23] can enable network-based relevance\nscoring (e.g., PageRank-style importance measures, co-citation\npatterns), which may enhance both retrieval accuracy and lit-\nerature synthesis. Finally, optimizing query formulation tech-\nniques will be considered to increase recall while maintaining\nhigh relevance.\nThis work has shown the potential to reduce manual over-\nhead in writing literature reviews, while maintaining relevance\non topics, proving it is an scalable and accessible alternative\nto traditional SLR systems.\nREFERENCES\n[1] T. Wittenborg, O. Karras, and S. Auer, “SWARM-\nSLR - Streamlined Workflow Automation for Machine-\nactionable Systematic Literature Reviews,” in Proc. Int.\nConf. Theory Pract. Digit. Libr., 2024. [Online]. Avail-\nable: https://arxiv.org/abs/2407.18657\n[2] Cornell University, “arXiv API.” [Online]. Available:\nhttps://arxiv.org/help/api. [Accessed: Jul. 17, 2025]\n[3] X. Tang, X. Duan, and Z. G. Cai, “Large Language\nModels for Automated Literature Review: An Evalu-\nation of Reference Generation, Abstract Writing, and\nReview Composition,” in Proc. Int. Conf. Theory Pract.\nDigit. Libr., 2024. [Online]. Available: https://arxiv.org/\nabs/2412.13612\n[4] Y. Li, L. Chen, A. Liu, K. Yu, and L. Wen, “ChatCite:\nLLM Agent with Human Workflow Guidance for Com-\nparative Literature Summary,” in Proc. Int. Conf. Com-\nput. Linguistics, 2024. [Online]. Available: https://arxiv.\norg/abs/2403.02574\n[5] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence\nEmbeddings using Siamese BERT-Networks,” in Proc.\nConf. Empirical Methods Nat. Lang. Process., 2019.\n[Online]. Available: https://arxiv.org/abs/1908.10084\n[6] T. Kasanishi, M. Isonuma, J. Mori, and I. Sakata,\n“SciReviewGen: A Large-scale Dataset for Automatic\nLiterature Review Generation,” in Proc. Annu. Meeting\nAssoc. Comput. Linguistics, 2023. [Online]. Available:\nhttps://arxiv.org/abs/2305.15186\n[7] G. Izacard and E. Grave, “Leveraging Passage Retrieval\nwith Generative Models for Open Domain Question An-\nswering,” arXiv preprint arXiv:2007.01282, 2021. [On-\nline]. Available: https://arxiv.org/abs/2007.01282\n[8] S. Wang, H. Scells, B. Koopman, and G. Zuccon, “Can\nChatGPT Write a Good Boolean Query for Systematic\nReview Literature Search?,” in Proc. 46th Int. ACM\nSIGIR Conf. Res. Develop. Inf. Retrieval, 2023. [Online].\nAvailable: https://arxiv.org/pdf/2302.03495\n[9] Google,\n“Gemini\n2.0\nFlash\nLarge\nLanguage\nModel,”\n2025.\n[Online].\nAvailable:\nhttps:\n//ai.google.dev/gemini-api/docs/models\n[10] B. Kang and Y. Shin, “Empirical Study of Zero-shot\nKeyphrase Extraction with Large Language Models,” in\nProc. 31st Int. Conf. Comput. Linguistics, Abu Dhabi,\nUAE, Jan. 2025, pp. 3670–3686. [Online]. Available:\nhttps://aclanthology.org/2025.coling-main.248/\n[11] N.\nReimers\nand\nI.\nGurevych,\n“all-MiniLM-L6-v2\nSentence Transformer,” HuggingFace, 2021. [Online].\nAvailable: https://huggingface.co/sentence-transformers/\nall-MiniLM-L6-v2. [Accessed: Aug. 19, 2025]\n[12] J. W. Tukey, Exploratory Data Analysis. Reading, MA,\nUSA: Addison-Wesley, 1977.\n[13] I. Ben-Gal, “Outlier Detection,” in Data Mining and\nKnowledge Discovery Handbook, O. Maimon and L.\nRokach, Eds. Boston, MA: Springer, 2005, pp. 131–146.\n[14] X. Zeng, H. He, H. Shi, and T. Li, “Towards Multiple\nReferences Era – Addressing Data Leakage and Limited\nReference Diversity in Machine Translation Evaluation,”\nin Findings Assoc. Comput. Linguistics: ACL 2024, pp.\n259–272, 2024.\n[15] A. Singh, M. D’Arcy, A. Cohan, D. Downey, and\nS.\nFeldman,\n“SciRepEval:\nA\nMulti-Format\nBench-\nmark\nfor\nScientific\nDocument\nRepresentations,”\nin\nProc. Conf. Empirical Methods Nat. Lang. Process.,\n2022. [Online]. Available: https://api.semanticscholar.\norg/CorpusID:254018137\n[16] J. X. McKie et al., “PyMuPDF: Python bindings for\nMuPDF, a high-performance PDF and document pro-\ncessing library,” Artifex Software, Version 1.26.3, 2025.\n[Online]. Available: https://pypi.org/project/PyMuPDF/\n[17] S. Teufel, A. Siddharthan, and D. Tidhar, “Automatic\nclassification of citation function,” in Proc. Conf. Em-\npirical Methods Nat. Lang. Process., Sydney, Australia,\n2006, pp. 103–110.\n[18] D. Jurgens, T. Finethy, J. McCorriston, Y. Xu, and D.\nRuths, “Citation Intent Classification Using Deep Neural\nNetworks,” in Proc. 56th Annu. Meeting Assoc. Comput.\nLinguistics (ACL), pp. 3724–3733, 2018.\n[19] A. Cohan, N. Goharian, and E. Grave, “Structural Scaf-\nfolds for Citation Intent Classification in Scientific Pub-\nlications,” in Proc. 57th Annu. Meeting Assoc. Comput.\nLinguistics (ACL), pp. 3809–3819, 2019.\n[20] Y. Gu et al., “Domain-Specific Language Model Pretrain-\ning for Biomedical Natural Language Processing,” ACM\nTrans. Comput. Healthcare, vol. 3, no. 1, pp. 1–23, Oct.\n2021. doi: 10.1145/3458754\n[21] I. Beltagy, K. Lo, and A. Cohan, “SciBERT: A Pre-\ntrained Language Model for Scientific Text,” in Proc.\nConf. Empirical Methods Nat. Lang. Process. (EMNLP-\nIJCNLP), Hong Kong, China, Nov. 2019, pp. 3615–3620.\ndoi: 10.18653/v1/D19-1371\n[22] A. Cohan, S. Feldman, I. Beltagy, D. Downey, and\nD. Weld, “SPECTER: Document-level Representation\nLearning\nusing\nCitation-informed\nTransformers,”\nin\nProc. 58th Annu. Meeting Assoc. Comput. Linguistics,\nJul. 2020, pp. 2270–2282. doi: 10.18653/v1/2020.acl-\nmain.207\n[23] Allen Institute for AI, “Semantic Scholar.” [Online].\nAvailable: https://www.semanticscholar.org/. [Accessed:\nAug. 11, 2025]\n",
    "content": "```markdown\n# Paper Interpretation: An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature Review\n\n## 1. Core Content and Key Contributions\n\nThis paper introduces **AutoLit**, an automated literature review pipeline that leverages **semantic similarity** to accelerate the preliminary research survey process for scientists. The system only requires a user to provide a research paper's title and abstract, then automatically completes the entire workflow—from keyword generation and related paper retrieval, semantic filtering, content extraction, summarization, citation intent labeling, contribution classification, to final review report generation.\n\n### Key Contributions:\n\n- **End-to-end automation design**: A complete, low-human-intervention literature review pipeline covering keyword extraction, paper acquisition, semantic filtering, text extraction, summary generation, citation intent annotation, contribution categorization, and final review composition.\n- **Efficient semantic similarity-based filtering mechanism**: Uses transformer models (e.g., all-MiniLM-L6-v2) to generate sentence embeddings, combined with cosine similarity and statistical thresholding (IQR method) to intelligently filter large volumes of candidate papers, improving relevance.\n- **Comparative analysis of multiple models**: Systematically evaluates three embedding models (TF-IDF, all-MiniLM-L6-v2, and Specter2) in scientific paper matching tasks, revealing their respective strengths and limitations.\n- **Structured information extraction and classification**: Employs large language models (LLMs) to generate structured summaries and annotate each paper with \"citation intent\" and \"academic contribution type\" tags, enhancing interpretability and practical utility.\n- **Integration of open resources**: Fully relies on open platforms (e.g., arXiv API), ensuring accessibility and scalability suitable for broad research applications.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### (1) Minimal Input, High-Relevance Output Design Philosophy\nUnlike traditional systematic literature reviews (SLRs) requiring complex configurations or multi-stage workflows like Swarm-SLR, AutoLit emphasizes \"minimal input, maximum relevance\"—only needing a title and abstract to initiate the full process—significantly lowering the barrier to entry, especially beneficial during early exploratory research phases.\n\n### (2) Dynamic Statistical Threshold Filtering Strategy\nInnovatively applies the **IQR (Interquartile Range) method** to set semantic similarity thresholds (Q3 + 0.5×IQR), avoiding biases from fixed thresholds. This distribution-free approach adaptively adjusts to varying similarity distributions across different queries, enhancing robustness and generalization.\n\n### (3) Empirical Study on Embedding Model Selection and Trade-offs\nExperimental findings reveal:\n- **TF-IDF**: Precise but lacks semantic understanding;\n- **Specter2**: Excels in scientific texts but suffers from \"similarity saturation\" (most scores cluster at high values, making distinctions difficult);\n- **all-MiniLM-L6-v2**: Achieves a strong balance between broad semantic coverage and precision, making it the preferred choice.\n\nThese insights provide valuable guidance for model selection in future similar systems.\n\n### (4) LLM-Powered Structured Knowledge Organization\nGoes beyond simple summarization by using LLMs not only for keyword extraction and summary generation, but also for **citation intent recognition** (e.g., background, comparison, extension) and **contribution type classification** (e.g., algorithm, framework, dataset), enabling higher-level information structuring.\n\n### (5) Closed-loop, Full-Process Design\nForms a complete cycle—from retrieval → filtering → extraction → summarization → classification → review generation—with real-world deployment potential, rather than optimizing just one isolated step.\n\n---\n\n## 3. Viable Startup Ideas (Inspired by the Paper)\n\n### Startup Idea 1: **\"LitPilot\" — AI-Powered Research Navigator SaaS Platform**\n\n#### Product Positioning:\nA \"one-click literature review generator\" targeting graduate students, early-career researchers, and R&D teams, helping users quickly grasp the state-of-the-art in any research field.\n\n#### Core Features:\n- Input title/abstract → auto-generate structured literature reports (including highly relevant paper lists, thematic evolution maps, key technology trends)\n- Export options: BibTeX, Word/PDF reports, visual charts\n- \"Research Gap Suggestions\" module: LLM analyzes existing conclusions and recommends potential research directions\n\n#### Business Model:\n- Free tier: Up to 3 queries/month, basic features\n- Pro subscription ($9.9/month): Unlimited queries, advanced exports, team collaboration, API access\n- Institutional license: Bulk licensing for university libraries/labs, integration into internal knowledge management systems\n\n#### Competitive Advantages:\n- More focused on \"automated writing\" vs. manual curation tools like Zotero or Connected Papers\n- More specialized, reliable, and reproducible than generic tools like ChatGPT, tailored specifically for academic use\n\n---\n\n### Startup Idea 2: **\"CiteMind\" — Academic Impact Intelligence Engine**\n\n#### Product Positioning:\nA \"deep paper impact analyzer\" serving journal editorial boards, grant review agencies, and university HR departments.\n\n#### Key Features:\n- Automatically analyze a submission’s reference network to identify its true contribution type (innovative or merely applied?)\n- Generate a \"citation intent map\": Visualize how a target paper is cited by others (support, critique, extension, etc.)\n- Produce a \"research contribution heatmap\" to assist in evaluating a candidate’s academic influence\n\n#### Use Cases:\n- Automated pre-screening for research evaluation\n- Preliminary review of grant proposals\n- Supporting detection of academic misconduct (e.g., fake citations, excessive self-citation)\n\n#### Revenue Model:\nB2B custom services + API usage-based billing\n\n---\n\n### Startup Idea 3: **\"ResearchStart\" — AI-Powered Research Proposal Assistant Platform**\n\n#### Product Positioning:\nAn \"intelligent thesis starter\" designed to help novice researchers (undergraduates, master’s students) quickly define viable research topics.\n\n#### Core Workflow:\n1. User inputs interest keywords or a brief description\n2. System retrieves recent, highly relevant papers\n3. Applies the AutoLit pipeline to generate a \"state-of-the-art review\" for that topic\n4. LLM analyzes gaps and suggests: \"unsolved problems,\" \"improvable methods,\" \"transferable techniques\"\n5. Outputs 3–5 concrete research topics + preliminary literature support package\n\n#### Market Potential:\n- Huge education market (millions of graduate students globally)\n- Partnerships with MOOCs and research bootcamps\n- Supports Chinese localization for domestic university needs\n\n#### Extended Features:\n- Integration with historical NSFC (National Natural Science Foundation of China) funded project database to offer \"successful proposal templates\"\n- \"Advisor Matching\" feature recommending professors with aligned research interests\n\n---\n\n## Summary\n\nThe AutoLit pipeline proposed in this paper is not only a technically sound innovation but also a significant step toward automating scientific research. Centered on **semantic similarity** and powered by modern NLP techniques, it delivers an efficient, scalable, and cost-effective solution for literature review. Its conceptual framework holds substantial commercial potential, particularly for developing intelligent writing and decision-support tools for researchers. Future enhancements could include domain-specific fine-tuning, human feedback reinforcement learning (RLHF), and graph neural networks (for modeling citation networks), enabling continuous iteration and improvement.\n```",
    "github": "",
    "hf": ""
}