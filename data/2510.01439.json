{
    "id": "2510.01439",
    "title": "Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons",
    "summary": "This paper systematically reviews the development历程, current status, and future directions of edge artificial intelligence through a multidimensional classification system, and explores related challenges and emerging opportunities.",
    "abstract": "Edge Artificial Intelligence (Edge AI) embeds intelligence directly into devices at the network edge, enabling real-time processing with improved privacy and reduced latency by processing data close to its source. This review systematically examines the evolution, current landscape, and future directions of Edge AI through a multi-dimensional taxonomy including deployment location, processing capabilities such as TinyML and federated learning, application domains, and hardware types. Following PRISMA guidelines, the analysis traces the field from early content delivery networks and fog computing to modern on-device intelligence. Core enabling technologies such as specialized hardware accelerators, optimized software, and communication protocols are explored. Challenges including resource limitations, security, model management, power consumption, and connectivity are critically assessed. Emerging opportunities in neuromorphic hardware, continual learning algorithms, edge-cloud collaboration, and trustworthiness integration are highlighted, providing a comprehensive framework for researchers and practitioners.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Mohamad Abou Ali,Fadi Dornaika",
    "subjects": [
        "Machine Learning (cs.LG)"
    ],
    "comments": "",
    "keypoint": "- Edge AI embeds intelligence directly into devices at the network edge for real-time processing, improved privacy, and reduced latency.\n- The evolution of Edge AI traces back to content delivery networks (CDNs), fog computing, and mobile edge computing (MEC) as foundational technologies.\n- A multi-dimensional taxonomy is introduced, covering deployment location, processing capability, application domain, and hardware type.\n- Core enabling technologies include specialized hardware accelerators (ASICs, FPGAs, GPUs, neuromorphic chips), optimized software frameworks (TensorFlow Lite, PyTorch Mobile, OpenVINO), and communication protocols (MQTT, CoAP, 5G).\n- TinyML enables ultra-low-power AI on microcontrollers with KBs of memory and µW–mW power budgets.\n- TinyDL extends deep learning capabilities to embedded systems like Raspberry Pi or Jetson platforms with MBs of RAM and mW–W power.\n- TinyRL brings reinforcement learning to edge devices, supporting adaptive decision-making in robotics and industrial control under extreme constraints.\n- Federated Learning allows collaborative model training across distributed edge devices without sharing raw data, enhancing privacy in domains like healthcare and finance.\n- Application domains include healthcare (remote monitoring, diagnostics), industrial IoT (predictive maintenance, quality control), autonomous vehicles (real-time object detection, navigation), smart cities (traffic management, public safety), and retail (inventory tracking, customer analytics).\n- Key challenges include resource constraints (compute, memory, energy), security vulnerabilities due to distributed attack surfaces, model management complexity, power consumption limitations, and unreliable connectivity.\n- Hardware-software co-design is essential to balance performance, efficiency, and adaptability across diverse edge environments.\n- Interoperability and standardization remain major hurdles due to fragmented ecosystems and vendor-specific solutions.\n- Future directions include neuromorphic computing, in-memory computing, analog and optical AI accelerators, and self-powered intelligent systems.\n- Advanced algorithms will focus on continual learning, few-shot learning, meta-learning, and explainable AI (XAI) tailored for edge constraints.\n- Edge-cloud collaboration will evolve into a cognitive continuum with dynamic workload offloading and hierarchical intelligence distribution.\n- Trustworthiness in Edge AI will require robustness against adversarial attacks, bias detection/mitigation in decentralized data, and on-device explainability.\n- 6G networks will act as intelligent connective tissue, enabling sub-millisecond latency, integrated sensing, and AI-native radio access networks.\n- The future vision is a \"Cognitive Edge\" — a self-adapting, trustworthy fabric of distributed intelligence spanning device-to-cloud.",
    "date": "2025-10-04",
    "paper": "Edge Artificial Intelligence: A Systematic\nReview of Evolution, Taxonomic Frameworks,\nand Future Horizons\nMohamad Abou Ali1, 3, 4 and Fadi Dornaika*1, 2\n1University of the Basque Country, 2IKERBASQUE, 3Lebanese\nInternational University (LIU), 4The International University of Beirut,\nmohamad.abouali01@liu.edu.lb, fadi.dornaika@ehu.eus\nAbstract\nEdge Artificial Intelligence (Edge AI) embeds intelligence directly into devices\nat the network edge, enabling real-time processing with improved privacy and re-\nduced latency by processing data close to its source. This review systematically ex-\namines the evolution, current landscape, and future directions of Edge AI through\na multi-dimensional taxonomy including deployment location, processing capabil-\nities such as TinyML and federated learning, application domains, and hardware\ntypes. Following PRISMA guidelines, the analysis traces the field from early con-\ntent delivery networks and fog computing to modern on-device intelligence. Core\nenabling technologies such as specialized hardware accelerators, optimized soft-\nware, and communication protocols are explored. Challenges including resource\nlimitations, security, model management, power consumption, and connectivity\nare critically assessed. Emerging opportunities in neuromorphic hardware, contin-\nual learning algorithms, edge-cloud collaboration, and trustworthiness integration\nare highlighted, providing a comprehensive framework for researchers and practi-\ntioners.\nKeywords— Edge Artificial Intelligence, Systematic Review, Tiny Machine Learning (TinyML),\nTiny Deep Learning (TinyDL), Tiny Reinforcement Learning (TinyRL), Federated Learning,\nMulti-Access Edge Computing (MEC), Hardware Accelerators, AI Privacy and Security\n1\nIntroduction\n1.1\nThe Imperative for Edge AI: A Paradigm Shift\nThe convergence of massive-scale Internet of Things (IoT) deployment and the critical need for\nreal-time, intelligent decision-making has necessitated a fundamental evolution beyond tradi-\ntional cloud-centric computing architectures. This transition is driven by the impracticalities of\n*Corresponding author\n1\narXiv:2510.01439v1  [cs.LG]  1 Oct 2025\ncloud-dependent models—namely latency, bandwidth, privacy, and operational resilience—in\napplications ranging from autonomous vehicles to personalized healthcare. Edge Artificial In-\ntelligence (Edge AI) emerges as the foundational response to these challenges, representing a\nparadigm that embeds computational intelligence directly into devices at the network periphery\n[1, 2]. By processing data locally, at or near its source, Edge AI enables unprecedented respon-\nsiveness, privacy preservation, and operational efficiency [3, 4].\n1.2\nMethodological Foundation and Analytical Framework\nThis review adopts a systematic methodology guided by PRISMA 2020 guidelines [5] to ensure\na comprehensive, unbiased, and reproducible analysis of the Edge AI landscape. From an ini-\ntial corpus of over 2,200 identified records, our rigorous screening process yielded 79 primary\nstudies for in-depth qualitative synthesis, forming the analytical core of this review.\nThe cornerstone of our analysis is a novel multi-dimensional taxonomy (Figure 1) that pro-\nvides an integrated framework for classifying and understanding Edge AI research [6]. This\ntaxonomy synthesizes four critical dimensions: deployment location (D1), which spans from\nDevice Edge and Network Edge to Regional Edge/Multi-Access Edge Computing (MEC)\nand Cloud Edge [7, 8]; processing capability (D2), encompassing TinyML, TinyDL [9, 10],\nTinyRL [11], and federated learning [12, 13] paradigms; application domain (D3), includ-\ning healthcare [14], industrial IoT [15], autonomous systems [16], and smart cities [17];\nand hardware architecture (D4), which covers CPUs, ASICs, FPGAs, GPUs, and neuromor-\nphic chips [18, 19, 20, 21, 22]. This integrated framework enables the systematic identification\nof research gaps, technological trade-offs, and future opportunities across the entire Edge AI\necosystem.\n1.3\nResearch Gaps and Contributions\nOur systematic analysis reveals significant limitations within the current Edge AI literature,\nwhich this review addresses through its novel methodological approach. First, a notable his-\ntorical fragmentation exists, as prior surveys [23, 24] lack comprehensive historical contextu-\nalization by failing to connect modern Edge AI developments to their technological origins in\ncontent delivery networks (CDNs) and fog computing. Second, current works exhibit isolated\ntechnological analysis; studies such as [25, 26] examine hardware, software, and application lay-\ners in isolation, thereby neglecting their critical interdependencies. Third, there is an incomplete\nchallenge assessment across the literature, where existing reviews [27, 28] provide only partial\ncoverage of the Edge AI challenge landscape by emphasizing either optimization or security in\nisolation. A systematic comparison of key Edge AI surveys in Table 1 further elucidates these\nresearch gaps.\n1.4\nOur Contributions\nThis review makes three significant contributions that advance the field of Edge AI.\nFirst, it provides a novel historical synthesis by tracing the complete evolutionary trajectory\nfrom early distributed systems, such as content delivery networks (CDNs), to modern Edge AI\nparadigms, thereby establishing a critical historical continuity absent from previous surveys.\nSecond, it introduces a unified framework through its multi-dimensional taxonomy, which\nenables an integrated analysis across hardware, software, and application domains to reveal their\nessential interdependencies and inherent trade-offs.\nThird, it offers a comprehensive challenge analysis that delivers complete coverage of the\nlandscape, including technical constraints, deployment challenges, and fundamental performance\n2\nFigure 1: Multi-dimensional analytical framework for Edge AI systems, integrating\ndeployment locations, processing capabilities, application domains, and hardware ar-\nchitectures.\n3\nTable 1: Comparison of Key Edge AI Surveys\nRef.\nFocus\nStrengths\nLimitations\n[23]\nArchitectures\n• Broad coverage\n• HW/SW analysis\n• Shallow AI depth\n• No benchmarks\n[29]\nAlgorithms\n• Technical depth\n• Algorithm comparison\n• Dense presentation\n• Lacks tools\n[24]\nLightweight AI\n• Historical context\n• Application diversity\n• Edge computing bias\n• Weak metrics\n[27]\nOptimization\n• Multi-layer taxonomy\n• Privacy focus\n• Conceptual\n• No benchmarks\n[30]\nEdge + LML\n• Future insights\n• Trade-off analysis\n• No validation\n• Vague tools\n[6]\nTechnologies\n• Case studies\n• Real-time focus\n• Too brief\n• Weak analysis\n[31]\nChallenges\n• Interdisciplinary\n• Agenda-setting\n• Abstract\n• No tools\n[25]\nTaxonomy\n• Robust methodology\n• Collaboration focus\n• Surface-level\n• No toolchain\n[26]\nOptimization\n• Model compression\n• HW-aware\n• Dense taxonomy\n• Narrow scope\n[32]\nOn-Device AI\n• Acceleration focus\n• Foundation models\n• Scalability gaps\n• Technical overload\n[28]\nTrustworthy AI\n• Trust framework\n• XAI integration\n• Few examples\n• Tool gaps\n4\ntrade-offs across the entire Edge AI stack. Together, these contributions provide a foundational\nand holistic perspective for future research.\n1.5\nPaper Organization\nThe remainder of this paper is structured as follows: Section 2 delineates the PRISMA-guided\nsystematic methodology and multi-dimensional analytical framework. Section 3traces the his-\ntorical evolution of Edge AI from centralized cloud to distributed intelligence. Section 4 presents\na taxonomic analysis of the contemporary Edge AI ecosystem. Section 5 examines the systemic\nchallenges and fundamental trade-offs. Section 6 projects future research horizons and emerging\nparadigms. Finally, Section 7 concludes the review by synthesizing key insights and implica-\ntions.\n2\nResearch Methodology: A Systematic Multi-Dimensional\nReview\nThis review employs a Systematic Literature Review (SLR) methodology, conducted in strict ad-\nherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\n2020 guidelines [5].\nThis rigorous approach ensures a transparent, reproducible, and unbi-\nased synthesis of the extant literature on Edge Artificial Intelligence (Edge AI). The process\nencompassed the formulation of research questions, a comprehensive search strategy, a multi-\nstage study selection process, systematic data extraction, and analysis through a novel analytical\nframework.\n2.1\nResearch Questions\nThe review is guided by the following primary Research Questions (RQs), designed to compre-\nhensively map the past, present, and future of Edge AI:\n1. RQ1: What are the historical milestones and foundational technologies that have shaped\nthe evolution of Edge AI?\n2. RQ2: What constitutes the current state-of-the-art, including core technologies, architec-\ntural paradigms, and prevalent types of Edge AI (e.g., TinyML, TinyDL)?\n3. RQ3: What are the significant application domains and their respective impacts?\n4. RQ4: What are the predominant challenges and limitations inherent in Edge AI systems?\n5. RQ5: What are the emerging opportunities and promising future research directions?\nThese questions provide a structured lens through which the vast body of literature is ana-\nlyzed and synthesized.\n2.2\nSearch Strategy\nA systematic and multi-faceted search strategy was deployed to maximize the retrieval of rele-\nvant, high-quality academic literature.\n5\n2.2.1\nKeywords and Search Strings\nA comprehensive set of keywords was derived from the research questions and pilot searches\nto cover the breadth of the domain. The terms included: \"Edge AI\", \"Edge Artificial Intelli-\ngence\", \"Edge Computing\", \"TinyML\", \"Tiny Deep Learning\", \"Federated Learning at Edge\",\n\"On-Device AI\", and \"Edge Intelligence\", among others.\nThese keywords were combined using Boolean operators (AND, OR) to construct complex\nsearch queries tailored to each database’s syntax. For example:\n(\"Edge AI\" OR \"Edge Intelligence\") AND (\"survey\" OR \"review\")\n2.2.2\nData Sources and Search Period\nThe search was executed across eleven leading academic databases and publishers renowned\nfor their coverage of computer science and engineering: IEEE Xplore, ACM Digital Library,\nSpringerLink, ScienceDirect (Elsevier), arXiv, Wiley Online Library, MDPI, Taylor & Francis\nOnline, Hindawi, Nature, and Science. An additional 36 records were identified through citation\nsearching of relevant articles.\nThe initial search yielded 2,220 records. After removing 520 duplicates, a total of 1,700\nrecords were screened by title and abstract. The search period was demarcated from January 1,\n2000, to June 30, 2025. This timeframe was selected to capture the foundational work in edge\ncomputing and distributed systems, the emergence of key enabling technologies, and the most\nrecent advancements in Edge AI, ensuring a complete historical contextualization.\n2.3\nStudy Selection and Eligibility Criteria\nThe study selection process followed the PRISMA 2020 protocol, as detailed in the flow diagram\n(Figure 2).\nThe initial pool of 1,700 records was rigorously screened against formal inclusion and ex-\nclusion criteria (Table 2) to ensure both relevance and academic rigor. The title and abstract\nscreening phase resulted in the exclusion of 1,490 records. The full text of the remaining 210\nreports was sought for retrieval, of which 10 were not accessible. Consequently, 200 reports\nwere thoroughly assessed for eligibility.\nOf these, 121 reports were excluded for specific reasons: 76 were off-topic or lacked a\nprimary focus on Edge AI, 25 presented no novel technical contribution, 15 were not peer-\nreviewed, and 5 were published in a language other than English.\nThis meticulous process yielded a final corpus of 79 primary studies deemed suitable for\nqualitative synthesis.\n2.4\nData Extraction and Synthesis\nData from the 79 included studies were extracted into a standardized template. The extracted\nfields included: bibliographic information (authors, title, year, source), key contributions and\nfindings, identified challenges, and proposed future directions.\n2.5\nAnalytical Framework: A Multi-Dimensional Taxonomy\nTo enable a structured and insightful synthesis that moves beyond a descriptive summary, the\nextracted data was analyzed through a novel multi-dimensional analytical framework, as visu-\nalized in Figure 1. This framework categorizes each contribution across four interdependent\ndimensions.\n6\nFigure 2: PRISMA 2020 flow diagram of the systematic literature identification,\nscreening, and inclusion process.\n7\nTable 2: Study Inclusion and Exclusion Criteria\nCriterion\nDescription\nInclusion\nPublication Type\nPeer-reviewed journal articles, conference proceedings, and\ncomprehensive survey papers.\nLanguage\nEnglish.\nTopic\nPrimary focus on Edge AI technologies, architectures, applica-\ntions, challenges, or futures.\nTime Frame\nJanuary 2000 – June 2025.\nExclusion\nPublication Type\nShort papers (<4 pages), posters, abstracts, editorials, books,\ntheses, and non-academic sources (e.g., blogs, whitepapers).\nTopic\nFocus solely on cloud computing or traditional data centers\nwithout an explicit edge component.\nAccessibility\nFull text not retrievable.\nThe first dimension, deployment location (D1), concerns the physical or logical placement\nof intelligence, which ranges from the Device Edge — encompassing microcontrollers and sen-\nsors—to the Network Edge, including gateways and MEC servers, and finally the Cloud Edge.\nThe second dimension, processing capability (D2), identifies the computational paradigm\nemployed, spanning from ultra-constrained TinyML to more capable TinyDL and collaborative\nFederated Learning.\nThe third dimension, application domain (D3), classifies the sector or use-case addressed,\nsuch as Healthcare, Industrial IoT, Autonomous Vehicles, or Smart Cities. Finally, the fourth\ndimension, hardware type (D4), specifies the underlying processing substrate, which includes\nASICs, FPGAs, GPUs, and Neuromorphic Chips.\nThis integrated framework facilitates a nuanced analysis of trade-offs, synergies, and re-\nsearch gaps across the entire ecosystem. It thereby allows for the identification of under-explored\nintersections, such as federated learning algorithms optimized for neuromorphic hardware. Con-\nsequently, the synthesis presented in subsequent sections is structured to critically examine the\nliterature through this cohesive and original lens.\n3\nHistorical Evolution: From Cloud to the Intelligent\nEdge\n3.1\nFrom Cloud to Edge: A Paradigm Shift\nThe evolution of computing paradigms represents a continuous quest for optimal efficiency, re-\nsponsiveness, and scalability, progressively distributing resources closer to the point of demand.\nThis trajectory began with centralized mainframes, evolved through client-server architectures,\nand culminated in the cloud computing era, which revolutionized data processing through on-\ndemand access to immense, shared computational resources [33]. However, the explosive prolif-\neration of connected devices and the emergence of applications requiring real-time processing of\nmassive data volumes at the network periphery exposed critical limitations of the cloud-centric\n8\nmodel. Intrinsic bottlenecks related to latency, bandwidth consumption, and data privacy became\nfundamentally incompatible with the requirements of autonomous systems, real-time analytics,\nand privacy-sensitive applications [34, 35].\nThis analysis, guided by our methodological framework, reveals that the shift to edge-based\nprocessing was not a single event but a series of innovations, each addressing specific dimensions\nof the cloud limitation problem. As illustrated in Figure 3, this progression established the foun-\ndational layers that constitute the modern Edge AI landscape, directly informing the deployment\nlocation (D1) and processing capability (D2) dimensions of our taxonomy.\nFigure 3: Evolution from centralized cloud computing to distributed Edge AI, mapping\nkey technological milestones to the dimensions of our analytical framework.\nEdge AI, therefore, represents the culmination of this evolutionary trajectory—a deliberate\nparadigm shift from reliance on distant cloud data centers to the strategic distribution of intelli-\ngence adjacent to data sources. This architectural decentralization minimizes long-distance data\ntransmission, thereby critically reducing latency, conserving bandwidth, and enhancing the pri-\nvacy of sensitive information through localized processing [36, 37]. The driving force behind this\nshift is the ubiquitous deployment of smart devices and IoT endpoints that generate continuous\ndata streams in environments where immediate, autonomous decision-making is paramount, such\nas in industrial automation, autonomous vehicles, and remote healthcare monitoring [38, 39].\n3.2\nFoundational Technologies: The Pillars of Distributed Intelli-\ngence\nThe emergence of Edge AI is intrinsically linked to and built upon several foundational tech-\nnologies that established the core principles of distributed processing. Our systematic review\nidentifies three pivotal technologies that sequentially paved the way for modern Edge AI, each\ncontributing a critical piece to the architectural puzzle.\n9\n3.2.1\nContent Delivery Networks (CDNs): The Precursor to Edge Locality\nContent Delivery Networks (CDNs) constituted the earliest widespread form of distributed com-\nputing, designed primarily to optimize web content delivery by geographically dispersing cached\ncontent closer to end-users [40]. While their initial focus was content replication rather than\ncomputation, CDNs introduced the seminal concept of leveraging network proximity to enhance\nperformance and reduce congestion. This demonstrated the fundamental benefits of localized re-\nsource allocation and established the core architectural principle of bringing computation closer\nto the consumer, laying the essential conceptual groundwork for more sophisticated edge pro-\ncessing paradigms [41]. CDNs represent the initial instantiation of what would become the\nDeployment Location (D1) dimension in our taxonomy.\n3.2.2\nFog Computing: Bridging the Cloud-Edge Divide\nFog computing emerged as a strategic extension of cloud computing, explicitly designed to\nbridge the conceptual and architectural gap between centralized cloud resources and edge de-\nvices [42]. By extending cloud services to the network edge, fog computing enabled computa-\ntion, storage, and networking capabilities to be performed in closer proximity to data sources.\nFog nodes—often implemented on routers, switches, or dedicated servers—functioned as intel-\nligent intermediaries between edge devices and the cloud, providing crucial localized processing\nand reducing the dependency on continuous, high-bandwidth cloud connectivity [8, 43]. This\narchitecture introduced a hierarchical computing model, a concept central to modern Edge AI,\nwhich explicitly defined different processing tiers at varying distances from the data source. Fog\ncomputing directly informs the hierarchical nature of both the Deployment Location (D1) and\nProcessing Capability (D2) dimensions in our taxonomy.\n3.2.3\nMobile Edge Computing (MEC): The Ultralow-Latency Enabler\nMobile Edge Computing (MEC), standardized as Multi-access Edge Computing, advanced the\ndistribution paradigm by integrating cloud computing capabilities directly within the radio ac-\ncess network (RAN) infrastructure [7]. By positioning computation and storage resources at the\nbase station level, MEC brings unprecedented proximity to mobile users and devices, offering\nultralow latency and high bandwidth essential for advanced applications. This proximity is crit-\nical for latency-sensitive use cases such as augmented reality, virtual reality, and real-time video\nanalytics, where millisecond-scale delays significantly impact user experience and system per-\nformance [44, 45]. MEC platforms enable application deployment at cellular base stations or\naccess points, facilitating immediate data processing and responses, a capability that defines the\nhigh-performance end of the Processing Capability (D2) dimension for mobile scenarios.\n3.3\nThe Rise of On-Device AI: The Ultimate Realization of Edge\nIntelligence\nThe convergence of these foundational technologies, coupled with breakthroughs in hardware\nminiaturization and energy-efficient AI algorithms, has catalyzed the most significant evolution:\nthe rise of On-Device AI. This paradigm refers to the execution of sophisticated AI models di-\nrectly on end-user devices—such as smartphones, wearables, sensors, and microcontrollers—often\nindependent of continuous cloud or fog connectivity [46, 47].\nOn-Device AI represents the ultimate expression of edge intelligence, offering transforma-\ntive advantages in privacy, latency, and operational autonomy. By ensuring data never leaves the\ndevice, it fundamentally mitigates privacy and security risks associated with data transmission.\n10\nThe elimination of network latency ensures genuine real-time inference, and devices maintain\nintelligent functionality even in offline or intermittently connected environments [48, 49]. This\nshift has precipitated remarkable innovation in highly optimized, compact AI models, spurring\nnot only TinyML but also the more capable Tiny Deep Learning (TinyDL) and Tiny Reinforce-\nment Learning (TinyRL) paradigms, alongside the development of specialized hardware accel-\nerators tailored for severely resource-constrained environments.\nThis historical analysis, structured through our methodological framework, demonstrates\nthat the evolution of Edge AI has been a process of continuous refinement across the dimensions\nof deployment, processing, and hardware. The following section will delve into the current\nlandscape of Edge AI, utilizing our taxonomy to provide a structured analysis of the technologies\nand architectures that have emerged from this evolutionary journey.\n4\nA Taxonomic Analysis of the Contemporary Edge AI\nEcosystem\nThe contemporary state of Edge AI is defined by a complex, synergistic ecosystem of special-\nized hardware, optimized software stacks, and efficient communication protocols, all orches-\ntrated to enable intelligent processing under stringent resource constraints. This section employs\nthe multi-dimensional taxonomy introduced in Figure 1 to provide a structured analysis of this\nlandscape. We deconstruct the ecosystem into its core technological pillars and then examine the\ndominant paradigms (TinyML, TinyDL, Federated Learning) that emerge from the interplay of\nthese pillars, concluding with a analysis of their application across critical domains.\n4.1\nThe Core Technology Stack: Hardware, Software, and Com-\nmunication\nThe effective deployment of Edge AI is predicated on the co-design of hardware, software, and\ncommunication layers. This tripartite foundation directly maps to the Hardware Type (D4) and\nProcessing Capability (D2) dimensions of our taxonomy, enabling the diverse Deployment\nLocations (D1) discussed in Section 3.\nThe Edge AI technology stack (Figure 4) is not a collection of isolated components but a\ntightly integrated hierarchy. breakthroughs in hardware accelerators (e.g., Google’s Edge TPU)\ncreate the substrate for efficient inference, which software frameworks (e.g., PyTorch Mobile)\nleverage through advanced optimization techniques, while communication protocols (e.g., 5G)\nenable the orchestration of intelligence across the edge-to-cloud continuum.\n4.1.1\nHardware Accelerators for Edge AI: The Physical Substrate (D4)\nThe computational exigencies of contemporary artificial intelligence models, particularly deep\nneural networks, substantially surpass the capabilities of conventional general-purpose proces-\nsors in resource-constrained edge environments. This discrepancy has precipitated the devel-\nopment of specialized hardware accelerators, each embodying distinct architectural trade-offs\nwithin the Hardware Type (D4) dimension of our taxonomic framework. These co-processors\nare fundamentally engineered to maximize computational efficiency, quantified as trillions of\noperations per second per watt (TOPS/W) [18, 19].\nFour principal architectural paradigms dominate this landscape. Application Specific In-\ntegrated Circuits (ASICs), epitomized by Google’s Edge TPU, represent the zenith of perfor-\nmance and energy efficiency for fixed-function, high-volume inference workloads; however, this\n11\nFigure 4: The Edge AI technology stack, illustrating the synergistic relationship be-\ntween hardware accelerators, software frameworks, and communication protocols that\nenable intelligent processing across the deployment continuum.\noptimization comes at the expense of architectural flexibility due to their hardened circuitry.\nField-Programmable Gate Arrays (FPGAs), such as the Xilinx Versal series, occupy a mid-\ndle ground by offering reconfigurable logic fabrics that balance computational efficiency with\npost-deployment adaptability, making them particularly suitable for evolving algorithmic re-\nquirements and prototyping applications [20]. Graphics Processing Units (GPUs), including\npower-optimized variants like the NVIDIA Jetson platform, leverage massive parallel processing\ncapabilities to accelerate complex computational workloads such as real-time video analytics,\nthough this often necessitates accepting higher power envelopes [21]. Finally, neuromorphic\ncomputing platforms, exemplified by Intel’s Loihi architecture, constitute a paradigm-shifting\napproach that emulates biological neural networks through event-driven, asynchronous process-\ning, thereby offering transformative potential for ultra-low-power operation on sparse, temporal\ndata streams [22].\nThis heterogeneous ecosystem of accelerator architectures demonstrates that no single so-\nlution optimally addresses all edge computing constraints, thereby necessitating careful archi-\ntectural co-design across the hardware-software stack to meet specific application requirements\nwithin the Edge AI domain.\nSelecting the appropriate accelerator (Table 3) is a critical system-level decision dictated\nby the constraints of the Application Domain (D3) and the target Deployment Location (D1).\nASICs deliver unmatched efficiency for static workloads at the network edge, while FPGAs\nprovide crucial adaptability for industrial settings. Neuromorphic chips, though nascent, offer\na disruptive potential for next-generation always-on sensing applications at the extreme device\nedge.\n12\nTable 3: Performance characteristics and trade-offs of dominant Edge AI hardware\nplatforms [18, 20, 22], cataloged by the Hardware Type (D4) dimension.\nType (D4)\nExample\nPros\nCons\nOptimal\nDeployment\n(D1) & Use\nCase\nASICs\nGoogle Edge\nTPU\nHigh\nTOPS/W,\nlow latency\nFixed\nar-\nchitecture,\ncostly design\nNetwork/Cloud\nEdge; Fixed,\nhigh-volume\ninference\nFPGAs\nXilinx Versal\nReconfigurable,\nenergy-\nefficient\nHigh\nde-\nvelopment\ncomplexity\nNetwork\nEdge; Evolv-\ning\nmodels,\nprototyping\nGPUs\nNVIDIA Jet-\nson\nHigh\npar-\nallelism,\nflexible\nPower-\nhungry,\nexpensive\nDevice/Network\nEdge;\nVideo\nanalytics,\ntraining\nNeuromorphic\nIntel Loihi\nEvent-driven,\nultra-low\npower\nNiche\npro-\ngramming\nmodel\nDevice Edge;\nSensor\nfu-\nsion,\nsparse\ndata\n13\n4.1.2\nEdge AI Frameworks and Runtimes: The Software Abstraction Layer\nTo effectively harness the computational capabilities of heterogeneous edge hardware, a sophis-\nticated suite of software frameworks has emerged, specializing in model optimization, quan-\ntization, and efficient inference execution. These frameworks constitute the critical software\nabstraction layer that operationalizes the Processing Capability (D2) dimension for any given\nHardware Type (D4) within our taxonomy [50, 51].\nSeveral prominent frameworks exemplify this technological stratum. TensorFlow Lite and\nPyTorch Mobile provide lightweight runtimes derived from their respective mainstream ma-\nchine learning ecosystems, specifically engineered for deployment on mobile and embedded\ndevices with stringent requirements for low latency and minimal binary footprint. The Open-\nVINO Toolkit represents a vendor-specific optimization suite that enhances deep learning model\nperformance across Intel’s heterogeneous hardware portfolio, including CPUs, GPUs, FPGAs,\nand Vision Processing Units (VPUs), thereby maximizing inference efficiency on dedicated sil-\nicon architectures. In contrast, ONNX Runtime embodies a cross-platform inference paradigm\nthat promotes framework interoperability by enabling models trained within one ecosystem (e.g.,\nPyTorch) to be deployed through hardware-optimized runtimes from alternative toolchains, ef-\nfectively mitigating vendor lock-in concerns.\nCollectively, these frameworks provide crucial abstraction of underlying hardware complex-\nity, enabling developers to concentrate on algorithmic innovation and model design while the\nsoftware stack manages the intricate challenges of executing computational graphs efficiently\nacross diverse accelerator architectures. This architectural separation between hardware capabil-\nities and software implementation fundamentally enables the practical deployment of advanced\nAI models within the constrained environments characteristic of edge computing scenarios.\n4.1.3\nCommunication Protocols for Edge AI: The Connectivity Fabric\nEfficient and reliable data exchange is the connective tissue of distributed Edge AI systems,\nenabling collaborations between devices at different Deployment Locations (D1). The choice\nof protocol is dictated by the constraints of the Application Domain (D3), particularly its latency\nand bandwidth requirements [52, 53].\n• MQTT & CoAP: Lightweight messaging protocols designed for constrained devices and\nunreliable networks, ideal for telemetry data transmission in IoT scenarios.\n• 5G and Beyond: The ultra-reliable low-latency communication (URLLC) and enhanced\nmobile broadband (eMBB) capabilities of 5G are transformative enablers, facilitating\nreal-time collaboration between edge devices and servers for advanced applications like\nautonomous driving and augmented reality [54].\n4.2\nParadigms of Processing: From TinyML to Federated Learn-\ning\nEdge AI is not monolithic but comprises a spectrum of paradigms tailored to specific resource\nconstraints and processing requirements. These paradigms represent different points along the\nProcessing Capability (D2) dimension, as visualized in Figure 5.\nThe strategic selection of a paradigm involves navigating a complex trade-off space, as sys-\ntematically compared in Table 4. This decision is primarily driven by the constraints of the\nHardware Type (D4) and the requirements of the Application Domain (D3).\n14\nFigure 5: The spectrum of Edge AI paradigms, classified by Processing Capability\n(D2), ranging from ultra-constrained TinyML to collaborative Federated Learning.\n15\nTable 4: Comparative Analysis of Edge AI Paradigms along the Processing Capability (D2) dimension.\nFeature\nTinyML\nTinyDL\nTinyRL\nFederated Learning at the Edge\nTarget Hardware (D4)\nMicrocontrollers\nEmbedded SoCs\nEmbedded SoCs\nHeterogeneous Device Fleets\nCompute Resources\nKBs of RAM, µW–mW\nMBs of RAM, mW–W\nMBs of RAM, mW–W\nAggregated resources\nTypical Applications (D3)\nAnomaly detection\nObject detection\nRobotic control,\nPrivacy-preserving analytics\nKey Advantage\nUltra-low power\nEfficient deep learning\nAdaptive decision-making\nPrivacy, leverage distributed data\nKey Challenge\nExtreme constraints\nModel optimization\nSample efficiency,\npolicy\ncompression\nCommunication overhead, heterogene-\nity\n16\n4.2.1\nTinyML: Intelligence at the Extremes (D2)\nTinyML operates at the most constrained end of the Processing Capability (D2) spectrum, de-\nploying highly optimized models onto microcontrollers with kilobytes of memory and microwatt\npower budgets [55, 10]. Its value proposition is enabling always-on, always-sensing capabilities\nfor applications like industrial monitoring and wearable health devices, where data privacy and\npower autonomy are paramount [56, 57]. The core innovation lies in extreme model compres-\nsion and quantization techniques that strip neural networks down to their bare essentials without\nsacrificing critical functionality.\n4.2.2\nTinyDL: Embedded Deep Learning (D2)\nTinyDL occupies the middle ground, enabling more complex deep learning tasks (e.g., image\nclassification, object detection) on embedded systems like Raspberry Pi or NVIDIA Jetson plat-\nforms [9, 58]. These devices possess marginally more resources (MBs of RAM, watt-level\npower), allowing for the execution of deeper networks. The focus shifts from extreme com-\npression to sophisticated optimization techniques like pruning, knowledge distillation, and neu-\nral architecture search to balance accuracy, latency, and power consumption for applications in\nsmart cameras, drones, and robotics [59, 60].\n4.2.3\nTinyRL: Reinforcement Learning at the Edge (D2)\nTiny Reinforcement Learning (TinyRL) represents the cutting edge of on-device learning, en-\nabling autonomous decision-making and control policies directly on resource-constrained hard-\nware [11]. Unlike the supervised learning focus of TinyML and TinyDL, TinyRL algorithms\nlearn optimal behaviors through interaction with their environment, making them ideal for ap-\nplications in robotics, industrial control, and network management where systems must adapt in\nreal-time without cloud dependency [61]. The primary challenge lies in compressing the high\nsample complexity and memory requirements of traditional RL into the extreme constraints of\nthe device edge, often leveraging techniques like policy distillation and efficient experience re-\nplay.\n4.2.4\nFederated Learning: Collaborative Intelligence\nFederated Learning (FL) is a unique paradigm that transcends a single Deployment Location\n(D1). It is a privacy-preserving distributed training method that leverages the collective compu-\ntational power of a heterogeneous fleet of edge devices (D4) [62, 13]. Instead of centralizing raw\ndata, FL trains models locally on each device and aggregates only model updates. This makes\nit particularly suited for Application Domains (D3) with stringent privacy concerns, such as\nhealthcare and finance, though it introduces challenges in communication efficiency and han-\ndling device heterogeneity [63, 64].\n4.2.5\nOther Variants: Deployment Location (D1) Specialization\nBeyond the core computational paradigms previously examined, the Edge AI ecosystem exhibits\nspecialized architectural implementations distinguished primarily by their positioning within the\nDeployment Location (D1) dimension. Two particularly significant specializations merit ex-\nplicit consideration.\nFirst, Edge AI on Gateways/Servers (including Regional Edge/MEC nodes) represents an\napproach that deploys more capable models (TinyDL, TinyRL) on robust appliances, enabling\n17\nsophisticated processing like real-time multi-sensor fusion in smart manufacturing and urban\ninfrastructure [65, 66].\nSecond, Edge AI in Mobile Devices constitutes a distinct paradigm that harnesses dedicated\nneural processing units (NPUs) integrated within smartphones to facilitate advanced on-device\napplications including augmented reality interfaces and computational photography enhance-\nments [67, 12].\nThese deployment-specific implementations demonstrate that the physical and logical place-\nment of computational resources serves as a fundamental architectural determinant that directly\ngoverns system capabilities, operational constraints, and performance characteristics across di-\nverse Edge AI applications. The strategic selection of deployment location thus represents a\ncritical design consideration that profoundly influences both the technical feasibility and practi-\ncal efficacy of Edge AI solutions within their intended operational environments.\nTable 5: Deployment Tiers of Edge AI, synthesizing Hardware Type (D4), resource\nconstraints, and typical Application Domains (D3).\nCategory\nDevice\nExam-\nples (D4)\nMemory\nPower\nTypical Use Cases (D3)\nTinyML\nMicro-controllers\nKBs\nµW–mW\nKeyword spotting, vibra-\ntion monitoring\nTinyDL\nEmbedded SoCs\nMBs\nmW–W\nReal-time object detec-\ntion, drones\nEdge Servers\nGateways, MEC\nnodes\nGBs\nW–kW\nSmart\ncity\nanalytics,\nmulti-sensor fusion\nThe stratification of the Edge AI landscape into distinct deployment tiers (Table 5) is a direct\nconsequence of the trade-offs analyzed through our taxonomic framework. The choice of tier\ndictates the feasible paradigms and ultimately the applications that can be successfully deployed.\n4.3\nApplication Domains: Transformative Impact Across Indus-\ntries\nEdge AI is fundamentally transforming industries by enabling intelligent, real-time decision-\nmaking at the data source. Its value proposition—local processing, reduced latency, and en-\nhanced privacy—makes it indispensable across diverse sectors. The following analysis, struc-\ntured by the Application Domain (D3) dimension, highlights the most impactful use cases, with\ntheir benefits summarized in Table 6.\n4.3.1\nSmart Homes and Cities\nIn smart homes, Edge AI enhances privacy and responsiveness. Devices like smart speakers,\nsecurity cameras, and thermostats can process voice commands, detect intruders, or optimize\nenergy consumption locally without sending sensitive data to the cloud [68, 69]. This on-device\nprocessing ensures immediate responses and reduces concerns about data breaches. For instance,\na smart doorbell with Edge AI can identify known visitors or detect suspicious activity in real-\ntime, sending alerts only when necessary [70].\nIn smart cities, Edge AI plays a crucial role in managing urban infrastructure and services.\nApplications include intelligent traffic management systems that optimize signal timings based\n18\nTable 6: Edge AI Applications categorized by Application Domain (D3), highlighting\ndomain-specific benefits.\nDomain (D3)\nApplication Examples\nKey Benefits of Edge AI\nSmart Cities\nTraffic management, public\nsafety\nReal-time processing, low-\nlatency decision-making\nIndustrial IoT\nPredictive\nmaintenance,\nquality control\nOn-device analytics, mini-\nmized downtime\nAutonomous Vehicles\nObject detection,\nnaviga-\ntion\nUltra-low\nlatency,\nen-\nhanced\nsafety,\nindepen-\ndence\nHealthcare\nRemote patient monitoring,\ndiagnostics\nStrong data privacy, real-\ntime insights\nRetail\nInventory\ntracking,\ncus-\ntomer analytics\nReal-time\ninsights,\nim-\nproved user experience\non real-time traffic flow detected by edge cameras [17], smart streetlights that adjust illumination\nbased on pedestrian and vehicle presence [71], and waste management systems that optimize\ncollection routes using AI-powered sensors on bins [72]. These deployments leverage Edge AI\nto improve efficiency, sustainability, and public safety.\n4.3.2\nIndustrial IoT (IIoT)\nEdge AI is a cornerstone of Industry 4.0, enabling predictive maintenance, quality control, and\noperational optimization in manufacturing and industrial settings. By deploying AI models di-\nrectly on factory equipment, sensors can monitor machine health, detect anomalies, and pre-\ndict potential failures before they occur, minimizing downtime and reducing maintenance costs\n[73, 15]. For example, vibration sensors with embedded AI can analyze machine vibrations to\nidentify early signs of wear and tear, triggering alerts for proactive maintenance [74].\nFurthermore, Edge AI facilitates real-time quality inspection on production lines, where\ncameras with embedded AI can identify defects in products with high accuracy and speed, ensur-\ning consistent product quality [75]. This localized processing is critical in environments where\nnetwork latency to the cloud could lead to significant production delays or errors.\n4.3.3\nAutonomous Vehicles\nAutonomous vehicles are perhaps one of the most demanding applications for Edge AI, requiring\nultra-low latency and highly reliable real-time decision-making. Self-driving cars must process\nvast amounts of sensor data (from cameras, LiDAR, radar, etc.) instantaneously to perceive their\nenvironment, predict the behavior of other road users, and make critical navigation decisions [76,\n77]. Cloud-based processing for such tasks is impractical due to the inherent latency, which could\nlead to catastrophic delays. Edge AI enables these vehicles to operate autonomously and safely\nby performing complex AI computations directly on board [16]. This includes object detection,\nlane keeping assistance, pedestrian recognition, and real-time path planning, all executed at the\nedge to ensure immediate responses to dynamic road conditions [78].\n19\n4.3.4\nHealthcare\nIn healthcare, Edge AI offers transformative potential, particularly in remote patient monitoring,\ndiagnostics, and personalized medicine. Wearable health devices equipped with Edge AI can\ncontinuously monitor vital signs, detect anomalies, and alert patients or healthcare providers\nto critical changes, all while preserving patient privacy by processing sensitive data on-device\n[79, 14]. For instance, an Edge AI-powered ECG device can detect arrhythmias in real-time,\nproviding immediate feedback to the user or triggering emergency services [80].\nEdge AI also supports intelligent medical imaging analysis at the point of care, allowing for\nfaster preliminary diagnoses in remote clinics or emergency settings without relying on cloud\nconnectivity [81]. This can significantly reduce the time to diagnosis and improve patient out-\ncomes, especially in underserved areas.\n4.3.5\nRetail\nEdge AI is revolutionizing the retail sector by enhancing customer experience, optimizing store\noperations, and improving inventory management. In smart retail environments, Edge AI-powered\ncameras can analyze customer traffic patterns, optimize product placement, and detect shoplift-\ning in real-time, without transmitting continuous video feeds to the cloud [82, 83]. This not only\nimproves security but also provides valuable insights into customer behavior while maintaining\nprivacy.\nFurthermore, Edge AI can facilitate automated checkout systems, smart shelves that monitor\ninventory levels, and personalized advertising displays that adapt to customer demographics or\npreferences, all processed locally to ensure immediate and relevant interactions [84, 85]. These\napplications demonstrate how Edge AI can create more efficient, secure, and customer-centric\nretail experiences.\n5\nSystemic Challenges and Fundamental Trade-Offs\nDespite its transformative potential, the widespread deployment of Edge AI is contingent upon\novercoming significant and interconnected challenges. These limitations are not merely tech-\nnical hurdles but fundamental trade-offs inherent to the distributed, resource-constrained, and\nheterogeneous nature of edge environments. As illustrated in Figure 6, these five core chal-\nlenges—resource constraints, data privacy, model management, power consumption, and con-\nnectivity—and their interconnections span the entire stack, from hardware and algorithms to\nsecurity and systems management. This section analyzes these barriers through the lens of our\nmulti-dimensional taxonomy, examining how constraints in one dimension (e.g., Hardware Type\n- D4) precipitate challenges in another (e.g., Processing Capability - D2 or Deployment - D1).\n5.1\nResource Constraints: The Fundamental Trade-Off\nThe most fundamental challenge in Edge AI stems from the severe resource constraints intrin-\nsic to devices at the Device Edge (D1), particularly those running TinyML (D2). Furthermore,\nemerging paradigms like TinyRL introduce additional complexity, as their training-inference cy-\ncles and memory requirements for experience replay present novel optimization hurdles beyond\nthose of static inference or even federated learning. These constraints—encompassing compu-\ntational power, memory (RAM and storage), and energy budgets—define the design space and\nforce a continuous trade-off between model accuracy, latency, inference speed, and power con-\nsumption [86, 87].\n20\nFigure 6: Systemic challenges in Edge AI, depicting five core challenges (resource\nconstraints, data privacy & security, model management, power consumption, and con-\nnectivity) and their causal interrelationships (e.g., \"exacerbates\", \"requires\") across the\ntechnology stack.\nWhile techniques like model compression (pruning, quantization), efficient neural architec-\ntures (e.g., MobileNets), and specialized hardware accelerators (D4) are crucial mitigations,\nthey are not panaceas. For instance, aggressive quantization can achieve 3–5× energy savings\nbut often at the cost of non-negligible accuracy loss, especially for complex models [88, 89].\nConsequently, developing robust AI capabilities for devices with kilobyte-scale memory and\nmilliwatt power budgets remains a formidable engineering challenge that dictates the feasible\nApplication Domains (D3) for a given hardware class.\n5.2\nData Privacy and Security: The Expanded Attack Surface\nWhile Edge AI inherently enhances privacy by processing data locally, reducing its exposure\nover networks, it simultaneously introduces a new set of security vulnerabilities by distributing\nthe attack surface. Physically exposed edge devices (D1) are susceptible to tampering, theft, and\nside-channel attacks, threatening the integrity of both the AI models and the sensitive data they\nprocess [90, 91].\nThe distributed nature of Edge AI complicates centralized security management. Ensuring\ntrust across a heterogeneous fleet of devices—from microcontrollers to edge servers—requires\nrobust mechanisms like secure boot, trusted execution environments (TEEs), and homomor-\nphic encryption. Furthermore, paradigms like Federated Learning (D2) are being adopted as a\nprivacy-preserving training method, but they themselves introduce new challenges related to the\nsecurity of aggregated model updates [92, 93]. A comprehensive, standardized security frame-\nwork for these heterogeneous ecosystems is still nascent, representing a critical gap for sensitive\nApplication Domains (D3) like healthcare and industrial control.\n21\n5.3\nModel Management and Deployment: The Operational Bottle-\nneck\nThe lifecycle management of AI models across vast, geographically dispersed edge deployments\npresents a profound operational challenge. The core of this problem is the extreme hardware\nand software heterogeneity (D4) across the edge continuum. Deploying, updating, and main-\ntaining models on millions of devices, each with potentially different capabilities, requires so-\nphisticated orchestration platforms that are both robust and secure [94, 95].\nOver-the-air (OTA) updates must be efficient and fault-tolerant, especially for devices with\nintermittent connectivity. Strategies such as A/B partitioning and rollback capabilities are essen-\ntial to ensure reliability. Furthermore, maintaining model consistency and version control across\nthis diverse fleet, while simultaneously debugging issues in the field, adds significant complexity\nto the DevOps cycle for Edge AI, potentially stalling the deployment of new applications [96].\n5.4\nPower Consumption: The Energy Efficiency Frontier\nEnergy efficiency represents arguably the paramount challenge for battery-operated or energy-\nharvesting devices operating at the Device Edge (D1). The fundamental tension between the\nobjective of continuous, always-on sensing and inference capabilities and the requirement for\nmulti-year operational lifespans necessitates innovative solutions. Although specialized low-\npower accelerators (D4) provide partial mitigation, a comprehensive, system-wide approach to\npower management remains essential, requiring optimization across every system component\nfrom sensors to communication modules [97, 98].\nCurrent research converges on three synergistic strategies to advance the energy-efficiency\nfrontier. First, hardware-level optimization employs quantized models—utilizing 8-bit integers\nrather than floating-point representations—to achieve 3–5× reductions in memory bandwidth and\ncomputational energy consumption. This approach is complemented by dedicated low-power\ncores that handle simple always-on tasks at microwatt power levels, thereby maintaining main\nprocessors in deep sleep states for extended durations [88, 97]. Second, algorithmic efficiency\ntechniques, including pruning and sparsity exploitation, eliminate redundant operations to re-\nduce energy consumption by 30–60%. Event-triggered inference further enhances efficiency by\nradically reducing the duty cycle, processing data exclusively in response to meaningful sen-\nsor events such as detected motion [89, 98]. Third, system-wide power gating implements\ntechniques including dynamic voltage and frequency scaling (DVFS) to adjust computational re-\nsources in real-time, while selective peripheral disabling powers down unused sensors and radios\nbetween inference cycles [99, 100].\nSignificant challenges persist, particularly regarding the minimization of accuracy penal-\nties associated with extreme quantization and the development of standardized power manage-\nment interfaces capable of operating across heterogeneous hardware platforms (D4). The most\npromising trajectory forward appears to lie in hybrid approaches that combine hardware-software\nco-design with adaptive, learning-based algorithms [98, 99].\n5.5\nConnectivity: The Reliability Bottleneck\nThe efficacy of distributed Edge AI systems is fundamentally dependent on reliable and suf-\nficient network connectivity, encompassing both bandwidth and reliability [53]. Unlike cloud\ncomputing, the edge continuum often operates in environments with unstable or low-bandwidth\nconnections, such as rural areas, moving vehicles, or dense industrial settings. This variability\ndirectly complicates model management (e.g., failed OTA updates) and constrains the feasibil-\nity of collaborative paradigms like Federated Learning, which require frequent model update\n22\nexchanges. Furthermore, strategies to overcome poor connectivity, such as data buffering or\nmore complex compression algorithms, can exacerbate power consumption challenges, creating\na critical trade-off between operational reliability and energy efficiency.\n5.6\nInteroperability and Standardization: The Integration Chal-\nlenge\nThe fragmented nature of the Edge AI ecosystem presents a critical barrier to widespread adop-\ntion. The proliferation of proprietary hardware accelerators (D4), diverse software frameworks,\nand incompatible communication protocols frequently results in vendor lock-in, increased devel-\nopment complexity, and substantial challenges in integrating disparate components into cohesive,\nscalable systems [101, 102].\nThis absence of common standards significantly impedes innovation and elevates develop-\nment costs. While industry consortia and open-source initiatives are actively working to establish\ncommon APIs—such as the Open Neural Network Exchange (ONNX)—standardized data for-\nmats, and consistent deployment methodologies, the remarkable diversity of the edge landscape\nrenders widespread adoption a long-term objective [103, 104]. Achieving genuine interoper-\nability remains crucial for unlocking the full potential of Edge AI technologies and enabling\nseamless collaboration between devices operating across different Deployment Locations (D1),\nas required by many advanced Applications (D3).\n5.7\nSynthesis of Challenges\nThe challenges facing Edge AI are not isolated but are deeply interconnected. The drive for\ngreater energy efficiency (Power) can exacerbate security vulnerabilities. Unreliable Connec-\ntivity complicates model management and can constrain processing capabilities. The resource\nconstraints of a chosen Hardware (D4) platform directly limit the complexity of models that can\nbe deployed, affecting the achievable Processing Capability (D2) and thus the feasible Appli-\ncations (D3). Navigating this complex web of trade-offs is the central task for Edge AI system\narchitects, requiring a holistic approach informed by the multi-dimensional perspective provided\nby our taxonomic framework.\n6\nFuture Research Horizons and Emerging Paradigms\nThe evolution of Edge AI is accelerating, driven by the imperative to overcome current limita-\ntions and unlock new frontiers of decentralized intelligence. The future landscape will be charac-\nterized by unprecedented hardware specialization, algorithms capable of autonomous adaptation,\nand the seamless, trust-aware orchestration of intelligence across the edge-to-cloud continuum.\nThis section projects these future trajectories, framing them as natural progressions within the\nmulti-dimensional taxonomy that structures this review. These emerging opportunities, summa-\nrized in Figure 7, promise to address the challenges outlined in Section 5 and radically expand\nthe application horizon of Edge AI.\n6.1\nNext-Generation Hardware: Redefining the Performance-Energy\nFrontier (D4)\nThe relentless pursuit of enhanced energy efficiency and computational density is driving a\nparadigm shift within the Hardware Type (D4) dimension, heralding a transition from incre-\nmental improvements to fundamentally novel computing architectures. Future Edge AI hardware\n23\nFigure 7: Emerging research vectors in Edge AI, spanning next-generation hardware\nparadigms, advanced algorithmic capabilities, and systemic architectures for collabo-\nrative intelligence.\ndevelopment will be characterized by four transformative trajectories that collectively redefine\nthe performance-energy frontier.\nFirst, the emergence of hyper-specialized accelerators marks a departure from general-\npurpose AI chips toward processors meticulously optimized for specific model architectures—including\ntransformers and graph neural networks—and specialized data modalities such as sparse and\nevent-based data. This architectural evolution promises order-of-magnitude efficiency gains for\nniche Application Domains (D3), particularly in real-time sensor fusion and edge-based natural\nlanguage understanding [105, 106].\nSecond, in-memory and near-memory computing (IMC/NMC) architectures represent\na fundamental rethinking of computational paradigms by executing operations directly within\nor adjacent to memory cells through emerging technologies including memristors and mag-\nnetoresistive random-access memory (MRAM). This approach substantially mitigates the von\nNeumann bottleneck, dramatically reducing data movement energy and enabling unprecedented\nultra-low-power, high-throughput inference capabilities essential for data-intensive applications\nat the Device Edge (D1) [107, 108].\nThird, post-digital computing paradigms are transitioning from theoretical constructs to\npractical implementations, with analog AI and optical computing offering transformative po-\ntential. Analog compute-in-memory techniques, leveraging fundamental physical principles in-\ncluding Ohm’s and Kirchhoff’s laws for matrix operations, demonstrate potential for 10–100×\nenergy reduction in always-on processing scenarios, particularly for sparse data applications in\nindustrial monitoring and environmental sensing [109, 110].\nFinally, the development of self-powered intelligent systems embodies the convergence\nof advanced energy harvesting technologies—spanning solar, kinetic, and radio frequency har-\nvesting—with ultra-low-power AI processors. This integration enables perpetually operational,\nmaintenance-free devices that fundamentally expand viable Deployment Locations (D1) to in-\nclude remote, inaccessible, and hazardous environments, thereby transcending conventional power\nconstraints in intelligent sensing applications [111, 112].\nCollectively, these innovations represent not merely incremental advances but rather a fun-\n24\ndamental reimagining of computational approaches that will enable previously impossible Edge\nAI applications while addressing critical energy constraints.\n6.2\nAdvanced Algorithms: Towards Adaptive and Resource-Aware\nIntelligence (D2)\nAlgorithmic research is poised to fundamentally transform Edge AI systems by imbuing them\nwith greater autonomy, efficiency, and contextual understanding, thereby substantially advancing\nthe capabilities within the Processing Capability (D2) dimension. This evolution will manifest\nthrough several critical research directions that collectively address the unique constraints and\nopportunities of edge computing environments.\nA primary focus will be the development of continual and lifelong learning systems, which\nwill progressively replace static, pre-deployed models with architectures capable of incremental\nlearning from continuous data streams. Such systems will enable models to adapt to concept\ndrift and personalize to local environments without suffering catastrophic forgetting, thereby\ndrastically reducing the need for costly retraining and redeployment cycles [113, 114].\nConcurrently, meta-learning and few-shot learning approaches will emerge as crucial en-\nablers for agile deployment in diverse and dynamic scenarios where labeled data is inherently\nscarce. These techniques will facilitate \"plug-and-play\" intelligence, allowing pre-optimized\nmodels to rapidly specialize for new sensors or environmental conditions at the edge [115, 116].\nThe deployment of Tiny Reinforcement Learning (TinyRL) agents directly on edge de-\nvices will represent another significant advancement, enabling autonomous, real-time decision-\nmaking and control loops. This capability proves particularly transformative for Application\nDomains (D3) such as robotics, industrial automation, and network management, where systems\nmust learn and react to complex environments without the latency of cloud dependency [11, 61].\nFinally, as Edge AI penetrates increasingly critical applications, research will focus on de-\nveloping explainable AI (XAI) techniques specifically designed for resource-constrained envi-\nronments. These methods will provide interpretable rationales for model decisions without over-\nwhelming the limited computational and memory resources of edge hardware, thereby address-\ning growing demands for transparency and accountability in autonomous systems [117, 118].\nTogether, these algorithmic advancements will create a new generation of adaptive, resource-\naware intelligence systems capable of operating effectively within the stringent constraints of\nedge computing environments while maintaining sophisticated learning and decision-making\ncapabilities.\n6.3\nEdge-Cloud Collaborative Intelligence: The Emergence of a\nCognitive Continuum\nThe conventional rigid dichotomy between edge and cloud computing is evolving toward a fluid,\nhierarchical intelligence continuum, representing the maturation of the Deployment Location\n(D1) dimension into a dynamic, integrated system. This architectural shift will be characterized\nby several defining features that collectively enable more efficient and responsive intelligent\nsystems.\nFuture architectures will be defined by adaptive hierarchical systems that dynamically dis-\ntribute intelligence across computational tiers. In this paradigm, TinyML models will perform\ninitial filtering and time-critical inference at the Device Edge (D1), while more complex ana-\nlytical tasks—such as multi-sensor fusion and long-term trend analysis—will be offloaded to\nNetwork Edge servers. The cloud will increasingly specialize in large-scale model training and\n25\nglobal aggregation, thereby establishing a seamless flow of computation and data across the en-\ntire continuum [119, 120].\nComplementing this architectural evolution, intelligent dynamic offloading mechanisms\nwill employ AI-powered controllers to make real-time decisions about workload placement\nbased on a comprehensive assessment of network conditions, computational load, energy avail-\nability, and application requirements. This approach will systematically optimize the complex\ntrade-offs between latency, bandwidth, accuracy, and power consumption across the entire sys-\ntem [121, 122].\nFurthermore, enhanced federated learning frameworks will advance beyond simple up-\ndate averaging to incorporate stronger privacy guarantees—including differential privacy and\nhomomorphic encryption—along with robust aggregation algorithms capable of handling ex-\ntreme non-IID data and device heterogeneity. These frameworks will also integrate mechanisms\nfor detecting and mitigating malicious participants, thereby creating a truly scalable and secure\nsolution for privacy-sensitive domains [123, 124].\nThis collaborative paradigm, illustrated in Figure 8, envisions a future where embedded\nTinyDL models in smart cameras perform real-time object detection, edge servers aggregate\nmultiple feeds for sophisticated crowd analytics, and cloud-based federated learning systems\nsecurely improve global models—all functioning as a single, cohesive intelligent system that\ndynamically adapts to changing conditions and requirements.\n6.4\nTrustworthy and Explainable Edge AI: The Foundation for\nAdoption\nFor Edge AI systems to achieve deployment in truly critical applications, trust must be elevated\nto a first-class design constraint, systematically integrated across both the Processing Capabil-\nity (D2) and Application Domain (D3) dimensions. This imperative will drive several criti-\ncal research thrusts that collectively address the unique challenges of trustworthy computing in\nresource-constrained environments.\nA primary research focus will center on developing robustness against adversarial attacks\nthrough defense mechanisms specifically designed for the computational constraints of edge\nhardware. This includes advancing efficient adversarial training techniques, input purification\nmethods, and runtime detection algorithms that can secure Edge AI systems against evolving\nthreat landscapes without compromising operational efficiency [125, 126].\nEqually important will be addressing the unique challenges of bias detection and mitiga-\ntion in models trained on decentralized, non-IID edge data. Research must develop specialized\ntechniques for detecting, quantifying, and mitigating bias directly on edge devices or during fed-\nerated learning processes, ensuring equitable and ethical AI outcomes across diverse deployment\nscenarios [127, 128].\nFinally, achieving practical on-device explainability will require methods capable of gener-\nating concise, meaningful explanations for model decisions directly on edge hardware, utilizing\nonly a fraction of the resources required for inference itself. This capability will prove paramount\nfor establishing user trust, ensuring regulatory compliance, and enabling effective developer de-\nbugging in field deployments [129, 130].\nTogether, these research directions will establish the foundational trustworthiness necessary\nfor Edge AI systems to expand into increasingly critical applications while maintaining the effi-\nciency requirements inherent to edge computing environments.\n26\nFigure 8: The future cognitive continuum: A dynamic, hierarchical architecture that in-\ntelligently partitions and orchestrates AI workloads across the device-edge-cloud spec-\ntrum based on real-time constraints and requirements.\n27\n6.5\nThe Role of 6G and Advanced Networking: The Connective\nTissue\nNext-generation wireless networks, particularly 6G, will transcend their conventional role as\nmere data conduits to become the intelligent connective tissue that unifies the edge computing\ncontinuum, thereby directly enabling transformative Application Domains (D3). This evolution\nwill manifest through several foundational capabilities that collectively redefine the relationship\nbetween networking and distributed intelligence.\n6G networks are anticipated to provide sub-millisecond latency and holographic-type\ncommunication capabilities, which will fundamentally unlock applications requiring unprece-\ndented responsiveness. These advancements will enable collaborative swarm robotics, autonomous\nvehicle platooning, and immersive extended reality (XR) experiences where haptic feedback and\nreal-time interaction demand near-instantaneous communication [131, 132].\nFurthermore, the deep integration of artificial intelligence and communication tech-\nnologies will transform the network infrastructure itself into an inherently intelligent system.\nThrough the embedding of AI capabilities directly within the radio access network (RAN), future\nnetworks will dynamically allocate resources, predict network states, and optimize performance\nparameters specifically tailored to Edge AI workload requirements [133, 134].\nAdditionally, the emergence of sensing-as-a-service capabilities will represent a paradigm\nshift in network functionality. 6G infrastructure is projected to incorporate integrated sensing\ntechnologies, effectively transforming the network into a distributed sensor array that can pro-\nvide rich perceptual context to Edge AI devices, thereby significantly enhancing their situational\nawareness and operational capabilities across diverse environments.\nCollectively, these advancements will establish next-generation networks as active enablers\nof Edge AI systems rather than passive communication channels, creating a symbiotic rela-\ntionship between networking capabilities and distributed intelligence that will drive innovation\nacross the entire computing continuum.\n6.6\nSynthesis: Towards a Cognitive Edge\nThe convergence of these directions points to a future of a \"Cognitive Edge\"—an intelligent, self-\nadapting, and trustworthy fabric of distributed computation. This evolution will be characterized\nby a shift from deploying static models on isolated devices to orchestrating dynamic intelligence\nacross a continuum of heterogeneous resources. The multi-dimensional taxonomy presented in\nthis review provides the essential framework for navigating this complex and exciting future,\noutlining the inter-dependencies between hardware, algorithms, deployment strategies, and ap-\nplications that will define the next decade of Edge AI innovation.\n7\nConclusion\nEdge Artificial Intelligence represents a paradigm shift in computational architecture, fundamen-\ntally redefining how intelligent systems are designed, deployed, and integrated into the physical\nworld. This comprehensive review has systematically charted the evolution, current state, and\nfuture trajectory of Edge AI, offering a holistic analysis through a novel multi-dimensional tax-\nonomy that integrates deployment location, processing capability, application domain, and hard-\nware type.\nOur analysis reveals that the journey from cloud-centric computing to distributed edge in-\ntelligence has been neither accidental nor instantaneous.\nIt is the result of a coherent evo-\nlution through foundational technologies—CDNs, fog computing, and mobile edge comput-\n28\ning—each solving critical limitations of its predecessor and collectively paving the way for\nmodern paradigms like TinyML, TinyDL, TinyRL, and federated learning. This historical con-\ntextualization, often neglected in prior surveys, provides essential perspective for understanding\ncurrent developments and future directions.\nThrough our systematic methodology and taxonomic framework, we have demonstrated that\nthe contemporary Edge AI landscape is characterized by sophisticated hardware-software co-\ndesign across a spectrum of resource constraints. We have further shown how these technolog-\nical capabilities enable transformative applications across diverse sectors—from healthcare and\nindustrial automation to autonomous systems and smart cities—each with unique requirements\nfor latency, privacy, and autonomy.\nHowever, this review also identifies significant challenges that constrain widespread adop-\ntion. Resource constraints, security vulnerabilities, model management complexities, and power\nconsumption limitations present formidable hurdles that require continued innovation. These\nchallenges are not isolated but interconnected, demanding holistic solutions that address trade-\noffs across hardware, software, and deployment architectures.\nLooking forward, we project that Edge AI’s future lies in several key directions: (1) next-\ngeneration hardware paradigms that redefine energy-performance trade-offs through in-memory\ncomputing, analog AI, and specialized accelerators; (2) advanced algorithms capable of continu-\nous adaptation, few-shot learning, and explainable decision-making within resource constraints;\n(3) seamless edge-cloud collaborative intelligence that dynamically distributes workloads across\nthe computational continuum; and (4) the integration of trustworthiness and explainability as\nfundamental design principles rather than afterthoughts.\nThe realization of Edge AI’s full potential will require unprecedented interdisciplinary col-\nlaboration across hardware engineering, computer systems, algorithm design, and application\ndomains. As 5G/6G networks mature and AI workloads become increasingly pervasive, the\nprinciples and architectures discussed in this review will become central to next-generation in-\ntelligent systems.\nUltimately, Edge AI is poised to create a future where artificial intelligence becomes truly\nubiquitous—embedded not just in devices but woven into the very fabric of our environment,\nenabling responsive, intelligent, and autonomous systems that operate seamlessly within our\nphysical world while respecting the constraints of resources, privacy, and energy. This survey\nprovides a comprehensive foundation for researchers, practitioners, and policymakers to navigate\nand contribute to this rapidly evolving field.\nA\nComplete Edge AI Reference Taxonomy\n29\nTable 7: Systematic classification of seminal Edge AI literature (2017–2025) organized by the review’s taxonomic categories: hardware\naccelerators, TinyML/TinyDL/TinyRL paradigms, federated learning, edge systems (Fog/MEC/CDN), application domains, and emerging\nchallenges.\nYear\nCategory\nSubcategory\nReference\nKey Contribution\n2020\nHardware\nASIC\n[19]\nGoogle Edge TPU architecture analysis\n2021\nHardware\nGPU\n[21]\nNVIDIA Jetson performance profiling\n2022\nHardware\nFPGA\n[20]\nReal-time SRAM-based acceleration\n2023\nHardware\nNeuromorphic\n[22]\nIntel Loihi2 edge deployment\n2024\nHardware\nCiM\n[107]\nMemristor-based compute-in-memory\n2024\nHardware\nSurvey\n[18]\nComparative analysis of 32 accelerators\n2025\nHardware\nAnalog\n[109]\nMythic analog AI chip case study\n2017\nTinyML\nDL\n[59]\nFirst just-in-time DL compilation\n2020\nTinyML\nTools\n[56]\nTensorFlow Lite Micro framework\n2022\nTinyML\nMCU\n[55]\nARM Cortex-M4 optimizations\n2023\nTinyML\nVision\n[60]\nNeural architecture search for MCUs\n2024\nTinyML\nSurvey\n[10]\nState-of-the-art techniques review\n2023\nTinyDL\nSurvey\n[9]\nFrom TinyML to TinyDL: A comprehensive survey\n2023\nTinyDL\nArchitecture\n[58]\nAnalysis of deep learning on microcontrollers\n2024\nTinyDL\nOptimization\n[60]\nHardware-aware NAS for TinyDL models\n2024\nTinyRL\nAlgorithms\n[11]\nDesign principles for RL on edge devices\n2023\nTinyRL\nApplications\n[61]\nDiffusion-based RL for edge-generated content\n2019\nFederated\nFoundational\n[12]\nFirst edge FL framework\n2021\nFederated\nPrivacy\n[63]\nDifferential privacy enhancements\n2022\nFederated\nSurvey\n[13]\nAnalysis of 58 deployments\n2023\nFederated\nRobustness\n[92]\nAdversarial attack defenses\nContinued on next page\n30\nYear\nCategory\nSubcategory\nReference\nKey Contribution\n2018\nEdge\nFog\n[8]\nFog computing architecture\n2020\nEdge\nMEC\n[7]\n5G MEC standardization\n2021\nEdge\nCDN\n[40]\nAI-enhanced content delivery\n2022\nEdge\nSurvey\n[42]\n10-year evolution analysis\n2024\nEdge\nMEC\n[44]\nMEC for video streaming and VR\n2020\nApps\nHealthcare\n[14]\nWearable ECG monitoring\n2021\nApps\nAutomotive\n[16]\nReal-time object detection\n2022\nApps\nIndustrial IoT\n[15]\nResource-efficient Edge AI for predictive maintenance\n2022\nApps\nIndustry\n[15]\nPredictive maintenance systems\n2019\nChallenges\nPrivacy\n[37]\nEdge data protection framework\n2021\nChallenges\nPower\n[97]\nEnergy harvesting techniques\n2022\nChallenges\nSecurity\n[90]\nAttack vector taxonomy\n2022\nFuture\nContinual\n[113]\nLifelong learning algorithms\n2023\nFuture\nBio\n[111]\nNeuromorphic edge systems\n2025\nFuture\n6G\n[132]\nAI-optimized RAN architectures\n31\nReferences\n[1] S. Vasuki. Edge AI: A comprehensive survey of technologies, applications, and chal-\nlenges. In 2024 1st International Conference on Advanced Computing and Emerging\nTechnologies (ACET), pages 1–6, 2024.\n[2] T. Sipola, J. Alatalo, T. Kokkonen, and M. Rantonen. Artificial intelligence in the IoT era:\nA review of edge AI hardware and software. In 2022 31st Conference of Open Innovations\nAssociation (FRUCT), pages 320–331, 2022.\n[3] A. Karras, A. Giannaros, C. Karras, K. C. Giotopoulos, D. Tsolis, and S. Sioutas. Edge\nartificial intelligence in large-scale IoT systems, applications, and big data infrastructures.\nIn 2023 8th South-East Europe Design Automation, Computer Engineering, Computer\nNetworks and Social Media Conference (SEEDA-CECNSM), pages 1–8, 2023.\n[4] M. Sibanda, E. Bhero, and J. Agee. AI edge processing - a review of distributed embed-\nded systems. In 2023 31st Southern African Universities Power Engineering Conference\n(SAUPEC), pages 1–6, 2023.\n[5] Matthew J Page et al. The prisma 2020 statement: an updated guideline for reporting\nsystematic reviews. Systematic reviews, 10(1):1–11, 2021.\n[6] Vikram Shankar. Edge ai: A comprehensive survey of technologies, applications, and\nchallenges. In Proceedings of the 1st International Conference on Advanced Computing\nand Emerging Technologies (ACET), pages 1–6, 2024.\n[7] S. Ahmed, H. Khalid, M. Hamza, and D. Farhat. Mobile edge computing. arXiv [cs.DC],\n2024.\n[8] H. Gupta and A. K. Bharti. Fog computing & IoT: Overview, architecture and appli-\ncations. International Journal of Advanced Research in Computer and Communication\nEngineering, 7(5):30–34, May 2018.\n[9] S. Somvanshi et al. From tiny machine learning to tiny deep learning: A survey. arXiv\n[cs.LG], 2025.\n[10] J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, and S. Han. Tiny machine learning: Progress\nand futures [feature]. IEEE Circuits Syst. Mag., 23(3):8–34, 2023.\n[11] G. Wu, D. Zhang, Z. Miao, W. Bao, and J. Cao. How to design reinforcement learn-\ning methods for the edge: An integrated approach toward intelligent decision making.\nElectronics (Basel), 13(7):1281, 2024.\n[12] X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen. In-edge AI: Intelligentizing\nmobile edge computing, caching and communication by federated learning. IEEE Netw.,\n33(5):156–165, 2019.\n[13] H. G. Abreha, M. Hayajneh, and M. A. Serhani. Federated learning in edge computing:\nA systematic survey. Sensors (Basel), 22(2):450, 2022.\n[14] A. Rocha et al. Edge AI for internet of medical things: A literature review. Comput.\nElectr. Eng., 116(109202):109202, 2024.\n[15] V. Artiushenko, S. Lang, C. Lerez, T. Reggelin, and M. Hackert-Oschätzchen. Resource-\nefficient edge AI solution for predictive maintenance. Procedia Comput. Sci., 232:348–\n357, 2024.\n[16] J. Xie, X. Zhou, and L. Cheng. Edge computing for real-time decision making in au-\ntonomous driving: Review of challenges, solutions, and future trends. Int. J. Adv. Comput.\nSci. Appl., 15(7), 2024.\n32\n[17] G. P. Sharma. Real-time traffic management using IoT sensors and edge computing in\nsmart cities. International Journal of Trend in Research and Development, 11(6):96–100,\nDec 2024.\n[18] S. Alam, C. Yakopcic, Q. Wu, M. Barnell, S. Khan, and T. M. Taha. Survey of deep\nlearning accelerators for edge and emerging computing. Electronics (Basel), 13(15):2988,\n2024.\n[19] B. Liang. Design of ASIC accelerators for AI applications. In International Conference\non Electrical Engineering and Intelligent Control (EEIC 2024), pages 147–154, 2024.\n[20] H. Liu et al. A high-performance accelerator for real-time super-resolution on edge FP-\nGAs. ACM Trans. Des. Automat. Electron. Syst., 29(3):1–25, 2024.\n[21] H. Bouzidi, H. Ouarnoughi, S. Niar, and A. A. E. Cadi. Performance modeling of com-\nputer vision-based CNN on edge GPUs. ACM Trans. Embed. Comput. Syst., 21(5):1–33,\n2022.\n[22] R. S. Das. Emerging neuromorphic computing for edge AI application: A systematic\nliterature review. Journal of Technological Innovations, 5(1):1–8, 2024.\n[23] Jorge Mendez et al. Edge intelligence: Concepts, architectures, applications, and future\ndirections. ACM Transactions on Embedded Computing Systems (TECS), 21(5):1–36,\n2022.\n[24] Rajdeep Singh and Sukhpal Singh Gill. Edge ai: A survey. IoT and Cyber-Physical\nSystems, 3:1–20, 2023.\n[25] Sukhpal Singh Gill et al. Edge ai: A taxonomy, systematic review and future directions.\nCluster Computing, 28(1):1–25, 2025.\n[26] Xiaolong Wang and Wenlong Jia. Optimizing edge ai: A survey on data, model, and\nsystem strategies. Qeios, 2025.\n[27] Chellammal Surianarayanan et al. A survey on optimization techniques for edge ai. Sen-\nsors, 23(3):1279, 2023.\n[28] Xiaolong Wang et al.\nA survey on trustworthy edge intelligence: From security and\nreliability to transparency and sustainability. IEEE Communications Surveys & Tutorials,\n27(3):2102–2138, 2025.\n[29] Wei Su et al. Ai on the edge: A comprehensive review. Artificial Intelligence Review,\n55(8):6125–6184, 2022.\n[30] Kyle Hoffpauir et al. A survey on edge intelligence and lightweight ml. ACM Journal of\nData and Information Quality (JDIQ), 15(2), 2023.\n[31] Tobias Meuser et al. Revisiting edge ai: Opportunities and challenges. IEEE Internet\nComputing, 28(4):45–53, 2024.\n[32] Xiaolong Wang et al. Empowering edge intelligence: A comprehensive survey on on-\ndevice ai models. ACM Computing Surveys, 57(9):1–39, 2025.\n[33] J. H. Kiswani, S. M. Dascalu, and A. F. C. Jr Harris. Cloud computing and its applications:\nA comprehensive survey. IEEE Trans. Pattern Anal. Mach. Intell., 28(1):3–24, Mar 2021.\n[34] B. R. Cherukuri. Edge computing vs. cloud computing: A comparative analysis for real-\ntime AI applications. IEEE Access, 6(5):1–17, Oct 2024.\n[35] B. Charyyev, E. Arslan, and M. H. Gunes. Latency comparison of cloud datacenters and\nedge servers. In GLOBECOM 2020 - 2020 IEEE Global Communications Conference,\npages 1–6, 2021.\n33\n[36] M G Avram. Advantages and challenges of adopting cloud computing from an enterprise\nperspective. Procedia Technol., 12:529–534, 2014.\n[37] Panjun Sun. Security and privacy protection in cloud computing: Discussions and chal-\nlenges. J. Netw. Comput. Appl., 160(102642):102642, 2020.\n[38] A. Choudhary. Internet of things: a comprehensive overview, architectures, applications,\nsimulation tools, challenges and future directions. Discover Internet of Things, 4(1):1–41,\nDec 2024.\n[39] J. Wang, M. K. Lim, C. Wang, and M.-L. Tseng. The evolution of the internet of things\n(IoT) over the past 20 years. Computers & Industrial Engineering, 155:107174, May\n2021.\n[40] B. Zolfaghari et al. Content delivery networks: State of the art, trends, and future roadmap.\nACM Comput. Surv., 53(2):1–34, 2021.\n[41] Vagmi and R. K. Gupta. Content delivery networks in the modern age: Analyzing trends,\novercoming challenges, and pioneering developments. In Smart Innovation, Systems and\nTechnologies, pages 793–806. Springer Nature Singapore, Singapore, 2024.\n[42] S. N. Srirama. A decade of research in fog computing: Relevance, challenges, and future\ndirections. arXiv [cs.DC], 2023.\n[43] S. H. Han and V. Naik. A review on fog computing: Architecture, fog with IoT, algorithms\nand research challenges. ICT Express, 7(2):162–176, 2021.\n[44] M. A. Khan et al. A survey on mobile edge computing for video streaming: Opportunities\nand challenges. arXiv [cs.MM], 2022.\n[45] Z. Li et al. Optimizing mobile edge computing for virtual reality rendering via UAVs: A\nmulti-agent deep reinforcement learning approach. IEEE Internet Things J., pages 1–1,\n2025.\n[46] J. J. Moon et al. A new frontier of AI: On-device AI training and personalization. arXiv\n[cs.LG], 2022.\n[47] X. Wang et al. Empowering edge intelligence: A comprehensive survey on on-device AI\nmodels. ACM Comput. Surv., 57(9):1–39, 2025.\n[48] S. Ubale. On-device AI models: Advancing privacy-first machine learning for mobile\napplications. Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol, 11(1):61–68, 2025.\n[49] D. Xu et al.\nFast on-device LLM inference with NPUs.\nIn Proceedings of the 30th\nACM International Conference on Architectural Support for Programming Languages\nand Operating Systems, Volume 1, pages 445–462, 2025.\n[50] H. Rexha and S. Lafond. Data collection and utilization framework for edge AI applica-\ntions. arXiv [cs.LG], 2021.\n[51] R. Dagli and S. Eken. Deploying a smart queuing system on edge with intel OpenVINO\ntoolkit. Soft Comput., 25(15):10103–10115, 2021.\n[52] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief. Communication-efficient edge AI:\nAlgorithms and systems. arXiv [cs.IT], 2020.\n[53] C. Mwase, Y. Jin, T. Westerlund, H. Tenhunen, and Z. Zou. Communication-efficient\ndistributed AI strategies for the IoT edge. Future Gener. Comput. Syst., 131:292–308,\n2022.\n[54] E. Kartsakli et al. AI-powered edge computing evolution for beyond 5g communication\nnetworks. In 2023 Joint European Conference on Networks and Communications & 6G\nSummit (EuCNC/6G Summit), pages 478–483, 2023.\n34\n[55] R. Immonen and T. Hämäläinen. Tiny machine learning for resource-constrained micro-\ncontrollers. J. Sens., 2022:1–11, 2022.\n[56] P. P. Ray. A review on TinyML: State-of-the-art and prospects. J. King Saud Univ. -\nComput. Inf. Sci., 34(4):1595–1623, 2022.\n[57] H. Han and J. Siebert. TinyML: A systematic review and synthesis of existing research.\nIn 2022 International Conference on Artificial Intelligence in Information and Communi-\ncation (ICAIIC), 2022.\n[58] M. Roveri. Is tiny deep learning the new deep learning? In Computational Intelligence\nand Data Analytics, pages 23–39. Springer Nature Singapore, Singapore, 2023.\n[59] B. Darvish Rouhani, A. Mirhoseini, and F. Koushanfar. TinyDL: Just-in-time deep learn-\ning solution for constrained embedded systems. In 2017 IEEE International Symposium\non Circuits and Systems (ISCAS), 2017.\n[60] A. Burrello, M. Risso, B. A. Motetti, E. Macii, L. Benini, and D. J. Pagliari. Enhancing\nneural architecture search with multiple hardware constraints for deep learning model\ndeployment on tiny IoT devices. IEEE Trans. Emerg. Top. Comput., 12(3):780–794, 2024.\n[61] H. Du et al. Diffusion-based reinforcement learning for edge-enabled AI-generated con-\ntent services. arXiv [cs.NI], 2023.\n[62] S. G. Thomas and P. K. Myakala. Beyond the cloud: Federated learning and edge AI for\nthe next decade. J. Comput. Commun., 13(02):37–50, 2025.\n[63] A. Brecko, E. Kajati, J. Koziorek, and I. Zolotova. Federated learning for edge computing:\nA survey. Appl. Sci. (Basel), 12(18):9124, 2022.\n[64] A. Hemmati, H. M. Arzanagh, and A. M. Rahmani. Fundamentals of edge AI and fed-\nerated learning. In Model Optimization Methods for Efficient and Edge AI, pages 1–23.\nWiley, 2025.\n[65] T. Wang, J. Guo, B. Zhang, G. Yang, and D. Li. Deploying AI on edge: Advancement\nand challenges in edge intelligence. Mathematics, 13(11):1878, 2025.\n[66] L. H. Nguyen, K. D. Tran, X. Zeng, and K. P. Tran. Human-centered edge artificial in-\ntelligence for smart factory applications in industry 5.0: A review and perspective. In\nArtificial Intelligence for Safety and Reliability Engineering, pages 79–100. Springer Na-\nture Switzerland, Cham, 2024.\n[67] M. Hirsch, C. Mateos, and T. A. Majchrzak. Exploring smartphone-based edge AI infer-\nences using real testbeds. Sensors (Basel), 25(9), 2025.\n[68] P. Thakur, S. Goel, and E. Puthooran. Edge AI enabled IoT framework for secure smart\nhome infrastructure. Procedia Comput. Sci., 235:3369–3378, 2024.\n[69] A. M. Sheikh, M. R. Islam, M. H. Habaebi, S. A. Zabidi, A. R. Bin Najeeb, and A. Kab-\nbani. A survey on edge computing (EC) security challenges: Classification, threats, and\nmitigation strategies. Future Internet, 17(4):175, 2025.\n[70] V. Patel, S. Kanani, T. Pathak, P. Patel, M. I. Ali, and J. Breslin. A demonstration of smart\ndoorbell design using federated deep learning. arXiv [cs.DC], 2020.\n[71] A. Omar et al. Smart city: Recent advances in intelligent street lighting systems based on\nIoT. J. Sens., 2022:1–10, 2022.\n[72] A. Choubey, S. Mishra, R. Misra, A. K. Pandey, and D. Pandey. Smart e-waste manage-\nment: a revolutionary incentive-driven IoT solution with LPWAN and edge-AI integration\nfor environmental sustainability. Environ. Monit. Assess., 196(8):720, 2024.\n35\n[73] J. I. Argungu et al. A survey of edge computing approaches in smart factory. Nternational\nJ. Adv. Res. Comput. Commun. Eng., 12(9), 2023.\n[74] D. Ji, J. Y. Kim, H. W. Kim, and Y. Park. MEMS vibration sensor-based edge AI for ma-\nchinery fault prediction: feasibility study using a petrochemical plant process simulation\nfacility. J. Incl. Phenom. Macrocycl. Chem., 105(3–4):249–259, 2025.\n[75] S. Mandapaka, C. Diaz, H. Irisson, A. Akundi, V. Lopez, and D. Timmer. Application of\nautomated quality control in smart factories - a deep learning-based approach. In 2023\nIEEE International Systems Conference (SysCon), pages 1–8, 2023.\n[76] M. Rahmati. Edge AI-powered real-time decision-making for autonomous vehicles in\nadverse weather conditions. arXiv [cs.RO], 2025.\n[77] S. Manivannan, V. Muralidharan, S. Kumar, B. Sundarambal, Kirubakaran, and C. Sel-\nvaganesan. Edge intelligence in autonomous vehicle navigation. In 2025 International\nConference on Data Science and Business Systems (ICDSBS), pages 1–7, 2025.\n[78] I. Ahmed, M. Ahmad, M. U. R. Siddiqi, A. Chehri, and G. Jeon. Toward AI-powered\nedge intelligence for object detection in self-driving cars: Enhancing IoV efficiency and\nsafety. IEEE Internet Things J., 12(11):16990–16997, 2025.\n[79] A. Sathiya, D. Angel, M. Iswarya, R. Poonkodi, K. M. Angelo, and N. Priyadharshini.\nIoT enabled healthcare framework using edge AI and advanced wearable sensors for real\ntime health monitoring. In 2025 International Conference on Multi-Agent Systems for\nCollaborative Intelligence (ICMSCI), pages 384–392, 2025.\n[80] Z. Huang et al.\nEfficient edge-AI models for robust ECG abnormality detection on\nresource-constrained hardware. J. Cardiovasc. Transl. Res., 17(4):879–892, 2024.\n[81] Y. Xu, T. M. Khan, Y. Song, and E. Meijering. Edge deep learning in computer vision\nand medical diagnostics: a comprehensive survey. Artif. Intell. Rev., 58(3), 2025.\n[82] A. Biswas, A. Jain, and Mohana. Survey on edge computing–key technology in retail\nindustry. In Computer Networks and Inventive Communication Technologies, pages 97–\n106. Springer Nature Singapore, Singapore, 2021.\n[83] N. Rashvand, G. A. Noghre, A. D. Pazho, S. Yao, and H. Tabkhi. Exploring pose-based\nanomaly detection for retail security: A real-world shoplifting dataset and benchmark.\narXiv [cs.CV], 2025.\n[84] A. Savit and A. Damor. Revolutionizing retail stores with computer vision and edge AI:\nA novel shelf management system. In 2023 2nd International Conference on Applied\nArtificial Intelligence and Computing (ICAAIC), pages 69–74, 2023.\n[85] R. Islam, M. S. Arafat, M. A. M. Jony, S. M. S. Rafi, M. S. Jalil, and F. Hossen. Hyper-\npersonalization with AI and edge computing: The future of customer experience in e-\ncommerce and retail.\nAdvanced International Journal of Multidisciplinary Research,\n2(6):1–18, Dec 2024.\n[86] R. Dong, Y. Mao, and J. Zhang. Resource-constrained edge AI with early exit prediction.\nJ. Commun. Inf. Netw., 7(2):122–134, 2022.\n[87] Z. Jia et al. The importance of resource awareness in artificial intelligence for healthcare.\nNat. Mach. Intell., 5(7):687–698, 2023.\n[88] E. J. Husom et al. Sustainable LLM inference for edge AI: Evaluating quantized LLMs\nfor energy efficiency, output accuracy, and inference latency. arXiv [cs.CY], 2025.\n[89] A. C. Muhoza, E. Bergeret, C. Brdys, and F. Gary. Power consumption reduction for IoT\ndevices thanks to edge-AI: Application to human activity recognition. Internet Things\n(Amst.), 24(100930):100930, 2023.\n36\n[90] A. Shafee, S. R. Hasan, and T. A. Awaad. Privacy and security vulnerabilities in edge in-\ntelligence: An analysis and countermeasures. Comput. Electr. Eng., 123(110146):110146,\n2025.\n[91] A. Shafee, T. A. Awaad, and A. Moro. A survey of edge computing privacy and security\nthreats and their countermeasures. In 2024 IEEE Computer Society Annual Symposium\non VLSI (ISVLSI), pages 484–489, 2024.\n[92] E. Villar-Rodriguez, M. A. Pérez, A. I. Torre-Bastida, C. R. Senderos, and J. López-de\nArmentia.\nEdge intelligence secure frameworks: Current state and future challenges.\nComput. Secur., 130(103278):103278, 2023.\n[93] R. Jayanth, N. Gupta, and V. Prasanna.\nBenchmarking edge AI platforms for high-\nperformance ML inference. arXiv [cs.AI], 2024.\n[94] S. Choudhary, Vijitha, D. D. Bhavani, Bhuvaneswari, M. Tiwari, and Subburam. Edge AI\ndeploying artificial intelligence models on edge devices for real-time analytics. ITM Web\nConf., 76:01009, 2025.\n[95] J. V. Anchitaalagammai, S. Kavitha, R. Buurvidha, T. S. Santhiya, M. D. Roopa, and S. S.\nSankari. Edge artificial intelligence for real-time decision making using NVIDIA jetson\norin, google coral edge TPU and 6g for privacy and scalability. In 2025 International\nConference on Visual Analytics and Data Visualization (ICVADV), pages 150–155, 2025.\n[96] A. L. Baresi and D. F. Mendonca. Towards a serverless platform for edge computing. In\n2019 IEEE International Conference on Fog Computing (ICFC), pages 1–10, 2019.\n[97] S. Soro. TinyML for ubiquitous edge AI. arXiv [cs.LG], 2021.\n[98] M. Lee, X. She, B. Chakraborty, S. Dash, B. Mudassar, and S. Mukhopadhyay. Reliable\nedge intelligence in unreliable environment. In 2021 Design, Automation & Test in Europe\nConference & Exhibition (DATE), pages 896–901, 2021.\n[99] Y. Meng, Z. Xudong, Z. Jianwen, X. Xinxin, W. Changling, and W. Fang. A ultra-low\npower system design method of AI edge computation. In 2023 19th International Confer-\nence on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD),\npages 1–5, 2023.\n[100] D. Katare, M. Zhou, Y. Chen, M. Janssen, and A. Y. Ding. Energy-aware vision model\npartitioning for edge AI. In Proceedings of the 40th ACM/SIGAPP Symposium on Applied\nComputing, pages 671–678, 2025.\n[101] D. M. K. Dave and B. K. Mittapally. Data integration and interoperability in IOT: Chal-\nlenges, strategies and future direction. International Journal of Computer Engineering\nand Technology, 15(1):45–60, Feb 2024.\n[102] A. Stanko, O. Duda, A. Mykytyshyn, O. Totosko, and R. Koroliuk. Artificial intelligence\nof things (AIoT): Integration challenges, and security issues. Bioinformatics and Applied\nInformation Technologies (BAIT’2024), 3842:1–14, 2024.\n[103] K. B. Letaief, Y. Shi, J. Lu, and J. Lu. Edge artificial intelligence for 6g: Vision, enabling\ntechnologies, and applications. IEEE J. Sel. Areas Commun., 40(1):5–36, 2022.\n[104] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang. Edge intelligence: Paving the\nlast mile of artificial intelligence with edge computing. Proc. IEEE Inst. Electr. Electron.\nEng., 107(8):1738–1762, 2019.\n[105] W. Li and M. Liewig. A survey of AI accelerators for edge environment. In Trends and In-\nnovations in Information Systems and Technologies, pages 35–44. Springer International\nPublishing, Cham, 2020.\n37\n[106] J. Haris, R. Saha, W. Hu, and J. Cano. SECDA-LLM: Designing efficient LLM accelera-\ntors for edge devices. arXiv [cs.AR], 2024.\n[107] W.-S. Khwa et al.\nA mixed-precision memristor and SRAM compute-in-memory AI\nprocessor. Nature, 639(8055):617–623, 2025.\n[108] T.-H. Wen et al.\nFusion of memristor and digital compute-in-memory processing for\nenergy-efficient edge computing. Science, 384(6693):325–332, 2024.\n[109] D. Fick. Analog compute-in-memory for AI edge inference. In 2022 International Elec-\ntron Devices Meeting (IEDM), pages 21.8.1–21.8.4, 2022.\n[110] D. Pile. Optical computing and artificial intelligence. Nat. Photonics, 2024.\n[111] A. R. Trivedi, J. Kung, and J. H. Ko. Architectures for self-powered edge intelligence. In\nHandbook of Computer Architecture, pages 89–125. Springer Nature Singapore, Singa-\npore, 2025.\n[112] M. Ben Ammar, I. Ben Dhaou, D. El Houssaini, S. Sahnoun, A. Fakhfakh, and O. Ka-\nnoun. Requirements for energy-harvesting-driven edge devices using task-offloading ap-\nproaches. Electronics (Basel), 11(3):383, 2022.\n[113] A. Soltoggio et al. A collective AI via lifelong learning and sharing at the edge. Nat.\nMach. Intell., 6(3):251–264, 2024.\n[114] H. Wang, S. Lin, and J. Zhang. Continual and reinforcement learning for edge AI: Frame-\nwork, foundation, and algorithm design. Springer Nature Switzerland, Cham, 2025.\n[115] Y. Chen, H. Yu, Q. Guo, S. Zhao, and T. Taleb. Dynamic edge AI service management\nand adaptation via off-policy meta-reinforcement learning and digital twin. In ICC 2024\n- IEEE International Conference on Communications, volume 80, pages 867–872, 2024.\n[116] J. Lu. Few-shot learning on edge devices using CLIP: A resource-efficient approach for\nimage classification. Information Technology and Control, 53(3):833–845, Jun 2024.\n[117] A. E. Hassanien, D. Gupta, A. K. Singh, and A. Garg, editors. Explainable edge AI: A\nfuturistic computing perspective. Springer International Publishing, Cham, Switzerland,\n1st edition, 2023.\n[118] H. T. T. Nguyen, L. P. T. Nguyen, and H. Cao. XEdgeAI: A human-centered indus-\ntrial inspection framework with data-centric explainable edge AI approach. Inf. Fusion,\n116(102782):102782, 2025.\n[119] N. S. Dhakad, Y. Malhotra, S. K. Vishvakarma, and K. Roy. SHA-CNN: Scalable hierar-\nchical aware convolutional neural network for edge AI. arXiv [cs.NE], 2024.\n[120] F. Firouzi, B. Farahani, and A. Marinšek. The convergence and interplay of edge, fog,\nand cloud in the AI-driven internet of things (IoT). Inf. Syst., 107(101840):101840, 2022.\n[121] S. Kumari, M. Rakshith, C. K. S. Sibi, and T. Sayyed. Leveraging artificial intelligence for\ndynamic workload management in edge and cloud environments. International Journal\nfor Research Trends and Innovation, 9(7):309–316, Jul 2024.\n[122] Y. Li, S. Cheng, H. Zhang, and J. Liu. Dynamic adaptive workload offloading strategy in\nmobile edge computing networks. Comput. Netw., 233(109878):109878, 2023.\n[123] J. Rane, O. Kaya, S. K. Mallick, and N. L. Rane. Federated learning for edge artificial\nintelligence: Enhancing security, robustness, privacy, personalization, and blockchain in-\ntegration in IoT. In Future Research Opportunities for Artificial Intelligence in Industry\n4.0 and 5.0. Deep Science Publishing, 2024.\n38\n[124] K. Wang, Q. He, F. Chen, H. Jin, and Y. Yang. FedEdge: Accelerating edge-assisted\nfederated learning. In Proceedings of the ACM Web Conference 2023, pages 2895–2904,\n2023.\n[125] D. Zhong, B. Li, X. Chen, and C. Liu. EdgeShield: A universal and efficient edge com-\nputing framework for robust AI. arXiv [cs.CR], 2024.\n[126] Z. Yi, Y. Qian, M. Chen, S. A. Alqahtani, and M. S. Hossain. Defending edge computing\nbased metaverse AI against adversarial attacks.\nAd Hoc Netw., 150(103263):103263,\n2023.\n[127] W. Hutiri. Design patterns for detecting and mitigating bias in edge AI. PhD thesis, Delft\nUniversity of Technology, Delft, Netherlands, 2023.\n[128] D. Katare, N. Kourtellis, S. Park, D. Perino, M. Janssen, and A. Y. Ding. Bias detection\nand generalization in AI algorithms on edge for autonomous driving. In 2022 IEEE/ACM\n7th Symposium on Edge Computing (SEC), pages 342–348, 2022.\n[129] B. Xu et al. Towards explainability for AI-based edge wireless signal automatic modula-\ntion classification. J. Cloud Comput. Adv. Syst. Appl., 13(1), 2024.\n[130] J. Xu et al. On-device language models: A comprehensive review. arXiv [cs.CL], 2024.\n[131] P. Zhang and H. Zhu. The fusion of edge computing and artificial intelligence in 5g\ncommunication. In Proceedings of the 2024 3rd International Conference on Frontiers of\nArtificial Intelligence and Machine Learning, pages 106–112, 2024.\n[132] Y. Zhou and X. Chen. Edge intelligence: Edge computing for 5g and the internet of things.\nFuture Internet, 17(3):101, 2025.\n[133] P. Zhang, D. Wen, G. Zhu, Q. Chen, K. Han, and Y. Shi. Collaborative edge AI inference\nover cloud-RAN. IEEE Trans. Commun., 72(9):5641–5656, 2024.\n[134] R. Barker. Advancements in mobile edge computing and open RAN: Leveraging artificial\nintelligence and machine learning for wireless systems. arXiv [cs.NI], 2025.\n39\n",
    "content": "# **Edge AI: Evolution, Taxonomy Framework, and Future Outlook – Paper Review**\n\n## 1. Core Content and Key Contributions\n\nThis systematic review paper offers a comprehensive overview of the **evolution, current state, and future directions of Edge Artificial Intelligence (Edge AI)**. Following the PRISMA 2020 guidelines, the authors screened over 2,200 publications and conducted an in-depth analysis of 79 core studies.\n\n### **Core Content**\n- **Definition and Background**: Edge AI embeds intelligence directly into devices at the network edge, enabling real-time data processing close to the source. This significantly reduces latency, enhances privacy protection, and improves system resilience.\n- **Historical Evolution**: Traces the development from early content delivery networks (CDNs) and fog computing, through mobile edge computing (MEC), to modern on-device AI.\n- **Multidimensional Taxonomy Framework**: Introduces an innovative four-dimensional classification system:\n  - **D1 Deployment Location**: Device Edge → Network Edge → Regional/MEC Edge → Cloud Edge\n  - **D2 Processing Capability**: TinyML, TinyDL, TinyRL, Federated Learning, etc.\n  - **D3 Application Domains**: Healthcare, Industrial IoT, Autonomous Driving, Smart Cities, etc.\n  - **D4 Hardware Architecture**: CPU, ASIC, FPGA, GPU, Neuromorphic Chips, etc.\n- **Key Technology Stack**: Covers specialized hardware accelerators, optimized software frameworks (e.g., TensorFlow Lite, OpenVINO), and communication protocols (e.g., 5G URLLC, MQTT).\n- **Challenges Analysis**: Explores five major systemic challenges—resource constraints, security & privacy, model management, power consumption, and connectivity.\n- **Future Directions**: Predicts emerging trends such as next-generation hardware (in-memory computing, analog AI), continual learning algorithms, edge-cloud synergy, and trustworthy AI.\n\n### **Main Contributions**\n1. **First Complete Historical Tracing**: Establishes clear lineage between modern Edge AI and its predecessors (CDN, fog computing), filling a gap in prior reviews that lacked historical continuity.\n2. **Unified Multidimensional Taxonomy**: Integrates deployment, processing, application, and hardware dimensions into a single framework, revealing interdependencies and trade-offs across layers—a powerful tool for systematic analysis.\n3. **Comprehensive Challenge Mapping**: Goes beyond isolated issues like performance or security by systematically mapping interconnected challenges across the entire technology stack.\n4. **Reproducible Research Foundation**: Employs the rigorous PRISMA methodology, ensuring transparency and reproducibility, thus providing a reliable reference for future research.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nThe paper achieves significant advances in several key areas:\n\n| **Innovation Dimension** | **Specific Highlights** |\n|--------------------------|-------------------------|\n| **Methodological Innovation** | Applies PRISMA 2020 standards for systematic literature review—an uncommon but highly rigorous approach in AI-related surveys, enhancing academic credibility. |\n| **Framework Innovation** | Proposes the first unified four-dimensional taxonomy (“Deployment–Capability–Application–Hardware”), surpassing traditional single-axis classifications and better capturing the complexity of the Edge AI ecosystem (see Figure 1). |\n| **Perspective Innovation** | Emphasizes **cross-layer co-design**, arguing that hardware, algorithms, and applications must not be considered in isolation. For example, choosing a low-power MCU limits model complexity, which in turn constrains feasible use cases. |\n| **Historical Perspective Innovation** | Positions Edge AI as the natural evolution of \"distributed intelligence\" rather than a sudden technological disruption, offering deeper insight into current trajectories and developmental logic. |\n| **Challenge Integration Innovation** | Synthesizes fragmented issues—security, resources, energy—into an interconnected **challenge network** (see Figure 6), highlighting how solving one problem may exacerbate another (e.g., energy-saving measures potentially weakening security). |\n\nMoreover, the paper presents a forward-looking vision of the \"**Cognitive Edge**\"—a future where Edge AI evolves from static model deployment to dynamic, adaptive, and explainable agents capable of flexibly offloading tasks between edge and cloud, forming a true intelligent continuum.\n\n---\n\n## 3. Startup Ideas Based on the Paper\n\nLeveraging the technological trends and market gaps identified in the paper, here are several high-potential entrepreneurial opportunities:\n\n### 🚀 Project 1: \"TinyML-as-a-Service\" Platform for Micro Devices  \n> **Positioning**: A one-stop TinyML solution provider for SMEs and developers, lowering the barrier to entry for on-device AI.\n\n#### **Pain Points Addressed**\n- As noted in the paper, TinyML operates under extreme resource constraints (KB-level memory, µW-level power), requiring highly customized model compression and quantization.\n- Current tools lack usability; developers must manually handle pruning, quantization, and deployment adaptation.\n\n#### **Solution**\n- Build a cloud-based automated pipeline: Users upload pre-trained models → platform automatically compresses, quantizes, and generates code → outputs lightweight firmware runnable on MCUs.\n- Support popular MCUs (STM32, ESP32, nRF series) and sensor interfaces.\n- Provide a visual debugging interface with OTA update support and real-time performance monitoring.\n\n#### **Business Model**\n- SaaS subscription (charged per device or API call)\n- Partner with chipmakers to pre-install SDKs and share licensing revenue\n- Offer educational kits targeting academic and training markets\n\n#### **Technical Barriers**\n- Automated model compression algorithm library\n- Cross-platform compiler and runtime optimization\n- Low-power inference scheduling engine\n\n---\n\n### 🏥 Project 2: Medical-Grade Wearables + Edge Federated Learning Platform  \n> **Positioning**: A closed-loop personal health management system compliant with HIPAA/GDPR privacy regulations.\n\n#### **Pain Points Addressed**\n- Medical data is highly sensitive; traditional cloud-based training poses privacy risks (highlighted in D3 application domain).\n- Individual devices generate limited data, making it difficult to train robust personalized models.\n\n#### **Solution**\n- Develop wearable devices (e.g., ECG wristbands, glucose patches) with onboard AI processing—ensuring raw data never leaves the device.\n- Build an **edge-based federated learning network**: Devices only upload encrypted gradient updates to an edge server, which aggregates them into a global model and redistributes it.\n- Allow users to opt into “population health studies” in exchange for enhanced predictive services (e.g., atrial fibrillation detection, sleep disorder identification).\n\n#### **Differentiation**\n- Full local data processing ensures compliance with strictest privacy laws\n- Models continuously improve with usage (\"the more you use, the smarter it gets\")\n- Can integrate with hospital systems to assist doctors in preliminary screening\n\n#### **Revenue Streams**\n- Device sales + subscription-based health services\n- Sell anonymized population trend reports to pharmaceutical companies or research institutions (with user consent)\n- Partner with insurers on behavior-based wellness incentive programs\n\n---\n\n### 🌐 Project 3: \"Cognitive Edge\" Intelligent Gateway – Hardware + OS  \n> **Positioning**: Serve as the \"edge brain\" in smart homes/factories, enabling coordinated multi-device decision-making.\n\n#### **Background Support**\n- The paper identifies “edge-cloud collaborative intelligence” (Section 6.3) as a key future trend, necessitating an intelligent intermediary layer for task orchestration.\n- Existing home/industrial gateways are functionally limited and lack native AI inference capabilities.\n\n#### **Product Design**\n- **Hardware**: High-performance gateway equipped with NPU + FPGA, supporting multiple camera and sensor inputs.\n- **Operating System**: Embedded **dynamic task offloading engine** that autonomously decides—based on network status, battery level, and compute availability:\n  - What runs locally (e.g., facial recognition door access)\n  - What goes to the edge server (e.g., cross-camera tracking)\n  - What is sent to the cloud (e.g., long-term behavioral analytics)\n\n#### **Use Cases**\n- **Smart Home**: Fall detection → local alarm + video snippet sent to family’s phones\n- **Industrial Plant**: Vibration anomaly → local shutdown + data upload to MES + maintenance ticket request\n\n#### **Competitive Advantages**\n- Vendor-agnostic; supports heterogeneous device integration\n- Delivers ultra-low latency and high reliability\n- Offers open APIs to attract third-party developers for plugin creation\n\n#### **Monetization Paths**\n- B2B sales to system integrators\n- Custom solutions for premium B2C customers\n- Revenue from remote maintenance and model update services\n\n---\n\n### 🔮 Summary: Fundamental Insights Behind the Three Ventures\n\n| **Project** | **Trend Captured** | **Core Conflict Resolved** |\n|------------|--------------------|----------------------------|\n| TinyML-as-a-Service | Democratization of AI | **High technical barrier vs. Broad application demand** |\n| Medical Federated Learning Platform | Privacy-first AI | **High data value vs. Strict privacy requirements** |\n| Cognitive Edge Gateway | Distributed Intelligence | **Weak individual intelligence vs. Strong collaborative decision-making** |\n\nAll three ventures are grounded in the technological evolution patterns revealed in the paper. They address critical \"break points\" in the existing ecosystem by delivering integrated solutions—each backed by strong technical moats and clear commercial viability.",
    "github": "",
    "hf": ""
}