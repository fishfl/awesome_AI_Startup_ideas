{
    "id": "2509.13268",
    "title": "LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt",
    "summary": "Research findings show that large language models, when fine-tuned parameter-efficiently based on textual descriptions, can accurately predict energy and macronutrient values from 24-hour dietary recalls of adolescents.",
    "abstract": "BACKGROUND: Most artificial intelligence tools used to estimate nutritional content rely on image input. However, whether large language models (LLMs) can accurately predict nutritional values based solely on text descriptions of foods consumed remains unknown. If effective, this approach could enable simpler dietary monitoring without the need for photographs. METHODS: We used 24-hour dietary recalls from adolescents aged 12-19 years in the National Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM was prompted using a 10-shot, chain-of-thought approach to estimate energy and five macronutrients based solely on text strings listing foods and their quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate whether predictive accuracy improved. NHANES-calculated values served as the ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male, mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across endpoints. In contrast, the fine-tuned model performed substantially better, with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed solely to text input can accurately predict energy and macronutrient values from 24-hour dietary recalls. This approach holds promise for low-burden, text-based dietary monitoring tools.",
    "category1": "Application Implementation",
    "category2": "Healthcare",
    "category3": "Non-Agent",
    "authors": "Rodrigo M Carrillo-Larco",
    "subjects": [
        "Machine Learning (cs.LG)"
    ],
    "comments": "Comments:this https URL",
    "keypoint": "The vanilla LLM without fine-tuning produced poor predictions for energy and macronutrients.\nMean absolute error (MAE) for energy in the vanilla model was 652.08.\nLin’s CCC for all outcomes in the vanilla model was below 0.46.\nA 10-shot chain-of-thought prompt was used to guide the LLM.\nParameter-efficient fine-tuning (PEFT) was applied using one data subset (n = 1,129).\nFine-tuning significantly improved prediction accuracy across all nine test subsets.\nAfter fine-tuning, energy MAE ranged from 171.34 to 190.99.\nLin’s CCC exceeded 0.89 for all outcomes after fine-tuning.\nFor total fat, Lin’s CCC ranged from 0.831 to 0.916 post-fine-tuning.\nFor dietary fiber, Lin’s CCC ranged from 0.874 to 0.925 after fine-tuning.\nFor total sugars, the lowest Lin’s CCC after fine-tuning was 0.896.\nBland-Altman plots showed good agreement with minimal bias after fine-tuning.\nThe Mistral-Small-24B-Instruct-2501 (4-bit) model was used for the experiments.\nUnsloth ecosystem was used to perform PEFT efficiently.\nThe study used NHANES 24-hour dietary recall data from adolescents aged 12–19 years.\nThe dataset included 11,281 participants from the second day of dietary recalls.\nText inputs consisted of food names and quantities in grams.\nNo image data were used as input to the LLM.\nGround truth values came directly from NHANES-calculated nutrient data.\nEnergy, protein, carbohydrates, total sugar, fiber, and total fat were the predicted outcomes.",
    "date": "2025-09-18",
    "paper": "1 \n \nLLMs for energy and macronutrients estimation using only text data from 24-hour dietary \nrecalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt \n \nRodrigo M Carrillo-Larco1,2 \n \n1. Hubert Department of Global Health, Rollins School of Public Health, Emory University, \nAtlanta, GA, USA. \n2. Emory Global Diabetes Research Center of Woodruff Health Sciences Center, Emory \nUniversity, Atlanta, USA \n \n \nCORRESPONDING AUTHOR \nRodrigo M Carrillo-Larco, MD, PhD \nRollins School of Public Health, Emory University, Atlanta, GA, USA. \nrmcarri@emory.edu \n \n2 \n \nABSTRACT \nBACKGROUND: Most artificial intelligence tools used to estimate nutritional content rely on \nimage input. However, whether large language models (LLMs) can accurately predict nutritional \nvalues based solely on text descriptions of foods consumed remains unknown. If effective, this \napproach could enable simpler dietary monitoring without the need for photographs. METHODS: \nWe used 24-hour dietary recalls from adolescents aged 12–19 years in the National Health and \nNutrition Examination Survey (NHANES). An open-source quantized LLM was prompted using a \n10-shot, chain-of-thought approach to estimate energy and five macronutrients based solely on \ntext strings listing foods and their quantities. We then applied parameter-efficient fine-tuning \n(PEFT) to evaluate whether predictive accuracy improved. NHANES-calculated values served as \nthe ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber and total fat. \nRESULTS: In a pooled dataset of 11,281 adolescents (49.9% male, mean age 15.4 years), the \nvanilla LLM yielded poor predictions. The mean absolute error (MAE) was 652.08 for energy and \nthe Lin’s CCC <0.46 across endpoints. In contrast, the fine-tuned model performed substantially \nbetter, with energy MAEs ranging from 171.34 to 190.99 across subsets, and Lin’s CCC \nexceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a chain-of-thought \napproach and fine-tuned with PEFT, open-source LLMs exposed solely to text input can \naccurately predict energy and macronutrient values from 24-hour dietary recalls. This approach \nholds promise for low-burden, text-based dietary monitoring tools. \n \nKeywords: generative artificial intelligence; nutritional assessment; precision nutrition. \n3 \n \nINTRODUCTION \nTechnologies, including artificial intelligence (AI), are increasingly being leveraged for health-\nrelated applications, including the areas of nutrition monitoring and dietary assessment.1-5 Among \nthese, AI tools that use image analysis have been widely adopted to help individuals monitor their \nfood intake.6-11 For instance, several mobile applications allow users to take photos of their meals \nand receive feedback on estimated energy and nutritional content.6-11 Although the accuracy of \nthese applications varies, they are generally considered reliable enough for pragmatic use.6-11 \n \nWith the rapid rise of generative AI—most notably large language models (LLMs), which now \nengage millions of users daily—the analysis of text-based data has expanded substantially. In the \nfield of nutrition, LLMs capable of processing both text and images have begun to be explored as \ntools for estimating energy and nutrient intake.12-15 However, it remains unknown whether LLMs \nthat receive only text input—without images—can perform this task accurately. Texting what one \neats throughout the day, in the form of a dietary diary, may represent a simpler and less resource-\nintensive alternative to photographing meals. Moreover, given the widespread use of messaging \napps and social media platforms, text-based dietary logging may align more naturally with users’ \ndaily behaviors. \n \nIn this study, we aimed to evaluate whether an LLM, provided solely with a string of text listing all \nfoods and their respective quantities as collected in a 24-hour dietary recall, could accurately \nestimate energy and macronutrient intake. We also investigated whether fine-tuning the LLM \nwould enhance its predictive performance. \n \n4 \n \nMETHODS \nData sources \nWe used data from the National Health and Nutrition Examination Survey (NHANES) in the United \nStates (US).16,17 NHANES samples non-institutionalized individuals across the country and is \ndesigned to produce health estimates representative at the national level. We pooled all \nconsecutive NHANES cycles from 2003-2004 to 2021-2023. Earlier cycles were excluded \nbecause they included only a single 24-hour dietary recall, whereas the selected cycles included \ntwo non-consecutive 24-hour dietary recalls. All NHANES cycles follow a consistent protocol, \nincluding standardized methods for dietary data collection.16,17 \n \nSample \nWe included only participants with high-quality 24-hour dietary recalls, as determined by NHANES \nprotocols.16,17 Individuals who were breastfeeding at the time of data collection were excluded. \nThis analysis included only adolescents aged 12 to 19 years, inclusive. Younger participants were \nexcluded because their dietary recall was completed by a proxy or in the presence of a proxy.16,17 \nWe also excluded adults, as adolescents represent a distinct population for this experiment. \nAdolescents frequently interact with mobile devices via text, and most prior research using novel \ntechnologies for dietary assessment (e.g., imaging-based nutrient analysis) has focused on \nadults.6-11 Thus, adolescents provide a novel and relevant population to assess whether LLMs \ncan accurately estimate energy and macronutrient intake. \n \nInput data for the LLM \nThe pooled dataset of adolescents aged 12-19 years with two high-quality 24-hour dietary recalls \nand who were not breastfeeding included 24,111 observations. From this pooled dataset, we \n5 \n \nrandomly selected 10 records to construct the 10-shot prompt (Supplementary Table 1). We then \nused only data from the second day of dietary recall, which included 11,281 unique participants. \nWe chose the second day to reduce computational demands, as using both days (24,111 \nobservations) or only the first day (12,830 observations) would require more computational \nresources without proportionate added value for our primary objective—to assess the accuracy \nof LLM estimates for energy and macronutrient intake. Finally, to facilitate processing, we divided \nthe dataset from the second day (11,281 participants) into 10 subsets: one containing 1,129 \nparticipants and nine others with 1,128 each. These data subsets were used at different stages \nof the analytical approach. \n \nExposure variable \nThe exposure information—later formatted as input data for the LLM—consisted of dietary data \nprovided by NHANES participants during the 24-hour dietary recalls.16,17 The first recall was \nconducted in person at a mobile examination center, while the second was conducted by \ntelephone three to five days later, on a different day of the week.16,17 NHANES nutritional datasets \ninclude USDA food codes, each of which can be mapped to a corresponding text descriptor (e.g., \nfood code 11000000 maps to “MILK, HUMAN”). For each participant, all reported food items were \ncombined into a single string, with each item paired with the quantity consumed. This text string, \nlisting all foods and their respective amounts, was presented to the LLM and had the following \nform: \"PORK CHOP, BREADED, FRIED, LEAN ONLY (22); CHICKEN PATTY/FILLET/TENDERS, \nBREADED, COOKED (48); BREAD, WHITE (26); SALTY SNACKS, CORN OR CORNMEAL, \nCORN PUFFS, TWISTS (14); ORANGE, RAW (96); MUSTARD SAUCE (10); TAFFY (15.6); \nSOFT DRINK, FRUIT-FLAVORED, CAFFEINE FREE (314.68); FRUIT-FLAVORED DRINK, \nFROM SWEETENED POWDER, FORTIFIED WITH VITAMIN C (203.13).\" Values in parentheses \n6 \n \nrepresent the amount consumed (grams). Aside from formatting the food entries as described, we \ndid not alter the NHANES data in any way. \n \nOutcome variable \nThe outcomes of interest—the variables the LLM was tasked with predicting—included energy \nand five macronutrients. Specifically, these were: energy (kilocalories; NHANES variable \nDRx1KCAL), protein (grams; DRxIPROT), carbohydrates (grams; DRxICARB), total sugars \n(grams; DRxISUGR), dietary fiber (grams; DRxIFIBE), and total fat (grams; DRxITFAT). These \noutcomes were selected because they are commonly targeted in dietary tracking and weight \nmanagement applications. We used the NHANES data exactly as provided, without any additional \nprocessing or modification to the energy and macronutrient variables. These outcome variables \nwere treated as the ground truth and served as the reference standard against which the LLM’s \npredictions were benchmarked. \n \nAnalytical approach \nWe conducted two experiments. First, in the baseline LLM evaluation, we used the second data \nsubset (n = 1,128) to prompt the LLM to estimate energy and macronutrient values based solely \non the food list, using the 10-shot prompt. Notably, the 10-shot prompt was structured using a \nchain-of-thought approach, in which each step of the reasoning process was explicitly articulated \nto guide the LLM through the task. Second, in the fine-tuned LLM evaluation, we applied \nParameter-Efficient Fine-Tuning (PEFT) to the LLM using the first data subset (n = 1,129) and \nthen used the fine-tuned model to generate predictions across the remaining data subsets (nine \ndata subsets; n = 1,128 each). In summary, the first subset (n = 1,129) was used exclusively for \n7 \n \nPEFT, the second subset (n = 1,128) was used to evaluate both the vanilla and fine-tuned LLM, \nand the remaining subsets (n = 1,128 each) were used to evaluate the fine-tuned LLM. \n \nPEFT was conducted using the Unsloth ecosystem,18 which enables fast and efficient fine-tuning \nof quantized LLMs. We ran PEFT for 10 epochs on the first data subset using the 10-shot prompt. \nAmong the models available in Unsloth,18 we used the Mistral-Small-24B-Instruct-2501 (4-bit), as \npreliminary testing showed that other LLMs failed to return output in the desired format. \n \nTo assess the accuracy of the LLM predictions—both from the vanilla model (before fine-tuning) \nand the fine-tuned model—we compared them against the ground truth values (i.e., energy and \nmacronutrients reported in the NHANES datasets). We computed the following metrics: mean \nsquared error (MSE), mean absolute error (MAE), mean absolute percentage error (MAPE), root \nmean squared error (RMSE), R2, and Lin’s concordance correlation coefficient (Lin’s CCC). We \nalso computed paired t-tests. Finally, we generated Bland–Altman plots. Ideally, the MSE, MAE, \nMAPE, and RMSE should be close to zero, indicating minimal error between the ground truth and \nthe predictions. In contrast, R2 and Lin’s CCC should be close to 1, reflecting strong concordance \nbetween the predicted and observed values. The Bland-Altman plots provide a visual \nrepresentation of the differences between the ground truth and the predictions, where most values \nshould fall within the limits of agreement (green dotted lines), and the mean difference (red solid \nline) should be close to zero indicating minimal bias between the ground truth and the predictions. \n \nIn a few instances, the LLM returned predictions that were not in the expected format and, \ntherefore, these cases were excluded from the computation of validation metrics. In such cases, \nthe model failed to respond as instructed. The effective sample size used to compute the \nvalidation metrics is reported and was always >89% of the respective data subset. \n \n8 \n \nThe analysis was conducted using Python, and the analysis code is provided as Supplementary \nMaterials. We did not account for the complex survey design of the NHANES (e.g., primary \nsampling units or sampling weights), because our goal was to compare the ground truth values \nwith the LLM-based predictions rather than to produce nationally representative population-\nweighted estimates. \n \nEthical considerations \nWe analyzed publicly available data that contain no personal identifiers or sensitive information. \nHuman subjects were not directly involved in this research. Ethical approval was not sought as \nthe study posed minimal risk. NHANES participants provided informed consent.16,17 \n \nRole of the funding source \nNo specific funding supported this work. \n \nPublic involvement \nNeither members of the public nor patients were involved in the design, analysis, or interpretation \nof this study. \n \nRESULTS \nStudy sample \nIn the pooled dataset including only the second day of 24-hour dietary recalls (n = 11,281), the \nsample was evenly split by sex, with 49.9% male and 50.1% female participants. The mean age \n9 \n \nwas 15.4 years, and the median age was 15.0 years. A descriptive summary of the observed \noutcomes (energy and macronutrients) is presented in Table 1. \n \nPredictions – before fine-tuning \nThe vanilla model—i.e., the LLM using the 10-shot prompt without fine-tuning—produced poor \npredictive performance. The MAE was highest for energy (652.08) and lowest for dietary fiber \n(4.90; Table 2). Lin’s CCC was consistently low, with values of 0.463 for energy and as low as \n0.113 for total fat. Bland-Altman plots also indicated poor agreement between predicted and \nobserved values, with a tendency for error magnitude to increase at higher levels of energy and \nmacronutrient intake (Supplementary Figure 1). \n \nPredictions – after fine-tuning \nPredictions improved substantially after PEFT (Table 2). This improvement was consistent across \nall nine test data subsets. Energy remained the outcome with the highest MAE, ranging from \n171.34 (data subset 5) to 190.99 (data subset 9). Fiber consistently showed the lowest error, with \nMAEs ranging from 1.94 (data subset 2) to 2.60 (data subset 8). Lin’s CCC exceeded 0.91 for all \noutcomes across all data subsets, with a few exceptions. For total fat, Lin’s CCC ranged from \n0.831 (data subset 5) to 0.916 (data subset 2). For fiber, the coefficient dropped to 0.874-0.878 \nin two subsets (data subsets 9 and 10). For total sugars, the lowest Lin’s CCC was 0.896 in data \nsubset 9. Bland-Altman plots showed good agreement between predicted and reference values, \nwith no evident systematic bias across the range of measurements (Supplementary Figures 2-\n10). \n \n10 \n \nDISCUSSION \nMain findings \nUsing a 10-shot chain-of-thought prompt and an open-source LLM presented with text inputs—\nno images—describing foods and their respective quantities, we found that the LLM in its baseline \nform did not produce accurate predictions for energy and five macronutrients (protein, \ncarbohydrates, sugars, fiber, and fat). However, after PEFT, the LLM’s predictive performance \nimproved substantially. For instance, the fine-tuned LLM showed strong agreement with reference \nvalues, with Lin’s CCC exceeding 89% for all outcomes across nine independent data subsets. \n \nThese findings suggest that open-source LLMs, when fine-tuned using high-quality 24-hour \ndietary recall data, can accurately estimate energy and macronutrient content from text alone—\nwithout requiring accompanying food images. This capability has potential applications for \nfacilitating dietary data collection in research settings, as well as for enhancing real-time \nmonitoring and personalized dietary feedback among individuals aiming to manage their weight \nor improve nutritional habits. \n \nStrengths and limitations \nThis study has important strengths. We leveraged a large, nationally representative dataset from \nthe NHANES,16,17 spanning multiple years. NHANES is widely regarded as a gold standard in \npopulation-based health research, with high-quality data collected using rigorous protocols.16,17 \nWe relied on 24-hour dietary recalls administered by trained professionals, and the outcome \nvariables—energy \nand \nmacronutrients—were \nderived using \nstate-of-the-art, validated \nmethods.16,17 In addition, we used an open-source LLM and PEFT,18 enhancing the accessibility, \nreproducibility, and scalability of our approach using emerging technologies. \n \n11 \n \nHowever, limitations must be acknowledged. First, the input data from NHANES had already \nundergone preprocessing, which may not reflect the complexity or variability of real-world free \ntext inputs. Future work should evaluate LLM performance using free-text dietary descriptions \nprovided by lay individuals in everyday contexts—similar to how one might text a friend or report \nto a health worker what they ate the previous day. Second, we limited our analysis to English-\nlanguage inputs. While many LLMs are multilingual, their performance may vary depending on \nthe volume and quality of non-English data included in their pretraining.19 Thus, the accuracy \nreported in this study may not generalize to inputs in other languages. Future studies should \nexplore multilingual performance and consider cultural or regional food naming differences.14 \nThird, we focused exclusively on energy and macronutrients—nutrients most commonly tracked \nin diet-monitoring apps and interventions for weight management. However, other dietary \ncomponents, such as micronutrients (e.g., sodium, calcium, vitamin D), are important for specific \nsubpopulations. For instance, salt intake is particularly relevant for individuals with hypertension, \nwhile iron and folate are crucial for pregnant women. Future research should assess LLMs' ability \nto estimate a broader array of dietary constituents. Fourth, we relied on a single fine-tuned model \napplied to 24-hour dietary recall data, which may limit generalizability. Other dietary assessment \ntools (e.g., food frequency questionnaires) might pose unique challenges to LLM performance \nand should be tested separately. Lastly, although PEFT is computationally efficient,18 fully fine-\ntuning the model—or training a new LLM exclusively on dietary data—may yield much better \npredictive performance. \n \nImplications \nThis study demonstrates that LLMs can accurately predict energy and macronutrient content \nwhen provided with a plain-text string listing food items and their consumed quantities. These \nfindings expand upon existing literature,1-5  which has largely focused on image-based dietary \n12 \n \nassessment.6-15 While recent applications of LLMs in nutrition have harnessed their ability to \nprocess visual inputs,12-15 few or none have investigated the potential of text-only inputs for \nnutritional predictions. For instance, Haman et al.20 used an OpenAI LLM to estimate the \nnutritional content of 236 individual food items, rather than complete 24-hour dietary recalls. \n \nPending external validation and further refinement to address the limitations discussed, our \nfindings offer preliminary yet compelling evidence that LLMs can serve as effective tools for \ndietary monitoring. Future applications, ideally tested in trials, should evaluate whether mobile \napps that accept only text input can deliver accurate nutritional assessments. These assessments \ncould then be leveraged to trigger tailored or personalized dietary recommendations, \ncomplementing the growing literature on how LLMs can aid in personalized dietary prescription.20-\n25 This also aligns with the increasing interest in precision nutrition.26,27 In parallel with the \nwidespread use of images in nutrition tracking,6-11 text-based LLM solutions may represent a \nsimpler and more accessible alternative, especially in settings where image capture is impractical. \n \nBeyond personal health applications, our findings may also hold implications for research and \npublic health nutrition. The collection and analysis of 24-hour dietary recalls are typically labor- \nand time-intensive processes requiring trained interviewers and specialized coding. Our results \nsuggest that, with appropriate fine-tuning, LLMs could automate and streamline this process—\nreceiving plain-text dietary inputs and instantly returning estimates of energy and macronutrient \nintake. This would represent a major reduction in the time, cost, and expertise traditionally \nrequired for dietary data analysis. \n \nFuture research should evaluate this approach using free-text input provided directly by lay \nindividuals, including more diverse food descriptions, regional or culturally specific foods, and \nlanguages other than English. Additionally, comparisons across different LLM architectures—\n13 \n \nincluding commercial models—could help identify the most effective models for dietary analysis, \nalthough cost considerations may limit the scalability of proprietary solutions. Further work is also \nwarranted to expand prediction targets beyond macronutrients, incorporating micronutrients and \nclinically relevant nutrients such as sodium. Finally, there is an unmet need for domain-specific \nLLMs trained explicitly for nutrition science. While a few LLMs tailored for medicine now exist \n(e.g., Meditron28), we are unaware of any that focus specifically on dietary assessment or \nnutritional guidance—a promising area for future model development. \n \nConclusion \nWhen prompted using a chain-of-thought approach and fine-tuned through parameter-efficient \nmethods, LLMs exposed solely to text data describing foods and quantities consumed can \naccurately predict energy and five macronutrient values. This was demonstrated using 24-hour \ndietary recall data from a nationally representative sample of adolescents. These findings \nhighlight the potential of fine-tuned, open-source LLMs as scalable tools for dietary assessment \nusing text input, without the need for images or specialized hardware. \n \n14 \n \nTABLES \nTable 1. Distribution of energy and macronutrients in the pooled dataset. \n \n \nDRxIKCAL DRxIPROT DRxICARB DRxISUGAR \nDRxIFIBE \nDRxITFAT \nMean \n1957 \n73.77 \n251.80 \n114.38 \n13.56 \n74.34 \nMedian \n1811 \n66.29 \n231.30 \n101.44 \n11.80 \n66.51 \n1st Qu. \n1296 \n45.55 \n162.60 \n63.34 \n7.70 \n43.80 \n3rd Qu. \n2444 \n93.41 \n318.00 \n149.35 \n17.40 \n94.91 \nMin. \n0 \n0 \n0 \n0 \n0 \n0 \nMax. \n9119 \n555.67 \n1416.5 \n952.98 \n86.30 \n425.54 \nSD \n946.29 \n40.99 \n127.48 \n73.04 \n8.49 \n43.46 \n1st and 3rd Qu. refer to the first and third quartiles. Min: minimum value. Max: maximum value. SD: \nstandard deviation. \n \n15 \n \nTable 2. Predictions using vanilla and the fine-tuned model. \n \n \nData subset #2 - Vanilla model \n(N=1,005) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n912441.584 \n1046.947 \n18906.107 \n5340.230 \n46.667 \n1731.315 \nMAE \n652.081 \n23.088 \n94.299 \n51.474 \n4.901 \n28.340 \nMAPE \n0.290 \n0.391 \n0.309 \n0.419 \n0.493 \n0.379 \nRMSE \n955.218 \n32.357 \n137.500 \n73.077 \n6.831 \n41.609 \nR2 \n0.105 \n0.398 \n0.041 \n0.146 \n0.412 \n0.116 \nT-test p-value \n<0.001 \n0.002 \n<0.001 \n<0.001 \n0.929 \n<0.001 \nLin's CCC \n0.463 \n0.385 \n0.326 \n0.171 \n0.133 \n0.113 \n \nData subset #2 - Fine-tuned model \n(N=1,005) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n62810.385 \n126.909 \n1482.704 \n563.437 \n8.525 \n212.702 \nMAE \n180.548 \n7.716 \n29.835 \n16.896 \n1.948 \n9.134 \nMAPE \n0.094 \n0.113 \n0.121 \n0.140 \n0.157 \n0.124 \nRMSE \n250.620 \n11.265 \n38.506 \n23.737 \n2.920 \n14.584 \nR2 \n0.938 \n0.927 \n0.925 \n0.910 \n0.893 \n0.891 \nT-test p-value \n0.001 \n0.015 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \nLin's CCC \n0.955 \n0.948 \n0.947 \n0.946 \n0.921 \n0.916 \n \nData subset #3 - Fine-tuned model \n(N=1,072) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n61449.243 \n108.671 \n1718.436 \n874.721 \n11.492 \n198.861 \nMAE \n175.312 \n7.631 \n28.940 \n16.522 \n2.097 \n9.484 \nMAPE \n0.096 \n0.122 \n0.126 \n0.149 \n0.162 \n0.139 \nRMSE \n247.890 \n10.425 \n41.454 \n29.576 \n3.390 \n14.102 \nR2 \n0.933 \n0.930 \n0.902 \n0.866 \n0.836 \n0.895 \nT-test p-value \n<0.001 \n<0.001 \n<0.001 \n0.002 \n<0.001 \n<0.001 \nLin's CCC \n0.954 \n0.950 \n0.933 \n0.928 \n0.917 \n0.877 \n \nData subset #4 - Fine-tuned model \n(N=1,072) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n61046.742 \n119.346 \n1429.177 \n485.027 \n13.671 \n221.262 \nMAE \n181.072 \n7.791 \n29.123 \n15.895 \n2.151 \n9.659 \nMAPE \n0.103 \n0.120 \n0.134 \n0.176 \n0.171 \n0.139 \nRMSE \n247.076 \n10.925 \n37.805 \n22.023 \n3.697 \n14.875 \nR2 \n0.937 \n0.930 \n0.915 \n0.911 \n0.812 \n0.890 \nT-test p-value \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \nLin's CCC \n0.955 \n0.950 \n0.947 \n0.936 \n0.923 \n0.849 \n \nData subset #5 - Fine-tuned model \n(N=1,074) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n64264.381 \n133.774 \n1692.458 \n687.688 \n14.765 \n190.105 \n16 \n \nMAE \n171.342 \n8.094 \n27.697 \n17.069 \n2.239 \n9.368 \nMAPE \n0.099 \n0.128 \n0.128 \n0.172 \n0.170 \n0.144 \nRMSE \n253.504 \n11.566 \n41.140 \n26.224 \n3.843 \n13.788 \nR2 \n0.925 \n0.918 \n0.897 \n0.874 \n0.797 \n0.893 \nT-test p-value \n<0.001 \n<0.001 \n<0.001 \n0.012 \n<0.001 \n0.001 \nLin's CCC \n0.947 \n0.944 \n0.929 \n0.925 \n0.925 \n0.831 \n \nData subset #6 - Fine-tuned model \n(N=1,064) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n63258.827 \n172.389 \n1324.973 \n534.148 \n10.858 \n245.302 \nMAE \n181.718 \n8.549 \n27.010 \n14.937 \n2.233 \n10.458 \nMAPE \n0.105 \n0.129 \n0.124 \n0.163 \n0.163 \n0.166 \nRMSE \n251.513 \n13.130 \n36.400 \n23.112 \n3.295 \n15.662 \nR2 \n0.931 \n0.914 \n0.912 \n0.893 \n0.851 \n0.875 \nT-test p-value \n<0.001 \n<0.001 \n<0.001 \n0.686 \n<0.001 \n0.113 \nLin's CCC \n0.950 \n0.939 \n0.934 \n0.933 \n0.914 \n0.885 \n \nData subset #7 - Fine-tuned model \n(N=1,071) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n63345.123 \n157.469 \n1430.194 \n575.557 \n13.730 \n232.483 \nMAE \n190.414 \n9.141 \n28.813 \n16.409 \n2.514 \n10.522 \nMAPE \n0.122 \n0.152 \n0.154 \n0.202 \n0.187 \n0.182 \nRMSE \n251.685 \n12.549 \n37.818 \n23.991 \n3.705 \n15.247 \nR2 \n0.917 \n0.899 \n0.895 \n0.857 \n0.817 \n0.861 \nT-test p-value \n<0.001 \n<0.001 \n<0.001 \n0.055 \n<0.001 \n0.884 \nLin's CCC \n0.940 \n0.926 \n0.926 \n0.922 \n0.909 \n0.858 \n \nData subset #8 - Fine-tuned model \n(N=1,077) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n58318.585 \n192.412 \n1583.278 \n579.210 \n15.962 \n236.609 \nMAE \n183.805 \n9.657 \n30.426 \n16.356 \n2.607 \n10.808 \nMAPE \n0.118 \n0.154 \n0.161 \n0.225 \n0.192 \n0.178 \nRMSE \n241.492 \n13.871 \n39.790 \n24.067 \n3.995 \n15.382 \nR2 \n0.925 \n0.890 \n0.884 \n0.859 \n0.808 \n0.855 \nT-test p-value \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n0.003 \nLin's CCC \n0.947 \n0.921 \n0.921 \n0.919 \n0.905 \n0.859 \n \nData subset #9 - Fine-tuned model \n(N=1,075) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n67077.364 \n181.110 \n1661.594 \n726.481 \n14.328 \n311.955 \nMAE \n190.994 \n9.332 \n29.906 \n16.840 \n2.597 \n11.425 \nMAPE \n0.118 \n0.166 \n0.163 \n0.265 \n0.204 \n0.165 \nRMSE \n258.993 \n13.458 \n40.763 \n26.953 \n3.785 \n17.662 \nR2 \n0.914 \n0.886 \n0.872 \n0.814 \n0.774 \n0.838 \nPaired t-test p-\nvalue \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \nLin's CCC \n0.940 \n0.918 \n0.909 \n0.896 \n0.878 \n0.843 \n17 \n \n \nData subset #10 - Fine-tuned model \n(N=1,080) \nDRxIKCAL \nDRxIPROT DRxICARB DRxISUGAR DRxIFIBE DRxITFAT \nMSE \n64043.464 \n171.631 \n1610.967 \n638.163 \n12.324 \n301.646 \nMAE \n184.221 \n9.344 \n30.424 \n17.063 \n2.417 \n11.483 \nMAPE \n0.122 \n0.175 \n0.180 \n0.275 \n0.214 \n0.167 \nRMSE \n253.068 \n13.101 \n40.137 \n25.262 \n3.511 \n17.368 \nR2 \n0.917 \n0.876 \n0.873 \n0.823 \n0.803 \n0.844 \nT-test p-value \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \n<0.001 \nLin's CCC \n0.943 \n0.912 \n0.906 \n0.902 \n0.874 \n0.869 \nThe t-test p-value refers to the paired t-test between the ground truth and the predictions. Lin’s \nCCC: Lin’s concordance correlation coefficient; MSE: mean square error; MAE: mean absolute \nerror; MAPE: mean absolute percentage error; RMSE: root mean square error. \n \n18 \n \nDECLARATIONS \nAcknowledgements \nWe used a Large Language Model (LLM) for text editing. We wrote a full first draft which was then \npassed to Chat GPT 4.o (free version) with the task of improving clarity and impact. The edited \ntext was verified by the lead author and further edited by all co-authors. \nData sharing \nAll datasets are available at: https://wwwn.cdc.gov/nchs/nhanes/Default.aspx The analysis code \nwill be uploaded to a Figshare repository upon publication.  \nContributions \nRMC-L conceived the idea, conducted the analysis, wrote the first draft and received feedback \nfrom coauthors. \nFunding \nNone. \nConflict of interest \nNone to declare. \nClinical trial registration \nNot a clinical trial. \n \n19 \n \nREFERENCES \n1. \nCofre S, Sanchez C, Quezada-Figueroa G, López-Cortés XA. Validity and accuracy of \nartificial intelligence-based dietary intake assessment methods: a systematic review. Br J Nutr \n2025: 1-13. \n2. \nZheng J, Wang J, Shen J, An R. Artificial Intelligence Applications to Measure Food and \nNutrient Intakes: Scoping Review. J Med Internet Res 2024; 26: e54557. \n3. \nTheodore Armand TP, Nfor KA, Kim JI, Kim HC. Applications of Artificial Intelligence, \nMachine Learning, and Deep Learning in Nutrition: A Systematic Review. Nutrients 2024; 16(7). \n4. \nZhang L, Misir A, Boshuizen H, Ocké M. A Systematic Review and Meta-Analysis of \nValidation Studies Performed on Dietary Record Apps. Adv Nutr 2021; 12(6): 2321-32. \n5. \nEldridge AL, Piernas C, Illner AK, et al. Evaluation of New Technology-Based Tools for \nDietary Intake Assessment-An ILSI Europe Dietary Intake and Exposure Task Force Evaluation. \nNutrients 2018; 11(1). \n6. \nShonkoff E, Cara KC, Pei XA, et al. AI-based digital image dietary assessment methods \ncompared to humans and ground truth: a systematic review. Ann Med 2023; 55(2): 2273497. \n7. \nDalakleidi KV, Papadelli M, Kapolos I, Papadimitriou K. Applying Image-Based Food-\nRecognition Systems on Dietary Assessment: A Systematic Review. Adv Nutr 2022; 13(6): 2590-\n619. \n8. \nHo DKN, Tseng SH, Wu MC, et al. Validity of image-based dietary assessment methods: \nA systematic review and meta-analysis. Clin Nutr 2020; 39(10): 2945-59. \n9. \nBoushey CJ, Spoden M, Zhu FM, Delp EJ, Kerr DA. New mobile methods for dietary \nassessment: review of image-assisted and image-based dietary assessment methods. Proc Nutr \nSoc 2017; 76(3): 283-94. \n10. \nGemming L, Utter J, Ni Mhurchu C. Image-assisted dietary assessment: a systematic \nreview of the evidence. J Acad Nutr Diet 2015; 115(1): 64-77. \n20 \n \n11. \nMartin CK, Nicklas T, Gunturk B, Correa JB, Allen HR, Champagne C. Measuring food \nintake with digital photography. J Hum Nutr Diet 2014; 27 Suppl 1(0 1): 72-81. \n12. \nO'Hara C, Kent G, Flynn AC, Gibney ER, Timon CM. An Evaluation of ChatGPT for \nNutrient Content Estimation from Meal Photographs. Nutrients 2025; 17(4). \n13. \nJi Y, Waki K, Yamauchi T, Nangaku M, Ohe K. Using One-Shot Prompting of Non-Fine-\nTuned Commercial Artificial Intelligence to Assess Nutrients from Photographs of Japanese \nMeals. J Diabetes Sci Technol 2025: 19322968241309889. \n14. \nLo FP, Qiu J, Wang Z, et al. Dietary Assessment With Multimodal ChatGPT: A Systematic \nAnalysis. IEEE J Biomed Health Inform 2024; 28(12): 7577-87. \n15. \nLee H-A, Huang T-T, Yen L-H, et al. Precision Nutrient Management Using Artificial \nIntelligence Based on Digital Data Collection Framework. Applied Sciences, 2022.  (accessed. \n16. \nCenters for Disease Control and Prevention. National Center for Health Statistics. National \nHealth and Nutrition Examination Survey. [internet]. Accessed 10 October 2025. URL: \nhttps://wwwn.cdc.gov/nchs/nhanes/Default.aspx. \n17. \nTerry AL, Chiappa MM, McAllister J, Woodwell DA, Graber JE. Plan and operations of the \nNational Health and Nutrition Examination Survey, August 2021–August 2023. National Center \nfor Health Statistics. Vital Health Stat 1(66). 2024. DOI: https://dx.doi.org/10.15620/ cdc/151927. \n18. \nUnsloth Easily finetyne & train LLMs. [internet]. Accessed 17 June 2025. URL: \nhttps://unsloth.ai/. \n19. \nMenezes MCS, Hoffmann AF, Tan ALM, et al. The potential of Generative Pre-trained \nTransformer 4 (GPT-4) to analyse medical notes in three different languages: a retrospective \nmodel-evaluation study. Lancet Digit Health 2025; 7(1): e35-e43. \n20. \nHaman M, Školník M, Lošťák M. AI dietician: Unveiling the accuracy of ChatGPT's \nnutritional estimations. Nutrition 2024; 119: 112325. \n21. \nBayram HM, Çelik ZM, Barcın Güzeldere HK. Can artificial intelligence (AI) chatbot tools \nbe used effectively for nutritional management in obesity? Nutr Health 2025: 2601060251329070. \n21 \n \n22. \nHieronimus B, Hammann S, Podszun MC. Can the AI tools ChatGPT and Bard generate \nenergy, macro- and micro-nutrient sufficient meal plans for different dietary patterns? Nutr Res \n2024; 128: 105-14. \n23. \nPapastratis I, Konstantinidis D, Daras P, Dimitropoulos K. AI nutrition recommendation \nusing a deep generative model and ChatGPT. Sci Rep 2024; 14(1): 14620. \n24. \nPapastratis I, Stergioulas A, Konstantinidis D, Daras P, Dimitropoulos K. Can ChatGPT \nprovide appropriate meal plans for NCD patients? Nutrition 2024; 121: 112291. \n25. \nYou Q, Li X, Shi L, Rao Z, Hu W. Still a Long Way to Go, the Potential of ChatGPT in \nPersonalized Dietary Prescription, From a Perspective of a Clinical Dietitian. J Ren Nutr 2025. \n26. \nBush CL, Blumberg JB, El-Sohemy A, et al. Toward the Definition of Personalized \nNutrition: A Proposal by The American Nutrition Association. Journal of the American College of \nNutrition 2020; 39(1): 5-15. \n27. \nWu X, Oniani D, Shao Z, et al. A Scoping Review of Artificial Intelligence for Precision \nNutrition. Adv Nutr 2025: 100398. \n28. \nChen Z, Cano AH, Romanou A, et al. Meditron-70b: Scaling medical pretraining for large \nlanguage models. arXiv preprint arXiv:231116079 2023. \n \nSUPPLEMENTARY MATERIALS \n \nLLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a \nparameter-efficient fine-tuning experiment using a 10-shot prompt \n \n \nSupplementary Table 1. Ten-shot Prompt. \nSystem Message: \n\"\"\" \nSYSTEM: \n \nYou are a highly experienced clinical dietitian and nutrition scientist with advanced training in \nmacronutrient metabolism, dietary pattern analysis, and expert proficiency with the Nutrition Data \nSystem for Research (NDSR). \nYour task is to analyze a patient's 24-hour dietary recall and estimate the following six nutritional \nvalues: \n1. Total energy (kcal) \n2. Total protein (g) \n3. Total carbohydrates (g) \n4. Total sugars (g) \n5. Total dietary fiber (g) \n6. Total fat (g) \n \nYou must base your estimates **solely** on the foods listed in the dietary recall. Each food is formatted \nas: \n- 'FOOD NAME (grams or milliliters)' \n- Foods are separated by semicolons. \n \nYou must reason internally using the following process: \n--- \nChain-of-Thought Reasoning: \n1. **Identify Foods and Portions**   \nParse the list of foods and quantities (in grams/mL), identifying each food type and its contribution to \nenergy and macronutrients. \n \n2. **Reference Internal Knowledge from NDSR**   \nUse your mental simulation of the NDSR nutrient database to retrieve nutrient values per 100g for each \nfood item, adjusting proportionally based on the reported intake. \n \n3. **Estimate Nutrient Totals**   \nAccumulate values for each nutrient: total kcal, protein, carbohydrate, sugars, fiber, and fat. \n \n4. **Format Your Output**   \nRound your final estimates to two decimal places and report them in the exact required order and \nformat. \n--- \n \nCritical Output Rules: \n- Your output must be exactly six numeric values separated by semicolons. No additional text, \ncomments, labels, or formatting is allowed. \n- If your answer contains anything else, immediately reprint the six values only. \n- Any output that deviates from this exact six-number, semicolon-separated format will be considered \ninvalid. \n- Failure to follow this format will be considered an invalid output. \n- Do **not** include units, labels, or introductory phrases. \n- Do **not** infer unlisted foods or make assumptions beyond what is provided. \n- You must **always** return a complete prediction, even with minimal input. \n- Output **only** the six values, separated by semicolons, in this exact order: 'kcal; protein; \ncarbohydrate; sugars; fiber; fat' \n- If any nutrient is likely to be zero based on the input, explicitly return '0' for that value. \n- Do **not** include missing values, comments, or formatting variations. \n \nExamples for Calibration: \nPatient Input: \n24-hour dietary recall: MILK, LOW FAT (1%) (76.25); BEEF, NS AS TO CUT, COOKED, NS AS TO FAT \nEATEN (12.56); BEEF, NS AS TO CUT, COOKED, LEAN ONLY EATEN (134); BEEF, NS AS TO CUT, \nCOOKED, LEAN ONLY EATEN (134); TORTILLA, CORN (168); CEREAL, READY-TO-EAT, NFS \n(52.5); APPLE JUICE, 100% (325.5); POTATO, NFS (120); BROCCOLI, COOKED, FROM FRESH, \nFAT NOT ADDED IN COOKING (117); BROCCOLI, COOKED, FROM FRESH, FAT NOT ADDED IN \nCOOKING (117); CARROTS, COOKED, FROM FRESH, FAT NOT ADDED IN COOKING (117); SOFT \nDRINK, FRUIT FLAVORED, CAFFEINE FREE (248) \nExpected Output: 1630; 107.97; 233.28; 79.83; 27.7; 33.68 \n \nPatient Input: \n24-hour dietary recall: ICE CREAM, REGULAR, NOT CHOCOLATE (141.31); CHEESE, NFS (24); \nBOLOGNA, NFS (28); SUNFLOWER SEEDS, HULLED, ROASTED, SALTED (46); BREAD, WHITE \n(52); COOKIE, MARSHMALLOW, W/ RICE CEREAL (NO-BAKE) (60); MILK 'N CEREAL BAR (24); \nPASTA W/ TOMATO SAUCE & MEAT/MEATBALLS, CANNED (280.13); SOFT DRINK, FRUIT-\nFLAVORED, CAFFEINE FREE (368) \nExpected Output: 1629; 43.29; 205.67; 113.29; 14.9; 74.29 \n \nPatient Input: \n24-hour dietary recall: CHICKEN NUGGETS, FROM FROZEN (96); CHICKEN TENDERS OR STRIPS, \nBREADED, FROM SCHOOL LUNCH (80); BIG MAC (MCDONALDS) (135); MACARONI OR \nNOODLES WITH CHEESE, MADE FROM PACKAGED MIX (57.5); APPLE, RAW (125); \nSTRAWBERRIES, RAW (108); POTATO, FRENCH FRIES, FAST FOOD (55); POTATO, MASHED, \nFROM SCHOOL LUNCH (62.5); WATER, BOTTLED, PLAIN (20); WATER, BOTTLED, PLAIN (345) \nExpected Output: 1293; 48.28; 135.41; 29.22; 13.2; 62.15 \n \nPatient Input: \n24-hour dietary recall: MILK, COW'S, FLUID, 2% FAT (259.25); CHICKEN, THIGH, STEWED, W/ SKIN \n(88); BREAD, GARLIC (333); RICE, WHITE, COOKED, REGULAR, NO FAT ADD IN COOKING (79); \nFROSTED FLAKES, KELLOGG'S (74.31); PIZZA, CHEESE, THIN CRUST (136.78); PLUM, RAW \n(66); GRAPE JUICE (332.06); FRUIT JUICE DRINK (449.5); FRUIT JUICE DRINK (449.5) \nExpected Output: 2923; 81.63; 443.26; 206.48; 14.8; 93.63 \n \nPatient Input: \n24-hour dietary recall: ICE CREAM CONE, VANILLA, PREPACKAGED (95); CHICKEN, NS AS TO \nPART AND COOKING METHOD, SKIN NOT EATEN (75.94); RICE, WHITE, COOKED, NO ADDED \nFAT (138.25); TACO, MEAT, NO CHEESE (180); CARROTS, RAW (45); TOMATOES, RAW (67.5); \nLETTUCE, RAW (19.69); SOFT DRINK, COLA (264.5); SOFT DRINK, COLA (264.5); WATER, \nBOTTLED, PLAIN (1740) \nExpected Output: 1338; 57.38; 162.67; 81.11; 8; 51.38 \n \nPatient Input: \n24-hour dietary recall: GENERAL TSO CHICKEN (866.88); WAFFLE, FRUIT (78); RICE, FRIED, W/ \nPORK (210.38); SYRUP, DIETETIC (5); GRAPE JUICE DRINK (250) \nExpected Output: 2473; 129.47; 215.26; 71.96; 9.1; 121.12 \n \nPatient Input: \n24-hour dietary recall: MILK, COW'S, FLUID, 1% FAT (533.75); MILK, SOY, READY-TO-DRINK, NOT \nBABY (535.94); CHEESE, NATURAL, CHEDDAR OR AMERICAN TYPE (56.7); HAM, SLICED, \nPREPACKAGED OR DELI, LUNCHEON MEAT (56); CHEESEBURGER, W/ MAYO & \nTOMATO/CATSUP, ON BUN CHEESEBURGER, (314); EGGS, WHOLE, FRIED (INCL SCRAMBLED, \nNO MILK ADDED) (46); PEANUT BUTTER (32); PEANUT BUTTER (32); BREAD, RYE (50); BREAD, \nRYE (25); COOKIE, OATMEAL, W/ RAISINS OR DATES (39); OATMEAL, CKD, INST, MADE W/ \nMILK, FAT NOT ADDED IN COOKING (307.13); RICE, WHITE, COOKED, REGULAR, NO FAT ADD \nIN COOKING (207.38); RICE W/ BEANS AND BEEF (433.19); WHITE POTATO, BOILED, W/O PEEL, \nNS AS TO FAT (516); TOMATOES, RAW (40); LETTUCE, RAW (24); SNICKERS CANDY BAR (17); \nWATER, TAP (9480) \nExpected Output: 4270; 201.78; 503.17; 127.52; 38.9; 164.7 \n \nPatient Input: \n24-hour dietary recall: ICE CREAM, REGULAR, NOT CHOCOLATE (141.31); FISH STICK/FILLET, NS \nTYPE, FLOURED/BREADED, FRIED (51); WHITE POTATO, FRENCH FRIES, FROM FROZEN, \nDEEP-FRIED (60.56); TOMATO CATSUP (15); TOMATO CATSUP (15); FRUIT JUICE DRINK, W/ VIT \nB1 & VIT C (546.88); WATER, BOTTLED, UNSWEETENED (518.44); WATER, BOTTLED, \nUNSWEETENED (518.44) \nExpected Output: 854; 18.29; 126.89; 73.54; 4.5; 31.65 \n \nPatient Input: \n24-hour dietary recall: MILK, LOW FAT (1%) (106.75); PORK, CRACKLINGS, COOKED (51.19); \nPINTO/CALICO/RED MEX BEANS, DRY, CKD, FAT ADD, NS TYPE FAT (100.13); TORTILLA, FLOUR \n(WHEAT) (225); FRUITY PEBBLES CEREAL (52.5); APPLE, RAW (182); WHITE POTATO, CHIPS, \nRESTRUCTURED, BAKED (21); SOFT DRINK, FRUIT-FLAVORED, W/ CAFFEINE (241.5); WATER, \nBOTTLED, UNSWEETENED (2610) \nOutExpected Outputput: 1693; 51.67; 257.79; 83.32; 22; 51.17 \n \nPatient Input: \n24-hour dietary recall: PUDDING, TAPIOCA, MADE FROM DRY MIX, MADE WITH MILK (299.06); \nOYSTERS, COOKED, NS AS TO COOKING METHOD (81.81); BEEF WITH VEGETABLES \nEXCLUDING CARROTS, BROCCOLI, AND DARK-G (132.28); PORK AND VEGETABLES \nEXCLUDING  CARROTS, BROCCOLI, AND DARK-G (132.28); RICE, WHITE, COOKED, NS AS TO \nFAT ADDED IN COOKING (213.94); BEEF NOODLE SOUP, CANNED OR READY-TO-SERVE \n(808.25); TEA, ICED, INSTANT, BLACK, DECAFFEINATED, PRE-SWEETENED WITH (333.5); SOFT \nDRINK, COLA, DECAFFEINATED (372); SOFT DRINK, FRUIT FLAVORED, CAFFEINE FREE (372); \nWATER, BOTTLED, UNSWEETENED (720) \nExpected Output: 1742; 58.25; 278.91; 167.56; 7.9; 45.46 \n \nBegin reasoning internally and return your prediction in the exact required output format. \nDo not explain your reasoning. \nDo not repeat or preface the answer. \nOutput only the final six numbers in this format: kcal; protein; carbohydrate; sugars; fiber; fat. \nDo not prefix with \"Assistant:\" or \"Answer:\". \nOutput the six values ONCE and nothing else. Failure to follow this format will be considered incorrect. \n \n\"\"\" \nUser Message: \n\"\"\" \n \nUSER: \n \nPlease analyze the patient's dietary intake and return the six requested nutrition estimates. \nPatient Input: \n24-hour dietary recall: {diet} \n \nReturn only the six numeric values in this format: \n1234.56; 78.90; 123.45; 67.89; 10.00; 50.00 \nDo not include any text, explanations, or extra formatting. Only output the six numbers, separated by \nsemicolons, rounded to two decimals. \nDo not explain your reasoning. \nDo not repeat or preface the answer. \nOutput only the final six numbers in this format: kcal; protein; carbohydrate; sugars; fiber; fat. \nDo not prefix with \"Assistant:\" or \"Answer:\". \nOutput the six values ONCE and nothing else. Failure to follow this format will be considered incorrect. \n \n\"\"\" \n \n \n \nSupplementary Figure 1. Bland-Altman plot for the vanilla model, data partition #2. \n \n \n \nSupplementary Figure 2. Bland-Altman plot for the fine-tuned model, data partition #2. \n \n \n \nSupplementary Figure 3. Bland-Altman plot for the fine-tuned model, data partition #3. \n \n \n \nSupplementary Figure 4. Bland-Altman plot for the fine-tuned model, data partition #4. \n \n \n \nSupplementary Figure 5. Bland-Altman plot for the fine-tuned model, data partition #5. \n \n \n \nSupplementary Figure 6. Bland-Altman plot for the fine-tuned model, data partition #6. \n \n \n \nSupplementary Figure 7. Bland-Altman plot for the fine-tuned model, data partition #7. \n \n \n \nSupplementary Figure 8. Bland-Altman plot for the fine-tuned model, data partition #8. \n \n \n \nSupplementary Figure 9. Bland-Altman plot for the fine-tuned model, data partition #9. \n \n \n \nSupplementary Figure 10. Bland-Altman plot for the fine-tuned model, data partition #10. \n \n",
    "content": "# LLMs for Energy and Macronutrient Estimation Using Only Text Data: Paper Summary\n\n---\n\n## 1. Core Content and Key Contributions of the Paper\n\nThe central focus of this paper is to explore **whether large language models (LLMs) using only textual input can accurately estimate energy and macronutrient intake from 24-hour dietary recalls**. Based on adolescent data from the U.S. National Health and Nutrition Examination Survey (NHANES), the study employs an open-source quantized LLM—Mistral-Small-24B-Instruct-2501—and uses a \"10-shot prompting + Chain-of-Thought (CoT)\" approach to predict calories, protein, carbohydrates, total sugars, dietary fiber, and total fat based solely on text descriptions of food items and portion sizes (e.g., “chicken nuggets (100g)”).\n\n### Main Contributions:\n\n- **Demonstrates feasibility of text-only LLMs in nutritional assessment**: Unlike mainstream approaches relying on image recognition, this work shows that with structured textual inputs (food name + weight), properly fine-tuned LLMs can achieve high-accuracy nutrient estimation.\n  \n- **Highlights the critical role of Parameter-Efficient Fine-Tuning (PEFT)**: The base (unfine-tuned) LLM performs poorly (e.g., MAE of 652 kcal for energy), but after PEFT, performance improves significantly (MAE drops to ~171–191 kcal), with Lin’s Concordance Correlation Coefficient (CCC) exceeding 0.89—indicating strong clinical-level agreement.\n\n- **Proposes a low-cost, low-burden paradigm for dietary monitoring**: Users need not take photos; simply recording meals via text (like sending a message) enables precise nutritional analysis—ideal for mobile health apps and public health research.\n\n- **Enhances transparency and reproducibility through public datasets and code**: The study leverages NHANES—a nationally representative dataset—and commits to releasing its code, promoting further research and real-world deployment.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### Key Innovations:\n\n| Dimension | Breakthrough / Innovation |\n|--------|----------------------------|\n| **Input Modality Shift** | First systematic validation of **purely text-based input** (no images or multimodal systems) for full 24-hour dietary nutrient estimation, challenging the dominant image-centric AI nutrition paradigm. |\n| **Prompt Engineering Design** | Employs a \"10-shot + Chain-of-Thought\" strategy, guiding the model to simulate a dietitian's reasoning process (identify food → consult database → sum nutrients), improving logical consistency and accuracy. |\n| **Application of PEFT** | Uses lightweight fine-tuning tools like Unsloth on quantized LLMs under resource constraints, achieving major performance gains without full-parameter training—lowering computational barriers and boosting practicality. |\n| **Real-World Data Foundation** | Leverages authentic population-level dietary records from NHANES (not lab-controlled data), ensuring high ecological validity and applicability. |\n| **Rigorous Evaluation Metrics** | Goes beyond common metrics (MAE, R²) by incorporating **Lin’s Concordance Correlation Coefficient (CCC)** and **Bland-Altman plots**, demonstrating medical-grade rigor in assessing agreement. |\n\n> 💡 **Key Insight**: The paper clearly demonstrates that raw, off-the-shelf LLMs perform poorly—highlighting that general-purpose models are insufficient for specialized tasks like nutrition analysis. However, even small-scale fine-tuning leads to dramatic improvements, underscoring the **importance of domain adaptation**. This provides crucial guidance for developing future medical and nutritional AI systems.\n\n---\n\n## 3. Entrepreneurial Opportunities Inspired by This Paper\n\nHere are **five high-potential startup ideas** derived from the core technology and applications explored in the paper, spanning consumer, healthcare, and public policy domains:\n\n---\n\n### 🚀 Startup Idea 1: **TextDiet — Minimalist Diet Tracking App**\n\n#### Concept:\nA **no-photo, text-only diet logging app** where users type entries like “rice (150g), chicken breast (100g), broccoli (80g)” and instantly receive detailed macronutrient breakdowns.\n\n#### Key Advantages:\n- **Extremely user-friendly**: Eliminates hassles of photo-taking, lighting issues, and privacy concerns;\n- **Voice-to-text support**: Further reduces input effort;\n- **Offline functionality**: Lightweight models run locally, enhancing privacy;\n- **Personalized feedback**: Offers tailored advice based on goals (fat loss, muscle gain, blood sugar control).\n\n#### Business Model:\n- Freemium model: Free basic tracking + premium features (nutritional insights, trend reports, exportable summaries)\n- API licensing to fitness platforms and smartwatch manufacturers\n\n#### Technical Path:\n- Adapt the paper’s method to build a Chinese-optimized model (accounting for local food naming conventions)\n- Use LoRA or other PEFT techniques to reduce deployment costs\n- Integrate with Chinese food composition databases (e.g., *Chinese Food Composition Table*)\n\n---\n\n### 🏥 Startup Idea 2: **NutriBot — Medical-Grade Remote Nutrition Management SaaS**\n\n#### Concept:\nAn automated nutrition assessment system for hospitals, clinics, and chronic disease centers. Patients submit daily meal logs via WeChat or mini-programs, and the system generates professional nutrition reports for clinicians.\n\n#### Use Cases:\n- Monitoring carbohydrate intake for diabetic patients\n- Managing protein levels for kidney disease patients\n- Tracking fiber and iron intake during pregnancy\n\n#### Differentiating Features:\n- Handles vague expressions (e.g., “a bowl of rice,” “half a cup of milk”) by converting them into grams\n- Real-time alerts for abnormal intakes (e.g., 30% over daily fat limit)\n- Auto-generates structured EHR-compatible entries\n\n#### Revenue Model:\n- Subscription fees for healthcare institutions\n- Integration with insurance systems for digital chronic care reimbursement\n\n---\n\n### 🌐 Startup Idea 3: **GlobalNutri — Multilingual, Cross-Cultural Nutrition AI Engine**\n\n#### Concept:\nA global platform capable of estimating nutrition across **major languages and regional cuisines**, addressing biases in current models trained primarily on Western/English data.\n\n#### Key Innovations:\n- Builds multilingual food dictionaries (e.g., “ramen” ↔ “拉面”, “injera” ↔ “阿瓦摩西”)\n- Trains models to understand regional cooking methods (e.g., “braised in soy sauce,” “curry stew”)\n- Offers open APIs for international health apps\n\n#### Market Opportunity:\n- Global health apps lack localized nutrition support\n- Potential partnerships with FAO, WHO for nutrition surveillance in developing countries\n\n---\n\n### 📊 Startup Idea 4: **AutoRecall — Automated Epidemiological Research Tool**\n\n#### Concept:\nA service for researchers that **automatically processes free-text 24-hour dietary recall data**, reducing months of manual coding to minutes.\n\n#### Key Features:\n- Converts unstructured dietary notes → standardized nutrient outputs\n- Supports multiple formats (NHANES, CAPI, OCR-scanned surveys)\n- Delivers quality assurance reports (confidence scores, anomaly detection)\n\n#### Target Customers:\n- Public health departments at universities\n- Contract Research Organizations (CROs) conducting clinical trials\n- Government CDCs conducting national nutrition surveys\n\n#### Social Impact:\nDramatically lowers cost and time for large-scale nutritional studies, supporting initiatives like “Healthy China 2030.”\n\n---\n\n### 🧠 Startup Idea 5: **NutriGPT — Domain-Specific Nutrition Large Language Model**\n\n#### Concept:\nDevelop a **specialized large language model dedicated to nutrition science** (Nutrition-LLM), surpassing general-purpose models in accuracy and reliability.\n\n#### Development Roadmap:\n1. **Phase 1**: Apply PEFT to fine-tune existing open-source LLMs using the method in this paper;\n2. **Phase 2**: Collect high-quality food-nutrient paired datasets and pre-train a compact, domain-specific model;\n3. **Phase 3**: Integrate food composition databases and metabolic pathway knowledge graphs to enable **explainable nutritional reasoning**.\n\n#### Strategic Significance:\nFills a critical gap in medical AI—while models like Med-PaLM and Meditron exist for medicine, there is currently **no dedicated Nutrition-LLM**. This could become foundational infrastructure for next-generation precision nutrition.\n\n---\n\n## Conclusion\n\n> 🔍 **One-Sentence Summary of the Paper’s Value**:  \n> This study proves that **open-source LLMs, when fine-tuned efficiently, can accurately estimate dietary nutrients from text alone**, paving the way from the old \"take-a-picture-of-your-meal\" paradigm toward a new era of seamless, conversational nutrition tracking.\n\n> 💡 **Entrepreneurial Insight**:  \n> The biggest opportunity isn’t in copying photo-based apps—but in **reimagining human-AI interaction**. Let users describe their meals naturally in everyday language, while AI silently performs expert-level calculations. This enables truly **invisible, effortless nutrition management**—a vital step toward **accessible, scalable, and sustainable digital health for all**.",
    "github": "",
    "hf": ""
}