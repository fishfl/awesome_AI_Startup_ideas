{
    "id": "/NVIDIA/garak",
    "issues": "288",
    "watch": "45",
    "fork": "581",
    "star": "5.4k",
    "topics": [
        "ai",
        "vulnerability-assessment",
        "security-scanners",
        "llm-security",
        "llm-evaluation"
    ],
    "license": "Apache License 2.0",
    "languages": [
        "Python,99.5%",
        "Jinja,0.5%"
    ],
    "contributors": [
        "https://avatars.githubusercontent.com/u/121934?s=64&v=4",
        "https://avatars.githubusercontent.com/u/7873740?s=64&v=4",
        "https://avatars.githubusercontent.com/u/43585941?s=64&v=4",
        "https://avatars.githubusercontent.com/u/69960846?s=64&v=4",
        "https://avatars.githubusercontent.com/u/19983789?s=64&v=4",
        "https://avatars.githubusercontent.com/u/45014214?s=64&v=4",
        "https://avatars.githubusercontent.com/u/64374762?s=64&v=4",
        "https://avatars.githubusercontent.com/u/5998450?s=64&v=4",
        "https://avatars.githubusercontent.com/u/6442600?s=64&v=4",
        "https://avatars.githubusercontent.com/u/31307962?s=64&v=4",
        "https://avatars.githubusercontent.com/u/25726586?s=64&v=4",
        "https://avatars.githubusercontent.com/u/69222710?s=64&v=4",
        "https://avatars.githubusercontent.com/u/11455652?s=64&v=4"
    ],
    "about": "the LLM vulnerability scanner",
    "is_AI": "y",
    "category": "Network Attack/Protection",
    "summary": "Sure! Here's the English translation of your content in Markdown format:\n\n---\n\n## Project Summary: LLM Vulnerability Scanner **garak**\n\n### 1. Core Content and Problems Solved\n\n**garak** is a security evaluation tool for Large Language Models (LLMs), similar to `nmap` and `Metasploit` in the field of cybersecurity, but focused on red teaming and vulnerability scanning for LLMs.\n\n#### Core Features:\n- **Vulnerability Detection**: garak can detect whether an LLM exhibits the following security risks:\n  - Hallucination\n  - Data leakage\n  - Prompt injection\n  - Misinformation generation\n  - Toxicity\n  - Jailbreaking\n  - Cross-Site Scripting (XSS)\n  - Package hallucination\n  - Malware generation\n  - ...and many other failure modes\n\n- **Multi-Model Support**: Compatible with various LLM interfaces including Hugging Face, OpenAI, Replicate, Groq, Cohere, NVIDIA NIM, ggml, and REST APIs.\n\n- **Logging and Reporting**: Automatically generates detailed logs, JSONL-formatted test reports, and hit records for easy security analysis.\n\n#### Problems Solved:\n- **Difficulty in LLM Security Assessment**: Helps developers and enterprises evaluate whether their deployed LLMs have potential risks of being attacked or misused.\n- **Lack of Standardized Testing Tools**: Provides a unified, modular testing framework, enabling comparable security testing across different models.\n- **Automated Red Teaming**: Eliminates the need for manually writing attack prompts, with garak offering automated detection mechanisms to improve test efficiency and coverage.\n\n---\n\n### 2. Breakthroughs and Innovations\n\n- **Standardization of LLM Security Testing**:\n  - First to introduce the concept of traditional cybersecurity tools (e.g., Metasploit) into the LLM security domain.\n  - Offers a unified interface supporting multiple model platforms, forming a general-purpose LLM security testing framework.\n\n- **Modular Plugin Architecture**:\n  - Supports a flexible plugin system (detectors, generators, evaluators, etc.), allowing users to customize test logic.\n  - Facilitates community contributions of new detection modules, fostering an open-source ecosystem.\n\n- **Multiple Detection Strategies**:\n  - Combines static, dynamic, and adaptive detection strategies to enhance accuracy and coverage in vulnerability discovery.\n\n- **Open Source and Openness**:\n  - Fully open source, supports local deployment, and is suitable for enterprise internal security testing.\n  - Provides detailed documentation and developer guides for secondary development and integration.\n\n- **Academic and Industrial Relevance**:\n  - Backed by academic research, ensuring scientific rigor.\n  - Gaining attention from top security conferences like DEF CON, indicating practical application value.\n\n---\n\n### 3. Entrepreneurial Project Ideas Based on garak\n\nBelow are several startup ideas based on garak's features and positioning:\n\n---\n\n#### ðŸš€ Startup Idea 1: LLM Security as a Service\n\n**Description**:\nOffer a cloud-based LLM security testing service where users simply upload their models or API endpoints to receive a comprehensive security evaluation.\n\n**Feature Suggestions**:\n- Support for major LLM platforms (OpenAI, Anthropic, Google, Alibaba Tongyi, etc.)\n- Provides visual reports, risk scores, and remediation suggestions\n- Offers SaaS platform + API integration for CI/CD workflows\n- Subscription-based model with pricing based on models or API calls\n\n**Target Customers**:\n- Enterprise AI teams\n- LLM application development companies\n- Compliance and security teams\n\n---\n\n#### ðŸš€ Startup Idea 2: AI Security Compliance Platform\n\n**Description**:\nBuild an AI security compliance assessment platform aligned with international standards (e.g., ISO/IEC 23894, NIST AI RMF, GDPR), integrating garak as the LLM module.\n\n**Feature Suggestions**:\n- Multi-dimensional risk assessment: security, ethics, bias, privacy, etc.\n- Automatically generates compliance reports in PDF, Word, and other formats\n- Supports private deployment for government and financial industry needs\n- Offers custom plugin development services\n\n**Target Customers**:\n- Financial institutions\n- Healthcare industry\n- Government agencies\n- Legal and compliance departments\n\n---\n\n#### ðŸš€ Startup Idea 3: LLM Security Training and Attack-Defense Simulation Platform\n\n**Description**:\nCreate a hands-on LLM security training and red teaming platform for AI engineers and security professionals, simulating real-world attack scenarios to enhance team awareness and response capabilities.\n\n**Feature Suggestions**:\n- Built-in garak detectors as an attack sample library\n- Provides virtual target environments for users to upload models and practice attacks\n- Includes educational courses, challenge-based tasks, and leaderboards for motivation\n- Customizable internal training platforms for enterprises\n\n**Target Customers**:\n- Universities and training institutions\n- Internal training for tech companies\n- Government and military cybersecurity departments\n\n---\n\n#### ðŸš€ Startup Idea 4: LLM Security Plugin Marketplace\n\n**Description**:\nDevelop a plugin marketplace offering various LLM security protection modules (e.g., input filtering, output sanitization, jailbreak detection), forming an ecosystem complementary to garak.\n\n**Feature Suggestions**:\n- Plugins support multiple deployment methods (API, local, Docker)\n- Offers both open-source and commercial plugins\n- Integrates with garak to link detection results with mitigation recommendations\n- Provides plugin development SDKs and certification mechanisms\n\n**Target Customers**:\n- AI application developers\n- DevOps teams\n- Cloud service providers\n\n---\n\n## Summary\n\ngarak is a highly practical and extensible LLM security assessment tool. Its open-source nature and modular architecture provide a solid foundation for entrepreneurial ventures. Based on its core capabilities, various business models can be developed, such as SaaS platforms, compliance testing, training and simulation, and plugin marketplaces, catering to diverse industries and user groups.\n\n--- \n\nLet me know if you'd like this translated into another format (e.g., PDF, HTML) or need a version tailored for a specific audience (e.g., investors, developers).",
    "text": "garak, LLM vulnerability scanner\nGenerative AI Red-teaming & Assessment Kit\ngarak\nchecks if an LLM can be made to fail in a way we don't want.\ngarak\nprobes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know\nnmap\nor\nmsf\n/ Metasploit Framework, garak does somewhat similar things to them, but for LLMs.\ngarak\nfocuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.\ngarak\n's a free tool. We love developing it and are always interested in adding functionality to support applications.\nGet started\n> See our user guide!\ndocs.garak.ai\n> Join our\nDiscord\n!\n> Project links & home:\ngarak.ai\n> Twitter:\n@garak_llm\n> DEF CON\nslides\n!\nLLM support\ncurrently supports:\nhugging face hub\ngenerative models\nreplicate\ntext models\nopenai api\nchat & continuation models\nlitellm\npretty much anything accessible via REST\ngguf models like\nllama.cpp\nversion >= 1046\n.. and many more LLMs!\nInstall:\ngarak\nis a command-line tool. It's developed in Linux and OSX.\nStandard install with\npip\nJust grab it from PyPI and you should be good to go:\npython -m pip install -U garak\nInstall development version with\npip\nThe standard pip version of\ngarak\nis updated periodically. To get a fresher version from GitHub, try:\npython -m pip install -U git+https://github.com/NVIDIA/garak.git@main\nClone from source\ngarak\nhas its own dependencies. You can to install\ngarak\nin its own Conda environment:\nconda create --name garak \"python>=3.10,<=3.12\"\nconda activate garak\ngh repo clone NVIDIA/garak\ncd garak\npython -m pip install -e .\nOK, if that went fine, you're probably good to go!\nNote\n: if you cloned before the move to the\nNVIDIA\nGitHub organisation, but you're reading this at the\ngithub.com/NVIDIA\nURI, please update your remotes as follows:\ngit remote set-url origin https://github.com/NVIDIA/garak.git\nGetting started\nThe general syntax is:\ngarak <options>\ngarak\nneeds to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:\ngarak --list_probes\nTo specify a generator, use the\n--model_type\nand, optionally, the\n--model_name\noptions. Model type specifies a model family/interface; model name specifies the exact model to be used. The \"Intro to generators\" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set\n--model_type\nto\nhuggingface\nand\n--model_name\nto the model's name on Hub (e.g.\n\"RWKV/rwkv-4-169m-pile\"\n). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.\ngarak\nruns all the probes by default, but you can be specific about that too.\n--probes promptinject\nwill use only the\nPromptInject\nframework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a\n.\n; for example,\n--probes lmrc.SlurUsage\nwill use an implementation of checking for models generating slurs based on the\nLanguage Model Risk Cards\nframework.\nFor help and inspiration, find us on\nTwitter\nor\ndiscord\n!\nExamples\nProbe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)\nexport OPENAI_API_KEY=\"sk-123XXXXXXXXXXXX\"\npython3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding\nSee if the Hugging Face version of GPT2 is vulnerable to DAN 11.0\npython3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0\nReading the results\nFor each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.\nHere are the results with the\nencoding\nmodule on a GPT-3 variant:\nAnd the same results for ChatGPT:\nWe can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections.  The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.\nErrors go in\ngarak.log\n; the run is logged in detail in a\n.jsonl\nfile specified at analysis start & end. There's a basic analysis script in\nanalyse/analyse_log.py\nwhich will output the probes and prompts that led to the most hits.\nSend PRs & open issues. Happy hunting!\nIntro to generators\nHugging Face\nUsing the Pipeline API:\n--model_type huggingface\n(for transformers models to run locally)\n--model_name\n- use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!\nUsing the Inference API:\n--model_type huggingface.InferenceAPI\n(for API-based model access)\n--model_name\n- the model name from Hub, e.g.\n\"mosaicml/mpt-7b-instruct\"\nUsing private endpoints:\n--model_type huggingface.InferenceEndpoint\n(for private endpoints)\n--model_name\n- the endpoint URL, e.g.\nhttps://xxx.us-east-1.aws.endpoints.huggingface.cloud\n(optional) set the\nHF_INFERENCE_TOKEN\nenvironment variable to a Hugging Face API token with the \"read\" role; see\nhttps://huggingface.co/settings/tokens\nwhen logged in\nOpenAI\n--model_type openai\n--model_name\n- the OpenAI model you'd like to use.\ngpt-3.5-turbo-0125\nis fast and fine for testing.\nset the\nOPENAI_API_KEY\nenvironment variable to your OpenAI API key (e.g. \"sk-19763ASDF87q6657\"); see\nhttps://platform.openai.com/account/api-keys\nwhen logged in\nRecognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.\nReplicate\nset the\nREPLICATE_API_TOKEN\nenvironment variable to your Replicate API token, e.g. \"r8-123XXXXXXXXXXXX\"; see\nhttps://replicate.com/account/api-tokens\nwhen logged in\nPublic Replicate models:\n--model_type replicate\n--model_name\n- the Replicate model name and hash, e.g.\n\"stability-ai/stablelm-tuned-alpha-7b:c49dae36\"\nPrivate Replicate endpoints:\n--model_type replicate.InferenceEndpoint\n(for private endpoints)\n--model_name\n- username/model-name slug from the deployed endpoint, e.g.\nelim/elims-llama2-7b\nCohere\n--model_type cohere\n--model_name\n(optional,\ncommand\nby default) - The specific Cohere model you'd like to test\nset the\nCOHERE_API_KEY\nenvironment variable to your Cohere API key, e.g. \"aBcDeFgHiJ123456789\"; see\nhttps://dashboard.cohere.ai/api-keys\nwhen logged in\nGroq\n--model_type groq\n--model_name\n- The name of the model to access via the Groq API\nset the\nGROQ_API_KEY\nenvironment variable to your Groq API key, see\nhttps://console.groq.com/docs/quickstart\nfor details on creating an API key\nggml\n--model_type ggml\n--model_name\n- The path to the ggml model you'd like to load, e.g.\n/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin\nset the\nGGML_MAIN_PATH\nenvironment variable to the path to your ggml\nmain\nexecutable\nREST\nrest.RestGenerator\nis highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See\nhttps://reference.garak.ai/en/latest/garak.generators.rest.html\nfor examples.\nNIM\nUse models from\nhttps://build.nvidia.com/\nor other NIM endpoints.\nset the\nNIM_API_KEY\nenvironment variable to your authentication API token, or specify it in the config YAML\nFor chat models:\n--model_type nim\n--model_name\n- the NIM\nmodel\nname, e.g.\nmeta/llama-3.1-8b-instruct\nFor completion models:\n--model_type nim.NVOpenAICompletion\n--model_name\n- the NIM\nmodel\nname, e.g.\nbigcode/starcoder2-15b\nTest\n--model_type test\n(alternatively)\n--model_name test.Blank\nFor testing. This always generates the empty string, using the\ntest.Blank\ngenerator.  Will be marked as failing for any tests that\nrequire\nan output, e.g. those that make contentious claims and expect the model to refute them in order to pass.\n--model_type test.Repeat\nFor testing. This generator repeats back the prompt it received.\nIntro to probes\nProbe\nDescription\nblank\nA simple probe that always sends an empty prompt.\natkgen\nAutomated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2\nfine-tuned\non the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).\nav_spam_scanning\nProbes that attempt to make the model output malicious content signatures\ncontinuation\nProbes that test if the model will continue a probably undesirable word\ndan\nVarious\nDAN\nand DAN-like attacks\ndonotanswer\nPrompts to which responsible language models should not answer.\nencoding\nPrompt injection through text encoding\ngcg\nDisrupt a system prompt by appending an adversarial suffix.\nglitch\nProbe model for glitch tokens that provoke unusual behavior.\ngrandma\nAppeal to be reminded of one's grandmother.\ngoodside\nImplementations of Riley Goodside attacks.\nleakreplay\nEvaluate if a model will replay training data.\nlmrc\nSubsample of the\nLanguage Model Risk Cards\nprobes\nmalwaregen\nAttempts to have the model generate code for building malware\nmisleading\nAttempts to make a model support misleading and false claims\npackagehallucination\nTrying to get code generations that specify non-existent (and therefore insecure) packages.\npromptinject\nImplementation of the Agency Enterprise\nPromptInject\nwork (best paper awards @ NeurIPS ML Safety Workshop 2022)\nrealtoxicityprompts\nSubset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)\nsnowball\nSnowballed Hallucination\nprobes designed to make a model give a wrong answer to questions too complex for it to process\nxss\nLook for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.\nLogging\ngarak\ngenerates multiple kinds of log:\nA log file,\ngarak.log\n. This includes debugging information from\ngarak\nand its plugins, and is continued across runs.\nA report of the current run, structured as JSONL. A new report file is created every time\ngarak\nruns. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's\nstatus\nattribute takes a constant from\ngarak.attempts\nto describe what stage it was made at.\nA hit log, detailing attempts that yielded a vulnerability (a 'hit')\nHow is the code structured?\nCheck out the\nreference docs\nfor an authoritative guide to\ngarak\ncode structure.\nIn a typical run,\ngarak\nwill read a model type (and optionally model name) from the command line, then determine which\nprobe\ns and\ndetector\ns to run, start up a\ngenerator\n, and then pass these to a\nharness\nto do the probing; an\nevaluator\ndeals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.\ngarak/probes/\n- classes for generating interactions with LLMs\ngarak/detectors/\n- classes for detecting an LLM is exhibiting a given failure mode\ngarak/evaluators/\n- assessment reporting schemes\ngarak/generators/\n- plugins for LLMs to be probed\ngarak/harnesses/\n- classes for structuring testing\nresources/\n- ancillary items required by plugins\nThe default operating mode is to use the\nprobewise\nharness. Given a list of probe module names and probe plugin names, the\nprobewise\nharness instantiates each probe, then for each probe reads its\nrecommended_detectors\nattribute to get a list of\ndetector\ns to run on the output.\nEach plugin category (\nprobes\n,\ndetectors\n,\nevaluators\n,\ngenerators\n,\nharnesses\n) includes a\nbase.py\nwhich defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example,\ngarak.generators.openai.OpenAIGenerator\ndescends from\ngarak.generators.base.Generator\n.\nLarger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using\ngarak\n.\nDeveloping your own plugin\nTake a look at how other plugins do it\nInherit from one of the base classes, e.g.\ngarak.probes.base.TextProbe\nOverride as little as possible\nYou can test the new code in at least two ways:\nStart an interactive Python session\nImport the model, e.g.\nimport garak.probes.mymodule\nInstantiate the plugin, e.g.\np = garak.probes.mymodule.MyProbe()\nRun a scan with test plugins\nFor probes, try a blank generator and always.Pass detector:\npython3 -m garak -m test.Blank -p mymodule -d always.Pass\nFor detectors, try a blank generator and a blank probe:\npython3 -m garak -m test.Blank -p test.Blank -d mymodule\nFor generators, try a blank probe and always.Pass detector:\npython3 -m garak -m mymodule -p test.Blank -d always.Pass\nGet\ngarak\nto list all the plugins of the type you're writing, with\n--list_probes\n,\n--list_detectors\n, or\n--list_generators\nFAQ\nWe have an FAQ\nhere\n. Reach out if you have any more questions!\ngarak@nvidia.com\nCode reference documentation is at\ngarak.readthedocs.io\n.\nCiting garak\nYou can read the\ngarak preprint paper\n. If you use garak, please cite us.\n@article{garak,\n  title={{garak: A Framework for Security Probing Large Language Models}},\n  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},\n  year={2024},\n  howpublished={\\url{https://garak.ai}}\n}\n\"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly\"\n- Elim\nFor updates and news see\n@garak_llm\nÂ© 2023- Leon Derczynski; Apache license v2, see\nLICENSE",
    "readme": "# garak, LLM vulnerability scanner\n\n*Generative AI Red-teaming & Assessment Kit*\n\n`garak` checks if an LLM can be made to fail in a way we don't want. `garak` probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know `nmap` or `msf` / Metasploit Framework, garak does somewhat similar things to them, but for LLMs. \n\n`garak` focuses on ways of making an LLM or dialog system fail. It combines static, dynamic, and adaptive probes to explore this.\n\n`garak`'s a free tool. We love developing it and are always interested in adding functionality to support applications. \n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Tests/Linux](https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg)](https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml)\n[![Tests/Windows](https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg)](https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml)\n[![Tests/OSX](https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg)](https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml)\n[![Documentation Status](https://readthedocs.org/projects/garak/badge/?version=latest)](http://garak.readthedocs.io/en/latest/?badge=latest)\n[![arXiv](https://img.shields.io/badge/cs.CL-arXiv%3A2406.11036-b31b1b.svg)](https://arxiv.org/abs/2406.11036)\n[![discord-img](https://img.shields.io/badge/chat-on%20discord-yellow.svg)](https://discord.gg/uVch4puUCs)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/garak)](https://pypi.org/project/garak)\n[![PyPI](https://badge.fury.io/py/garak.svg)](https://badge.fury.io/py/garak)\n[![Downloads](https://static.pepy.tech/badge/garak)](https://pepy.tech/project/garak)\n[![Downloads](https://static.pepy.tech/badge/garak/month)](https://pepy.tech/project/garak)\n\n\n## Get started\n### > See our user guide! [docs.garak.ai](https://docs.garak.ai/)\n### > Join our [Discord](https://discord.gg/uVch4puUCs)!\n### > Project links & home: [garak.ai](https://garak.ai/)\n### > Twitter: [@garak_llm](https://twitter.com/garak_llm)\n### > DEF CON [slides](https://garak.ai/garak_aiv_slides.pdf)!\n\n<hr>\n\n## LLM support\n\ncurrently supports:\n* [hugging face hub](https://huggingface.co/models) generative models\n* [replicate](https://replicate.com/) text models\n* [openai api](https://platform.openai.com/docs/introduction) chat & continuation models\n* [litellm](https://www.litellm.ai/)\n* pretty much anything accessible via REST\n* gguf models like [llama.cpp](https://github.com/ggerganov/llama.cpp) version >= 1046\n* .. and many more LLMs!\n\n## Install:\n\n`garak` is a command-line tool. It's developed in Linux and OSX.\n\n### Standard install with `pip`\n\nJust grab it from PyPI and you should be good to go:\n\n```\npython -m pip install -U garak\n```\n\n### Install development version with `pip`\n\nThe standard pip version of `garak` is updated periodically. To get a fresher version from GitHub, try:\n\n```\npython -m pip install -U git+https://github.com/NVIDIA/garak.git@main\n```\n\n### Clone from source\n\n`garak` has its own dependencies. You can to install `garak` in its own Conda environment:\n\n```\nconda create --name garak \"python>=3.10,<=3.12\"\nconda activate garak\ngh repo clone NVIDIA/garak\ncd garak\npython -m pip install -e .\n```\n\nOK, if that went fine, you're probably good to go!\n\n**Note**: if you cloned before the move to the `NVIDIA` GitHub organisation, but you're reading this at the `github.com/NVIDIA` URI, please update your remotes as follows:\n\n```\ngit remote set-url origin https://github.com/NVIDIA/garak.git\n```\n\n\n## Getting started\n\nThe general syntax is:\n\n`garak <options>`\n\n`garak` needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:\n\n`garak --list_probes`\n\nTo specify a generator, use the `--model_type` and, optionally, the `--model_name` options. Model type specifies a model family/interface; model name specifies the exact model to be used. The \"Intro to generators\" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set `--model_type` to `huggingface` and `--model_name` to the model's name on Hub (e.g. `\"RWKV/rwkv-4-169m-pile\"`). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.\n\n`garak` runs all the probes by default, but you can be specific about that too. `--probes promptinject` will use only the [PromptInject](https://github.com/agencyenterprise/promptinject) framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a `.`; for example, `--probes lmrc.SlurUsage` will use an implementation of checking for models generating slurs based on the [Language Model Risk Cards](https://arxiv.org/abs/2303.18190) framework.\n\nFor help and inspiration, find us on [Twitter](https://twitter.com/garak_llm) or [discord](https://discord.gg/uVch4puUCs)!\n\n## Examples\n\nProbe ChatGPT for encoding-based prompt injection (OSX/\\*nix) (replace example value with a real OpenAI API key)\n \n```\nexport OPENAI_API_KEY=\"sk-123XXXXXXXXXXXX\"\npython3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding\n```\n\nSee if the Hugging Face version of GPT2 is vulnerable to DAN 11.0\n\n```\npython3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0\n```\n\n\n## Reading the results\n\nFor each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.\n\nHere are the results with the `encoding` module on a GPT-3 variant:\n![alt text](https://i.imgur.com/8Dxf45N.png)\n\nAnd the same results for ChatGPT:\n![alt text](https://i.imgur.com/VKAF5if.png)\n\nWe can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections.  The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.\n\nErrors go in `garak.log`; the run is logged in detail in a `.jsonl` file specified at analysis start & end. There's a basic analysis script in `analyse/analyse_log.py` which will output the probes and prompts that led to the most hits.\n\nSend PRs & open issues. Happy hunting!\n\n## Intro to generators\n\n### Hugging Face\n\nUsing the Pipeline API:\n* `--model_type huggingface` (for transformers models to run locally)\n* `--model_name` - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!\n\nUsing the Inference API:\n* `--model_type huggingface.InferenceAPI` (for API-based model access)\n* `--model_name` - the model name from Hub, e.g. `\"mosaicml/mpt-7b-instruct\"`\n\nUsing private endpoints:\n* `--model_type huggingface.InferenceEndpoint` (for private endpoints)\n* `--model_name` - the endpoint URL, e.g. `https://xxx.us-east-1.aws.endpoints.huggingface.cloud`\n\n* (optional) set the `HF_INFERENCE_TOKEN` environment variable to a Hugging Face API token with the \"read\" role; see https://huggingface.co/settings/tokens when logged in\n\n### OpenAI\n\n* `--model_type openai`\n* `--model_name` - the OpenAI model you'd like to use. `gpt-3.5-turbo-0125` is fast and fine for testing.\n* set the `OPENAI_API_KEY` environment variable to your OpenAI API key (e.g. \"sk-19763ASDF87q6657\"); see https://platform.openai.com/account/api-keys when logged in\n\nRecognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.\n\n### Replicate\n\n* set the `REPLICATE_API_TOKEN` environment variable to your Replicate API token, e.g. \"r8-123XXXXXXXXXXXX\"; see https://replicate.com/account/api-tokens when logged in\n\nPublic Replicate models:\n* `--model_type replicate`\n* `--model_name` - the Replicate model name and hash, e.g. `\"stability-ai/stablelm-tuned-alpha-7b:c49dae36\"`\n\nPrivate Replicate endpoints:\n* `--model_type replicate.InferenceEndpoint` (for private endpoints)\n* `--model_name` - username/model-name slug from the deployed endpoint, e.g. `elim/elims-llama2-7b`\n\n### Cohere\n\n* `--model_type cohere`\n* `--model_name` (optional, `command` by default) - The specific Cohere model you'd like to test\n* set the `COHERE_API_KEY` environment variable to your Cohere API key, e.g. \"aBcDeFgHiJ123456789\"; see https://dashboard.cohere.ai/api-keys when logged in\n\n### Groq\n\n* `--model_type groq`\n* `--model_name` - The name of the model to access via the Groq API\n* set the `GROQ_API_KEY` environment variable to your Groq API key, see https://console.groq.com/docs/quickstart for details on creating an API key\n\n### ggml\n\n* `--model_type ggml`\n* `--model_name` - The path to the ggml model you'd like to load, e.g. `/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin`\n* set the `GGML_MAIN_PATH` environment variable to the path to your ggml `main` executable\n\n### REST\n\n`rest.RestGenerator` is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See https://reference.garak.ai/en/latest/garak.generators.rest.html for examples.\n\n### NIM\n\nUse models from https://build.nvidia.com/ or other NIM endpoints.\n* set the `NIM_API_KEY` environment variable to your authentication API token, or specify it in the config YAML\n\nFor chat models:\n* `--model_type nim`\n* `--model_name` - the NIM `model` name, e.g. `meta/llama-3.1-8b-instruct`\n\nFor completion models:\n* `--model_type nim.NVOpenAICompletion`\n* `--model_name` - the NIM `model` name, e.g. `bigcode/starcoder2-15b`\n\n\n### Test\n\n* `--model_type test`\n* (alternatively) `--model_name test.Blank`\nFor testing. This always generates the empty string, using the `test.Blank` generator.  Will be marked as failing for any tests that *require* an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.\n\n* `--model_type test.Repeat`\nFor testing. This generator repeats back the prompt it received.\n\n## Intro to probes\n\n| Probe                | Description                                                                                                                   |\n|----------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| blank                | A simple probe that always sends an empty prompt.                                                                             |\n| atkgen               | Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 [fine-tuned](https://huggingface.co/garak-llm/artgpt2tox) on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now). |\n| av_spam_scanning     | Probes that attempt to make the model output malicious content signatures                                                     |\n| continuation         | Probes that test if the model will continue a probably undesirable word                                                       |\n| dan                  | Various [DAN](https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html) and DAN-like attacks                                 |\n| donotanswer          | Prompts to which responsible language models should not answer.                                                               |\n| encoding             | Prompt injection through text encoding                                                                                        |\n| gcg                  | Disrupt a system prompt by appending an adversarial suffix.                                                                   |\n| glitch               | Probe model for glitch tokens that provoke unusual behavior.                                                                  |\n| grandma              | Appeal to be reminded of one's grandmother.                                                                                   |\n| goodside             | Implementations of Riley Goodside attacks.                                                                                    |\n| leakreplay           | Evaluate if a model will replay training data.                                                                                |\n| lmrc                 | Subsample of the [Language Model Risk Cards](https://arxiv.org/abs/2303.18190) probes                                         |\n| malwaregen           | Attempts to have the model generate code for building malware                                                                 |\n| misleading           | Attempts to make a model support misleading and false claims                                                                  |\n| packagehallucination | Trying to get code generations that specify non-existent (and therefore insecure) packages.                                   |\n| promptinject         | Implementation of the Agency Enterprise [PromptInject](https://github.com/agencyenterprise/PromptInject/tree/main/promptinject) work (best paper awards @ NeurIPS ML Safety Workshop 2022) |\n| realtoxicityprompts  | Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)                      |\n| snowball             | [Snowballed Hallucination](https://ofir.io/snowballed_hallucination.pdf) probes designed to make a model give a wrong answer to questions too complex for it to process |\n| xss                  | Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.                           |\n\n## Logging\n\n`garak` generates multiple kinds of log:\n* A log file, `garak.log`. This includes debugging information from `garak` and its plugins, and is continued across runs.\n* A report of the current run, structured as JSONL. A new report file is created every time `garak` runs. The name of this file is output at the beginning and, if successful, also at the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's `status` attribute takes a constant from `garak.attempts` to describe what stage it was made at.\n* A hit log, detailing attempts that yielded a vulnerability (a 'hit')\n\n## How is the code structured?\n\nCheck out the [reference docs](https://reference.garak.ai/) for an authoritative guide to `garak` code structure.\n\nIn a typical run, `garak` will read a model type (and optionally model name) from the command line, then determine which `probe`s and `detector`s to run, start up a `generator`, and then pass these to a `harness` to do the probing; an `evaluator` deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.\n\n* `garak/probes/` - classes for generating interactions with LLMs\n* `garak/detectors/` - classes for detecting an LLM is exhibiting a given failure mode\n* `garak/evaluators/` - assessment reporting schemes\n* `garak/generators/` - plugins for LLMs to be probed\n* `garak/harnesses/` - classes for structuring testing\n* `resources/` - ancillary items required by plugins\n\nThe default operating mode is to use the `probewise` harness. Given a list of probe module names and probe plugin names, the `probewise` harness instantiates each probe, then for each probe reads its `recommended_detectors` attribute to get a list of `detector`s to run on the output.\n\nEach plugin category (`probes`, `detectors`, `evaluators`, `generators`, `harnesses`) includes a `base.py` which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, `garak.generators.openai.OpenAIGenerator` descends from `garak.generators.base.Generator`.\n\nLarger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using `garak`.\n\n\n## Developing your own plugin\n\n* Take a look at how other plugins do it\n* Inherit from one of the base classes, e.g. `garak.probes.base.TextProbe`\n* Override as little as possible\n* You can test the new code in at least two ways:\n  * Start an interactive Python session\n    * Import the model, e.g. `import garak.probes.mymodule`\n    * Instantiate the plugin, e.g. `p = garak.probes.mymodule.MyProbe()`\n  * Run a scan with test plugins\n    * For probes, try a blank generator and always.Pass detector: `python3 -m garak -m test.Blank -p mymodule -d always.Pass`\n    * For detectors, try a blank generator and a blank probe: `python3 -m garak -m test.Blank -p test.Blank -d mymodule`\n    * For generators, try a blank probe and always.Pass detector: `python3 -m garak -m mymodule -p test.Blank -d always.Pass`\n  * Get `garak` to list all the plugins of the type you're writing, with `--list_probes`, `--list_detectors`, or `--list_generators`\n\n\n## FAQ\n\nWe have an FAQ [here](https://github.com/NVIDIA/garak/blob/main/FAQ.md). Reach out if you have any more questions! [garak@nvidia.com](mailto:garak@nvidia.com)\n\nCode reference documentation is at [garak.readthedocs.io](https://garak.readthedocs.io/en/latest/).\n\n## Citing garak\n\nYou can read the [garak preprint paper](garak-paper.pdf). If you use garak, please cite us.\n\n```\n@article{garak,\n  title={{garak: A Framework for Security Probing Large Language Models}},\n  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},\n  year={2024},\n  howpublished={\\url{https://garak.ai}}\n}\n```\n\n<hr>\n\n_\"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly\"_ - Elim\n\nFor updates and news see [@garak_llm](https://twitter.com/garak_llm)\n\nÂ© 2023- Leon Derczynski; Apache license v2, see [LICENSE](LICENSE)\n",
    "author": "NVIDIA",
    "project": "garak",
    "date": "2025-09-14"
}