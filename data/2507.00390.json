{
    "id": "2507.00390",
    "title": "MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE",
    "summary": "This paper proposes a novel expert pruning method called Mixture-of-Novices-and-Experts (MoNE), which achieves effective model compression by replacing redundant experts with lightweight \"novices.\" It demonstrates superior performance over baseline methods across multiple dimensions.",
    "abstract": "Mixture-of-Experts (MoE) enables efficient scaling of large language models by activating only a subset of experts per input token. However, deploying MoE-based models incurs significant memory overhead due to the need to retain all experts in memory. While structured pruning is promising to reduce memory costs, existing methods often show suboptimal performance and unstable degradation in three dimensions: model architectures, calibration data sources, and calibration sample sizes. This paper proposes Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that replaces redundant experts with lightweight novices to achieve effective and robust model compression. MoNE evaluates expert redundancy based on two metrics: access frequency and output variance. Experts exhibiting low usage and stable outputs are pruned and replaced with lightweight novices-unbiased estimations of their original outputs-minimizing performance degradation. Extensive experiments demonstrate that MoNE consistently outperforms baseline methods with minimal accuracy degradation across the three dimensions, confirming its effectiveness and robustness. Notably, it improves the average zero shot accuracy across nine downstream tasks by up to 2.71 under 25\\% pruning ratio and 3.61 under 50\\% pruning. The code is available atthis https URL.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You",
    "subjects": [
        "Machine Learning (cs.LG)"
    ],
    "comments": "",
    "keypoint": "- MoNE is a novel expert pruning method that replaces redundant experts with lightweight novices to compress Mixture-of-Experts (MoE) models effectively.\n- Expert redundancy in MoE models is evaluated based on access frequency and output variance, enabling the identification of less critical experts for replacement.\n- Lightweight novices are introduced as unbiased estimations of original expert outputs, minimizing performance degradation post-pruning.\n- Extensive experiments show that MoNE consistently outperforms existing structured pruning methods across multiple MoE architectures, calibration data sources, and sample sizes.\n- MoNE demonstrates robustness and effectiveness by maintaining higher zero-shot accuracy across nine downstream tasks after pruning.\n- The method improves average zero-shot accuracy by up to 2.71 under a 25% pruning ratio and 3.61 under a 50% pruning ratio.\n- Redundant experts identified by MoNE exhibit consistency across various downstream tasks, indicating reliable redundancy evaluation metrics.\n- Replacing experts with novices adaptively reduces activated parameters for tokens routed to novices, enhancing computational efficiency without significant accuracy loss.\n- MoNE advances the Pareto frontier in model compression, achieving better accuracy versus memory trade-offs than baseline methods.\n- Ablation studies confirm that combining expert redundancy scores with novice replacement yields superior performance, especially under aggressive pruning scenarios.",
    "date": "2025-07-03",
    "paper": "arXiv:2507.00390v1  [cs.LG]  1 Jul 2025\nMoNE: Replacing Redundant Experts with\nLightweight Novices for Structured Pruning of MoE\nGeng Zhang\nYuxuan Han\nYuxuan Lou\nWangbo Zhao\nYiqi Zhang\nYang You\nNational University of Singapore\n{zhangg,youy}@comp.nus.edu.sg, wangbo.zhao96@gmail.com,\n{han_yuxuan, yuxuanlou, yiqi.zhang}@u.nus.edu\nAbstract\nMixture-of-Experts (MoE) enables efficient scaling of large language models by\nactivating only a subset of experts per input token. However, deploying MoE-based\nmodels incurs significant memory overhead due to the need to retain all experts\nin memory. While structured pruning is promising to reduce memory costs, ex-\nisting methods often show suboptimal performance and unstable degradation in\nthree dimensions: model architectures, calibration data sources, and calibration\nsample sizes. This paper proposes Mixture-of-Novices-and-Experts (MoNE), a\nnovel expert pruning method that replaces redundant experts with lightweight\nnovices to achieve effective and robust model compression. MoNE evaluates expert\nredundancy based on two metrics: access frequency and output variance. Experts\nexhibiting low usage and stable outputs are pruned and replaced with lightweight\nnovices—unbiased estimations of their original outputs—minimizing performance\ndegradation. Extensive experiments demonstrate that MoNE consistently out-\nperforms baseline methods with minimal accuracy degradation across the three\ndimensions, confirming its effectiveness and robustness. Notably, it improves the\naverage zero shot accuracy across nine downstream tasks by up to 2.71 under 25%\npruning ratio and 3.61 under 50% pruning ratio1.\n1\nIntroduction\nMixture-of-Experts (MoE) has emerged as a powerful architecture for advancing the capabilities of\nlarge language models (LLMs) [20, 21, 25]. MoE-based LLMs achieve higher parameter efficiency\nthan vanilla transformer-based LLMs by replacing the MLP module with a set of smaller MLP\nmodules (experts) and sparsely activating partial amount of experts for each input token [17]. Despite\nits performance benefits, the deployment of MoE-based models often incur additional memory\noverhead to maintain the non-activated experts in memory, which is valuable but limited for existing\naccelerators such as GPU and TPU [16].\nWhile diverse structured pruning methods have been proposed to reduce deployment memory costs\nby removing different model components while minimizing the performance degradation [32, 12, 34],\nwe observe that these approaches often exhibit suboptimal performance and unstable degradation\nwhen applied to MoE models. Specifically, we identify three critical dimensions where existing\nmethods fall short: model architectures, calibration data sources and calibration sample sizes, as\nshown by experiments in Section 5.3. These limitations are evident across two main categories of\nstructured pruning approaches for MoE models: general structured pruning and expert pruning as\nshown in Figure 1 (a). First, general structured pruning methods that remove model layers (Angular\n[11]) or weight matrix channels (FLAP [2]) fail to account for the sparse computation scheme of\n1The code is available at https://github.com/zxgx/mode-pd.\nPreprint. Under review.\n(a)\n(b)\nExpert pruning\nMC-SMoE, RS\nRouter\n✂️\n✂️\nMoNE\nRouter\nGeneral structured pruning\nLayer pruning\nAngular\n✂️\n✂️\nWeight pruning\nFLAP\n Transformer Layer\n Transformer Layer\n Transformer Layer\n Transformer Layer\n✂️\n✂️\nWeight\nmatrix\nFigure 1: (a) Different structured pruning methods. (b) Layer-wise normalized expert access frequency\nand output variance of Deepseek-V2-Lite for three downstream tasks. Experts with high access\nfrequency or output variances are the same across downstream tasks. Expert in blue circles has both\nhigh frequency and variance. Expert in red circles only has high variance. Expert in green circles\nonly has high frequency. Similar observations on other models and tasks are in Appendix B.\nMoE models when evaluating component importance, resulting in inconsistent performance across\nthe aforementioned three dimensions. Second, existing expert pruning methods such as MC-SMoE\n[18] and RS [12] remove experts from MoE models primarily based on the expert access frequency.\nHowever, as shown in Figure 1 (b), this feature alone fails to fully capture the expert redundancy.\nBesides, these methods lack mechanisms to recover the performance loss caused by pruning.\nTo improve the effectiveness and robustness of structured pruning for MoE models, this paper\nproposes a novel expert pruning method, Mixture-of-Novices-and-Experts (MoNE) which replaces\nredundant experts with a lightweight structure, novice. Specifically, to prune an MoE model with\nMoNE, it first evaluates the expert redundancy by the access frequency and the output variance for\neach expert on a calibration dataset. Then, it identifies and removes redundant experts that show\nlow access frequency and stable output activations. Finally, the unbiased estimation of the expert\noutput is employed as the lightweight novice to reduce the computation and memory overhead for\nthe pruned expert. The intuition behind MoNE is that experts with low access frequency contribute\nless to the final outputs and experts whose outputs have low variance can be replaced with a constant\nbut introduce less discrepancy. Moreover, Figure 1 (b) reveals that experts with less redundancy\nidentified by MoNE exhibit strong consistency across various downstream tasks.\nThe contribution of this paper is summarized as follows:\n• We propose a novel expert pruning method named MoNE which replaces redundant experts\nwith lightweight novices to compress MoE models.\n• We exploit the expert access frequency and output variance to measure the expert redundancy\nand employ the unbiased estimation of the expert output to minimize the output discrepancy\nafter pruning, thus achieving effective and robust pruning results.\n• Extensive experiment results demonstrate that MoNE consistently outperforms baseline\nmethods under varying MoE architectures, calibration data sources and calibration sample\nsizes. Notably, it improves the average zero shot accuracy across nine downstream tasks by\nup to 2.71 with 25% pruning ratio and 3.61 with 50% pruning ratio.\n2\n2\nRelated Work\nModel pruning compresses a model by removing certain redundant model parameters while preserving\naccuracy. Existing pruning methods generally fall into two categories: unstructured pruning and\nstructured pruning. Unstructured pruning eliminates any model parameter that has minimal impact\non model performance. Methods such as SparseGPT [9], Wanda [30], SparseLLM [3] excel in\nmaintaining accuracy while achieving high compression ratios. However, the resulting irregular\nsparsity patterns hinder efficient representation and execution on hardware accelerators.\nIn contrast, structured pruning removes certain modules of a model, preserving hardware-friendly\nstructures. Early researches prune redundant transformer layers of a LLM [8, 19, 11]. LLM-Pruner\n[23] and FLAP [2] remove rows or columns of individual weight matrices. Recent work also proposes\nto delete components such as attention, MLP or MoE modules within each transformer layer [32, 12].\nMinitron [26] and Sheared LLaMA [34] combine different granularity and automatically search for\nthe optimal structures to prune. Despite their versatility, existing structured pruning methods often\nexhibit inconsistent performance across MoE architectures.\nThis work focuses on expert pruning, a unique direction of structured pruning for MoE models\n[17, 20]. Expert pruning targets on deleting individual experts for each layer to compress an MoE\nmodel. Previous expert pruning methods either require exhaustive search to identify redundant experts\n[22], or heavily rely on retraining to recover accuracy due to the suboptimal pruning performance\n[18, 12]. However, the exhaustive search is not applicable to modern MoE model architectures such\nas Deepseek [7, 21] or OLMoE [25], as their MoE layer contains 64 experts or even more, yielding a\ntremendous search space that is intractable. Retraining obscures the advantages of expert pruning\nover other structured pruning methods.\n3\nPreliminaries\n3.1\nMixture-of-Experts (MoE)\nMoE-based LLMs replace the traditional MLP module in the transformer layer with MoE module.\nEach MoE module consists of a router network G and a set of experts E = {E1, E2, . . . , EM}, where\nM is the number of experts and each expert is a smaller MLP. Let x ∈Rd be the hidden state of an\ninput token, where d is the hidden size of the model, the output of an MoE module is computed as:\nMoE(x, G, E) =\nX\nEi∈Sk,x\nGi(x) · Ei(x)\n(1)\nThe output of the router network G(x) ∈RM represents the routing scores for all experts, and\nSk,x ⊆E denotes the top k experts with the highest routing scores for input x. The final output of\nthe MoE module is the weighted sum of outputs from the top k experts. While Equation 1 captures\nthe general MoE computation, implementations for G and Ei may vary across model architectures\n[7, 20, 25, 21].\n3.2\nExpert pruning formulation\nPrevious studies have revealed that not all experts contribute equally, and pruning less important\nones can reduce memory overhead with marginal performance degradation [22, 18, 14]. However,\nsearching for the target experts to prune at the global model perspective falls into a tremendous search\nspace, as the number of experts per transformer layer increases with the evolving of the MoE model\narchitectures [17, 15, 7, 20]. Following the layer-wise pruning scheme [9, 2, 22, 19], our goal of\nexpert pruning is to identify a subset of redundant experts P ⊆E such that we can minimize the\noutput difference after compressing their parameters:\nmin\nP⊆E∥MoE(x, G, E \\ P) −MoE(x, G, E)∥2\n(2)\nTo achieve this goal, the core problem is twofold: (1) find a metric to evaluate the importance of the\nexperts in each layer, so that we can identify the expert subset P, and (2) find an pruning method to\ncompress the parameters of P, so that we can reduce the model size. While E \\ P implies directly\nremoving redundant experts [12, 22], existing methods have also explored expert merging to mitigate\nthe expert redundancy [18].\n3\nMixture of Novices and Experts\nRouter\nAveraged \nexpert outputs\nMixture of Experts\nRouter\n❌❌\n❌\n❌\n❌\n❌\n❌❌\nSample n\nSample 3\nSample 2\nSample 1\nSample n\nSample 3\nSample 2\nSample 1\nRouting scores\nExpert redundancy\nReplaced Experts\nPreserved Experts\nReplace redundant experts\nExpert outputs\nOutput variance\nAccess frequency\nConstructing novices\nFigure 2: The overview of MoNE. Given an MoE model, it first exploits a calibration dataset to\nevaluate the expert access frequency and output variance. Then, the two metrics are fused to get the\nexpert redundancy. Finally, the novices are derived from the averaged outputs for redundant experts.\n4\nMixture of Novices and Experts\nThis section introduces Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method\ndesigned to achieve effective and robust compression for MoE models while minimizing performance\ndegradation. Section 4.1 presents the computational framework of MoNE. Section 4.2 defines the\nmetric to evaluate the redundancy of experts. Section 4.3 explains the pruning process that compresses\nthe redundant experts to lightweight novices. The overview of MoNE is depicted in Figure 2.\n4.1\nMoNE framework\nMoE models are often trained with auxiliary losses to ensure load balance among experts in each layer,\nenabling each expert to learn certain aspect of knowledge [17, 7, 25]. However, most existing expert\npruning methods directly remove experts [12, 18], often leading to inconsistent performance drops\nacross different model architectures or calibration data. MoNE addresses this issue by introducing\nlightweight structures called novices to replace the pruned experts. A novice is designed to capture\nthe essential knowledge previously held by the removed experts. In contrast to simply removing\nredundant experts, MoNE compensates for knowledge loss by leveraging novices, thereby preserving\nthe overall performance of the model while maintaining compression efficiency. Specifically, the\noutput of the MoNE is computed as:\nMoNE(x) =\n\n\nX\nEi∈Sk,x\\P\nGi(x) · Ei(x)\n\n+\n\n\nX\nEi∈Sk,x∩P\nGi(x) · Ni\n\n\n(3)\nwhere Sk,x \\ P and Sk,x ∩P denote the preserved and pruned experts among the top k activated\nexperts respectively. Ni ∈Rd is the novice i that retains the essential knowledge of the pruned expert.\nNotably, Ni is a compressed vector that does not involve any computation with the input token x.\nAs a result, the computation and memory overhead is nearly identical to directly removing experts.\nFurthermore, replacing experts with novices introduces adaptive computation overhead for different\ntokens, leading to fewer activated parameters for tokens routed to novices. Nevertheless, empirical\nresults in Section 5.2 demonstrate that MoE models pruned by MoNE maintain more zero shot\nperformance on downstream tasks compared to existing expert pruning methods that only remove\nexperts but keep the same activated parameters.\n4.2\nExpert redundancy evaluation\nTo identify the expert subset P, we introduce an expert redundancy score ϕ to assess the redundancy\nof experts. To ensure the pruned experts contribute minimally to the model’s overall performance, the\nexpert redundancy score ϕ takes two aspects into consideration: the variance in an expert’s output\nacross a calibration dataset C, and the frequency of an expert selected by the router network G.\n4\nVariance-based redundancy\nAs the novices are constant vectors to ensure reduced computation\nand memory overhead, the outputs of the pruned experts are expected to have low variance across a\ncalibration dataset C. In other words, experts with high output variance should be retained to contribute\nmore discriminative information during inference, whereas experts with low output variance could\nbe compressed into a more efficient representation, i.e., a novice. The second row of Figure 1\n(b) visualizes this motivation. Expert outputs exhibit diverse variances, but we can find experts in\nblue and red circles that maintain high variances across different downstream tasks. Therefore, we\nintroduce a variance-based redundancy ϕvar\ni\nto measure the output variance for expert Ei. Concretely,\nϕvar\ni\nis the L2 norm of the unbiased estimation for the output variance:\nϕvar\ni\n=\n\r\r\r\r\r\r\nsP\nx∈C(Ei(x) −Ei)2 · I(Ei ∈Sk,x)\nP\nx∈C I(Ei ∈Sk,x) −1\n\r\r\r\r\r\r\n2\n(4)\nEi =\nP\nx∈C Ei(x) · I(Ei ∈Sk,x)\nP\nx∈C I(Ei ∈Sk,x)\n(5)\nwhere I(Ei ∈Sk,x) is the indicator function to show whether Ei is among top k experts for the input\ntoken x of the calibration dataset C.\nFrequency-based redundancy\nThe routing scores and access frequencies of the router network\nG serve as strong indicators of the overall redundancy of an expert [12, 18]. Intuitively, experts\nwhich are rarely selected or consistently assigned lower routing scores are likely to have a minimal\nimpact on the model’s output. As shown in Figure 1 (b), we can identify typical experts in blue and\ngreen circles that show consistent high frequency over the three downstream tasks. Notably, the\nexpert in green circles only has high frequency. Therefore, the frequency and variance information\ncan complement the discrepancy ignored by each other. Based on this observation, we define the\nfrequency-based redundancy ϕfreq\ni\nof the expert Ei as the average routing score across a calibration\ndataset C of which Ei is among the top k selected experts. Formally, the frequency-based redundancy\nϕfreq\ni\nis defined as:\nϕfreq\ni\n=\nP\nx∈C Gi(x) · I(Ei ∈Sk,x)\nP\nx∈C I(Ei ∈Sk,x)\n(6)\nFinally, the two redundancy metrics are fused to obtain the expert redundancy score ϕ:\nϕ = ϕvar · ϕfreq\n(7)\nA lower expert redundancy score ϕi indicates higher redundancy for expert Ei, making it a suitable\ncandidate for pruning and replacement with a novice Ni.\n4.3\nExpert replacement with novice\nAfter identifying the pruned expert subset P, we need to construct lightweight novices to replace them.\nAccording to Equation 2, the general objective for expert pruning is to minimize the discrepancy\nintroduced by the removed expert outputs. Since the output after applying MoNE is formulated as\nEquation 3, the concrete objective for MoNE can be translated to:\nmin\nEi∈P∥\nX\nx∈C\n(Ei(x) −Ni)∥2\n(8)\nBecause Ni is a constant vector, the optimal novice vector Ni that best approximates the output of a\npruned expert Ei can be obtained in a closed form, i.e., Ei in Equation 5.\nTo sum up, MoNE uses the unbiased estimations of mean expert outputs to replace experts that have\nthe minimum output variance. As a result, MoNE achieve the goal that effectively and robustly\ncompresses the MoE experts while minimizing performance degradation.\n5\nEvaluation\n5.1\nExperiment setup\nBase MoE models\nTo validate the effectiveness and robustness of MoNE, we conducted structured\npruning on three open source MoE models with diverse architectures and model scales: OLMoE\n5\nTable 1: Zero shot performance with 100 calibration samples from Zyda2 dataset. Best results are in\nbold, and the second best are underlined. Green cells indicate results no less than original models.\n(a) OLMoE\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande\nAvg.\n0%\nOLMoE\n49.23\n76.89\n70.09\n85.00\n53.54\n44.40\n79.76\n71.84\n68.90\n66.63\n25%\nAngular\n32.76\n61.91\n61.71\n74.00\n23.13\n37.60\n71.65\n53.07\n55.09\n52.33\nFLAP\n40.53\n67.55\n62.69\n78.00\n41.16\n37.80\n74.81\n61.37\n60.93\n58.32\nMC-SMoE\n35.67\n54.92\n63.49\n73.00\n29.04\n30.60\n67.19\n55.23\n65.75\n52.77\nRS\n25.85\n43.01\n59.08\n74.00\n29.63\n36.20\n66.16\n56.68\n59.98\n50.07\nMoNE\n42.32\n64.81\n67.19\n85.00\n40.13\n40.80\n78.07\n64.62\n66.46\n61.04\n50%\nAngular\n27.22\n37.50\n53.91\n62.00\n23.96\n26.60\n58.27\n52.35\n51.85\n43.74\nFLAP\n29.18\n54.92\n62.17\n68.00\n30.51\n29.60\n67.57\n55.23\n56.27\n50.39\nMC-SMoE\n24.49\n31.44\n59.33\n67.00\n23.01\n26.00\n53.92\n53.43\n53.12\n43.53\nRS\n21.50\n28.62\n39.45\n61.00\n23.27\n26.00\n52.34\n51.99\n51.85\n39.56\nMoNE\n28.16\n40.24\n63.12\n78.00\n25.21\n32.40\n63.33\n60.65\n63.54\n50.52\n(b) Moonlight\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande\nAvg.\n0%\nMoonlight\n58.28\n82.49\n80.40\n92.00\n67.30\n45.60\n81.12\n65.70\n71.11\n71.56\n25%\nAngular\n39.76\n52.69\n38.90\n79.00\n42.57\n32.20\n68.50\n61.01\n62.04\n52.96\nFLAP\n48.55\n76.01\n75.93\n90.00\n55.84\n42.20\n77.97\n64.26\n68.19\n66.55\nMC-SMoE\n47.61\n73.15\n78.72\n89.00\n46.11\n43.60\n80.36\n56.32\n71.43\n65.14\nRS\n55.80\n80.64\n78.69\n90.00\n46.73\n46.40\n81.01\n58.84\n72.30\n67.82\nMoNE\n55.89\n80.60\n79.57\n90.00\n55.23\n46.80\n80.85\n61.01\n71.98\n69.10\n50%\nAngular\n27.90\n28.54\n48.01\n49.00\n25.67\n28.80\n52.56\n51.99\n47.75\n40.02\nFLAP\n33.87\n61.36\n63.30\n75.00\n36.80\n36.00\n69.37\n57.04\n62.12\n54.98\nMC-SMoE\n29.52\n47.94\n59.54\n79.00\n23.94\n31.40\n67.30\n57.04\n60.46\n50.68\nRS\n37.80\n58.42\n70.86\n89.00\n23.27\n38.00\n78.18\n57.76\n70.80\n58.23\nMoNE\n43.09\n70.03\n76.12\n90.00\n23.57\n40.80\n78.78\n58.84\n70.17\n61.27\n(c) Deepseek-V2-Lite\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande\nAvg.\n0%\nDeepseek-V2-Lite\n48.72\n76.18\n79.88\n88.00\n54.96\n43.60\n80.25\n61.37\n71.51\n67.16\n25%\nAngular\n32.00\n53.28\n64.92\n75.00\n26.95\n34.00\n71.33\n58.84\n61.01\n53.04\nFLAP\n43.69\n71.46\n75.26\n84.00\n47.28\n41.40\n78.18\n62.82\n67.72\n63.53\nMC-SMoE\n36.69\n60.77\n71.31\n84.00\n42.22\n36.60\n75.57\n58.48\n68.67\n59.37\nRS\n49.32\n74.41\n69.39\n90.00\n50.35\n43.80\n80.14\n62.09\n70.24\n65.53\nMoNE\n46.67\n74.62\n78.47\n90.00\n49.05\n43.00\n79.76\n62.09\n71.43\n66.12\n50%\nAngular\n24.06\n32.79\n40.40\n61.00\n23.22\n26.80\n56.42\n57.76\n49.09\n41.28\nFLAP\n35.24\n60.31\n69.66\n79.00\n36.13\n35.20\n74.76\n56.68\n64.09\n56.79\nMC-SMoE\n24.57\n35.82\n56.36\n67.00\n28.29\n27.40\n61.37\n52.35\n53.91\n45.23\nRS\n36.01\n57.45\n57.98\n89.00\n24.91\n40.80\n78.02\n54.15\n62.75\n55.67\nMoNE\n37.20\n67.17\n73.39\n84.00\n42.30\n36.80\n75.30\n59.57\n67.88\n60.40\n[25], Moonlight [21] and DeepSeek-V2-Lite [7]. OLMoE has 7B parameters with 1B activated\nparameters per token. Both Moonlight and Deepseek-V2-Lite have 16B parameters with 3B activated\npamaters per token. OLMoE and Moonlight represent SOTA MoE models at their respective scales.\nWe chose the base version of the three models for experiments2.\nBaseline methods\nWe selected structured pruning methods for different structures as baseline.\nNotably, unless explicitly stated, we did not apply any weight update to compare the effect of pruning\nmethods. Specifically, for general structured pruning methods, we used Angular for layer pruning\n[11], which evaluates the layer importance by the angular distance between the input activations for\ndifferent layers, and we used FLAP for weight pruning [2], which evaluates the channel importance\nby the fluctuation of the input activations and compensates the performance loss with the averaged\noutput activations. For expert pruning methods, we adopted the expert merging method in MC-SMoE\n[18] for one of the expert pruning baselines. Another expert pruning baseline is RS [12], which uses\nrouting scores to evaluate the expert importance and discards less accessed ones. As there are 64\n2OLMoE:\nhttps://huggingface.co/allenai/OLMoE-1B-7B-0125,\nMoonlight:\nhttps:\n//huggingface.co/moonshotai/Moonlight-16B-A3B, Deepseek V2 Lite:\nhttps://huggingface.\nco/deepseek-ai/DeepSeek-V2-Lite\n6\n2\n3\n4\n5\n6\nStd variance of accuracy drop\n50\n55\n60\n65\nAvg accuracy over 9 benchmarks\nAngular\nFLAP\nMC-SMoE\nRS\nMoNE\nVarying model architecture\nGeneral pruning\nExpert pruning\nMoNE\nPareto Frontier\n0.0\n0.5\n1.0\n1.5\nStd variance of accuracy drop\nAngular\nFLAP\nMC-SMoE\nRS\nMoNE\nVaring calibration data source\nGeneral pruning\nExpert pruning\nMoNE\nPareto Frontier\n0.0\n0.5\n1.0\n1.5\nStd variance of accuracy drop\nAngular\nFLAP\nMC-SMoE\nRS\nMoNE\nVarying calibration sample size\nGeneral pruning\nExpert pruning\nMoNE\nPareto Frontier\n(a) 25% pruning ratio.\n2\n3\n4\n5\n6\n7\nStd variance of accuracy drop\n40\n45\n50\n55\nAvg accuracy over 9 benchmarks\nAngular\nFLAP\nMC-SMoE\nRS\nMoNE\nVarying model architecture\nGeneral pruning\nExpert pruning\nMoNE\nPareto Frontier\n0.0\n0.5\n1.0\n1.5\nStd variance of accuracy drop\nAngular\nFLAP\nMC-SMoE\nRS\nMoNE\nVaring calibration data source\nGeneral pruning\nExpert pruning\nMoNE\nPareto Frontier\n0.0\n0.5\n1.0\n1.5\nStd variance of accuracy drop\nAngular\nFLAP\nMC-SMoE\nRS\nMoNE\nVarying calibration sample size\nGeneral pruning\nExpert pruning\nMoNE\nPareto Frontier\n(b) 50% pruning ratio.\nFigure 3: Average accuracy versus accuracy drop variance. MoNE advances the Pareto frontier across\nvarying model architectures, calibration data sources and calibration sample sizes.\nexperts and 8 activated experts per layer for the three MoE models, the two expert pruning methods\nwill not change the activated parameters. In contrast, the proposed MoNE adaptively reduces the\nactivated parameters for tokens routed to novices.\nImplementation details\nWe tested two pruning ratios: 25% and 50%. To demonstrate the robust-\nness of MoNE to calibration data, we conducted experiments on two calibration data sources: Zyda2\n[31] and C4 [27]. Both datasets are constructed for LLM pretraining and C4 is commonly used as the\ncalibration dataset for model pruning [19, 9, 11, 34]. Besides, we also investigated the performance\nunder three calibration sample sizes: 100, 500 and 1000 in Section 5.3.\nTo evaluate performance recovery capabilities, we conducted continued pretraining on the 25%\ncompressed OLMoE model pruned by 100 Zyda2 samples, as only this model releases its pretraining\ndataset, OLMoE-mix-09243. The sequence length was set to 4096 and the global token size per step\nwas 4M. Each pruned model was trained with 2B tokens, i.e., 512 steps, and the peak and minimum\nlearning rate (lr) were 5e-5 and 5e-6, respectively. We employed the cosine lr scheduler with 50 warm\nup steps. Other hyperparameters were the same as the original configuration for OLMoE [25]. All\nthe experiments could run on a single H20 GPU, but we accelerated the training with 16 H20 GPUs.\nFollowing previous researches [23, 3, 19, 34, 2], we adopted lm-evaluation-harness4 [10] to measure\nthe zero shot accuracy and average results on nine downstream tasks: Arc-c and Arc-e [6], BoolQ [5],\nCOPA [28], MMLU [13], OBQA [24], PIQA [4], RTE [33] and Winogrande [29].\n5.2\nEffectiveness evaluation\nThis section validates the effectiveness of MoNE by comparing the zero shot performance with 100\ncalibration samples from the Zyda2 dataset. The results are presented in Table 1. This table indicates\nthat MoNE consistently outperforms baseline methods in terms of the average accuracy on the nine\ntasks. In particular, compared to baseline methods, it improves average accuracy by 2.72 for the 25%\npruned OLMoE and by 3.61 for the 50% pruned Deepseek-V2-Lite. Notably, it incurs accuracy drop\nas small as only 1.04 for the 25% pruned Deepseek-V2-Lite. Furthermore, MoNE-pruned models\nachieves either the best or the second best result for individual tasks only except for the 50%\npruned Moonlight on the MMLU task.\n3https://huggingface.co/datasets/allenai/OLMoE-mix-0924\n4https://github.com/EleutherAI/lm-evaluation-harness\n7\nPruning Ratio 25%\nFalse\nPruning Ratio 50%\nTrue\nTrue\nNovice\nReplacement\nPruning\nMertric\nFigure 4: Ablation study on expert access frequency, output variance and novice replacement.\nNumbers are the difference to the proposed MoNE. The detailed result is provided in Appendix C.\nTable 2: Zero shot performance of the 25% pruned OLMoE after continued pretraining with 2B\ntokens from OLMoE-mix-0924. Best results are in bold, and the second best are underlined.\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande\nAverage\nOLMoE\n49.23\n76.89\n70.09\n85.00\n53.54\n44.40\n79.76\n71.84\n68.90\n66.63\nAngular\n38.82\n64.69\n63.52\n82.00\n25.42\n39.80\n76.50\n51.62\n59.04\n55.71\nFLAP\n42.24\n69.07\n69.51\n80.00\n45.56\n40.40\n77.42\n50.18\n63.54\n59.77\nMC-SMoE\n42.75\n70.41\n69.76\n80.00\n44.13\n37.60\n75.79\n66.43\n64.96\n61.31\nRS\n44.97\n72.94\n70.73\n85.00\n43.28\n43.00\n78.67\n72.20\n65.98\n64.09\nMoNE\n47.35\n74.33\n71.56\n87.00\n43.30\n40.40\n78.89\n67.51\n67.25\n64.18\nAn interesting observation is that all the three expert pruning methods, MC-SMoE, RS and MoNE\ncan achieve results on par with or even better than the original models on certain tasks. The specific\nexamples are shown in green background in Table 1. All these results indicate that there is indeed\nredundancy existing in the expert level for the three pretrained MoE models, and expert pruning\ncan rule out such redundancy to achieve even better results on these tasks. Besides, compared to\nMC-SMoE and RS which keep the same activated parameters, MoNE introduces less activated\nparameters for tokens routed to novices. Nevertheless, MoNE consistently surpasses the two baseline\nmethods, demonstrating its strong capability.\n5.3\nRobustness evaluation\nThis section evaluates the robustness of MoNE across three key dimensions: model architecture\n(OLMoE, Moonlight and Deepseek-V2-Lite), calibration data sources (Zyda2 and C4), and calibration\nsample sizes (100, 500 and 1000). For each dimension, we vary one factor while averaging results\nover the other two, measuring both average accuracy and the standard deviation of accuracy drop.\nThe results are visualized in Figure 3, with detailed scores provided in Appendix A. As shown in\nFigure 3a, MoNE advances the Pareto frontier across all three dimensions at the 25% pruning\nratio, demonstrating superior robustness and effectiveness compared to existing structured pruning\nmethods. At the 50% pruning ratio (Figure 3b), MoNE exhibits slightly higher variance under varying\nmodel architectures and calibration sample sizes. Nevertheless, it remains the most effective method,\noutperforming baseline methods by a significant margin of 2.85.\n5.4\nAblation study\nThis section presents the ablation study to evaluate the effects of the two redundancy metrics and the\nimpact of novice replacement across the downstream tasks. Figure 4 displays the average accuracy\ndrop relative to our proposed methods, with lower values indicating greater degradation. Results are\naveraged over the three evaluation dimensions to provide a robust assessment.\nWe observe that integrating the fused expert redundancy score with novice replacement yields better\nperformance, particularly under higher pruning ratios. This indicates that our approach is especially\neffective in preserving model quality when pruning is more aggressive. Notably, for tasks such\nas BoolQ, COPA, and PIQA, our proposed method outperforms the ablation baselines by a large\nmargin—achieving accuracy gains of up to 8.85. However, for MMLU, pruning based solely on\nfrequency appears to offer a slight advantage, suggesting that frequently activated experts may play a\nmore critical role in domain-specific reasoning tasks.\n8\nOrigin Angular FLAP MC-SMoE\nRS\nMoNE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPruning ratio 25%\nNormalized latency\n1.00\n0.75\n1.01\n0.86\n0.83\n0.94\nOrigin Angular FLAP MC-SMoE\nRS\nMoNE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.00\n0.76\n0.95\n0.94\n0.92\n0.90\nOrigin Angular FLAP MC-SMoE\nRS\nMoNE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.00\n0.78\n1.02\n1.00\n1.04\n0.93\nOrigin Angular FLAP MC-SMoE\nRS\nMoNE\nOLMoE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPruning ratio 50%\nNormalized latency\n1.00\n0.52\n1.03\n0.59\n0.60\n0.93\nOrigin Angular FLAP MC-SMoE\nRS\nMoNE\nMoonlight\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.00\n0.48\n0.95\n0.95\n0.94\n0.85\nOrigin Angular FLAP MC-SMoE\nRS\nMoNE\nDeepseek-V2-Lite\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.00\n0.53\n1.08\n1.03\n1.01\n0.93\n0\n5\n10\n15\n20\n25\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\nMax allocated memory (GB)\n0\n5\n10\n15\n20\n25\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\nMax allocated memory (GB)\nFigure 5: Inference latency and memory footprint over downstream tasks for the three models\nwith different pruning ratios and methods. Latency are normalized with original models. Bars are\nnormalized latency and lines are maximum allocated memory.\n5.5\nAccuracy recovery with continued pretraining\nThis section demonstrates the capability of the MoNE framework with the 25% pruned OLMoE after\ncontinued pretraining. The results are summarized in Table 2. This table shows that MoNE achieves\nthe average accuracy closest to the original model with only 2B tokens from a pretraining dataset,\ndemonstrating the promising capability of the MoNE computation framework. Besides, MC-SMoE\nand RS reclaim 8.54 and 14.02 average accuracy, indicating that expert pruning is not only effective\nto eliminate redundancy, but also relatively easier to recover performance with pretraining.\n5.6\nInference latency and memory footprint\nThis section evaluates the inference latency and memory footprint for pruned models. For each model,\nwe used the original HuggingFace inference implementation. Experiments were conducted on a\nsingle H20 GPU, using the first 20 samples from each downstream task with batch size 1 to eliminate\npadding effects. The maximum generation length was fixed to 256. Though different calibration data\nmight affect the performance, no such variation was observed under this setup.\nFigure 5 depicts the normalized latency and maximum allocated memory for models pruned by\ndifferent methods and pruning ratios. Since the batch size is 1 and generation length is short, the\nmain memory overhead is from model parameters. With the same pruning ratio, different methods\nachieve similar peak memory reduction. In contrast, different structured pruning methods can yield\nvarying generation latency for MoE models. Angular prunes layers, thus achieving latency reduction\nlinear to pruning ratio. RS and MC-SMoE show descent speedup on OLMoE model but cannot\naccelerate other two models. This may because the two models need to move routing scores to CPU\nto decide the expert execution, which downgrades the GPU utilization. Notably, MoE models pruned\nby FLAP cannot show any speedup. This may be attributed to the tile-quantization effect from the\nsmall weight matrices with irregular shapes within each expert [1]. MoNE shows speedup around\n10% over original models, suggesting that a more efficient inference implementation is necessary to\nfully unlock the computational advantages of MoNE. We consider this as promising future work.\n6\nConclusion\nIn this paper, we proposes MoNE, a novel expert pruning method that replaces redundant experts\nwith lightweight novices to compress MoE models. MoNE evaluates the expert redundancy based on\nthe access frequency and output variance of each expert within a transformer layer. Experts with low\nusage and stable output are pruned and replaced by novices, which are the unbiased estimation of\ntheir outputs to compress the model size while incurring minimal performance degradation. Extensive\nexperiments on different MoE model architectures, calibration data sources and calibration sample\nsizes reveal that MoNE exceeds existing structured pruning methods by maintaining higher zero shot\nperformance across nine downstream tasks, demonstrating the effectiveness and robustness of MoNE.\n9\nAcknowledgments and Disclosure of Funding\nUse unnumbered first level headings for the acknowledgments. All acknowledgments go at the\nend of the paper before the list of references. Moreover, you are required to declare funding\n(financial activities supporting the submitted work) and competing interests (related financial activities\noutside the submitted work). More information about this disclosure can be found at: https:\n//neurips.cc/Conferences/2025/PaperInformation/FundingDisclosure.\nDo not include this section in the anonymized submission, only in the final paper. You can use\nthe ack environment provided in the style file to automatically hide this section in the anonymized\nsubmission.\nReferences\n[1] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gula-\nvani, Alexey Tumanov, and Ramachandran Ramjee. Taming {Throughput-Latency} tradeoff in\n{LLM} inference with {Sarathi-Serve}. In 18th USENIX Symposium on Operating Systems\nDesign and Implementation (OSDI 24), pages 117–134, 2024.\n[2] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Fluctuation-based adaptive\nstructured pruning for large language models. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 38, pages 10865–10873, 2024.\n[3] Guangji Bai, Yijiang Li, Chen Ling, Kibaek Kim, and Liang Zhao. SparseLLM: Towards global\npruning of pre-trained language models. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems, 2024.\n[4] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-\nical commonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pages 7432–7439, 2020.\n[5] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npages 2924–2936, 2019.\n[6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457, 2018.\n[7] DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language\nmodel, 2024.\n[8] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\nstructured dropout. In International Conference on Learning Representations, 2020.\n[9] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned\nin one-shot. In International Conference on Machine Learning, pages 10323–10337. PMLR,\n2023.\n[10] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles\nFoster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas\nMuennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,\nLintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework\nfor few-shot language model evaluation, 07 2024.\n[11] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Dan Roberts. The\nunreasonable ineffectiveness of the deeper layers. In The Thirteenth International Conference\non Learning Representations, 2025.\n[12] Shwai He, Daize Dong, Liang Ding, and Ang Li. Demystifying the compression of mixture-of-\nexperts through a unified framework. arXiv preprint arXiv:2406.02500, 2024.\n10\n[13] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In International\nConference on Learning Representations, 2021.\n[14] Wei Huang, Yue Liao, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang, Hongsheng Li,\nSi Liu, and XIAOJUAN QI. Mixture compressor for mixture-of-experts LLMs gains more. In\nThe Thirteenth International Conference on Learning Representations, 2025.\n[15] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,\net al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.\n[16] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil,\nSuvinay Subramanian, Andy Swing, Brian Towles, et al. Tpu v4: An optically reconfigurable\nsupercomputer for machine learning with hardware support for embeddings. In Proceedings of\nthe 50th annual international symposium on computer architecture, pages 1–14, 2023.\n[17] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with\nconditional computation and automatic sharding. In International Conference on Learning\nRepresentations, 2021.\n[18] Pingzhi Li, Zhenyu Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit Bansal, and Tianlong\nChen. Merge, then compress: Demystify efficient SMoe with hints from its routing policy. In\nThe Twelfth International Conference on Learning Representations, 2024.\n[19] Gui Ling, Ziyang Wang, and Qingwen Liu. Slimgpt: Layer-wise structured pruning for large\nlanguage models. Advances in Neural Information Processing Systems, 37:107112–107137,\n2024.\n[20] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437, 2024.\n[21] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin,\nWeixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint\narXiv:2502.16982, 2025.\n[22] Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, and\nHongsheng Li. Not all experts are equal: Efficient expert pruning and skipping for mixture-of-\nexperts large language models. In Proceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 6159–6172, 2024.\n[23] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large\nlanguage models. Advances in neural information processing systems, 36:21702–21720, 2023.\n[24] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pages 2381–2391, 2018.\n[25] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min,\nWeijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita\nBhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers,\nDouwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh\nHajishirzi. OLMoe: Open mixture-of-experts language models. In The Thirteenth International\nConference on Learning Representations, 2025.\n[26] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski,\nMostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov.\nCompact language models via pruning and knowledge distillation. Advances in Neural Informa-\ntion Processing Systems, 37:41076–41102, 2024.\n11\n0\n3\n6\n9\n12\n15\nFrequency\nLayers\nArc-c & Arc-e\nMMLU\nWinogrande\n0 4 8 12162024283236404448525660\nExperts\n0\n3\n6\n9\n12\n15\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 6: Layer-wise normalized expert access frequency and output variance of OLMoE for Arc-C\n& Arc-E, MMLU and Winogrande.\n[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.\n[28] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical\nformalizations of commonsense reasoning, pages 90–95, 2011.\n[29] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\nadversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106,\n2021.\n[30] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning\napproach for large language models. In The Twelfth International Conference on Learning\nRepresentations, 2024.\n[31] Yury Tokpanov, Paolo Glorioso, Ayush Dattagupta, Vibhu Jawa, Ryan Wolf, Vikranth Jeyakumar,\nArham Mehta, Quentin Anthony, and Beren Millidge. Building Zyda-2, a 5 Trillion Token\nHigh-Quality Dataset, with NVIDIA NeMo Curator, October 2024.\n[32] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-\nhead self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Anna\nKorhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics, pages 5797–5808, Florence, Italy, July 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P19-1580.\n[33] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems. Advances in neural information processing systems, 32, 2019.\n[34] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating\nlanguage model pre-training via structured pruning. In The Twelfth International Conference on\nLearning Representations, 2024.\nA\nMore detailed results\nThis section presents the experiment results on Zyda2 dataset with 500 and 1000 samples in Table 3\nand Table 4. Table 5, Table 6 and Table 7 presents the experiment results on C4 dataset with 100, 500\nand 1000 samples. The observations are the similar to those in Section 5.2 and 5.3.\nB\nRedundant expert visualization\nThis section complements the visualization of redundant experts for different models across the\ndownstream tasks. The results are depicted in Figure 6 - Figure 13. As mentioned in Figure 1, for\neach figure, expert in blue circles has both high frequency and variance. Expert in red circles only\n12\nTable 3: Zero shot performance with 500 calibration samples from Zyda2 dataset.\n(a) OLMoE\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nOLMoE\n49.23\n76.89\n70.09\n85.00\n53.54\n44.40\n79.76\n71.84\n68.90\n66.63\n25%\nAngular\n32.76\n61.91\n61.71\n74.00\n23.13\n37.60\n71.65\n53.07\n55.09\n52.33\nFLAP\n37.03\n63.43\n64.28\n81.00\n41.12\n38.80\n72.63\n54.51\n63.54\n57.37\nMC-SMoE\n33.36\n54.46\n70.03\n81.00\n37.05\n33.80\n68.17\n64.62\n65.19\n56.41\nRS\n23.81\n40.91\n57.92\n69.00\n27.79\n30.20\n63.71\n50.18\n57.38\n46.77\nMoNE\n41.04\n65.66\n70.24\n87.00\n41.21\n40.00\n76.61\n64.98\n66.61\n61.48\n50%\nAngular\n27.22\n37.50\n53.91\n62.00\n23.96\n26.60\n58.27\n52.35\n51.85\n43.74\nFLAP\n30.38\n52.99\n62.17\n70.00\n30.91\n33.20\n66.65\n59.21\n57.06\n51.40\nMC-SMoE\n25.43\n32.28\n54.80\n66.00\n22.95\n25.40\n55.22\n54.51\n54.14\n43.41\nRS\n25.34\n28.24\n42.48\n56.00\n23.07\n26.40\n52.34\n52.35\n51.78\n39.78\nMoNE\n26.28\n36.03\n65.17\n75.00\n26.02\n29.00\n61.64\n57.40\n62.67\n48.80\n(b) Moonlight\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nMoonlight\n58.28\n82.49\n80.40\n92.00\n67.30\n45.60\n81.12\n65.70\n71.11\n71.56\n25%\nAngular\n39.76\n52.69\n38.90\n79.00\n42.57\n32.20\n68.50\n61.01\n62.04\n52.96\nFLAP\n48.55\n76.05\n77.49\n89.00\n55.12\n42.40\n76.66\n65.34\n68.59\n66.58\nMC-SMoE\n37.46\n63.47\n76.82\n81.00\n48.27\n35.00\n71.11\n58.84\n70.17\n60.24\nRS\n55.63\n79.46\n78.93\n91.00\n46.60\n45.80\n80.90\n59.93\n72.14\n67.82\nMoNE\n55.03\n78.96\n79.36\n90.00\n54.39\n45.40\n80.69\n58.48\n71.74\n68.23\n50%\nAngular\n27.90\n28.54\n48.01\n49.00\n25.67\n28.80\n52.56\n51.99\n47.75\n40.02\nFLAP\n34.98\n61.32\n65.14\n73.00\n37.93\n35.60\n69.53\n56.68\n62.35\n55.17\nMC-SMoE\n22.87\n29.34\n58.93\n72.00\n23.91\n26.00\n55.06\n54.51\n52.72\n43.93\nRS\n37.71\n59.97\n71.65\n89.00\n25.48\n38.00\n76.22\n57.04\n68.82\n58.21\nMoNE\n38.23\n64.48\n75.90\n87.00\n23.89\n39.20\n77.42\n58.48\n70.56\n59.46\n(c) Deepseek-V2-Lite\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nDeepseek-V2-Lite\n48.72\n76.18\n79.88\n88.00\n54.96\n43.60\n80.25\n61.37\n71.51\n67.16\n25%\nAngular\n32.00\n53.28\n64.92\n75.00\n26.95\n34.00\n71.33\n58.84\n61.01\n53.04\nFLAP\n43.86\n72.18\n75.93\n85.00\n47.22\n41.80\n78.45\n62.09\n68.27\n63.87\nMC-SMoE\n33.53\n52.95\n73.67\n81.00\n41.68\n32.20\n66.70\n52.35\n70.17\n56.03\nRS\n48.98\n73.23\n71.77\n89.00\n52.68\n44.60\n79.33\n61.01\n70.32\n65.66\nMoNE\n44.62\n73.11\n78.01\n90.00\n48.29\n41.80\n79.43\n59.21\n71.35\n65.09\n50%\nAngular\n24.06\n32.79\n40.40\n61.00\n23.22\n26.80\n56.42\n57.76\n49.09\n41.28\nFLAP\n33.11\n60.73\n67.77\n78.00\n31.87\n36.80\n72.91\n55.23\n63.93\n55.59\nMC-SMoE\n25.34\n32.07\n47.98\n59.00\n26.28\n25.60\n56.20\n54.51\n53.75\n42.30\nRS\n38.14\n62.42\n53.03\n86.00\n38.98\n38.80\n74.37\n48.01\n64.64\n56.04\nMoNE\n36.69\n66.04\n73.36\n86.00\n41.07\n35.60\n75.73\n58.12\n69.69\n60.26\n0\n3\n6\n9\n12\n15\nFrequency\nLayers\nOBQA\nPIQA\nRTE\n0 4 8 12162024283236404448525660\nExperts\n0\n3\n6\n9\n12\n15\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 7: Layer-wise normalized expert access frequency and output variance of OLMoE for OBQA,\nPIQA and RTE.\nhas high variance. Expert in green circles only has high frequency. For each model across the nine\ndownstream tasks, we can always identify the same important experts, validating the effectiveness of\nthe redundancy metric, i.e., the expert access frequency and output variance.\n13\nTable 4: Zero shot performance with 1000 calibration samples from Zyda2 dataset.\n(a) OLMoE\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nOLMoE\n49.23\n76.89\n70.09\n85.00\n53.54\n44.40\n79.76\n71.84\n68.90\n66.63\n25%\nAngular\n32.76\n61.91\n61.71\n74.00\n23.13\n37.60\n71.65\n53.07\n55.09\n52.33\nFLAP\n38.91\n66.20\n64.65\n79.00\n40.05\n37.60\n74.65\n62.82\n63.77\n58.63\nMC-SMoE\n38.31\n61.66\n61.87\n73.00\n33.85\n33.20\n66.38\n57.04\n65.43\n54.53\nRS\n26.62\n43.35\n59.76\n70.00\n27.50\n30.00\n65.02\n49.46\n56.67\n47.60\nMoNE\n42.32\n64.52\n66.45\n88.00\n41.55\n40.80\n78.02\n64.26\n67.01\n61.44\n50%\nAngular\n27.22\n37.50\n53.91\n62.00\n23.96\n26.60\n58.27\n52.35\n51.85\n43.74\nFLAP\n30.46\n54.88\n62.32\n70.00\n29.63\n32.00\n67.95\n57.04\n57.54\n51.31\nMC-SMoE\n25.43\n35.31\n55.11\n67.00\n22.92\n25.40\n54.52\n51.99\n51.14\n43.20\nRS\n24.23\n29.50\n41.44\n58.00\n23.45\n24.20\n51.25\n50.54\n51.07\n39.30\nMoNE\n26.71\n37.21\n65.44\n75.00\n26.50\n31.80\n63.22\n55.96\n63.46\n49.48\n(b) Moonlight\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nMoonlight\n58.28\n82.49\n80.40\n92.00\n67.30\n45.60\n81.12\n65.70\n71.11\n71.56\n25%\nAngular\n39.76\n52.69\n38.90\n79.00\n42.57\n32.20\n68.50\n61.01\n62.04\n52.96\nFLAP\n49.49\n76.52\n76.91\n90.00\n54.47\n42.00\n77.26\n65.70\n69.06\n66.82\nMC-SMoE\n34.81\n57.74\n77.22\n85.00\n36.13\n34.60\n71.49\n58.12\n71.35\n58.50\nRS\n56.23\n79.25\n79.02\n91.00\n46.76\n45.20\n80.63\n59.57\n72.14\n67.76\nMoNE\n55.55\n79.29\n79.66\n90.00\n54.79\n45.60\n80.69\n59.21\n72.14\n68.55\n50%\nAngular\n27.90\n28.54\n48.01\n49.00\n25.67\n28.80\n52.56\n51.99\n47.75\n40.02\nFLAP\n36.95\n63.09\n64.56\n71.00\n37.14\n34.80\n69.86\n57.40\n63.22\n55.33\nMC-SMoE\n23.12\n41.04\n61.83\n68.00\n25.44\n26.80\n58.43\n53.07\n53.75\n45.72\nRS\n39.42\n64.56\n70.98\n90.00\n24.76\n40.60\n77.58\n58.12\n68.98\n59.45\nMoNE\n39.42\n65.82\n76.15\n88.00\n23.81\n40.40\n78.40\n58.12\n70.88\n60.11\n(c) Deepseek-V2-Lite\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nDeepseek-V2-Lite\n48.72\n76.18\n79.88\n88.00\n54.96\n43.60\n80.25\n61.37\n71.51\n67.16\n25%\nAngular\n32.00\n53.28\n64.92\n75.00\n26.95\n34.00\n71.33\n58.84\n61.01\n53.04\nFLAP\n44.28\n72.39\n76.12\n85.00\n47.16\n41.20\n78.29\n62.82\n67.40\n63.85\nMC-SMoE\n39.33\n64.31\n71.53\n85.00\n41.84\n40.00\n76.12\n58.48\n69.53\n60.68\nRS\n49.23\n73.32\n71.38\n89.00\n52.44\n45.00\n79.71\n60.29\n71.19\n65.73\nMoNE\n44.71\n73.99\n78.35\n88.00\n49.19\n42.20\n79.27\n59.93\n70.88\n65.17\n50%\nAngular\n24.06\n32.79\n40.40\n61.00\n23.22\n26.80\n56.42\n57.76\n49.09\n41.28\nFLAP\n32.59\n60.77\n68.13\n77.00\n31.43\n36.80\n73.94\n54.87\n61.64\n55.24\nMC-SMoE\n33.19\n47.81\n55.35\n81.00\n24.91\n33.80\n67.52\n51.99\n64.33\n51.10\nRS\n37.12\n61.95\n54.53\n88.00\n39.15\n38.40\n75.46\n51.62\n62.98\n56.58\nMoNE\n37.71\n65.36\n73.49\n84.00\n41.22\n36.60\n75.30\n57.40\n69.53\n60.07\n0\n3\n6\n9\n12\n15\nFrequency\nLayers\nBoolQ\nCOPA\n0 4 8 12162024283236404448525660\nExperts\n0\n3\n6\n9\n12\n15\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 8: Layer-wise normalized expert access frequency and output variance of OLMoE for BoolQ\nand COPA.\n14\nTable 5: Zero shot performance with 100 calibration samples from C4 dataset.\n(a) OLMoE\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nOLMoE\n49.23\n76.89\n70.09\n85.00\n53.54\n44.40\n79.76\n71.84\n68.90\n66.63\n25%\nAngular\n32.76\n61.91\n61.71\n74.00\n23.13\n37.60\n71.65\n53.07\n55.09\n52.33\nFLAP\n36.01\n62.67\n64.83\n75.00\n36.31\n37.00\n75.73\n58.84\n62.75\n56.57\nMC-SMoE\n32.76\n51.05\n54.71\n80.00\n23.05\n37.80\n70.89\n53.43\n66.61\n52.26\nRS\n34.56\n50.38\n63.64\n85.00\n26.51\n39.60\n76.01\n55.96\n64.01\n55.07\nMoNE\n36.69\n55.18\n67.03\n86.00\n25.53\n40.20\n77.80\n56.68\n67.64\n56.97\n50%\nAngular\n27.22\n37.50\n53.91\n62.00\n23.96\n26.60\n58.27\n52.35\n51.85\n43.74\nFLAP\n26.62\n50.51\n62.14\n62.00\n26.32\n28.80\n69.86\n52.71\n57.06\n48.45\nMC-SMoE\n25.68\n32.24\n57.65\n60.00\n22.83\n27.40\n58.60\n52.35\n51.38\n43.12\nRS\n23.12\n31.99\n45.02\n59.00\n23.80\n24.80\n53.59\n51.62\n49.57\n40.28\nMoNE\n26.37\n39.65\n62.29\n75.00\n22.93\n32.00\n66.10\n52.71\n61.01\n48.67\n(b) Moonlight\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nMoonlight\n58.28\n82.49\n80.40\n92.00\n67.30\n45.60\n81.12\n65.70\n71.11\n71.56\n25%\nAngular\n39.76\n52.69\n38.90\n79.00\n42.57\n32.20\n68.50\n61.01\n62.04\n52.96\nFLAP\n46.76\n75.04\n75.90\n85.00\n52.98\n41.00\n78.07\n65.70\n67.40\n65.32\nMC-SMoE\n48.38\n74.20\n78.69\n92.00\n50.41\n44.20\n81.18\n54.87\n71.19\n66.12\nRS\n55.97\n79.76\n78.93\n91.00\n52.74\n46.20\n81.39\n59.21\n72.38\n68.62\nMoNE\n54.27\n79.25\n78.07\n90.00\n53.78\n45.60\n81.34\n57.76\n72.14\n68.02\n50%\nAngular\n27.90\n28.54\n48.01\n49.00\n25.67\n28.80\n52.56\n51.99\n47.75\n40.02\nFLAP\n31.14\n54.63\n62.69\n73.00\n32.26\n30.40\n70.40\n58.12\n60.93\n52.62\nMC-SMoE\n29.10\n52.23\n57.22\n85.00\n22.92\n36.40\n71.71\n52.71\n63.54\n52.31\nRS\n37.80\n58.42\n70.86\n89.00\n23.27\n38.00\n78.18\n57.76\n70.80\n58.23\nMoNE\n33.87\n57.07\n72.75\n90.00\n22.97\n38.80\n78.62\n61.01\n70.17\n58.36\n(c) Deepseek-V2-Lite\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nDeepseek-V2-Lite\n48.72\n76.18\n79.88\n88.00\n54.96\n43.60\n80.25\n61.37\n71.51\n67.16\n25%\nAngular\n32.00\n53.28\n64.92\n75.00\n26.95\n34.00\n71.33\n58.84\n61.01\n53.04\nFLAP\n39.68\n66.58\n76.45\n84.00\n36.14\n39.80\n78.67\n57.04\n67.48\n60.65\nMC-SMoE\n36.52\n59.30\n59.76\n81.00\n36.80\n37.20\n75.30\n54.87\n69.22\n56.66\nRS\n49.32\n74.41\n69.39\n90.00\n50.35\n43.80\n80.14\n62.09\n70.24\n65.53\nMoNE\n47.44\n74.03\n77.71\n90.00\n49.10\n42.60\n80.25\n63.18\n70.24\n66.06\n50%\nAngular\n24.06\n32.79\n40.40\n61.00\n23.22\n26.80\n56.42\n57.76\n49.09\n41.28\nFLAP\n29.95\n52.36\n68.72\n75.00\n23.45\n34.20\n75.46\n52.71\n61.09\n52.55\nMC-SMoE\n28.75\n36.57\n59.45\n82.00\n23.65\n30.60\n64.96\n51.62\n59.75\n48.60\nRS\n36.01\n57.45\n57.98\n89.00\n24.91\n40.80\n78.02\n54.15\n62.75\n55.67\nMoNE\n35.24\n55.47\n68.20\n87.00\n22.94\n35.40\n76.50\n53.43\n68.35\n55.84\n0\n4\n8\n12\n16\n20\n24\nFrequency\nLayers\nArc-c & Arc-e\nMMLU\nWinogrande\n0 4 8 12162024283236404448525660\nExperts\n0\n4\n8\n12\n16\n20\n24\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 9: Layer-wise normalized expert access frequency and output variance of Moonlight for Arc-C\n& Arc-E, MMLU and Winogrande.\nC\nComprehensive ablation study results\nFigure 14 reports the detailed ablation study on the impacts of the three factors: expert access\nfrequency, output variance and novice replacement. The results in this figure validates that the three\nfactors play an important role in maintaining the effectiveness of the pruned models on different\n15\nTable 6: Zero shot performance with 500 calibration samples from C4 dataset.\n(a) OLMoE\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nOLMoE\n49.23\n76.89\n70.09\n85.00\n53.54\n44.40\n79.76\n71.84\n68.90\n66.63\n25%\nAngular\n32.76\n61.91\n61.71\n74.00\n23.13\n37.60\n71.65\n53.07\n55.09\n52.33\nFLAP\n36.26\n63.80\n63.49\n72.00\n35.09\n36.00\n75.35\n53.07\n61.96\n55.23\nMC-SMoE\n26.71\n48.61\n65.90\n69.00\n35.14\n31.00\n61.92\n54.87\n61.09\n50.47\nRS\n34.56\n49.41\n63.85\n86.00\n27.23\n39.80\n75.57\n56.32\n64.40\n55.24\nMoNE\n37.88\n55.89\n65.23\n86.00\n25.03\n41.20\n77.42\n57.40\n67.80\n57.10\n50%\nAngular\n27.22\n37.50\n53.91\n62.00\n23.96\n26.60\n58.27\n52.35\n51.85\n43.74\nFLAP\n26.71\n50.42\n62.20\n64.00\n27.52\n30.20\n70.29\n52.35\n55.09\n48.75\nMC-SMoE\n23.98\n32.07\n61.77\n63.00\n23.01\n25.40\n54.79\n57.40\n52.49\n43.77\nRS\n22.95\n31.78\n49.11\n59.00\n23.75\n23.00\n53.59\n53.43\n50.04\n40.74\nMoNE\n29.01\n42.51\n62.29\n79.00\n22.98\n32.00\n70.84\n52.71\n60.06\n50.16\n(b) Moonlight\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nMoonlight\n58.28\n82.49\n80.40\n92.00\n67.30\n45.60\n81.12\n65.70\n71.11\n71.56\n25%\nAngular\n39.76\n52.69\n38.90\n79.00\n42.57\n32.20\n68.50\n61.01\n62.04\n52.96\nFLAP\n45.22\n73.40\n72.29\n84.00\n53.81\n40.40\n78.13\n64.98\n67.32\n64.40\nMC-SMoE\n48.38\n74.16\n78.56\n89.00\n48.78\n43.00\n81.12\n58.48\n71.03\n65.84\nRS\n55.72\n79.80\n79.20\n91.00\n52.93\n44.60\n81.23\n59.57\n71.74\n68.42\nMoNE\n54.52\n78.87\n78.56\n90.00\n53.80\n45.20\n81.07\n57.04\n71.74\n67.87\n50%\nAngular\n27.90\n28.54\n48.01\n49.00\n25.67\n28.80\n52.56\n51.99\n47.75\n40.02\nFLAP\n31.83\n54.50\n63.18\n74.00\n29.18\n31.40\n71.16\n55.96\n61.88\n52.57\nMC-SMoE\n33.45\n54.59\n63.82\n79.00\n23.95\n33.20\n72.31\n52.71\n61.09\n52.68\nRS\n39.59\n58.67\n70.49\n86.00\n23.36\n38.60\n78.02\n54.15\n70.24\n57.68\nMoNE\n33.28\n56.40\n71.96\n91.00\n22.95\n37.60\n78.73\n58.12\n70.64\n57.85\n(c) Deepseek-V2-Lite\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nDeepseek-V2-Lite\n48.72\n76.18\n79.88\n88.00\n54.96\n43.60\n80.25\n61.37\n71.51\n67.16\n25%\nAngular\n32.00\n53.28\n64.92\n75.00\n26.95\n34.00\n71.33\n58.84\n61.01\n53.04\nFLAP\n40.53\n64.81\n77.43\n84.00\n37.70\n39.60\n79.43\n57.04\n67.32\n60.87\nMC-SMoE\n39.33\n61.99\n60.95\n84.00\n41.37\n38.60\n77.75\n58.48\n62.04\n58.28\nRS\n49.66\n74.49\n65.63\n91.00\n50.04\n43.60\n79.92\n61.01\n69.77\n65.01\nMoNE\n46.59\n73.19\n77.37\n89.00\n48.80\n43.40\n79.87\n62.09\n70.80\n65.68\n50%\nAngular\n24.06\n32.79\n40.40\n61.00\n23.22\n26.80\n56.42\n57.76\n49.09\n41.28\nFLAP\n30.97\n52.27\n68.47\n77.00\n23.16\n36.40\n75.08\n52.71\n62.75\n53.20\nMC-SMoE\n23.55\n35.73\n57.43\n71.00\n23.86\n32.20\n61.10\n51.62\n51.30\n45.31\nRS\n36.95\n58.71\n51.16\n87.00\n24.23\n40.20\n77.64\n54.15\n61.72\n54.64\nMoNE\n34.39\n55.72\n67.68\n84.00\n22.90\n35.60\n76.28\n54.51\n67.72\n55.42\n0\n4\n8\n12\n16\n20\n24\nFrequency\nLayers\nOBQA\nPIQA\nRTE\n0 4 8 12162024283236404448525660\nExperts\n0\n4\n8\n12\n16\n20\n24\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 10: Layer-wise normalized expert access frequency and output variance of Moonlight for\nOBQA, PIQA and RTE.\nmodel architectures, calibration data sources and calibration sample sizes. Fusing the three factors\nensures the robustness of the proposed MoNE.\n16\nTable 7: Zero shot performance with 1000 calibration samples from C4 dataset.\n(a) OLMoE\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nOLMoE\n49.23\n76.89\n70.09\n85.00\n53.54\n44.40\n79.76\n71.84\n68.90\n66.63\n25%\nAngular\n32.76\n61.91\n61.71\n74.00\n23.13\n37.60\n71.65\n53.07\n55.09\n52.33\nFLAP\n36.35\n61.24\n64.50\n72.00\n37.08\n36.00\n75.41\n57.40\n62.59\n55.84\nMC-SMoE\n40.10\n68.22\n61.65\n70.00\n36.91\n39.60\n71.65\n59.57\n57.93\n56.18\nRS\n34.39\n49.41\n64.89\n85.00\n26.64\n40.60\n75.95\n57.04\n63.46\n55.26\nMoNE\n38.74\n57.45\n65.57\n86.00\n24.59\n41.80\n77.42\n56.68\n68.43\n57.41\n50%\nAngular\n27.22\n37.50\n53.91\n62.00\n23.96\n26.60\n58.27\n52.35\n51.85\n43.74\nFLAP\n26.79\n49.92\n62.17\n63.00\n27.34\n31.60\n70.24\n53.43\n56.35\n48.98\nMC-SMoE\n26.96\n41.58\n58.56\n56.00\n23.00\n28.60\n60.23\n52.35\n50.83\n44.23\nRS\n23.81\n32.28\n46.09\n58.00\n23.76\n24.00\n53.59\n51.99\n48.86\n40.26\nMoNE\n29.78\n42.80\n62.32\n80.00\n22.99\n33.60\n71.49\n52.71\n60.06\n50.64\n(b) Moonlight\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nMoonlight\n58.28\n82.49\n80.40\n92.00\n67.30\n45.60\n81.12\n65.70\n71.11\n71.56\n25%\nAngular\n39.76\n52.69\n38.90\n79.00\n42.57\n32.20\n68.50\n61.01\n62.04\n52.96\nFLAP\n45.22\n74.20\n72.78\n86.00\n53.40\n41.20\n78.35\n65.34\n68.90\n65.04\nMC-SMoE\n51.62\n76.52\n77.86\n92.00\n43.08\n43.60\n80.25\n55.60\n71.11\n65.74\nRS\n55.97\n79.59\n78.75\n90.00\n53.01\n45.20\n81.34\n59.57\n71.90\n68.37\nMoNE\n53.75\n79.38\n78.53\n90.00\n53.75\n45.80\n81.39\n57.76\n71.59\n67.99\n50%\nAngular\n27.90\n28.54\n48.01\n49.00\n25.67\n28.80\n52.56\n51.99\n47.75\n40.02\nFLAP\n31.31\n55.26\n63.73\n72.00\n32.98\n33.00\n71.16\n56.32\n62.75\n53.17\nMC-SMoE\n28.75\n49.20\n58.90\n75.00\n23.72\n33.40\n66.32\n53.79\n59.75\n49.87\nRS\n38.74\n58.88\n69.85\n88.00\n23.32\n38.00\n77.80\n61.01\n70.17\n58.42\nMoNE\n34.81\n57.58\n71.96\n92.00\n22.95\n38.60\n78.18\n57.04\n70.72\n58.20\n(c) Deepseek-V2-Lite\nPruning ratio\nModel/Method\nArc-c\nArc-e\nBoolQ\nCOPA\nMMLU\nOBQA\nPIQA\nRTE\nWinogrande Average\n0%\nDeepseek-V2-Lite\n48.72\n76.18\n79.88\n88.00\n54.96\n43.60\n80.25\n61.37\n71.51\n67.16\n25%\nAngular\n32.00\n53.28\n64.92\n75.00\n26.95\n34.00\n71.33\n58.84\n61.01\n53.04\nFLAP\n39.33\n66.50\n76.33\n84.00\n38.14\n40.60\n78.84\n59.93\n67.88\n61.28\nMC-SMoE\n37.29\n58.42\n65.72\n81.00\n37.91\n39.00\n76.12\n55.60\n67.80\n57.65\nRS\n49.23\n74.33\n68.44\n88.00\n51.59\n44.40\n80.25\n62.45\n69.69\n65.38\nMoNE\n45.99\n72.98\n77.43\n89.00\n48.90\n42.60\n79.98\n61.01\n71.59\n65.50\n50%\nAngular\n24.06\n32.79\n40.40\n61.00\n23.22\n26.80\n56.42\n57.76\n49.09\n41.28\nFLAP\n31.66\n53.41\n69.36\n75.00\n23.04\n35.40\n75.41\n52.71\n62.51\n53.17\nMC-SMoE\n28.16\n36.49\n56.94\n81.00\n25.76\n31.80\n64.69\n52.71\n58.09\n48.40\nRS\n39.08\n61.32\n52.54\n83.00\n24.58\n39.40\n77.86\n54.51\n63.46\n55.08\nMoNE\n34.04\n55.51\n68.26\n85.00\n22.92\n36.40\n76.44\n54.15\n66.61\n55.48\n0\n4\n8\n12\n16\n20\n24\nFrequency\nLayers\nBoolQ\nCOPA\n0 4 8 12162024283236404448525660\nExperts\n0\n4\n8\n12\n16\n20\n24\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 11: Layer-wise normalized expert access frequency and output variance of Moonlight for\nBoolQ and COPA.\n17\n0\n4\n8\n12\n16\n20\n24\nFrequency\nLayers\nOBQA\nPIQA\nRTE\n0 4 8 12162024283236404448525660\nExperts\n0\n4\n8\n12\n16\n20\n24\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 12: Layer-wise normalized expert access frequency and output variance of Deepseek-V2-Lite\nfor OBQA, PIQA and RTE.\n0\n4\n8\n12\n16\n20\n24\nFrequency\nLayers\nBoolQ\nCOPA\n0 4 8 12162024283236404448525660\nExperts\n0\n4\n8\n12\n16\n20\n24\nVariance\nLayers\n0 4 8 12162024283236404448525660\nExperts\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 13: Layer-wise normalized expert access frequency and output variance of Deepseek-V2-Lite\nfor BoolQ and COPA.\nD\nLimitations\nThe proposed MoNE is an expert pruning method for the structured pruning of MoE models and the\nexperiments demonstrate excellent performance. However, there are a few limitations that are not\ncovered in this paper. First, we do not conduct experiments on larger MoE models. The largest MoE\nmodels employed in this paper are at the 16B scale. Larger models may require more redundancy\nfeatures other than the two metrics employed in this paper. However, those large models also\nneed more computation resources for experiments. Second, we do not explore the combination of\ndifferent structured pruning methods. While this paper focuses on expert pruning which is unique\nto MoE models and demonstrates the advantages of expert pruning over other structured pruning\nmethods, combining different pruning methods can potentially expand the search space, leaving finer\ngranularity to retain more model capabilities. We leave the two avenues as promising directions for\nfuture work.\n18\nPruning Ratio 25%\nPruning Ratio 50%\nFalse\nTrue\nTrue\nNovice\nReplacement\nPruning Mertric\nCalibration\nData\nModel\nDeepSeek-V2-Lite\nCalibration\nData\nFalse\nTrue\nTrue\nNovice\nReplacement\nPruning Mertric\nCalibration\nData\nModel\nMoonlight-16B-A3B\nCalibration\nData\nFalse\nTrue\nTrue\nNovice\nReplacement\nPruning Mertric\nCalibration\nData\nModel\nOLMoE-1B-7B-0125\nCalibration\nData\nFigure 14: Ablation study on expert access frequency, output variance and novice replacement with\ndetailed results. Numbers are the difference to the proposed MoNE.\n19\n",
    "content": "# Interpretation and Analysis of the MoNE Paper\n\n## 1. Core Content and Major Contributions\n\nThis paper introduces **MoNE (Mixture-of-Novices-and-Experts)**, a novel expert pruning method designed to achieve effective model compression by replacing redundant experts with lightweight novices. MoNE evaluates expert redundancy using two metrics—access frequency and output variance—and employs an unbiased estimation to substitute the outputs of pruned experts, thereby minimizing performance degradation.\n\n### Major Contributions:\n- Proposed the **MoNE method**, which enables effective and robust compression of Mixture-of-Experts (MoE) models by introducing lightweight novice structures to replace redundant experts.\n- Utilized two metrics—**access frequency** and **output variance**—to assess expert redundancy, and adopted **unbiased estimation** to substitute expert outputs, reducing output discrepancies after pruning.\n- Experimental results show that MoNE outperforms baseline methods across different MoE architectures, calibration data sources, and calibration sample sizes. On average across nine downstream tasks, zero-shot accuracy improved by **2.71% at a 25% pruning rate** and **3.61% at a 50% pruning rate**.\n\n## 2. Breakthroughs and Innovations\n\nThe key breakthrough of MoNE lies in its innovative pruning strategy:\n\n- **Dual Evaluation Metrics**: Combines access frequency and output variance to measure redundancy, offering a more comprehensive assessment than single metrics.\n- **Lightweight Novice Structure**: Replaces pruned experts with unbiased estimators, reducing computational and memory costs while maintaining model performance stability.\n- **Cross-task Consistency**: Experiments show that redundant experts identified by MoNE exhibit high consistency across various downstream tasks, validating the method's robustness and generalizability.\n- **No Fine-tuning Required**: Unlike existing approaches that rely on fine-tuning to recover performance, MoNE maintains model effectiveness directly through novice substitution, lowering training costs.\n\n## 3. Promising Startup Ideas Based on MoNE\n\nBased on the core ideas of MoNE, the following entrepreneurial directions can be explored:\n\n### 1. **Lightweight AI Inference Services**\n- **Project Description**: Provide lightweight MoE model inference services compressed via MoNE for SMEs or edge devices.\n- **Advantages**: MoNE significantly reduces memory usage while maintaining high performance, making it ideal for resource-constrained environments.\n- **Target Market**: IoT device manufacturers, mobile app developers, real-time data analytics companies.\n\n### 2. **Automated Model Compression Platform**\n- **Project Description**: Develop an automated model compression tool that allows users to upload their own MoE models, select a pruning ratio, and automatically generate optimized versions.\n- **Core Features**: Integrate the MoNE algorithm, support multiple MoE architectures, and offer visual analysis (e.g., redundancy distribution maps).\n- **Use Cases**: Enterprise deployment, cloud platforms, AI research institutions.\n\n### 3. **Personalized Learning Systems for Education**\n- **Project Description**: Build a personalized online learning recommendation system using lightweight expert models compressed via MoNE.\n- **Technical Highlights**: Each \"expert\" represents a knowledge domain, dynamically routed based on student needs; MoNE compresses infrequently used experts to reduce costs.\n- **Business Model**: SaaS subscription targeting K12 schools and vocational training institutions.\n\n### 4. **Low-code/No-code AI Model Customization Platform**\n- **Project Description**: Offer a graphical interface allowing users to create customized MoE models without coding, with efficient compression powered by MoNE.\n- **Technical Highlights**: Users can select \"expert modules\" from different domains; the platform automatically assembles and optimizes the model size.\n- **Target Customers**: Small-to-medium businesses, startups, non-technical professionals.\n\n### 5. **AI Model Sustainability Solutions**\n- **Project Description**: Launch green AI model compression services to help enterprises reduce carbon footprints and align with ESG standards.\n- **Key Selling Point**: MoNE lowers energy consumption and hardware requirements during model inference, helping companies meet environmental goals.\n- **Partnerships**: Large tech firms, government agencies, environmental organizations.\n\nThese startup ideas not only leverage the technical advantages of MoNE but also align with current trends in the AI industry, offering significant commercial potential.",
    "github": "https://github.com/zxgx/mode-pd",
    "hf": ""
}