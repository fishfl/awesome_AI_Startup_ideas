{
    "id": "2510.02719",
    "title": "TravelBench : Exploring LLM Performance in Low-Resource Domains",
    "summary": "The study created datasets for 14 travel domains, analyzing the performance of large language models on low-resource tasks. It found that general benchmark results are insufficient to understand the models' performance on low-resource tasks and highlighted that inference improves the performance of smaller models on certain tasks more significantly.",
    "abstract": "Results on existing LLM benchmarks capture little information over the model capabilities in low-resource tasks, making it difficult to develop effective solutions in these domains. To address these challenges, we curated 14 travel-domain datasets spanning 7 common NLP tasks using anonymised data from real-world scenarios, and analysed the performance across LLMs. We report on the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a variety of tasks. Our results confirm that general benchmarking results are insufficient for understanding model performance in low-resource tasks. Despite the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks in complex, domain-specific scenarios. Furthermore, reasoning provides a more significant boost for smaller LLMs by making the model a better judge on certain tasks.",
    "category1": "Application Implementation",
    "category2": "Personal Tools",
    "category3": "Non-Agent",
    "authors": "Srinivas Billa,Xiaonan Jing",
    "subjects": [
        "Computation and Language (cs.CL)",
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:10 pages, 3 figures",
    "keypoint": "General benchmarking results are insufficient for understanding model performance in low-resource tasks.\nOut-of-the-box LLMs hit performance bottlenecks in complex, domain-specific scenarios despite training FLOPs.\nPerformance improvements diminish significantly beyond 0.5 × 10^16 FLOPs.\nLightweight models can be better options for domain adaptation due to trade-offs between performance, cost, and latency.\nReasoning provides a more significant performance boost for smaller LLMs compared to larger ones.\nEnabling reasoning does not improve performance for larger models and can degrade it.\nLarger models show better average performance but insignificant gains in consistency across tasks.\nReasoning improves average performance in smaller models but introduces more score fluctuations.\nNo single model excels across all tasks; performance is highly task-dependent.\nInternal reasoning helps with retrieval but does not bridge the gap between model judgment and human annotation on unseen knowledge.",
    "date": "2025-10-07",
    "paper": "TravelBench : Exploring LLM Performance in Low-Resource Domains\nSrinivas Billa\nExpedia Group\nThe Angel Building, 407 St John St\nLondon, EC1V 4EX, UK\nnbilla@expediagroup.com\nXiaonan Jing\nExpedia Group\n1111 Expedia Group Wy W\nSeattle, WA 98119, USA\nxijing@expediagroup.com\nAbstract\nResults on existing LLM benchmarks capture\nlittle information over the model capabilities\nin low-resource tasks, making it difficult to de-\nvelop effective solutions in these domains. To\naddress these challenges, we curated 14 travel-\ndomain datasets spanning 7 common NLP tasks\nusing anonymised data from real-world sce-\nnarios, and analysed the performance across\nLLMs. We report on the accuracy, scaling be-\nhaviour, and reasoning capabilities of LLMs\nin a variety of tasks. Our results confirm that\ngeneral benchmarking results are insufficient\nfor understanding model performance in low-\nresource tasks. Despite the amount of training\nFLOPs, out-of-the-box LLMs hit performance\nbottlenecks in complex, domain-specific sce-\nnarios. Furthermore, reasoning provides a more\nsignificant boost for smaller LLMs by making\nthe model a better judge on certain tasks.\n1\nIntroduction\nThe rapid advancement of large language models\n(LLMs) in recent years has significantly facilitated\nthe prototyping of downstream natural language\nprocessing (NLP) tasks. However, this has also\nintroduced new challenges in selecting the most\nsuitable LLM from an ever-growing pool of state-\nof-the-art (SOTA) models. To address this issue,\na number of benchmarking datasets are proposed\nto evaluate the general capabilities of LLMs, in-\ncluding but not limited to: MMLU (Hendrycks\net al., 2020), ARC-C (Clark et al., 2018), GSM8K\n(Cobbe et al., 2021), HumanEval (Chen et al.,\n2021), MGSM (Shi et al., 2022). However, these\nbenchmarks provide limited insight into the perfor-\nmance of the model in low-resource domains such\nas the travel industry. In addition, prior study (Wi-\nher et al., 2022) has shown that different types of\ngeneration can affect the performance of an LLM’s\ndecoding strategy. Without in-depth domain eval-\nuation, streamlining LLM development becomes\nmore difficult.\nTraditionally, opinion mining has been the most\nprominent task in the travel domain, as it was of-\nten studied over customer reviews. For instance,\nhotel reviews have been widely used in sentiment\nclassification (Alam et al., 2016), rating prediction\n(Wang et al., 2010), and opinion spam detection\n(Ott et al., 2011). Similarly, restaurant reviews have\nbeen long leveraged for benchmarking the overall\nand aspect-based sentiment analysis, particularly in\nSemEval-2014 Task 4 (Pontiki et al., 2014). Most\nrecently, with the emergence of LLM agents, trip\nplanning has gained attention and benchmark such\nas TravelPlanner (Xie et al., 2024) has been intro-\nduced, which evaluates LLMs on tool usage and\ncomplex planning. To further understand LLM\nperformance in the travel domain beyond opinion\nmining and broaden the scope of real-world usage\nscenario benchmarking, we curated a comprehen-\nsive set of travel datasets including 14 tasks across\n7 categories for model evaluation. An overview of\nthe datasets is presented in Table 1.\nBy applying LLMs to generate predictions, we\ntreat each task as solving an autoregressive mod-\nelling problem. The prediction then reflects the\nnext token generation capability given specific con-\ntexts. Thus, the output is characterized by a com-\nbination of the model’s internal knowledge and\ninstruction-following ability. In the rest of this pa-\nper, we present in-depth evaluations of LLMs on\nour benchmark, aiming to bridge the gap between\nlow-resource, under-explored tasks in the travel\ndomain and the demand of real-world applications.\nIt should be noted that all of our datasets were col-\nlected in an anonymised form from real-world us-\nage scenarios, making the evaluation results more\nrepresentative of actual model performance in prac-\ntice. Our contributions are as follows:\n• We introduce a wide range of curated datasets\nin the travel domain for LLM evaluation, ex-\npanding the scope of current resources beyond\nopinion mining.\n1\narXiv:2510.02719v1  [cs.CL]  3 Oct 2025\nTask\n# Samples\nAvg Input Tokens\n# Labels\nAspect Based Sentiment Analysis\n∼500\n∼700\n6\nOverall Review Sentiment Analysis\n∼300\n3\nAspect Based Review Segmentation\n∼900\nOpen Ended Generation\nReview Topic Classification\n∼500\n17\nFaithfulness\n∼1500\n5\nRelevance\n∼700\n5\nInclusiveness\n∼1000\n2\nCompliance\n∼900\n2\nReview Moderation\n∼2000\n2\nManager Response Moderation\n∼1400\n2\nCustomer Service Intent Prediction\n∼800\n9\nReview Summarisation\n∼2500\nOpen Ended Generation\nReview Translation EN ->XX\n∼1000\n∼100\nOpen Ended Generation\nReview Translation XX ->EN\nOpen Ended Generation\nTable 1: Benchmark datasets categorised by NLP task. Each task is treated as a generation problem using an\nauto-regressive LLM. Avg Input Tokens denotes the number of tokens in the prompt which consists both instruction\ntokens and data tokens. For classification problems, number of labels are reported.\n• We present an in-depth analysis of various\nLLMs, verifying that out-of-the-box LLMs\nhave performance limitations when adapted\nto travel domain.\n• We share insights on LLM’s scaling and rea-\nsoning behaviours to shed light on the effects\nof model’s training FLOPs and size. 1\n2\nDataset\nIn this section, we describe the datasets, data col-\nlection process, and finally the evaluation metrics.\nThe selection of tasks was derived from real-world\ndownstream tasks in the travel domain. We sourced\ndata from real-world scenarios, i.e. publicly de-\nployed systems. The data was anonymised to re-\nmove Personally Identifiable Information (PII). All\nof the data was human annotated or verified without\nassistance from LLMs, minimizing implicit bias\nin the dataset towards models. For instance, we\nomitted sample responses to summary generation\nin the annotation guideline to reduce the impact of\nwriting styles from pre-defined summaries. Dataset\ncuration follows three steps:\n1. Stratified Random Sampling. From a large\ncollection of real-world usage, we randomly\nsample approximately 500 to 1000 rows per\ntask following the source distribution.\n2. Rubric Creation. We developed annotation\nguidelines in collaboration with human ex-\n1We are in the process of approval to release the datasets\nfor further research.\nperts to provide precise instructions and en-\nsure label consistency.\n3. Annotation.\nWe employed human coders\nfor annotation. It should be noted that al-\nthough the number of coders could vary across\ndatasets, the guidelines were designed to mini-\nmize variance introduced by these differences.\n2.1\nAspect-based Sentiment Analysis (ABSA)\nThe purpose of this task is to identify granular sen-\ntiment toward specific topics within a text. Given a\npre-defined set of topics (e.g., WiFi, pool, parking),\nwe randomly sampled an equal number of hotel re-\nviews per topic. Annotators labelled the sentiment\nof each aspect to positive, negative, mixed, neu-\ntral, not mentioned, wished for. Besides the widely\nused sentiment labels, we introduced two custom\nlabels, namely \"not mentioned: a topic not being\npresent in the text\" and \"wished for: an utterance\nwith the intent of wishing for it\". For example, \"I\nwish the breakfast was free\". We compute the F1-\nscore (Van Rijsbergen, 1979) between prediction\nand ground truth for evaluation.\n2.2\nOverall Sentiment Classification\nThe purpose of this task is to identify overall sen-\ntiment within a text. The data mainly consists of\nhotel reviews and customer support conversations.\nThe labels are positive, negative, neutral. We use\nF1-score for evaluation.\n2\n2.3\nAspect-based Text Segmentation\nThe purpose of this task is to extract relevant seg-\nments within a text with respect to a given aspect\nof interest (e.g. \"cleanliness\", \"service\"). Using\nthe same data as the ABSA task, human annota-\ntors were asked to label the start and the end of\ntext span indices. We used BLEU-score (Papineni\net al., 2002) to measure exact matches between the\nprediction and the ground truth.\n2.4\nTopic Classification\nThe purpose of this task is to identify the topics\nthat a text references. We use the same hotel re-\nview dataset as the ABSA task. Given a review the\nmodel is expected to identify all relevant topics are\nmentioned from the following list: amenities, bar,\nbeach, breakfast, cleanliness, comfort, location,\nnoise, pool, price, restaurant, room, service, spa or\ngym, view, wifi. We use F1-score for evaluation.\n2.5\nHELM\nHolistic Evaluation of Language Models (HELM)\n(Liang et al., 2022) defined an LLM-as-a-judge\n(Zheng et al., 2023) evaluation paradigm for natu-\nral language generation (NLG) such as faithfulness,\nrelevance, compliance, and inclusiveness. We em-\nployed two types of evaluation setups: 1) scoring\nin the range of 1 to 5 scale, with 5 being the most-\ndesired output (Chiang and Lee, 2023; Jing et al.,\n2025); and 2) binary classification with True or\nFalse labels (Wang et al., 2023; Liu et al., 2023).\n2.5.1\nHELM Faithfulness\nThe faithfulness metric measures the absence of\nhallucination or untrue facts derived from the\nLLM’s own knowledge.\nA score of 5 means\n\"highly-faithful\" and a score of 1 means \"high-\nunfaithful\". Root Mean Square Error (RMSE)(Chai\net al., 2014) is used for evaluation.\n2.5.2\nHELM Compliance\nCompliance provides a binary measure on whether\nthe presence of content is compliant with legal\npolicy. Example policies include toxic language,\ndefamation, and infringements of intellectual prop-\nerty. F1-score is used for evaluation\n2.5.3\nHELM Inclusiveness\nInclusiveness is a binary metric which measures\nwhether there exists the presence of hate speech,\nnon-respectful language, harassment, and threats\nin the NLG. F1-score is used for evaluation.\n2.5.4\nHELM Relevance\nRelevance measures how relevant the information\nis to the purpose of the task, for example : how rel-\nevant is the generated text given the task of review\nsummarisation, on a 1-5 scale. Each use case has\nits individual measure of relevance. The rubrics\nchange per use case. RMSE is used for evaluation.\n2.6\nReview Moderation\nThis task aims to identify whether a review is ap-\npropriate for publication. We evenly sampled hotel\nreviews and asked human annotators to label them\nas APPROVED or REJECTED. We use F1-score\nfor evaluation.\n2.7\nManager Response Moderation\nSimilarly to review moderation, this task aims to\nclassify property manager responses to customer\nreviews as APPROVED or REJECTED.. Some\ncommon reasons for rejection include exposing\nsensitive personal information, being irrelevant to\nthe review, or being overly promotional in order\nto bypass platform policies. We use F1-score for\nevaluation.\n2.8\nIntent Prediction\nThis task aims to determine the intent behind a cus-\ntomer’s utterance with a customer service agent.\nWe instructed human annotators to label each utter-\nance as one of the following intents: book, cancel,\nchange, contest_payment, feedback, help, retrieve,\nsmall_talk, unknown_scenario. We use F1-score\nfor evaluation.\n2.9\nReview Summarisation\nUsing the same hotel review data as the ABSA\ntask, we grouped the reviews by property and topic.\nHuman annotators were instructed to summarise\nreviews within the same group into a short sum-\nmary, which is less than 100 characters. We used\nMETEOR-score (Banerjee and Lavie, 2005) for\nevaluation, which is robust to paraphrasing and\nsynonyms.\n2.10\nTranslation\nWe split translation into two sub-tasks: 1) translate\nfrom English to a target language; and 2) trans-\nlate from a target language to English. To obtain\nthe annotations, we randomly sampled hotel re-\nviews from non-English languages and instructed\nhuman annotators to translate target texts into En-\nglish. METEOR-score was used for evaluation.\n3\nFigure 1: Performance Pm against training compute FLOPs. While performance generally improves with scale,\nthere are significant diminishing returns past 0.5 ∗1016.\n3\nBenchmark Setup\nWe leveraged prompt engineering by treating each\ntask as a chat completion problem. We used the\nOpenAI chat completion template via Python SDK\nto ensure response accuracy. For each task, we\ndesigned a few-shot prompt template and used the\nsame template consistently across all LLMs. Each\ntemplate consists of 1) the description of the task, 2)\nstep-by-step instructions to solve the specific prob-\nlem, and 3) Zero-shot or few-shot question/answer\npairs. For better comparison, we limited our scope\nto instruct-tuned models only, and that a temper-\nature of 0 and top-p of 1 were applied to obtain\ndeterministic outputs. For models with reasoning\ncapabilities, we adopted the default settings from\ntheir model cards. Additionally, to serve open-\nsource LLMs, we utilized vLLM framework and\nfor proprietary LLMs such as GPT-4o, we lever-\naged official APIs through a secure proxy. We fo-\ncused on two evaluations: 1) overall performance\ncomparison with 67 models across open and closed\nsource LLMs, and 2) the effect of reasoning of 8\nhybrid-reasoning models from the Qwen 3 family.\n3.1\nScaled performance across metrics\nIn order to ensure fair comparison of the models\nacross various tasks, each with a different evalu-\nation metric, we devised an overall performance\nmetric Pm. Firstly, we calculated Pt,m which de-\nnotes the scaled performance with respect to each\ntask as the following,\nPt,m =\n\n\n\n\n\n\n\n\n\n\n\n1 −clamp(xt,m, at, bt) −at\nbt −at\n,\nif invertt = True\nclamp(xt,m, at, bt) −at\nbt −at\n,\nif invertt = False\nwhere xt,m is the original score for model m on\ntask t, at is the theoretical minimum bound for task\nt, bt is the theoretical maximum bound for task\nt, invertt is an indicator if lower is better for task\nt, and clamp(x, a, b) = min(max(x, a), b) is the\nclamping function to bound the score within [a, b].\nThen, we can calculate the overall performance\nPm by averaging over all tasks as,\nPm =\n1\nNm\nX\nt\nPt,m\nwhere Pm represents the overall performance of\nmodel m, and Nm represents the total number of\nvalid (included) tasks for model m.\n4\nResults and Analysis\n4.1\nScaling Laws\nWe are interested in learning the correlation be-\ntween model performance and its scale. Using\nthe performance Pm calculated from above and the\ncompute approximation FLOPs following Kaplan\net al. (2020), a model’s scale can be defined as the\ntraining compute in floating-point operations per\n4\nFigure 2: The effect of enabling reasoning across the Qwen3 model family. While smaller models show some\nimprovements, the same does not apply to large models - performance degradation can be seen for the 235B model.\nsecond (FLOPs),\nFLOPs ≈6NT\nwhere T is the number of training tokens and N is\nthe number of model parameters\nAs illustrated in Figure 1, we can observe a pos-\nitive correlation between model performance and\nscale across all tasks. However, the scaling pro-\ngression rapidly diminishes as FLOPs increases.\nModels trained with compute budgets larger than\n≈0.5 ∗1016 FLOPs exhibits slow non-linear im-\nprovements, suggesting that unseen domain adap-\ntion remains a challenge even for larger models\nwith higher generalizability. This result suggests\nthat for domain adoption of out-of-the-box LLMs,\nlightweight models can sometimes serve as better\noptions given the trade off between performance,\ncompute costs, and inference latencies.\n4.2\nEffects of Reasoning\nRecent work on unified training frameworks with\ntoggle reasoning provided an opportunity to study\nits impact on the same model with in a model fam-\nily. We tested the Qwen3 (Yang et al., 2025) family\nmodels to measure the performance difference with\nand without enabling internal thinking.\nFigure 2 illustrates the performance differences\nfor the model of the same size. As the model pa-\nrameter increases, judging with reasoning does not\nalways outperform that without reasoning. In addi-\ntion, reasoning provides little to zero performance\nimprovements on bigger models, and in some cases\nslightly degrades performance, which is counter-\nintuitive.\n4.3\nPerformance Variance\nFigure 3 demonstrates the variance in performance\nacross the Qwen3 family models.\nWe observe\nthat larger models are on average better than their\nsmaller counterparts when pre-trained on the same\ndataset. However, improvements in performance\nconsistency are insignificant. We also notice that,\nwhile enabling reasoning improves the average per-\nformance of the smaller models, it can introduce\nmore fluctuations in the model scores across tasks.\nHowever, this behaviour is less prevalent in larger\nmodels. Our assumption is that larger models store\nabundant internal knowledge with greater context\nretrieval ability. When a task is complex enough to\noverwhelm the model’s natural retrieval capability,\nadding reasoning helps improve the recall. Yet rea-\nsoning does not provide the model with the ability\nto gain new knowledge, and thus the performance\ncaps at a certain level.\n5\nRelated Works\nScaling laws. Kaplan et al. (2020) and Hoffmann\net al. (2022) presented scaling laws for LLMs\nof varying sizes to understand the relationship be-\ntween scaling model size and training data. They\ncompared test loss and some general benchmark\nperformance such as MMLU (Hendrycks et al.,\n5\nFigure 3: Variance of model performance across tasks across the Qwen 3 family. While the performance overall\nincreases with model size, the spread of the performance does not follow the same pattern. This indicates that\nperformance is very task dependent and no one model is the best at every task.\n2020) and Big-Bench (Srivastava et al., 2023).\nDomain Specific Benchmarking. Within the\ntravel sector, TravelPlanner (Xie et al., 2024) in-\ntroduced a benchmark for evaluating LLMs on\ntool use and planning with synthetically generated\nqueries. They compared both open source and\nclosed source LLMs and showed that even the best\nperforming model GPT4 has a success rate of only\n0.6%. ChinaTravel (Shao et al., 2025) extends this\nwork by using open-ended human written queries\nfor the Chinese travel market, also showing simi-\nlar results with Deepseek V3 being the best model\nat 5% pass rate, the authors note this may be be-\ncause Deepseek has more context into chinese data.\nSemEval-2014 Task 4 (Pontiki et al., 2014) intro-\nduced an aspect-based sentiment analysis dataset\non restaurant and laptop reviews, which has been\nwidely used for benchmarking ABSA.\nEffects of reasoning.\nResearch such as\nDeepSeek-AI et al.\n(2025) and OpenAI et al.\n(2024) which train models with reinforcement\nlearning to enable native reasoning in LLMs show\nprominent improvements in model performance\nthrough test-time compute scaling. We extend this\nresearch and compare new hybrid-thinking models\nproposed by Yang et al. (2025) and show the effect\nof enabling reasoning traces. All prior research\nshows notable performance increases by introduc-\ning native reasoning.\n6\nConclusion\nWe presented an in-depth evaluation of LLM-as-\na-judge on a series of annotated datasets in the\nlow-resource travel domain. We expanded the task\nvariation from the previously prominent opinion\nmining to 7 common NLP tasks. Our results indi-\ncate that out-of-the-box LLMs, despite the num-\nber of parameters and the amount of training to-\nkens, reach performance bottlenecks over domain-\nspecific tasks.\nFurthermore, internal reasoning\ntends to have a more significant performance boost\non smaller LLMs over larger models. Lastly, the\nscoring fluctuation over the Qwen-family models\nindicates that although reasoning helps with re-\ntrieval, there remains a gap between human annota-\ntion and model judgement on unseen knowledge.\n7\nLimitations\n• Due to resource limitations, some of the mod-\nels (>200b) tested were only tested in their\nquantised FP8/INT4 versions. This may have\ncaused a slight degredation in performance for\nthese models at the higher end of the FLOPs\n6\nbudget in 1 however not enough to change the\nanalysis drawn from it.\n• We use a single prompt across all models,\nwhile this may not be optimal since different\nmodels might perform better with modified\nprompts we chose to keep the prompt identi-\ncal as a measure to test ease of use. Future\nwork could go into measuring how much the\nprompt matters in downstream performance.\n• The internal company proxy service we\nused for closed-source models has inbuilt\nguardrails that protect against accidental mis-\nuse. However this caused some issues with\ndealing with some tasks such as review mod-\neration as the harmful content filter was be-\ning triggered and caused the service to fail\nin providing a response. While this is testing\nwhether the model can successfully filter out\nharmful content the proxy service would not\nallow it. So we had to remove the failing rows\nin our testing for those models.\n• For closed-source models, we are not able to\nfind details on training data size, quantisation\nand parameter size which meant we could not\ninclude those in the FLOPs scaling figures.\nReferences\nMd Hijbul Alam, Woo-Jong Ryu, and SangKeun Lee.\n2016. Joint multi-grain topic sentiment: modeling\nsemantic aspects for online reviews. Information\nSciences, 339:206–223.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Arbor,\nMichigan. Association for Computational Linguis-\ntics.\nTianfeng Chai, Roland R Draxler, et al. 2014. Root\nmean square error (rmse) or mean absolute error\n(mae). Geoscientific model development discussions,\n7(1):1525–1534.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde De Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nCheng-Han Chiang and Hung-yi Lee. 2023. A closer\nlook into using large language models for automatic\nevaluation. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2023, pages 8928–\n8942.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong\nShao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue,\nBingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu,\nChenggang Zhao, Chengqi Deng, Chenyu Zhang,\nChong Ruan, Damai Dai, Deli Chen, Dongjie Ji,\nErhang Li, Fangyun Lin, Fucong Dai, Fuli Luo,\nGuangbo Hao, Guanting Chen, Guowei Li, H. Zhang,\nHan Bao, Hanwei Xu, Haocheng Wang, Honghui\nDing, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li,\nJianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.\nCai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai\nHu, Kaige Gao, Kang Guan, Kexin Huang, Kuai\nYu, Lean Wang, Lecong Zhang, Liang Zhao, Litong\nWang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan\nZhang, Minghua Zhang, Minghui Tang, Meng Li,\nMiaojun Wang, Mingming Li, Ning Tian, Panpan\nHuang, Peng Zhang, Qiancheng Wang, Qinyu Chen,\nQiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan,\nRunji Wang, R. J. Chen, R. L. Jin, Ruyi Chen,\nShanghao Lu, Shangyan Zhou, Shanhuang Chen,\nShengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng\nZhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing\nWu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,\nT. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu,\nWenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao\nZhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan\nWang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin\nLiu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li,\nXuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin,\nXiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxi-\nang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang,\nXinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang\nZhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng\nSun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi,\nYiliang Xiong, Ying He, Yishi Piao, Yisong Wang,\nYixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,\nYuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yu-\njia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You,\nYuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu,\nYanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu,\nYunxian Ma, Ying Tang, Yukun Zha, Yuting Yan,\nZ. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean\nXu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,\nZhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zi-\njia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song,\nZizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu\nZhang, and Zhen Zhang. 2025. Deepseek-r1: Incen-\n7\ntivizing reasoning capability in llms via reinforce-\nment learning. Preprint, arXiv:2501.12948.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models. Preprint, arXiv:2203.15556.\nXiaonan Jing, Srinivas Billa, and Danny Godbout. 2025.\nOn a scale from 1 to 5: Quantifying hallucination\nin faithfulness evaluation. In Findings of the Associ-\nation for Computational Linguistics: NAACL 2025,\npages 7765–7780, Albuquerque, New Mexico. Asso-\nciation for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. Preprint,\narXiv:2001.08361.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2511–2522.\nOpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer,\nAdam Richardson, Ahmed El-Kishky, Aiden Low,\nAlec Helyar, Aleksander Madry, Alex Beutel, Alex\nCarney, Alex Iftimie, Alex Karpenko, Alex Tachard\nPassos, Alexander Neitz, Alexander Prokofiev,\nAlexander Wei, Allison Tam, Ally Bennett, Ananya\nKumar, Andre Saraiva, Andrea Vallone, Andrew Du-\nberstein, Andrew Kondrich, Andrey Mishchenko,\nAndy Applebaum, Angela Jiang, Ashvin Nair, Bar-\nret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin\nSokolowsky, Boaz Barak, Bob McGrew, Borys Mi-\nnaiev, Botao Hao, Bowen Baker, Brandon Houghton,\nBrandon McKinzie, Brydon Eastman, Camillo Lu-\ngaresi, Cary Bassin, Cary Hudson, Chak Ming Li,\nCharles de Bourcy, Chelsea Voss, Chen Shen, Chong\nZhang, Chris Koch, Chris Orsinger, Christopher\nHesse, Claudia Fischer, Clive Chan, Dan Roberts,\nDaniel Kappler, Daniel Levy, Daniel Selsam, David\nDohan, David Farhi, David Mely, David Robinson,\nDimitris Tsipras, Doug Li, Dragos Oprica, Eben Free-\nman, Eddie Zhang, Edmund Wong, Elizabeth Proehl,\nEnoch Cheung, Eric Mitchell, Eric Wallace, Erik\nRitter, Evan Mays, Fan Wang, Felipe Petroski Such,\nFilippo Raso, Florencia Leoni, Foivos Tsimpourlas,\nFrancis Song, Fred von Lohmann, Freddie Sulit,\nGeoff Salmon, Giambattista Parascandolo, Gildas\nChabot, Grace Zhao, Greg Brockman, Guillaume\nLeclerc, Hadi Salman, Haiming Bao, Hao Sheng,\nHart Andrin, Hessam Bagherinezhad, Hongyu Ren,\nHunter Lightman, Hyung Won Chung, Ian Kivlichan,\nIan O’Connell, Ian Osband, Ignasi Clavera Gilaberte,\nIlge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina\nKofman, Jakub Pachocki, James Lennon, Jason Wei,\nJean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu,\nJiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero\nCandela, Joe Palermo, Joel Parish, Johannes Hei-\ndecke, John Hallman, John Rizzo, Jonathan Gordon,\nJonathan Uesato, Jonathan Ward, Joost Huizinga,\nJulie Wang, Kai Chen, Kai Xiao, Karan Singhal, Ka-\nrina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood,\nKendra Rimbach, Keren Gu-Lemberg, Kevin Liu,\nKevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad,\nLauren Yang, Leo Liu, Leon Maksin, Leyton Ho,\nLiam Fedus, Lilian Weng, Linden Li, Lindsay Mc-\nCallum, Lindsey Held, Lorenz Kuhn, Lukas Kon-\ndraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd,\nMaja Trebacz, Manas Joglekar, Mark Chen, Marko\nTintor, Mason Meyer, Matt Jones, Matt Kaufer,\nMax Schwarzer, Meghan Shah, Mehmet Yatbaz,\nMelody Y. Guan, Mengyuan Xu, Mengyuan Yan,\nMia Glaese, Mianna Chen, Michael Lampe, Michael\nMalek, Michele Wang, Michelle Fradin, Mike Mc-\nClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang,\nMira Murati, Mo Bavarian, Mostafa Rohaninejad,\nNat McAleese, Neil Chowdhury, Neil Chowdhury,\nNick Ryder, Nikolas Tezak, Noam Brown, Ofir\nNachum, Oleg Boiko, Oleg Murk, Olivia Watkins,\nPatrick Chao, Paul Ashbourne, Pavel Izmailov, Pe-\nter Zhokhov, Rachel Dias, Rahul Arora, Randall\nLin, Rapha Gontijo Lopes, Raz Gaon, Reah Mi-\nyara, Reimar Leike, Renny Hwang, Rhythm Garg,\nRobin Brown, Roshan James, Rui Shu, Ryan Cheu,\nRyan Greene, Saachi Jain, Sam Altman, Sam Toizer,\nSam Toyer, Samuel Miserendino, Sandhini Agarwal,\nSantiago Hernandez, Sasha Baker, Scott McKinney,\nScottie Yan, Shengjia Zhao, Shengli Hu, Shibani\nSanturkar, Shraman Ray Chaudhuri, Shuyuan Zhang,\nSiyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji,\nSuvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan\nClark, Tao Wang, Taylor Gordon, Ted Sanders, Te-\njal Patwardhan, Thibault Sottiaux, Thomas Degry,\nThomas Dimson, Tianhao Zheng, Timur Garipov,\nTom Stasi, Trapit Bansal, Trevor Creech, Troy Peter-\nson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju,\nVinnie Monaco, Vitchyr Pong, Vlad Fomenko,\nWeiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech\nZaremba, Yann Dubois, Yinghai Lu, Yining Chen,\nYoung Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yun-\nyun Wang, Zheng Shao, and Zhuohan Li. 2024. Ope-\nnai o1 system card. Preprint, arXiv:2412.16720.\nMyle Ott, Yejin Choi, Claire Cardie, and Jeffrey T\nHancock. 2011. Finding deceptive opinion spam\nby any stretch of the imagination. arXiv preprint\narXiv:1107.4557.\n8\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Suresh\nManandhar. 2014. Semeval-2014 task 4: Aspect\nbased sentiment analysis. In Proceedings of the 8th\nInternational Workshop on Semantic Evaluation (Se-\nmEval 2014), pages 27–35. 8th International Work-\nshop on Semantic Evaluation August 23-24, 2014. ;\nConference date: 23-08-2014 Through 24-08-2014.\nJie-Jing Shao, Bo-Wen Zhang, Xiao-Wen Yang, Baizhi\nChen, Si-Yu Han, Wen-Da Wei, Guohao Cai, Zhen-\nhua Dong, Lan-Zhe Guo, and Yu feng Li. 2025.\nChinatravel: An open-ended benchmark for lan-\nguage agents in chinese travel planning. Preprint,\narXiv:2412.13682.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush Vosoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.\nLanguage models are multilingual chain-of-thought\nreasoners. arXiv preprint arXiv:2210.03057.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane,\nAnantharaman S. Iyer, Anders Andreassen, Andrea\nMadotto, Andrea Santilli, Andreas Stuhlmüller, An-\ndrew Dai, Andrew La, Andrew Lampinen, Andy\nZou, Angela Jiang, Angelica Chen, Anh Vuong,\nAnimesh Gupta, Anna Gottardi, Antonio Norelli,\nAnu Venkatesh, Arash Gholamidavoodi, Arfa Tabas-\nsum, Arul Menezes, Arun Kirubarajan, Asher Mul-\nlokandov, Ashish Sabharwal, Austin Herrick, Avia\nEfrat, Aykut Erdem, Ayla Karaka¸s, B. Ryan Roberts,\nBao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,\nBatuhan Özyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk\nEkmekci, Bill Yuchen Lin, Blake Howald, Bryan\nOrinion, Cameron Diao, Cameron Dour, Cather-\nine Stinson, Cedrick Argueta, César Ferri Ramírez,\nChandan Singh, Charles Rathkopf, Chenlin Meng,\nChitta Baral, Chiyu Wu, Chris Callison-Burch, Chris\nWaites, Christian Voigt, Christopher D. Manning,\nChristopher Potts, Cindy Ramirez, Clara E. Rivera,\nClemencia Siro, Colin Raffel, Courtney Ashcraft,\nCristina Garbacea, Damien Sileo, Dan Garrette, Dan\nHendrycks, Dan Kilman, Dan Roth, Daniel Free-\nman, Daniel Khashabi, Daniel Levy, Daniel Moseguí\nGonzález, Danielle Perszyk, Danny Hernandez,\nDanqi Chen, Daphne Ippolito, Dar Gilboa, David Do-\nhan, David Drakard, David Jurgens, Debajyoti Datta,\nDeep Ganguli, Denis Emelin, Denis Kleyko, Deniz\nYuret, Derek Chen, Derek Tam, Dieuwke Hupkes,\nDiganta Misra, Dilyar Buzan, Dimitri Coelho Mollo,\nDiyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina\nShutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-\ngefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia,\nFatemeh Siar, Fernando Martínez-Plumed, Francesca\nHappé, Francois Chollet, Frieda Rong, Gaurav\nMishra, Genta Indra Winata, Gerard de Melo, Ger-\nmán Kruszewski, Giambattista Parascandolo, Gior-\ngio Mariani, Gloria Wang, Gonzalo Jaimovitch-\nLópez, Gregor Betz, Guy Gur-Ari, Hana Galijase-\nvic, Hannah Kim, Hannah Rashkin, Hannaneh Ha-\njishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin,\nHinrich Schütze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B. Simon,\nJames Koppel, James Zheng, James Zou, Jan Koco´n,\nJana Thompson, Janelle Wingfield, Jared Kaplan,\nJarema Radom, Jascha Sohl-Dickstein, Jason Phang,\nJason Wei, Jason Yosinski, Jekaterina Novikova,\nJelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen\nTaal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Ji-\naming Song, Jillian Tang, Joan Waweru, John Bur-\nden, John Miller, John U. Balis, Jonathan Batchelder,\nJonathan Berant, Jörg Frohberg, Jos Rozen, Jose\nHernandez-Orallo, Joseph Boudeman, Joseph Guerr,\nJoseph Jones, Joshua B. Tenenbaum, Joshua S. Rule,\nJoyce Chua, Kamil Kanclerz, Karen Livescu, Karl\nKrauth, Karthik Gopalakrishnan, Katerina Ignatyeva,\nKatja Markert, Kaustubh D. Dhole, Kevin Gim-\npel, Kevin Omondi, Kory Mathewson, Kristen Chi-\nafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros Colón, Luke Metz, Lütfi Kerem\n¸Senel, Maarten Bosma, Maarten Sap, Maartje ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramírez Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L. Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael A. Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mitch Walker,\nMo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor\nGeva, Mozhdeh Gheini, Mukund Varma T, Nanyun\nPeng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari\nKrakover, Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan\nWen, Oliver Zhang, Omar Agha, Omar Elbaghdadi,\nOmer Levy, Owain Evans, Pablo Antonio Moreno\n9\nCasares, Parth Doshi, Pascale Fung, Paul Pu Liang,\nPaul Vicol, Pegah Alipoormolabashi, Peiyuan Liao,\nPercy Liang, Peter Chang, Peter Eckersley, Phu Mon\nHtut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil,\nPouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing\nLyu, Qinlang Chen, Rabin Banjade, Rachel Etta\nRudolph, Raefer Gabriel, Rahel Habacker, Ramon\nRisco, Raphaël Millière, Rhythm Garg, Richard\nBarnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman\nNovak, Roman Sitelew, Ronan LeBras, Rosanne\nLiu, Rowan Jacobs, Rui Zhang, Ruslan Salakhut-\ndinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan\nTeehan, Rylan Yang, Sahib Singh, Saif M. Moham-\nmad, Sajant Anand, Sam Dillavou, Sam Shleifer,\nSam Wiseman, Samuel Gruetter, Samuel R. Bow-\nman, Samuel S. Schoenholz, Sanghyun Han, San-\njeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima, Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi,\nSiva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,\nSpencer Torene, Sriharsha Hatwar, Stanislas De-\nhaene, Stefan Divic, Stefano Ermon, Stella Bider-\nman, Stephanie Lin, Stephen Prasad, Steven T. Pi-\nantadosi, Stuart M. Shieber, Summer Misherghi, Svet-\nlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal\nSchuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto,\nTe-Lin Wu, Théo Desbordes, Theodore Rothschild,\nThomas Phan, Tianle Wang, Tiberius Nkinyili, Timo\nSchick, Timofei Kornev, Titus Tunduny, Tobias Ger-\nstenberg, Trenton Chang, Trishala Neeraj, Tushar\nKhot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera\nDemberg, Victoria Nyamai, Vikas Raunak, Vinay\nRamasesh, Vinay Uday Prabhu, Vishakh Padmaku-\nmar, Vivek Srikumar, William Fedus, William Saun-\nders, William Zhang, Wout Vossen, Xiang Ren, Xi-\naoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen,\nYadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,\nYasaman Bahri, Yejin Choi, Yichi Yang, Yiding\nHao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang\nHou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian\nWang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023.\nBeyond the imitation game: Quantifying and extrap-\nolating the capabilities of language models. Preprint,\narXiv:2206.04615.\nCornelis J Van Rijsbergen. 1979. Information retrieval,\n2nd edn. newton, ma.\nHongning Wang, Yue Lu, and Chengxiang Zhai. 2010.\nLatent aspect rating analysis on review text data: a rat-\ning regression approach. In Proceedings of the 16th\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 783–792.\nJiaan Wang, Yunlong Liang, Fandong Meng, Zengkui\nSun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,\nand Jie Zhou. 2023. Is chatgpt a good nlg evalua-\ntor? a preliminary study. In Proceedings of EMNLP\nWorkshop, page 1.\nGian Wiher, Clara Meister, and Ryan Cotterell. 2022.\nOn decoding strategies for neural text generators.\nTransactions of the Association for Computational\nLinguistics, 10:997–1012.\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze\nLou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024.\nTravelplanner: a benchmark for real-world planning\nwith language agents. In Proceedings of the 41st In-\nternational Conference on Machine Learning, pages\n54590–54613.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayi-\nheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi\nYang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai\nDang, Keqin Bao, Kexin Yang, Le Yu, Lianghao\nDeng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang,\nPeng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao\nYin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xu-\nancheng Ren, Yang Fan, Yang Su, Yichang Zhang,\nYinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang,\nZeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zi-\nhan Qiu. 2025. Qwen3 technical report. Preprint,\narXiv:2505.09388.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36:46595–46623.\n10\n",
    "content": "# TravelBench: Exploring LLM Performance in Low-Resource Domains\n\n## 1. Core Content and Key Contributions\n\nThis paper, *TravelBench: Exploring LLM Performance in Low-Resource Domains*, proposed by researchers from Expedia Group, addresses the insufficient evaluation of large language models (LLMs) on **domain-specific tasks with limited resources** in current benchmarking practices. The core content and main contributions are as follows:\n\n### **Core Content**\n- Introduced **TravelBench**, a comprehensive benchmark dataset tailored for the **travel industry**, encompassing 14 real-world NLP tasks across seven common natural language processing problem types.\n- Conducted systematic evaluations of 67 open- and closed-source LLMs using this benchmark, focusing on model performance in terms of **domain adaptability, scaling laws, and the impact of reasoning mechanisms**.\n- All data is derived from actual business scenarios and anonymized, ensuring that evaluation results closely reflect real-world applications.\n\n### **Key Contributions**\n1. **Created the first multi-task LLM benchmark focused specifically on the travel industry (TravelBench)**:\n   - Covers a wide range of tasks—from sentiment analysis and intent recognition to generation tasks (e.g., summarization, translation), and even text quality assessment based on the HELM framework (faithfulness, compliance, etc.), significantly expanding beyond existing research that primarily focuses on \"opinion mining.\"\n\n2. **Revealed performance bottlenecks of general-purpose LLMs in low-resource domains**:\n   - Even state-of-the-art models with massive parameter counts and high training compute exhibit clear performance ceilings on complex, specialized travel-related tasks when used without fine-tuning.\n\n3. **Conducted in-depth analysis of model scale and reasoning mechanisms**:\n   - Found that performance gains diminish significantly as training FLOPs increase, especially plateauing after approximately $0.5 \\times 10^{16}$ FLOPs.\n   - Reasoning mechanisms provide positive benefits for small models but show little to no improvement—and sometimes even slight degradation—for larger models.\n\n---\n\n## 2. Breakthroughs and Innovations\n\nThe paper presents several methodological and empirical innovations:\n\n### ✅ Innovation 1: Constructing a Realistic, Diverse, Domain-Specific Benchmark for Low-Resource Settings\n- Most existing LLM benchmarks (e.g., MMLU, GSM8K) focus on general knowledge or high-resource language tasks, failing to reflect real challenges faced by enterprises in vertical domains.\n- TravelBench is the first to systematically integrate **14 travel-related tasks drawn from real deployed systems**, covering classification, generation, and judgment tasks—filling a critical gap in evaluating low-resource industrial scenarios.\n\n### ✅ Innovation 2: Introducing the \"LLM-as-a-judge\" Paradigm with Standardized Metrics\n- Adopts evaluation dimensions from HELM—**Faithfulness, Relevance, Inclusiveness, Compliance**—applied directly to concrete tasks, particularly for scoring the quality of generated outputs like summaries and translations.\n- Proposes a unified normalized performance metric $P_m$, enabling cross-task and cross-metric model comparisons.\n\n### ✅ Innovation 3: Revealing the Counterintuitive Phenomenon — “Reasoning ≠ Always Better”\n- Experimental results show that enabling internal chain-of-thought (reasoning trace) improves performance for **small models**, but offers **no significant gain—and may slightly degrade performance—for large models**.\n- This finding challenges the widely held assumption that “larger models + stronger reasoning = better performance,” suggesting a need to rethink the design and applicability boundaries of reasoning mechanisms.\n\n### ✅ Innovation 4: Validating the Existence of “Diminishing Returns to Scale” in Domain-Specific Tasks\n- Figure 1 shows performance growth flattens after training FLOPs exceed ~$5 \\times 10^{15}$, indicating that simply increasing model size cannot effectively solve domain adaptation issues.\n- This provides empirical support for the feasibility of lightweight models in enterprise deployments.\n\n---\n\n## 3. Startup Ideas Inspired by the Paper\n\nBased on the insights revealed in TravelBench, here are several highly promising startup directions:\n\n### 💡 Startup Idea 1: **Vertical-Specific LLM Evaluation Platform (SaaS)**\n\n#### Project Name: **DomainEval.ai**  \n#### Positioning: Custom LLM selection and performance validation services for enterprises\n\n##### Core Value\n- Build professional benchmark suites similar to TravelBench for industries such as finance, healthcare, law, education, and tourism.\n- Help enterprise clients scientifically evaluate how different LLMs perform on their specific business tasks before deployment, avoiding blind model selection.\n\n##### Key Features\n- Support uploading private data to automatically construct evaluation sets (with anonymization)\n- Built-in evaluation dimensions: accuracy, faithfulness, compliance, latency, cost, etc.\n- Generate visual reports and recommend optimal cost-performance models (e.g., Qwen-7B vs. GPT-3.5-turbo)\n\n##### Business Model\n- B2B SaaS subscription + API usage billing\n- Potential integration partnerships with cloud providers (e.g., AWS Bedrock, Azure AI Studio)\n\n> 🔍 *Inspiration: The paper highlights that general benchmarks fail to predict domain performance → market gap exists*\n\n---\n\n### 💡 Startup Idea 2: **Lightweight Industry-Specific LLM Optimization Engine**\n\n#### Project Name: **LiteTune**  \n#### Positioning: Empowering SMEs to efficiently deploy high-performing small models\n\n##### Core Value\n- Leveraging the paper’s finding that “small models benefit significantly from reasoning,” develop a toolchain combining **instruction tuning + reasoning enhancement** tailored for small-to-medium models.\n- Enable compact models to match or even surpass large models on specific tasks while maintaining low inference costs.\n\n##### Technical Approach\n- Utilize lightweight fine-tuning techniques (LoRA, P-Tuning, etc.)\n- Design domain-adaptive Chain-of-Thought (CoT) prompt templates\n- Automate search for optimal prompt structures and reasoning activation strategies\n\n##### Use Cases\n- Customer service bots, review moderation, itinerary recommendations—common high-frequency tasks in travel companies\n- Can be packaged as an SDK for integration into OTAs or hotel management systems\n\n> 🔍 *Inspiration: The paper finds reasoning helps small models more → opportunity to enable “small model逆袭” (comeback)*\n\n---\n\n### 💡 Startup Idea 3: **AI-Generated Content Trustworthiness Audit Tool**\n\n#### Project Name: **TruthGuard**  \n#### Positioning: Detect hallucinations and compliance risks in LLM-generated content\n\n##### Core Value\n- Based on the paper’s emphasis on Faithfulness & Compliance tasks from HELM, build automated tools to detect **factual errors, policy violations, and biased expressions** in AI-generated text.\n\n##### Functionality\n- Input: AI-generated text (e.g., customer replies, marketing copy, travel guides)\n- Output:\n  - Hallucination score (likelihood of fabricated information)\n  - Compliance alerts (presence of sensitive topics)\n  - Inclusivity index (risk of discriminatory language)\n- Supports API integration into publishing workflows for pre-deployment risk control\n\n##### Target Customers\n- Online Travel Agencies (OTAs) – prevent false advertising\n- Social media / UGC platforms – content moderation\n- Publishers and news outlets – ensure factual integrity in AI-assisted writing\n\n> 🔍 *Inspiration: The paper identifies “faithfulness” as a key weakness → strong regulatory demand exists*\n\n---\n\n### 💡 Startup Idea 4: **Travel Agent Task Planning Benchmark Service**\n\n#### Project Name: **TripAgent Benchmark**  \n#### Positioning: Performance testing and optimization advisory for AI travel planners\n\n##### Background Extension\n- The paper mentions emerging benchmarks like TravelPlanner focused on tool use and complex planning.\n- With rising adoption of AI travel assistants (e.g., Google Trip Planning, Ctrip AI Guide), there's growing demand for standardized metrics to assess real-world usability.\n\n##### Product Form\n- Develop an open benchmark platform simulating complex user requests (e.g., “Plan a one-week Kansai trip with elderly parents and kids, budget ¥20k, avoid peak season”)\n- Test whether the AI can correctly invoke APIs for flights, hotels, visas, weather, etc., and generate coherent itineraries\n- Report success rate, tool-call accuracy, logical consistency, and other KPIs\n\n##### Business Opportunity\n- Charge AI travel product teams for stress-testing services\n- Publish annual *Global AI Travel Assistant Rankings* to establish industry authority\n\n> 🔍 *Inspiration: The paper calls for extending benchmarks to agent-style tasks → early mover advantage in an emerging niche*\n\n---\n\n## Summary\n\n| Dimension | Content |\n|---------|--------|\n| **Core Contribution** | Created the TravelBench benchmark; revealed performance bottlenecks of LLMs in low-resource travel tasks |\n| **Key Innovations** | Multi-task real-world dataset, reversed reasoning effect, validated diminishing returns to scale |\n| **Entrepreneurial Insights** | Vertical evaluation platforms, small-model enhancement tools, AI content auditing, agent benchmarking services |\n\n> 📌 **One-Sentence Takeaway**:  \n> This paper is not just a rigorous academic study—it offers a clear roadmap for deploying AI in real industries: **Don’t blindly trust big models; dive deep into domain data. Don’t just measure general capabilities; prioritize performance in real-world scenarios.** And this insight is precisely where entrepreneurs should start building in the next phase of AI.",
    "github": "",
    "hf": ""
}