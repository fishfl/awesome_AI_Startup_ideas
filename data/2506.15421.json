{
    "id": "2506.15421",
    "title": "Reward Models in Deep Reinforcement Learning: A Survey",
    "summary": "This paper provides a comprehensive review of reward modeling techniques in reinforcement learning, covering background, method classification, applications, evaluation methods, and future research directions.",
    "abstract": "In reinforcement learning (RL), agents continually interact with the environment and use the feedback to refine their behavior. To guide policy optimization, reward models are introduced as proxies of the desired objectives, such that when the agent maximizes the accumulated reward, it also fulfills the task designer's intentions. Recently, significant attention from both academic and industrial researchers has focused on developing reward models that not only align closely with the true objectives but also facilitate policy optimization. In this survey, we provide a comprehensive review of reward modeling techniques within the deep RL literature. We begin by outlining the background and preliminaries in reward modeling. Next, we present an overview of recent reward modeling approaches, categorizing them based on the source, the mechanism, and the learning paradigm. Building on this understanding, we discuss various applications of these reward modeling techniques and review methods for evaluating reward models. Finally, we conclude by highlighting promising research directions in reward modeling. Altogether, this survey includes both established and emerging methods, filling the vacancy of a systematic review of reward models in current literature.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Single-Agent",
    "authors": "Rui Yu,Shenghua Wan,Yucen Wang,Chen-Xiao Gao,Le Gan,Zongzhang Zhang,De-Chuan Zhan",
    "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:IJCAI 2025 Survey Track (To Appear)",
    "keypoint": "- Reward models are crucial in guiding policy optimization in reinforcement learning.\n- The survey provides a comprehensive review of reward modeling techniques within the deep RL literature, categorizing them based on source, mechanism, and learning paradigm.\n- Reward sources include human-provided rewards (manual reward engineering and human-in-the-loop reward learning) and AI-generated rewards using foundation models.\n- Reward mechanisms are divided into extrinsic rewards (task-specific incentives) and intrinsic motivation (encouraging exploration and skill development).\n- Learning paradigms for reward models include learning from demonstrations, goals, and preferences.\n- Applications of reward models span control problems, generative model post-training, and other fields like recommendation systems.\n- Evaluation methods for reward models involve assessing policy performance, using distance metrics, and interpreting representations.\n- Future research directions include vectorized rewards, interpretability of reward models, ethical alignment, and developing reward foundation models.",
    "date": "2025-06-22",
    "paper": "arXiv:2506.15421v1  [cs.LG]  18 Jun 2025\nReward Models in Deep Reinforcement Learning: A Survey\nRui Yu , Shenghua Wan , Yucen Wang , Chen-Xiao Gao ,\nLe Gan , Zongzhang Zhang , De-Chuan Zhan\nNational Key Laboratory for Novel Software Technology, Nanjing University, China\nSchool of Artificial Intelligence, Nanjing University, China\n{yur,wansh,wangyc,gaocx}@lamda.nju.edu.cn, {ganle,zzzhang,zhandc}@nju.edu.cn\nAbstract\nIn reinforcement learning (RL), agents continually\ninteract with the environment and use the feedback\nto refine their behavior. To guide policy optimiza-\ntion, reward models are introduced as proxies of\nthe desired objectives, such that when the agent\nmaximizes the accumulated reward, it also fulfills\nthe task designer‚Äôs intentions.\nRecently, signifi-\ncant attention from both academic and industrial\nresearchers has focused on developing reward mod-\nels that not only align closely with the true objec-\ntives but also facilitate policy optimization. In this\nsurvey, we provide a comprehensive review of re-\nward modeling techniques within the deep RL lit-\nerature.\nWe begin by outlining the background\nand preliminaries in reward modeling. Next, we\npresent an overview of recent reward modeling ap-\nproaches, categorizing them based on the source,\nthe mechanism, and the learning paradigm. Build-\ning on this understanding, we discuss various ap-\nplications of these reward modeling techniques and\nreview methods for evaluating reward models. Fi-\nnally, we conclude by highlighting promising re-\nsearch directions in reward modeling. Altogether,\nthis survey includes both established and emerging\nmethods, filling the vacancy of a systematic review\nof reward models in current literature.\n1\nIntroduction\nIn recent years, deep reinforcement learning (DRL), a ma-\nchine learning paradigm that combines RL with deep learn-\ning, has demonstrated its immense potential in applications\nacross various domains. For example, AlphaGo [Silver et\nal., 2016] showcased RL‚Äôs capability of complex decision-\nmaking in game scenarios; InstructGPT [Ouyang et al., 2022]\nmarked the irreplaceable role of RL in aligning language\nmodels with human intents; agents trained via large-scale\nRL, such as OpenAI-o1 and DeepSeek-R1 [Guo et al., 2025],\ndemonstrated impressive reasoning intelligence that is com-\nparable or even exceeds human capability. Unlike supervised\nlearning (SL) where the agent is required to imitate and repli-\ncate the behavior recorded in the dataset, RL sets itself apart\nby enabling the agent to explore, adapt, and optimize its be-\nhavior based on the outcome of its actions, thereby achieving\nunprecedented levels of autonomy and capability.\nA key component of reinforcement learning is the reward,\nwhich essentially defines the goal of interest in the task and\nguides the agents to optimize their behavior toward that in-\ntent [Sutton et al., 1998]. Just as dopamine motivates and\nreinforces adaptive actions in biological systems, rewards in\nRL encourage exploration of the environment and guide in-\ntelligent agents towards desired behaviors [Glimcher, 2011].\nHowever, while rewards are typically predefined in research\nenvironments [Towers et al., 2024], they are often absent or\ndifficult to specify in many real-world applications. In light of\nthis, a significant portion of modern RL research focuses on\nhow to extract effective rewards from various types of feed-\nback, after which standard RL algorithms can be applied to\noptimize the policies of agents.\nDespite the crucial role of reward modeling in RL, exist-\ning surveys [Arora and Doshi, 2021; Kaufmann et al., 2023]\nare often embedded within specific subdomains such as in-\nverse reinforcement learning (IRL) and reinforcement learn-\ning from human feedback (RLHF), with a limited focus on\nreward modeling as a standalone topic. To bridge this gap,\nwe provide a systematic review of reward models, cover-\ning their foundations, key methodologies, and applications\nacross diverse RL settings. We introduce a new categoriza-\ntion framework that addresses three fundamental questions:\n(1) The source: Where does the reward come from?\n(2)\nThe mechanism: What drives the agent‚Äôs learning? (3) The\nlearning paradigm: How to learn the reward model from var-\nious types of feedback? Furthermore, we highlight recent ad-\nvancements in reward models based on foundation models,\nsuch as large language models (LLMs) and vision-language\nmodels (VLMs), which have received relatively little atten-\ntion in previous surveys. The framework of reward modeling\nwe establish in this survey is illustrated in Figure 1. Specifi-\ncally, this survey is organized as follows:\n1. Background of reward modeling (Section 2). We first\nprovide the necessary background on RL and reward\nmodels;\n2. Categorization of reward models. We propose a clas-\nsification framework for reward models, distinguishing\nthem by three key factors: the source (Section 3), the\nSources\nHuman-provided\nAI-generated\nLearning paradigms\nDemonstrations\nGoals\nA ‚âªùêµPreferences\nReward Model\nAgent\nAction ùëé\nState ùë†\nReward ùëü\nMechanisms\nIntrinsic\nReward\nExtrinsic\nReward\nEnvironment\nFigure 1: A framework for reward modeling in RL, categorizing reward models by their sources, feedback types, and mechanisms to provide\na structured understanding of how rewards are derived and utilized in RL systems.\nmechanism that drives learning (Section 4), and the\nlearning paradigm used to derive rewards (Section 5).\nWe also list recent publications about reward modeling\nand categorize them based on our hierarchy in Table 1.\n3. Applications and evaluation methods of reward mod-\nels (Section 6 and Section 7). We provide a discussion\non the applications of reward models in practical scenar-\nios, together with evaluation methods for these models.\n4. Prosperous directions and discussions (Section 8).\nWe summarize this survey by presenting potential future\ndirections in this topic.\n2\nBackground\nRL is typically formulated as a Markov Decision Process\n(MDP) ‚ü®S, A, T, R, Œ≥‚ü©, where S and A denote the state space\nand the action space, respectively. The transition function\nT(¬∑|s, a) defines the distribution over the next states after tak-\ning action a at state s. The reward model R(s, a) specifies\nthe instantaneous reward that the agent will receive after tak-\ning action a at state s, and Œ≥ is the discount factor that bal-\nances the importance of future rewards. An RL agent aims\nto find the policy œÄ(a|s) maximizing the following expected\ndiscounted cumulative reward (a.k.a. return):\nJ (œÄ) = EœÄ,T\n\" ‚àû\nX\nt=0\nŒ≥tR(st, at)\n#\n.\n(1)\nwhere the expectation is taken over the distribution of states\nand actions that the agent will encounter following œÄ and T.\nThe fundamental objective of learning is to refine an\nagent‚Äôs behavior to accomplish predefined goals or tasks.\nWhile supervised learning (SL) offers a principled approach\nby training agents on human-annotated datasets to mimic hu-\nman behavior, this method is limited by the quantity and qual-\nity of available human demonstrations. Consequently, agents\ntrained solely by SL may make irrational decisions when\nhuman behavior is missing or sub-optimal. Reinforcement\nlearning instead offers another principled way that permits\nthe agent to explore the environment autonomously and adapt\nits behavior based on the rewards it receives. Such a trial-\nand-error approach exempts the agent from the constraints of\ndatasets and opens the possibility of achieving or even sur-\npassing human-level performance.\nAlthough S, A, and the transition model T are inherently\ndefined by the environment, the reward model R must be\ncarefully crafted by the task designer. This careful design\nis crucial to ensure that the specified rewards truly reflect the\nunderlying objectives. In many applications, only descriptive\nguidelines or standards of the intended goals are available,\nand therefore we need to convert them into statistical reward\nmodels. This process is termed as reward modeling through-\nout this survey.\n3\nSources of Rewards\nIn this section, we explore different sources of reward signals\nin RL. We categorize reward sources into two main types:\nhuman-provided rewards, which leverage human expertise\nand supervision, and AI-generated rewards, which rely on\nfoundation models typically trained by self-supervised learn-\ning on internet-scale datasets.\n3.1\nHuman-Provided Rewards\nManual Reward Engineering\nManual reward engineering refers to the process where\nresearchers meticulously design reward functions to steer\nagents toward optimal policies. Take the walker task in Gym-\nMuJoCo [Towers et al., 2024] as an example: its reward\nis manually designed as a combination of survival, forward\nmovement, and control cost penalties. However, reward en-\ngineering requires human experts to translate ambiguous task\nobjectives into precise statistical models. Such an undertak-\ning can be both resource-intensive and perilous: if the reward\nfunction is inadequately crafted, the agent may suffer from re-\nward hacking, leading to unpredictable behaviors [Kaufmann\net al., 2023].\nHuman-in-the-Loop Reward Learning\nInstead of directly crafting the reward models, human-in-the-\nloop reward learning derives rewards from indirect human su-\npervision, including demonstrations [Abbeel and Ng, 2004],\ngoals [Liu et al., 2022], and preferences [Kaufmann et al.,\n2023]. Compared to manual reward engineering, asking hu-\nman experts to provide demonstrations or feedback of such\nkind is much more straightforward.\nHowever, the reward\nlearning process needs to be specifically designed to accom-\nmodate different kinds of supervision and ensure alignment\nwith the intended task objectives.\n3.2\nAI-Generated Rewards\nFoundation models, such as large language models (LLMs)\nand vision-language models (VLMs) pre-trained on internet-\nscale human-generated data, have demonstrated a remark-\nable ability to interpret human intent and autonomously de-\nfine reward models for RL. For instance, LLMs have been\nemployed to design reward functions [Xie et al., 2023] and\ngenerate feedback for reward learning [Klissarov et al., 2023;\nBai et al., 2022; Lee et al., 2024]. VLMs, in particular, are\nhighly effective in specifying rewards and tasks within visu-\nally complex environments. Some studies [Fan et al., 2022;\nSontakke et al., 2023] compute semantic similarity between\nagent states and task descriptions, enabling dense reward sig-\nnals from visual observations. Others [Wang et al., 2024] uti-\nlize VLMs to analyze visual inputs and generate preference-\nbased feedback for reward model training. While certain ap-\nproaches [Baumli et al., 2023] leverage off-the-shelf founda-\ntion models for zero-shot reward specification, others [Fan et\nal., 2022; Sontakke et al., 2023] fine-tune these models on\ndomain-specific datasets to improve reward design.\n4\nReward Mechanisms\nIn this section, we focus on two different reward mechanisms\nthat drive RL agent‚Äôs learning.\n4.1\nExtrinsic Reward\nRewards are defined by incentives that drive the agent. The\nterm extrinsic reward corresponds to incentives that arise\nfrom external sources and directly relate to the desired task\nobjective, e.g., instructions or goals set by supervisors or em-\nployers. Defining extrinsic rewards requires the task designer\nto translate abstract goals into concrete, quantifiable rewards\nthat can be incorporated into a standard RL pipeline. The\napproach to accomplish this is detailed in Section 5.\n4.2\nIntrinsic Motivation\nIn contrast to extrinsic rewards, intrinsic motivation (IM) cap-\ntures an agent‚Äôs innate motivation to explore and refine its be-\nhavior in the environment [Ryan and Deci, 2000]. Harlow\n[1950] observed that even without extrinsic stimulus, mon-\nkeys have spontaneous desire and curiosity to solve com-\nplex puzzles. Later [Barto et al., 2004] introduced IM into\nthe reward mechanism, leading to the application of intrin-\nsic reward. Unlike extrinsic rewards, intrinsic rewards are\noften disentangled from specific task objectives; rather, they\nencapsulate the encouragement for beneficial behaviors for\nproblem-solving, such as exploration.\nTo coordinate the intrinsic reward and extrinsic reward, one\ncommon approach is to compute the agent‚Äôs reward r as a\nweighted sum of the intrinsic reward rint and the extrinsic re-\nward rext:\nr = Œªrint + (1 ‚àíŒª)rext,\n(2)\nwhere 0 ‚â§Œª ‚â§1 is a coefficient that balances the intrinsic\nreward rint and extrinsic reward rext.\nNext, we introduce three widely used types of intrinsic mo-\ntivation in reinforcement learning.\nExploration\nIM has long been used to encourage exploration. By lever-\naging concepts such as surprise [Pathak et al., 2017], epis-\ntemic uncertainty [Houthooft et al., 2016], and disagreement\n[Pathak et al., 2019; Sekar et al., 2020], many methods quan-\ntify the strangeness of states as the prediction errors of state\ntransition, and thus use the errors as intrinsic rewards to en-\ncourage the agent to explore unseen areas of the environment.\nThe strangeness of states can also be quantified using the dis-\ntillation error between randomly initialized networks [Burda\net al., 2018], which can be more flexible to implement.\nOther works design intrinsic rewards for exploration\nthrough the lens of data diversity. Among them, count-based\nmethods, such as the well-known upper confidence bound\n(UCB) [Lai and Robbins, 1985], maintain the state visita-\ntion counts and assign higher intrinsic rewards for less-visited\nstates. Later, static hashing [Tang et al., 2017] and density es-\ntimation [Bellemare et al., 2016; Ostrovski et al., 2017] are\nincorporated to extend count-based exploration to problems\nwith larger or even continuous state spaces. On the other\nhand, Liu and Abbeel [2021] and Badia et al. [2020] promote\ndiversity by estimating the data entropy and using the entropy\nas the intrinsic rewards. In this way, they can encourage the\nagent to explore novel and diverse states.\nEmpowerment\nEmpowerment, an information-theoretic intrinsic motivation\n(IM) concept, motivates an agent to maximize its influence\non the environment by seeking states where it possesses the\ngreatest control over future outcomes [Klyubin et al., 2005].\nAn intrinsic reward signal can then be formulated to guide the\nagent‚Äôs exploration towards states that offer greater control\nand a wider diversity of achievable consequences. Many pre-\nvious works leverage empowerment for skill discovery [Ey-\nsenbach et al., 2018; Mazzaglia et al., 2022]. These works\naim to find a skill-conditioned policy œÄ(a|s, z) that maxi-\nmizes the mutual information between the resulting trajectory\nand the latent variable z. The intrinsic reward is designed\nbased on the decomposition of this mutual information. The\nagent is then encouraged to recover the latent z from the tra-\njectory, implying that different z should produce distinctly\ndifferent trajectories, thereby defining z as the skill. By pro-\nviding an intrinsic reward based on the agent‚Äôs potential to\ninfluence the environment, skill learning through empower-\nment enables more generalizable agent behaviors and facili-\ntates rapid adaptation to new tasks.\nKnowledge-Driven IM\nMany approaches leverage high-level knowledge and struc-\ntured reasoning to generate intrinsic rewards, bridging the gap\nbetween abstract understanding and low-level sensorimotor\ninteractions. Some methods derive preferences from struc-\ntured event descriptions, comparing pairs of observations to\ninfer meaningful intrinsic signals [Klissarov et al., 2023]. Xu\net al. [2023] adopted a reward-shaping technique by treating\nvaluable propositional logic knowledge as intrinsic rewards\nfor the RL procedure. Du et al. [2023] generates goal can-\ndidates based on an agent‚Äôs current context and provides re-\nwards for achieving those inferred objectives. In recent work\n[Klissarov et al., 2023], large-scale models such as LLMs and\nVLMs have been employed to facilitate this process due to\ntheir broad knowledge and reasoning capabilities.\nSource\nMechanism\nFeedback\nMethod\nhuman\nintrinsic\n-\n[Pathak et al., 2017; Houthooft et al., 2016; Pathak et al., 2019; Sekar et al.,\n2020; Burda et al., 2018; Bellemare et al., 2016; Badia et al., 2020; Liu and\nAbbeel, 2021; Eysenbach et al., 2018; Mazzaglia et al., 2022; Wan et al.,\n2024]\nAI\nintrinsic\n-\n[Klissarov et al., 2023; Xu et al., 2023; Du et al., 2023]\nhuman\nextrinsic\ndemonstration\n[Abbeel and Ng, 2004; Ziebart et al., 2008; Finn et al., 2016a,b; Fu et al.,\n2017; Jeon et al., 2020]\nhuman\nextrinsic\ngoal\n[Liu et al., 2022; Nachum et al., 2018; Mazzaglia et al., 2024; Hartikainen\net al., 2019; Mendonca et al., 2021; Park et al., 2023; Myers et al., 2024;\nWang et al., 2025]\nAI\nextrinsic\ngoal\n[Sontakke et al., 2023; Fan et al., 2022; Rocamonde et al., 2023]\nhuman\nextrinsic\npreference\n[Christiano et al., 2017; Kim et al., 2023; Verma and Metcalf, 2024; Knox et\nal., 2022; Touvron et al., 2023; Liu et al., 2024a; Ouyang et al., 2022; K¬®opf\net al., 2023; Rafailov et al., 2023; Song et al., 2024; Liu et al., 2024b]\nAI\nextrinsic\npreference\n[Bai et al., 2022; Lee et al., 2024; Wang et al., 2024]\nTable 1: Summary of the algorithms mentioned in Section 3, Section 4, and Section 5.\n5\nLearning Paradigms\nIn this section, we focus on the paradigms of learning the\nreward model RŒ∏ from different kinds of human feedback.\nSpecifically, existing literature that involves reward learning\ncan be broadly categorized into three paradigms, namely:\n‚Ä¢ Learning from demonstrations, which extracts reward\nmodels based on demonstrations provided by human ex-\nperts. This is related to inverse RL (IRL) [Arora and\nDoshi, 2021].\n‚Ä¢ Learning from goals, which derives reward models\nfrom specified goal states.\nThis is related to goal-\nconditional RL (GCRL) [Liu et al., 2022].\n‚Ä¢ Learning from preferences, which extracts reward\nmodels from human preferences among two or more tra-\njectory segments. This is related to preference-based RL\n(PbRL) and reinforcement learning from human feed-\nback (RLHF) [Kaufmann et al., 2023].\nIn each subsection, we will provide a brief overview of the\nestablished methods in each setting.\n5.1\nLearning from Demonstrations\nMaximum-Entropy Inverse Reinforcement Learning\nPrevious approaches to IRL iteratively optimize the reward\nmodel to maximize the performance margin between demon-\nstrations and any other policy, such that the demonstrations\nappear optimal under the learned reward model [Abbeel and\nNg, 2004]. However, the IRL problem is inherently ill-posed,\nbecause multiple distinct rewards may explain the same ex-\npert behavior. A common strategy for resolving this ambigu-\nity is to incorporate additional regularization into the learn-\ning objective. As an example, the maximum-entropy IRL\n(MaxEnt-IRL) framework [Ziebart et al., 2008] introduces\nentropy regularization such that the expert demonstrations are\ndrawn from the Boltzmann distribution:\npŒ∏(œÑ) = exp(RŒ∏(œÑ))\nZŒ∏\n,\n(3)\nwhere œÑ = (s1, a1, . . . , s|œÑ|, a|œÑ|) denotes the demonstrated\ntrajectory, and RŒ∏(œÑ) = P|œÑ|\nt=1 RŒ∏(st, at) is the cumulative\nreward along œÑ. The partition function ZŒ∏ normalizes the dis-\ntribution, and it can be computed via dynamic programming\nin small, discrete domains [Ziebart et al., 2008] or approxi-\nmated by importance sampling in continuous settings [Finn et\nal., 2016b]. By parameterizing the reward model RŒ∏ as linear\nmodels or neural networks, we can perform maximum like-\nlihood training based on observed demonstrations and obtain\nthe reward models that explain the demonstrations.\nAdversarial Reward Learning\nFinn et al. [2016a] demonstrated that the MaxEnt-IRL prob-\nlem can be reformulated as a generative adversarial network\n(GAN) problem by employing a specifically structured dis-\ncriminator. Let the generator of the trajectories and the re-\nward model be qœà(œÑ) and RŒ∏(œÑ) respectively, the discrimina-\ntor is parameterized as:\nDŒ∏(œÑ) =\n1\nZ exp(RŒ∏(œÑ))\n1\nZ exp(RŒ∏(œÑ)) + qœà(œÑ),\n(4)\nwhere Z represents the partition function and can be esti-\nmated via importance sampling. The generator and the dis-\ncriminator are trained via standard GAN losses:\nL(Œ∏) = EœÑ‚àºDe [‚àílog DŒ∏(œÑ)] + EœÑ‚àºq [‚àílog(1 ‚àíDŒ∏(œÑ))] ,\nL(œà) = EœÑ‚àºqœà\n\u0014\nlog (1 ‚àíDŒ∏(œÑ))\nDŒ∏(œÑ)\n\u0015\n= EœÑ‚àºqœà [‚àíRŒ∏(œÑ)] ‚àíH(qœà) + log Z,\n(5)\nwhere De denotes the expert demonstrations and H is the\nentropy.\nBy optimizing (5), we can effectively optimize\nthe reward model RŒ∏.\nWhen the optimization converges,\nit follows from the maximum-entropy theory that q‚àó(œÑ) ‚àù\nexp(R‚àó(œÑ)), which exactly recovers the MaxEnt-IRL prob-\nlem in (3). However, conducting optimization over the tra-\njectories incurs high variance, and therefore the adversarial\ninverse RL (AIRL) framework [Fu et al., 2017] further de-\ncomposes the problem and operates on a state-action level:\nL(Œ∏) = EDe [‚àílog DŒ∏(s, a)] + Eqœà [‚àílog(1 ‚àíDŒ∏(s, a))] ,\n(6)\nwhere DŒ∏(s, a) =\nexp fŒ∏(s,a)\nexp(fŒ∏(s,a))+pœà(a|s). Once training is com-\nplete, fŒ∏ is shown to recover the optimal advantage func-\ntion A‚àó, from which reward models may subsequently be ex-\ntracted. Building on this foundation, the AIRL framework has\nbeen further extended ‚Äì for instance, to encompass a broader\nclass of regularizations [Jeon et al., 2020] .\n5.2\nLearning from Goals\nWhen our intended goals can be explicitly described or spec-\nified as a state g ‚ààS, the reward model can be conveniently\ndefined based on whether the goal is achieved [Liu et al.,\n2022]:\nR(s, g) = 1(s accomplishes g),\n(7)\nwhere 1 is the indicator function. However, this binary re-\nward structure is extremely sparse and inefficient for policy\noptimization, because the agent only receives a reward upon\nreaching the goal state, without intermediate supervision. To\naddress this sparsity, an alternative solution is to reshape the\nreward as the distance between the current and the desired\ngoal:\nR(s, g) = ‚àíd(œï(s), œà(g)),\n(8)\nwhere œï and œà are mapping functions that transform the state\ns and the goal g to the same latent space, and d(¬∑, ¬∑) is a spe-\ncific distance metric on that space. This distance-based re-\nward provides a more nuanced measurement of the agent‚Äôs\nprogress toward the specified goal. In the below, we will in-\ntroduce two commonly adopted distance metrics: spatial dis-\ntance and temporal distance.\nSpatial Distance\nSpatial distance directly quantifies the similarity between\nstates from the environment.\nCommon approaches utilize\nmeasures such as the L2 distance [Nachum et al., 2018], and\ncosine similarity [Mazzaglia et al., 2024] to assess the prox-\nimity between states. These metrics may be computed ei-\nther in the raw state space [Nachum et al., 2018], or within\na learned latent space [Mazzaglia et al., 2024] which better\ncaptures and exploits the problem structure.\nTemporal Distance\nOther works focus on the notion of temporal distance, which\nconceptually assigns higher rewards to states that are tempo-\nrally closer to the goal state. For instance, approaches like\nHartikainen et al. [2019] and Wang et al. [2025] train a dis-\ntance metric function dŒ∏, such that dŒ∏(s, g) approximates the\nnumber of time steps required for the agent to reach g from\ns. Using R = ‚àídŒ∏ as the reward model, the agent will be\nguided toward states that are in the proximity of the goal.\nMoreover, [Park et al., 2023] frames temporal distance learn-\ning as a constrained optimization problem, maintaining a dis-\ntance threshold between adjacent states while dispersing oth-\ners. Recently, [Myers et al., 2024] defines a temporal distance\nmetric based on successor features and temporal contrastive\nlearning, which is shown to satisfy the quasi-metric property.\nTemporal distance offers a more grounded reward signal by\neffectively reflecting the agent‚Äôs progress toward the goal and\ncapturing deeper task semantics beyond visual details.\nSemantic Similarity\nSemantic similarity-based rewards measure how closely the\nagent‚Äôs current state aligns with a given goal in a shared rep-\nresentation space. RoboCLIP [Sontakke et al., 2023] com-\nputes the reward as the dot product between the text embed-\nding of a language-specified goal and the video embedding of\nthe agent‚Äôs observed trajectory. MineCLIP [Fan et al., 2022]\ncomputes rewards as R = max\n\u0010\nPG ‚àí\n1\nNT , 0\n\u0011\n, where PG is\nthe probability of the observation video matching the goal\ndescription against negatives, and\n1\nNT serves as a baseline\nto filter out uncertain estimates. These embeddings can be\nobtained from VLMs, which map multimodal inputs into a\ncommon space, allowing the agent to learn from high-level\ninstructions or demonstrations.\n5.3\nLearning from Preferences\nIn many applications, obtaining human evaluations is com-\nparatively cost-effective compared to collecting demonstra-\ntions or identifying the goal states. Consider training lan-\nguage models to follow instructions as an example, it is\nboth tedious and time-consuming to require human annota-\ntors to generate template responses for every request. On the\ncontrary, comparing agent-generated responses using metrics\nsuch as helpfulness, harmlessness, and truthfulness is con-\nsiderably more straightforward.\nIn this section, we there-\nfore investigate methods for deriving rewards from human-\nannotated preferences among candidate options.\nIn this framework, annotators are asked to label their pref-\nerences y between a pair of trajectories (œÑ 0, œÑ 1), where œÑ =\n(s1, a1, . . . , s|œÑ|, a|œÑ|). A label y = 0 means œÑ 0 is preferred\nover œÑ 1 (denoted as œÑ 0 ‚âªœÑ 1), and y = 1 implies the opposite.\nTo build the connection between observed preferences and\nreward models, we need preference models. A widely used\nexample is Bradley-Terry (BT) models [Bradley and Terry,\n1952], which posit that the probability of preference can be\ndescribed by a Boltzmann distribution applied to the cumula-\ntive reward:\nPBT(œÑ 0 ‚âªœÑ 1; Œ∏) =\nexp(P\n(s0\nt ,a0\nt )‚ààœÑ 0 RŒ∏(s0\nt, a0\nt))\nP\nj‚àà{0,1} exp(P\n(sj\nt,aj\nt)‚ààœÑ j RŒ∏(sj\nt, aj\nt))\n.\n(9)\nTo optimize the reward model RŒ∏, we can maximize the like-\nlihood of the observed preferences:\nL(Œ∏) = ‚àí\nX\n(œÑ 0,œÑ 1,y)‚ààD\n(1 ‚àíy) log P(œÑ 0 ‚âªœÑ 1; Œ∏) + y log P(œÑ 1 ‚âªœÑ 0; Œ∏),\n(10)\nwhere P is defined according to the preference model in\n(9). After training, we can label the reward of each transi-\ntion pair and subsequently employ any RL algorithm to opti-\nmize the policies [Christiano et al., 2017]. Alternatively, we\ncan also directly train the policy via (10) by reparameterizing\nthe reward model through the policy in certain circumstances\n[Rafailov et al., 2023].\nPreference Models\nDespite its popularity in PbRL literature, BT models may\nnot align with reality [Kim et al., 2023]. Consequently, sev-\neral studies have proposed alternative preference models that\nmore closely reflect the mechanisms underlying human pref-\nerences. Preference Transformer [Kim et al., 2023] intro-\nduces importance weights over state-action pairs to account\nfor the dependence on certain critical states in the trajectory:\nPPT(œÑ 0 ‚âªœÑ 1; Œ∏) =\nexp(P\n(s0\nt ,a0\nt )‚ààœÑ 0 w0\nt RŒ∏(s0\nt, a0\nt))\nP\nj exp(P\n(sj\nt,aj\nt)‚ààœÑ j wj\ntRŒ∏(sj\nt, aj\nt))\n,\n(11)\nwhere j ‚àà{0, 1} and the weights wj\nt are the average attention\nweights of the pair (sj\nt, aj\nt) ‚ààœÑ j calculated by a bi-directional\nattention layer. Similarly, Verma and Metcalf [2024] replaced\nweights in (11) with attention weights from a transformer-\nbased transition model, thereby incorporating state impor-\ntance priors from the perspective of transition models. Be-\nsides, the regret-based models [Knox et al., 2022] propose to\nmodel human preferences by the sum of optimal advantages\nalong the trajectory, rather than the rewards:\nPReg(œÑ 0 ‚âªœÑ 1) =\nexp(‚àíRegret(œÑ 0))\nexp(‚àíRegret(œÑ 0)) + exp(‚àíRegret(œÑ 1)),\nRegret(œÑ) =\n|œÑ|\nX\nt=1\n[Q‚àó\nR(st, at) ‚àíV ‚àó\nR(st)],\n(12)\nwith V ‚àó\nR and Q‚àó\nR being the optimal state value function and\nQ-value function for the reward model R, respectively. Knox\net al. [2022] demonstrated that this approach may better pre-\ndict real human preference and the learned reward model may\nachieve superior performance in practice.\nExtension to Ordinal Feedback\nOrdinal feedback generalizes binary feedback by requiring\nannotators to additionally specify the strengths of their pref-\nerences (e.g., slightly better or significantly better). To inte-\ngrate this more nuanced information, existing studies mod-\nify BT models by incorporating soft margins [Touvron et al.,\n2023] or soft labels yi ‚àà[0, 1] [Liu et al., 2024a], where the\nmargin or the label reflects the strength of the preference.\nBeyond Pairwise Comparisons\nHuman feedback can also be provided in the form of rank-\nings among multiple candidates [Ouyang et al., 2022; K¬®opf et\nal., 2023]. Although such listwise comparisons put a greater\nburden on annotators, they also carry richer information than\npairwise comparisons. To accommodate rankings, Plackett-\nLuce (PL) models [Plackett, 1975] generalize BT models by\nextending the comparison to K candidates:\nPPL(œÑ 1 ‚âªœÑ 2 ‚âª. . . ‚âªœÑ K)\n=\nK\nY\nk=1\nexp(P\n(sk\nt ,ak\nt )‚ààœÑ k R(sk\nt , ak\nt ))\nPK\nj=k exp(P\n(sj\nt,aj\nt)‚ààœÑ j R(sj\nt, aj\nt))\n,\n(13)\nwhere (œÑ 1 ‚âªœÑ 2 ‚âª. . . ‚âªœÑ K) is the observed rank-\ning. Substituting (13) into (10) yields the objective of learn-\ning rewards from rankings [Rafailov et al., 2023; Song et\nal., 2024]. Another straightforward approach to rankings is\nbreaking the ranking into pairs by selecting two candidates\nfrom the list and assigning the label according to their ranks,\nthereby reducing the problem of applying BT models to all\npossible pairwise comparisons [Ouyang et al., 2022; Liu et\nal., 2024b].\n6\nApplications\nReward model designing constitutes an indispensable step\nbefore any practical applications of RL. Therefore, in this\nsection, we briefly review successful applications of reward\nmodels in deep RL, including control problems, generative\nmodel finetuning, and other fields.\n6.1\nControl Problems\nReward models play a pivotal role in control problems, as a\nfundamental mechanism for guiding decision-making in dy-\nnamic environments. Christiano et al. [2017] demonstrated\ntheir effectiveness in facilitating policy learning across di-\nverse domains, including game-playing and simulated contin-\nuous control tasks. In gameplay scenarios, Fan et al. [2022]\nleveraged generated rewards to enhance learning in Minecraft\ntasks. In robotics, Sontakke et al. [2023] employed reward\nmodels to train agents across various robotic tasks. Simi-\nlarly, in autonomous driving, the design of reward functions\nremains a critical aspect of training intelligent agents [Knox\net al., 2023].\n6.2\nGenerative Model Post-training\nModern generative models typically feature a two-stage train-\ning procedure, where the pre-training stage involves unsuper-\nvised learning on internet-scale data, and the post-training\nstage fine-tunes the models and fits them for downstream\ntasks. A prominent example is InstructGPT [Ouyang et al.,\n2022], which employs RL to optimize model outputs based\non human preference data. Specifically, it trains a reward\nmodel on human-ranked responses and fine-tunes the lan-\nguage model to maximize this reward. This approach has be-\ncome a standard method for enhancing the helpfulness, harm-\nlessness [Dai et al., 2023], and general task-solving capa-\nbilities of LLMs [Abramson et al., 2022]. In mathematical\nproblem-solving, golden rewards can be defined by compar-\ning the model-generated answers with ground-truth answers\n[Luong et al., 2024] or by verifying the correctness using for-\nmal solvers [Xin et al., 2024]. Some works also use LLM-\nbased verifiers [Zhang et al., 2024], further leveraging the in-\ncontext learning ability provided by LLMs.\n6.3\nOther Fields\nIn recommendation systems, Xue et al. [2023] trained reward\nmodels to allow RL recommendation systems to learn from\nusers‚Äô historical behaviors. Kim and Lee [2020] used a re-\nward model to automate peer-to-peer (P2P) energy trading,\nwhile Oueida et al. [2019] designed a reward model to im-\nprove the management of healthcare resources.\n7\nEvaluating Reward Models\nOnce reward models are developed, reliable evaluation tech-\nniques are essential for comparing or selecting models for\ndownstream policy optimization. However, due to the am-\nbiguous link between reward models and final policy perfor-\nmance, relying on a single evaluation perspective is often in-\nsufficient [Arora and Doshi, 2021]. We categorize commonly\nused reward evaluation techniques into the following three\ntypes, which are often used in combination to achieve a more\ncomprehensive assessment of reward models.\n7.1\nEvaluation via Policy Performance\nReward model quality can be evaluated by measuring the\nperformance of policies trained with it. Primary metrics in-\nclude ground-truth reward, task success rate, and training ef-\nficiency, with superior reward models yielding higher values\nacross these measures. This approach is widely adopted in\nreinforcement learning literature to assess the alignment be-\ntween reward models and actual objectives [Christiano et al.,\n2017]. However, these metrics are sensitive to policy opti-\nmization algorithms and environmental stochasticity, poten-\ntially limiting their ability to independently reflect the true\nperformance of the reward model itself.\n7.2\nEvaluation via Distance Metrics\nTo evaluate and compare reward models, another approach\nis to design distance metrics that accurately reflect the be-\nhavioral differences between the policies induced by these\nrewards. The pioneering work EPIC [Gleave et al., 2020]\nintroduces canonically shaped rewards to remove ambiguity\nand invariances from reward models and proposes to use the\nPearson coefficient between two canonically shaped rewards\nas a measure of the reward similarity. The EPIC distance be-\ntween two reward models is demonstrated to upper-bound the\nperformance difference between the induced policies. Lower\nEPIC distances to the ground truth reward indicate superior\nreward modeling capability.\nBased on EPIC, Wulfe et al. [2022] further incorporates the\ndynamics information when considering the invariant reward\nshaping and introduces the DARD metric, which is more pre-\ndictive and accurate in quantifying the differences in rewards.\nFurthermore, Skalse et al. [2023] presented a general frame-\nwork for designing such distance metrics. The STARC met-\nrics provided in this framework are shown to induce both the\nupper bound and the lower bound of the performance differ-\nences, and any other metrics that possess the same property\nmust be equivalent to the STARC metrics up to bilipschitz\nscaling. When datasets containing ground-truth rewards are\navailable, distance metrics are particularly suitable for offline\nevaluation, circumventing the necessity for policy learning.\n7.3\nEvaluation via Interpretable Representations\nAlthough the evaluation of reward models may not be\nstraightforward, we can transform them equivalently into in-\nterpretable representations. Jenner and Gleave [2022] pro-\nposed to transform reward models with potential-based shap-\ning and visualize the shaped reward instead. Since potential-\nbased shaping preserves the optimal policy, characteristics\nof the shaped reward may also apply to the original reward\nmodel.\nAlternatively, we can evaluate the reward model\nthrough the behavior of the induced policy [Rocamonde et\nal., 2023].\n8\nConclusions\nRecently, reward models have become a highly motivating\narea of research, driven by both theoretical challenges and\npractical needs across various domains. We consider the de-\nvelopment of reward models as a significant step before the\napplication of RL to real-world problems, and we hope this\nsurvey can offer valuable insights for both researchers and\npractitioners. Although our study provides a comprehensive\noverview of the topic, the design and variations of reward\nmodels still extend beyond the scope of this discussion. Inter-\nested readers can also refer to other survey papers [Eschmann,\n2021; Liu et al., 2022; Arora and Doshi, 2021; Kaufmann et\nal., 2023] that focus on RL subfields closely related to reward\nmodeling.\n8.1\nFuture Directions\nEfficient and accurate reward modeling is a valuable research\ndirection with significant application prospects. It combines\nincreasingly mature technologies such as large models and\ndiffusion models with reward design and generation in rein-\nforcement learning to provide behavioral feedback for agents\nin perception, planning, decision-making, and navigation.\nAlthough there is no definitive conclusion on which route can\nachieve efficient reward modeling, research on various tech-\nnologies in recent years has effectively promoted the develop-\nment of this field. With the continuous development of ma-\nchine learning and reinforcement learning, reward modeling\nhas many valuable research directions in the future, includ-\ning:\n1. Vectorized rewards: Constructing vectorized rewards\nto replace scalarized single rewards, dynamically bal-\nancing multiple competitive reward signals to provide\nagents with more comprehensive feedback.\n2. Interpretating reward models: Improving the trans-\nparency of reward functions and explaining the decision-\nmaking logic behind reward models.\n3. Ethical alignment and social value constraints: Quan-\ntifying ethical principles and embedding them into re-\nward functions while avoiding potential side effects dur-\ning the optimization process.\n4. Reward foundation models: Similar to constructing a\ngeneral representation space, consider training a founda-\ntion reward model that can obtain general reward values\nbased on diverse inputs (such as limb movements).\nAcknowledgements\nWe thank the anonymous reviewers for their insightful feed-\nback. This work was supported by the National Science and\nTechnology Major Project (Grant No. 2022ZD0114805) and\nYoung Scientists Fund of the National Natural Science Foun-\ndation of China (PhD Candidate) (Grant No. 624B200197).\nReferences\nPieter Abbeel and Andrew Y Ng. Apprenticeship learning via in-\nverse reinforcement learning. In Proceedings of the twenty-first\ninternational conference on Machine learning, page 1, 2004.\nJosh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev,\nAlex Goldin, Alden Hung, Jessica Landon, Jirka Lhotka, Timothy\nLillicrap, Alistair Muldal, et al. Improving multimodal interactive\nagents with reinforcement learning from human feedback. arXiv\npreprint arXiv:2211.11602, 2022.\nSaurabh Arora and Prashant Doshi. A survey of inverse reinforce-\nment learning: Challenges, methods and progress. Artificial In-\ntelligence, 297:103500, 2021.\nAdri`a Puigdom`enech Badia, Pablo Sprechmann, Alex Vitvitskyi,\nDaniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman,\nMart¬¥ƒ±n Arjovsky, Alexander Pritzel, Andew Bolt, et al. Never\ngive up: Learning directed exploration strategies. arXiv preprint\narXiv:2002.06038, 2020.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,\nJackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia\nMirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-\nlessness from ai feedback.\narXiv preprint arXiv:2212.08073,\n2022.\nAndrew G Barto, Satinder Singh, Nuttapong Chentanez, et al. Intrin-\nsically motivated learning of hierarchical collections of skills. In\nProceedings of the 3rd International Conference on Development\nand Learning, volume 112, page 19. Citeseer, 2004.\nKate Baumli, Satinder Baveja, Feryal Behbahani, Harris Chan, Ghe-\norghe Comanici, Sebastian Flennerhag, Maxime Gazeau, Kristian\nHolsheimer, Dan Horgan, Michael Laskin, et al. Vision-language\nmodels as a source of rewards. arXiv preprint arXiv:2312.09187,\n2023.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul,\nDavid Saxton, and Remi Munos. Unifying count-based explo-\nration and intrinsic motivation. Advances in neural information\nprocessing systems, 29, 2016.\nRalph Allan Bradley and Milton E Terry.\nRank analysis of in-\ncomplete block designs: I. the method of paired comparisons.\nBiometrika, 39(3/4):324‚Äì345, 1952.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.\nExploration by random network distillation.\narXiv preprint\narXiv:1810.12894, 2018.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane\nLegg, and Dario Amodei. Deep reinforcement learning from hu-\nman preferences. Advances in neural information processing sys-\ntems, 30, 2017.\nJosef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel\nLiu, Yizhou Wang, and Yaodong Yang.\nSafe rlhf: Safe re-\ninforcement learning from human feedback.\narXiv preprint\narXiv:2310.12773, 2023.\nYuqing Du, Olivia Watkins, Zihan Wang, C¬¥edric Colas, Trevor Dar-\nrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding\npretraining in reinforcement learning with large language models.\nIn International Conference on Machine Learning, pages 8657‚Äì\n8677. PMLR, 2023.\nJonas Eschmann. Reward function design in reinforcement learning.\nReinforcement learning algorithms: Analysis and Applications,\npages 25‚Äì33, 2021.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey\nLevine. Diversity is all you need: Learning skills without a re-\nward function. arXiv preprint arXiv:1802.06070, 2018.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong\nYang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and\nAnima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in Neural Infor-\nmation Processing Systems, 35:18343‚Äì18362, 2022.\nChelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine.\nA connection between generative adversarial networks, inverse\nreinforcement learning, and energy-based models. arXiv preprint\narXiv:1611.03852, 2016.\nChelsea Finn, Sergey Levine, and Pieter Abbeel.\nGuided cost\nlearning: Deep inverse optimal control via policy optimization.\nIn International conference on machine learning, pages 49‚Äì58.\nPMLR, 2016.\nJustin Fu, Katie Luo, and Sergey Levine. Learning robust rewards\nwith adversarial inverse reinforcement learning. arXiv preprint\narXiv:1710.11248, 2017.\nAdam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and\nJan Leike. Quantifying differences in reward functions. arXiv\npreprint arXiv:2006.13900, 2020.\nPaul W Glimcher.\nUnderstanding dopamine and reinforcement\nlearning: the dopamine reward prediction error hypothesis. Pro-\nceedings of the National Academy of Sciences, 108:15647‚Äì\n15654, 2011.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu\nZhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi,\net al. Deepseek-r1: Incentivizing reasoning capability in llms via\nreinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\nHarry F Harlow. Learning and satiation of response in intrinsically\nmotivated complex puzzle performance by monkeys. Journal of\ncomparative and physiological psychology, 43(4):289, 1950.\nKristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey\nLevine.\nDynamical distance learning for semi-supervised and\nunsupervised skill discovery. arXiv preprint arXiv:1907.08225,\n2019.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip\nDe Turck, and Pieter Abbeel. Vime: Variational information max-\nimizing exploration. Advances in neural information processing\nsystems, 29, 2016.\nErik Jenner and Adam Gleave. Preprocessing reward functions for\ninterpretability. arXiv preprint arXiv:2203.13553, 2022.\nWonseok Jeon, Chen-Yang Su, Paul Barde, Thang Doan, Derek\nNowrouzezahrai, and Joelle Pineau.\nRegularized inverse rein-\nforcement learning. arXiv preprint arXiv:2010.03691, 2020.\nTimo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H¬®ullermeier.\nA survey of reinforcement learning from human feedback. arXiv\npreprint arXiv:2312.14925, 10, 2023.\nJin-Gyeom Kim and Bowon Lee.\nAutomatic p2p energy trading\nmodel based on reinforcement learning using long short-term de-\nlayed reward. Energies, 13(20):5359, 2020.\nChangyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter\nAbbeel, and Kimin Lee.\nPreference transformer:\nModeling\nhuman preferences using transformers for rl.\narXiv preprint\narXiv:2303.00957, 2023.\nMartin Klissarov,\nPierluca D‚ÄôOro,\nShagun Sodhani,\nRoberta\nRaileanu, Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, and\nMikael Henaff. Motif: Intrinsic motivation from artificial intelli-\ngence feedback. arXiv preprint arXiv:2310.00166, 2023.\nAlexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv.\nEmpowerment: A universal agent-centric measure of control. In\n2005 ieee congress on evolutionary computation, volume 1, pages\n128‚Äì135. IEEE, 2005.\nW Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott\nNiekum, Peter Stone, and Alessandro Allievi.\nModels of hu-\nman preference for learning reward functions.\narXiv preprint\narXiv:2206.02231, 2022.\nW Bradley Knox, Alessandro Allievi, Holger Banzhaf, Felix\nSchmitt, and Peter Stone. Reward (mis) design for autonomous\ndriving. Artificial Intelligence, 316:103829, 2023.\nAndreas K¬®opf, Yannic Kilcher, Dimitri Von R¬®utte, Sotiris Anag-\nnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc\nNguyen, Oliver Stanley, Rich¬¥ard Nagyfi, et al.\nOpenassistant\nconversations-democratizing large language model alignment.\nAdvances in Neural Information Processing Systems, 36:47669‚Äì\n47681, 2023.\nTze Leung Lai and Herbert Robbins. Asymptotically efficient adap-\ntive allocation rules. Advances in applied mathematics, 6(1):4‚Äì\n22, 1985.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard,\nJohan Ferret, Kellie Ren Lu, Colton Bishop, Ethan Hall, Victor\nCarbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforce-\nment learning from human feedback with ai feedback. In Inter-\nnational Conference on Machine Learning, pages 26874‚Äì26901.\nPMLR, 2024.\nHao Liu and Pieter Abbeel. Behavior from the void: Unsupervised\nactive pre-training. Advances in Neural Information Processing\nSystems, 34:18459‚Äì18473, 2021.\nMinghuan Liu,\nMenghui Zhu,\nand Weinan Zhang.\nGoal-\nconditioned reinforcement learning:\nProblems and solutions.\narXiv:2201.08299, 2022.\nShang Liu, Yu Pan, Guanting Chen, and Xiaocheng Li.\nReward\nmodeling with ordinal feedback: Wisdom of the crowd. arXiv\npreprint arXiv:2411.12843, 2024.\nTianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman,\nRishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baum-\ngartner, Jialu Liu, et al.\nLipo: Listwise preference optimiza-\ntion through learning-to-rank. arXiv preprint arXiv:2402.01878,\n2024.\nTrung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran\nJin, and Hang Li. Reft: Reasoning with reinforced fine-tuning.\narXiv preprint arXiv:2401.08967, 2024.\nPietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste,\nand Sai Rajeswar. Choreographer: Learning and adapting skills\nin imagination. arXiv preprint arXiv:2211.13350, 2022.\nPietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, and\nSai Rajeswar. Genrl: Multimodal-foundation world models for\ngeneralization in embodied agents. Advances in neural informa-\ntion processing systems, 37:27529‚Äì27555, 2024.\nRussell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner,\nand Deepak Pathak. Discovering and achieving goals via world\nmodels.\nAdvances in Neural Information Processing Systems,\n34:24379‚Äì24391, 2021.\nVivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, and\nAnca Dragan. Learning to assist humans without inferring re-\nwards. arXiv preprint arXiv:2411.02623, 2024.\nOfir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine.\nData-efficient hierarchical reinforcement learning. Advances in\nneural information processing systems, 31, 2018.\nGeorg Ostrovski, Marc G Bellemare, A¬®aron Oord, and R¬¥emi Munos.\nCount-based exploration with neural density models.\nIn In-\nternational conference on machine learning, pages 2721‚Äì2730.\nPMLR, 2017.\nSoraia Oueida, Moayad Aloqaily, and Sorin Ionescu.\nA smart\nhealthcare reward model for resource allocation in smart city.\nMultimedia tools and applications, 78:24573‚Äì24594, 2019.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wain-\nwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Kata-\nrina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in neural informa-\ntion processing systems, 35:27730‚Äì27744, 2022.\nSeohong Park, Oleh Rybkin, and Sergey Levine. Metra: Scalable\nunsupervised rl with metric-aware abstraction.\narXiv preprint\narXiv:2310.08887, 2023.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell.\nCuriosity-driven exploration by self-supervised prediction. In In-\nternational conference on machine learning, pages 2778‚Äì2787.\nPMLR, 2017.\nDeepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised\nexploration via disagreement. In International conference on ma-\nchine learning, pages 5062‚Äì5071. PMLR, 2019.\nRobin L Plackett.\nThe analysis of permutations.\nJournal of the\nRoyal Statistical Society Series C: Applied Statistics, 24(2):193‚Äì\n202, 1975.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Man-\nning, Stefano Ermon, and Chelsea Finn. Direct preference op-\ntimization: Your language model is secretly a reward model.\nAdvances in Neural Information Processing Systems, 36:53728‚Äì\n53741, 2023.\nJuan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez,\nand David Lindner.\nVision-language models are zero-shot\nreward models for reinforcement learning.\narXiv preprint\narXiv:2310.12921, 2023.\nRichard M Ryan and Edward L Deci. Intrinsic and extrinsic mo-\ntivations: Classic definitions and new directions. Contemporary\neducational psychology, 25(1):54‚Äì67, 2000.\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel,\nDanijar Hafner, and Deepak Pathak. Planning to explore via self-\nsupervised world models. In International conference on machine\nlearning, pages 8583‚Äì8592. PMLR, 2020.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent\nSifre, George Van Den Driessche, Julian Schrittwieser, Ioannis\nAntonoglou, Veda Panneershelvam, Marc Lanctot, et al. Master-\ning the game of go with deep neural networks and tree search.\nnature, 529(7587):484‚Äì489, 2016.\nJoar Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner,\nAdam Gleave, and Alessandro Abate. Starc: A general frame-\nwork for quantifying differences between reward functions. arXiv\npreprint arXiv:2309.15257, 2023.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang,\nYongbin Li, and Houfeng Wang. Preference ranking optimization\nfor human alignment. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 38, pages 18990‚Äì18998, 2024.\nSumedh Sontakke, Jesse Zhang, S¬¥eb Arnold, Karl Pertsch, Erdem\nBƒ±yƒ±k, Dorsa Sadigh, Chelsea Finn, and Laurent Itti. Roboclip:\nOne demonstration is enough to learn robot policies. Advances in\nNeural Information Processing Systems, 36:55681‚Äì55693, 2023.\nRichard S Sutton, Andrew G Barto, et al. Reinforcement learning:\nAn introduction, volume 1. MIT press Cambridge, 1998.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI\nXi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter\nAbbeel. # exploration: A study of count-based exploration for\ndeep reinforcement learning.\nAdvances in neural information\nprocessing systems, 30, 2017.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2:\nOpen foundation and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288, 2023.\nMark Towers, Ariel Kwiatkowski, Jordan Terry, et al. Gymnasium:\nA standard interface for reinforcement learning environments.\narXiv:2407.17032, 2024.\nMudit Verma and Katherine Metcalf.\nHindsight priors for\nreward learning from human preferences.\narXiv preprint\narXiv:2404.08828, 2024.\nShenghua Wan, Hai-Hang Sun, Le Gan, and De-Chuan Zhan.\nMoser: learning sensory policy for task-specific viewpoint via\nview-conditional world model. In Proceedings of the Thirty-Third\nInternational Joint Conference on Artificial Intelligence, pages\n5046‚Äì5054, 2024.\nYufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik,\nDavid Held, and Zackory Erickson.\nRl-vlm-f: reinforcement\nlearning from vision language foundation model feedback.\nIn\nProceedings of the 41st International Conference on Machine\nLearning, pages 51484‚Äì51501, 2024.\nYucen Wang, Rui Yu, Shenghua Wan, Le Gan, and De-Chuan Zhan.\nFounder: Grounding foundation models in world models for\nopen-ended embodied decision making. In International Con-\nference on Machine Learning, 2025.\nBlake Wulfe, Ashwin Balakrishna, Logan Ellis, Jean Mercat, Rowan\nMcAllister, and Adrien Gaidon.\nDynamics-aware comparison\nof learned reward functions. arXiv preprint arXiv:2201.10081,\n2022.\nTianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo,\nVictor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Auto-\nmated dense reward function generation for reinforcement learn-\ning. arXiv preprint arXiv:2309.11489, 2023.\nHuajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao,\nHaocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du,\net al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback\nfor reinforcement learning and monte-carlo tree search.\narXiv\npreprint arXiv:2408.08152, 2024.\nJiacheng Xu, Chao Chen, Fuxiang Zhang, Lei Yuan, Zongzhang\nZhang, and Yang Yu.\nInternal logical induction for pixel-\nsymbolic reinforcement learning.\nIn Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and Data\nMining, pages 2825‚Äì2837, 2023.\nWanqi Xue, Qingpeng Cai, Zhenghai Xue, Shuo Sun, Shuchang\nLiu, Dong Zheng, Peng Jiang, Kun Gai, and Bo An. Prefrec:\nRecommender systems with human preferences for reinforcing\nlong-term user engagement.\nIn Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining,\npages 2874‚Äì2884, 2023.\nLunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi,\nAviral Kumar, and Rishabh Agarwal.\nGenerative verifiers:\nReward modeling as next-token prediction.\narXiv preprint\narXiv:2408.15240, 2024.\nBrian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K\nDey.\nMaximum entropy inverse reinforcement learning.\nIn\nProceedings of the 23rd national conference on Artificial\nintelligence-Volume 3, pages 1433‚Äì1438, 2008.\n",
    "content": "# Interpretation of AI Academic Papers\n\n## 1. What is the core content and main contribution of this paper?\n\nThis paper, *Reward Models in Deep Reinforcement Learning: A Survey*, provides a comprehensive review of reward models in deep reinforcement learning (DRL). The key contents include:\n\n- **Background and Preliminary Knowledge**: An introduction to the fundamental concepts of reinforcement learning and reward models.\n- **Classification of Reward Models**: Categorization of reward models based on their source, mechanism, and learning paradigm.\n- **Applications and Evaluation Methods**: Discussion on the practical applications of reward models in real-world scenarios and methods for evaluating these models.\n\nThe main contributions are as follows:\n1. Provides foundational knowledge, key methods, and applications of reward models across various DRL environments.\n2. Proposes a new classification framework addressing three fundamental questions: the source of rewards, the mechanisms driving learning, and how reward models can be learned from different types of feedback.\n3. Highlights recent advancements in reward models based on foundational models such as large language models (LLMs) and vision-language models (VLMs).\n\n---\n\n## 2. What are the breakthroughs or innovations in this paper?\n\nThe breakthroughs and innovations of this paper include the following aspects:\n\n1. **Systematic Classification Framework**: For the first time, it proposes a systematic classification framework that categorizes reward models by their source (human-provided or AI-generated), mechanism (intrinsic or extrinsic), and learning paradigm (learning from demonstrations, goals, or preferences).\n   \n2. **Emphasis on Foundational Models**: It highlights the critical role of LLMs and VLMs in generating reward models. These models can interpret human intent and autonomously define reward models through self-supervised learning of large-scale internet data.\n\n3. **Introduction of New Methods and Technologies**:\n   - Discusses reward computation methods based on semantic similarity, such as RoboCLIP and MineCLIP models.\n   - Explores reward design methods that combine spatial distance, temporal distance, and semantic similarity.\n   - Proposes several novel evaluation techniques, such as EPIC distance and STARC metrics, for more accurate comparison and assessment of reward models.\n\n4. **Coverage of Emerging Fields**: In addition to traditional reward modeling methods, it also explores the latest research directions, such as vectorized rewards, ethical alignment, and social value constraints.\n\n---\n\n## 3. Based on the content of this paper, what are some good ideas for startup projects?\n\nBased on the content of this paper, here are some potential startup project ideas:\n\n### 1. **Intelligent Reward Design Platform**\n   - **Project Description**: Develop an AI-based reward design platform that helps users quickly design efficient reward functions. This platform can leverage large language models and vision-language models to generate reward functions and support multiple input formats (e.g., text descriptions, images, etc.).\n   - **Application Scenarios**: Game development, robot control, autonomous driving, etc.\n\n### 2. **Automated Task Optimization Tool**\n   - **Project Description**: Create an automated tool that generates optimization strategies by analyzing user-provided goals or preferences. For example, users can set goals with simple natural language descriptions, and the tool will automatically construct reward models and optimize corresponding strategies.\n   - **Application Scenarios**: Business process optimization, personalized recommendation systems, smart home control, etc.\n\n### 3. **Preference-Based Recommendation System**\n   - **Project Description**: Design a recommendation system based on human preferences that uses reinforcement learning to extract preference information from user interaction behaviors, thereby improving recommendation accuracy.\n   - **Application Scenarios**: E-commerce recommendations, content distribution, social network interactions, etc.\n\n### 4. **Multimodal Reward Generation Engine**\n   - **Project Description**: Develop a multimodal reward generation engine capable of handling various data types such as text, images, and videos, generating reward models suitable for complex environments.\n   - **Application Scenarios**: Virtual assistant training, multimodal robot control, augmented reality applications, etc.\n\n### 5. **Ethically Aligned AI Assistant**\n   - **Project Description**: Build an ethically aligned AI assistant that ensures its behavior conforms to societal values. By quantifying ethical principles and embedding them into the reward function, it avoids potential negative side effects.\n   - **Application Scenarios**: Customer service bots, educational assistants, medical consultations, etc.\n\n### 6. **Reinforcement Learning Simulator**\n   - **Project Description**: Develop a reinforcement learning simulator that allows users to test and optimize reward models in a controlled environment. The simulator can support multiple reward modeling methods and provide real-time feedback.\n   - **Application Scenarios**: Scientific experiments, teaching tools, corporate training, etc.\n\n---\n\nBy implementing these projects, the technologies and methods mentioned in the paper can be fully utilized to promote the practical application and development of reinforcement learning.",
    "github": "",
    "hf": ""
}