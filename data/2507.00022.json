{
    "id": "2507.00022",
    "title": "GLU Attention Improve Transformer",
    "summary": "This paper proposes a new attention mechanism called GLU Attention, which improves model performance and convergence speed by introducing nonlinearity into the attention values, without requiring additional parameters or extra computational cost.",
    "abstract": "Gated Linear Units (GLU) have shown great potential in enhancing neural network performance. In this paper, I introduce a novel attention mechanism called GLU Attention, which introduces nonlinearity into the values of Attention. My experiments demonstrate that GLU Attention improves both model performance and convergence speed across text and vision modalities with zero additional parameters and negligible computational costs. GLU Attention is lightweight and can seamlessly integrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA). This project is open-sourced at github.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Zehao Wang",
    "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)",
        "Computation and Language (cs.CL)",
        "Neural and Evolutionary Computing (cs.NE)"
    ],
    "comments": "Comments:4 pages 4 figures",
    "keypoint": "- GLU Attention introduces nonlinearity into the values of Multi-Head Attention (MHA).\n- GLU Attention improves model performance and convergence speed in both text and vision tasks.\n- GLU Attention adds zero additional parameters and incurs negligible computational costs.\n- GLU Attention can seamlessly integrate with Flash Attention, RoPE, and MHA variants like GQA.\n- Experiments on Cifar-10 show GLU Attention outperforms baseline MHA in validation accuracy and training loss.\n- Experiments on WikiText-2 and WikiText-103 show GLU Attention consistently reduces training loss compared to baseline.\n- GLU Attention maintains the same number of parameters by adjusting dimensions in WV and WO matrices.\n- GLU Attention is open-sourced and easy to adopt for researchers.",
    "date": "2025-07-03",
    "paper": "arXiv:2507.00022v1  [cs.LG]  16 Jun 2025\nGLU ATTENTION IMPROVE TRANSFORMER\nWang Zehao\nNanjing University\nwangzehao_ai@163.com\nABSTRACT\nGated Linear Units (GLU) have shown great potential in enhancing neural network performance.\nIn this paper, I introduce a novel attention mechanism called GLU Attention, which introduces\nnonlinearity into the values of Attention. My experiments demonstrate that GLU Attention improves\nboth model performance and convergence speed across text and vision modalities with zero additional\nparameters and negligible computational costs. GLU Attention is lightweight and can seamlessly\nintegrate with other technologies, such as Flash Attention, Rotary Position Embedding (RoPE), and\nvarious Multi-Head Attention (MHA) variants such as Grouped-Query Attention (GQA). This project\nis open-sourced at github[1].\n1\nIntroduction\nTransformer[2] models have become the foundation of modern artificial intelligence. Transformer is a sequence-to-\nsequence model that uses Attention layer to capture relationships between tokens and Feed Forward Network (FFN)\nlayer to perform transformations on each token. GLU FFN[3] outperforms the original FFN and has been adopted in\npopular open source Large Language Model (LLM) Llama 3[4]. My study shows GLU Attention outperforms original\nMHA. In MHA there is a softmax function introduce nonlinearity for querys and keys, but the values are projected by\nlinear transformations. My study explores the integration of GLU nonlinearity into the values of MHA. Experiments\nshow that adding GLU to MHA values can improve model performance and convergence speed. GLU Attention is\na simple yet effective enhancement to the Transformer architecture, improving both training efficiency and model\nperformance.\n2\nBackgrounds\n2.1\nGated Linear Units\nGLU were first introduced to improve performance by introducing nonlinearity and has been successfully applied in\nvarious architectures, including convolutional neural networks (CNN)[5] and transformer FFN layer[3].\nGLU contains two inputs: a gate g and a gated input x, along with one Rectified Linear Unit[6] such as ReLU(x) =\nmax(0, x) or SiLU(x) = x ∗sigmoid(x). In this paper I use SiLU[7] as the Linear Unit, GLU is defined as:\nGLU(x, g) = x ∗SiLU(g)\n(1)\nOr just split the last dimension of input x into two parts, x1 and x2, and apply Rectified Linear Unit to x2:\nx1, x2 = split(x, dim = −1)\n(2)\nGLU(x) = x1 ∗SiLU(x2)\n(3)\n2.2\nMulti-Head Attention\nMHA is a key component of the Transformer architecture, enabling the model to focus on different parts of the input\nsequence simultaneously. The MHA layer has three inputs: queries Q, keys K, values V , and one output O. MHA\nGLU Attention Improve Transformer\napplies three linear transformations WQ, WK, and WV to project the inputs into different subspaces for each attention\nhead. A final linear transformation WO is used to project the output back to the original space. The MHA layer can be\nexpressed as:\nQ′ = WQ(Q)\n(4)\nK′ = WK(K)\n(5)\nV ′ = WV (V )\n(6)\nO′ = MHA(Q′, K′, V ′)\n(7)\nO = WO(O′)\n(8)\nIn Multi-Head Self-Attention, the same input X is used for queries, keys, and values. Q = K = V = X\n3\nMethod\nGLU Attention uses the projected value V ′ as the intput of Equation (3). The GLU Attention can be expressed as:\nV ′ = GLU(WV (V ))\n(9)\nBy replacing Equation (6) in MHA with Equation (9), while keeping other components unchanged, we obtain GLU\nMulti-Head Attention. To maintain the same number of parameters and computational costs, we use 4/3 of the original\nWV matrix output dimension and 2/3 of the original WO matrix input dimension.\n4\nExperiments\n4.1\nModels and Hyperparameters\nI conducted experiments using two Transformer models. The baseline model uses MHA, while the GLU Attention\nmodel uses GLU MHA. Each model consists of 1 embedding layer, 1 positional embedding layer, 6 transformer layers,\nand 1 linear layer for classification. Each transformer layer includes a self-attention layer and a GLU FFN layer, with\n384 model dimensions and 8 attention heads. The linear layer shapes are designed to match those of classic transformers:\n(1:1) * 4 in MHA, and 1:(16/3) + (8/3):1 in GLU FFN.\n4.2\nCifar-10\nFigure 1: Cifar-10 training loss of each epoch. The lower\nthe better.\nFigure 2: Cifar-10 validation accuracy of each epoch. The\nhigher the better.\nI trained these models from scratch on the Cifar-10 dataset, a widely used benchmark for image classification. The\ntraining dataset consists of 60,000 32x32 color images across 10 classes, while the validation set consists of 10,000\n2\nGLU Attention Improve Transformer\nimages. I followed the standard ViT[8] procedure, dividing each 32x32x3 image into 64 patches of size 4x4x3. Training\nwas conducted for 20 epochs with a batch size of 384. I used the AdamW optimizer with a learning rate of 1e-4 and a\ncosine annealing scheduler. The results are shown in Figure 1 and Figure 2. GLU Attention consistently outperformed\nthe baseline model.\nFigure 3: wikitext2 training loss for 10 epochs. The lower\nthe better.\nFigure 4: wikitext103 training loss for 1 epoch. The lower\nthe better.\n4.3\nWikiText-2\nI also trained these models from scratch on the WikiText-2 dataset which has 36,718 rows of token for language model\npre-training, which is to predict the next token. I used the GPT-2 tokenizer to tokenize the text and applied the same\ntraining settings as in Cifar-10, except that the batch size was set to 1 and a causal mask was used to prevent the model\nfrom seeing future tokens. Training was conducted for 10 epochs. The results are shown in Figure 3. GLU Attention\nconsistently outperformed the baseline model.\n4.4\nWikiText-103\nThen I trained these models from scratch on the WikiText-103 dataset which has 1,801,350 rows of token using learning\nrate 1e-5 for 1 epoch. The results are shown in Figure 4. GLU Attention consistently outperformed the baseline model.\n5\nConclusion\nGLU Attention offers a straightforward yet impactful improvement to the Transformer architecture. By introducing\nnonlinearity into the values of MHA, it enhances model performance and convergence speed.\nGLU Attention can be seamlessly integrated with other technologies, such as Flash Attention[9], RoPE[10], and various\nMHA variants like MQA and GQA[11], by simply adding a GLU function (Equation 3) after the value projection\nfunction and adjusting some parameters to accommodate the GLU function’s property that output dimension is half of\nthe input dimension.\n6\nFuture Work\nI highly recommend every researcher to test GLU Attention in your Transformers, as it is easy to adopt and provides a\nnearly cost-free performance boost. Future work could explore its application in different MHA variants with different\nFFN variants, on more datasets and tasks, as well as its scalability to larger models and datasets.\nReferences\n[1] Wang Zehao. Glu attention github repository. https://github.com/WangZehaoAI/GLU-Attention, 2025.\n3\nGLU Attention Improve Transformer\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need, 2023.\n[3] Noam Shazeer. Glu variants improve transformer, 2020.\n[4] Aaron Grattafiori et al. The llama 3 herd of models, 2024.\n[5] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional\nnetworks, 2017.\n[6] Kunihiko Fukushima. Cognitron: A self-organizing multilayered neural network, 1975.\n[7] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function\napproximation in reinforcement learning, 2017.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An\nimage is worth 16x16 words: Transformers for image recognition at scale, 2021.\n[9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient\nexact attention with io-awareness, 2022.\n[10] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding, 2023.\n[11] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa:\nTraining generalized multi-query transformer models from multi-head checkpoints, 2023.\n4\n",
    "content": "Here is the English translation of the provided text:\n\n---\n\n## A Detailed Review and Analysis of the Paper *\"GLU Attention Improves Transformers\"* (arXiv:2507.00022v1)\n\n---\n\n## I. Core Content and Key Contributions\n\n### **Core Content**\n\nThis paper introduces a novel attention mechanism called **GLU Attention**, which enhances the non-linear expressive power of traditional Multi-Head Attention (MHA) by incorporating Gated Linear Units (GLUs) into the value vector processing path. This modification significantly improves both the performance and training convergence speed of Transformer models, without increasing parameter count or computational cost.\n\n### **Key Contributions**\n\n1. **Introduction of GLU Attention**: The authors propose integrating GLU structures into the value branch of MHA, enhancing the model's non-linear modeling capabilities.\n2. **High Efficiency and Compatibility**: GLU Attention introduces almost no additional parameters or computational overhead and can be seamlessly integrated into existing Transformer variants such as Flash Attention, RoPE, MQA, and GQA.\n3. **Cross-modal Validation**: Experiments on image classification (CIFAR-10) and language modeling tasks (WikiText-2, WikiText-103) demonstrate the method’s general applicability and superior performance across multiple domains.\n4. **Open-source Implementation**: The authors have released the code repository, facilitating reproducibility and further research.\n\n---\n\n## II. Breakthroughs and Innovations\n\n| Innovation Dimension | Specific Manifestation |\n|----------------------|-------------------------|\n| **Architectural Innovation** | First exploration of introducing non-linearity in attention mechanisms by applying GLU to the value branch, previously used only in FFN layers. |\n| **Performance Improvement** | Experimental results show consistent improvements in model performance and convergence speed, especially on small datasets like CIFAR-10. |\n| **Lightweight Design** | By adjusting the dimensions of WV and WO after introducing GLU, the original MHA parameter scale and computational complexity are preserved. |\n| **Generalization Capability** | Highly versatile method applicable to Vision Transformers, GPT-style language models, and more; compatible with mainstream optimization techniques. |\n\n---\n\n## III. Startup Ideas Based on GLU Attention\n\nLeveraging GLU Attention as a core technology, here are several promising directions for startup projects with commercial potential:\n\n### 1. **Lightweight AI Inference Engine**\n- **Use Cases**: Edge devices (mobile phones, IoT), embedded systems, real-time speech/image recognition.\n- **Advantages**: GLU Attention inherently has low parameters and low latency. Combined with technologies like Flash Attention, it's ideal for building efficient inference frameworks.\n- **Business Model**: Offer SDKs or customized model compression services targeting smart security, smart manufacturing, consumer electronics, etc.\n\n### 2. **Multimodal AI Model Service Platform**\n- **Use Cases**: Image + text understanding, video summarization, cross-modal search.\n- **Advantages**: GLU Attention supports both image and text modalities, making it suitable for developing unified multimodal model platforms.\n- **Business Model**: SaaS platform charging per API call; provide APIs for enterprises to quickly integrate AI capabilities.\n\n### 3. **Vertical AI Solutions for Education or Healthcare**\n- **Use Cases**: Medical image analysis, personalized learning recommendations, online education content generation.\n- **Advantages**: Excellent performance on small-sample tasks (e.g., CIFAR-10), suitable for resource-constrained but high-accuracy fields.\n- **Business Model**: B2B partnerships offering custom AI solutions to hospitals, schools, or training institutions.\n\n### 4. **Open-source Community-driven AI Toolchain Ecosystem**\n- **Use Cases**: Developer tools, model acceleration libraries, automated training platforms.\n- **Advantages**: GLU Attention is open-sourced, making it ideal for building an ecosystem around Transformer optimizations.\n- **Business Model**: Open-source plus premium features; attract developer communities and later offer enterprise editions or technical support subscriptions.\n\n### 5. **AI-powered Creative Tools**\n- **Use Cases**: Content generation (text, images, video scripts), advertising copywriting, social media management.\n- **Advantages**: Language models based on GLU Attention achieve high performance even at smaller scales, ideal for lightweight content creation applications.\n- **Business Model**: ToC products with subscription or token-based pricing; combine with AI art, voiceover modules to build an all-in-one creative platform.\n\n---\n\n## Summary\n\nThe **GLU Attention** proposed in this paper represents a simple yet effective enhancement to the Transformer architecture, with broad application prospects. It not only boosts model performance but also offers advantages such as lightweight design and easy integration, making it well-suited for commercial deployment. Entrepreneurs can explore various angles—**model optimization services, vertical AI solutions for specific industries, or multimodal platform development**—to create competitive and market-ready products and services.\n\n---\n\nIf you'd like to dive deeper into any of these entrepreneurial directions—whether technically or from a business model perspective—feel free to let me know!",
    "github": "",
    "hf": ""
}