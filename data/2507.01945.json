{
    "id": "2507.01945",
    "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local Memory",
    "summary": "This paper proposes a new long animation coloring framework called LongAnimation, which extracts globally-consistent features relevant to the current generation through a dynamic global-local paradigm to achieve ideal long-term color consistency.",
    "abstract": "Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found atthis https URL.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Multi-Agent",
    "authors": "Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao",
    "subjects": [
        "Computer Vision and Pattern Recognition (cs.CV)"
    ],
    "comments": "",
    "keypoint": "- LongAnimation proposes a dynamic global-local paradigm to achieve long-term color consistency in animation colorization.\n- The framework includes three key components: SketchDiT, Dynamic Global-Local Memory (DGLM), and Color Consistency Reward.\n- SketchDiT efficiently extracts hybrid reference features from images, sketches, and text to support the DGLM module.\n- DGLM dynamically compresses historical video features and adaptively fuses them with current generation features for global-local consistency.\n- Color Consistency Reward further refines color consistency by aligning low-frequency features between generated and reference animations.\n- During inference, color consistency fusion is applied in the late denoising stage to ensure smooth transitions between segments.\n- LongAnimation can generate stable long animations averaging 500 frames, significantly outperforming previous methods.\n- Experimental results show improvements of 35.1% and 49.1% on short-term and long-term performance respectively using Frechet Video Distance (FVD).\n- Frequency domain analysis confirms that LongAnimation better preserves both low-frequency (color) and high-frequency (sketch) features over long sequences.\n- Ablation studies validate the effectiveness of each component, especially the DGLM and Color Consistency Fusion.",
    "date": "2025-07-04",
    "paper": "arXiv:2507.01945v1  [cs.CV]  2 Jul 2025\nLongAnimation: Long Animation Generation with Dynamic Global-Local\nMemory\nNan Chen1, Mengqi Huang1, Yihao Meng2, Zhendong Mao1†\n1University of Science and Technology of China\n2Hong Kong University of Science and Technology\n{chen nan,huangmq}@mail.ustc.edu.cn, ymengas@connect.ust.hk, {zdmao}@ustc.edu.cn\nAbstract\nAnimation colorization is a crucial part of real animation\nindustry production. Long animation colorization has high\nlabor costs. Therefore, automated long animation coloriza-\ntion based on the video generation model has significant\nresearch value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fus-\ning overlapping features to achieve smooth transitions be-\ntween local segments.\nHowever, the local paradigm ne-\nglects global information, failing to maintain long-term\ncolor consistency. In this study, we argue that ideal long-\nterm color consistency can be achieved through a dynamic\nglobal-local paradigm, i.e., dynamically extracting global\ncolor consistent features relevant to the current genera-\ntion.\nSpecifically, we propose LongAnimation, a novel\nframework, which mainly includes a SketchDiT, a Dynamic\nGlobal-Local Memory (DGLM), and a Color Consistency\nReward. The SketchDiT captures hybrid reference features\nto support the DGLM module. The DGLM module employs\na long video understanding model to dynamically compress\nglobal historical features and adaptively fuse them with\nthe current generation features. To refine the color con-\nsistency, we introduce a Color Consistency Reward. Dur-\ning inference, we propose a color consistency fusion to\nsmooth the video segment transition. Extensive experiments\non both short-term (14 frames) and long-term (average 500\nframes) animations show the effectiveness of LongAnima-\ntion in maintaining short-term and long-term color con-\nsistency for open-domain animation colorization task. The\ncode can be found at LongAnimation.\n1. Introduction\nAnimation is one of the most entertaining and aesthetic\nforms in the video industry. Animation colorization is a\ncrucial part of real animation industry production. The real\n†Zhendong Mao is the corresponding author.\nFigure 1. Comparison with existing paradigm. (a) Existing stud-\nies achieve local color consistency by fusing the overlaps of ad-\njacent video segments, suffering low long-term color consistency.\n(b) Our dynamic global-local paradigm dynamically extracts color\nfeatures of global historical segments as global memory and the\ncolor features of the latest generated segment as local memory,\nachieving high long-term color consistency. All segments are gen-\nerated from one same reference image.\nanimation industry first draws and colors one key frame (not\njust the first frame, but also the frame in the sequence).\nThen, sketch sequences around key frame, especially long\nsequences up to 10 ∼30 seconds (300 to 1000 frames),\nare colored based on the key frame. Coloring such long se-\nquences is extremely labor-intensive, which makes the re-\nsearch of automated long animation coloring highly signifi-\ncant. The primary goal of long animation coloring is main-\ntaining long-term color consistency, which means the same\nobject should have consistent colors in all frames.\nExisting methods mainly explore short-term (within 100\nframes) animation colorization. Early work [47] colors each\nframe based on image models and splices them into videos\nwith weak color consistency. Recent studies [25, 43, 45],\nbased on video models, explore various control on a fixed\nsegment (e.g., 16 frames), such as frame interpolation [43],\nsingle-layer coloring [45], and character-based coloring\n[25]. These methods do not consider longer animation col-\noring. LVCD [12], based on the Unet video model, further\nattempts to use local paradigm to extend colorization up to\n100 frames, that is, fusing the overlapping features of adja-\ncent segments, as shown in Fig. 1 (a). In summary, existing\nmethods are either limited to fixed short-term coloring or\nuse local paradigm to achieve limited coloring extension.\nHowever, existing methods have inherent defects, i.e.,\nneglecting the global color relationship, which makes it dif-\nficult to maintain color consistency over longer animations.\nAutoregressively using the last frame of the previous gen-\neration as the reference image for the next segment causes\nnoise error accumulation in long videos, while using only\none fixed reference image to control all segments ignores\nthe relationship between segments. Based on a fixed refer-\nence image, the local paradigm fuses the overlapping frame\nfeatures of adjacent segments to partially constrain the con-\nsistency of adjacent segments rather than the color consis-\ntency between all segments. Additionally, some animations\ninvolve large movements, which makes keeping long-term\nconsistency with local paradigm more challenging. As a re-\nsult, erratic color details appear in long animations (e.g., the\ngirl’s yellow hat turns red in the 100th frame, and the boy’s\nhat changes color in the 300th frame), as shown in Fig. 1(a).\nWe argue that an ideal long animation coloring per-\nformance could be achieved by the dynamic global-local\nparadigm, which aims to achieve long-term (e.g., 500\nframes) consistency by dynamically fusing global and local\ncolor features, as shown in Fig. 1(b). The history animations\ncontain color consistency features and redundant features.\nGlobal memory can dynamically extract color consistency\nfeatures, achieving global color consistency constraint. Ad-\nditionally, global memory can model low-frequency anima-\ntion changes to achieve more stable colorization, reducing\ncolor flickering, as shown in Fig. 8. Local memory is used\nfor short-term segments to ensure smooth local transitions.\nBy the joint memory of global and local segments, the dy-\nnamic global-local paradigm can effectively improve the\nlong-term color consistency of animations (e.g., the consis-\ntent color of the girl and boy’s hat in the Fig. 1(b)).\nIn this paper, we propose a novel DiT-based long anima-\ntion coloring framework, LongAnimation, which achieves\nlong-term color consistency by dynamically extracting\ncolor features related to current generation from global\ngeneration information.\nLongAnimation mainly includes\nthree key components: (1) SketchDiT, which aims to effi-\nciently extract reference features to support the subsequent\ndynamic memory mechanism; (2) Dynamic Global-Local\nMemory (DGLM) mechanism, which aims to extract long-\nterm consistency features related to the current generation\nfrom historical animations, and (3) Color Consistency Re-\nward, which aims to refine the color consistency. Specif-\nically, we first propose the SketchDiT architecture for the\nanimation colorization task, which achieves efficient fusion\nof hybrid features of reference images, sketches and text\nbased on DiT-based video model. The innovative DGLM\nmechanism, which first introduces the Long Video Under-\nstanding model to dynamically compress generated anima-\ntions, adaptively extracts global color consistent features.\nFurthermore, we propose the Color Consistency Reward to\nfurther refine the model’s coloring ability by aligning the\nlow-frequency features extracted from the Long Video Un-\nderstand model of the generated animation and reference\nanimation. During inference, we propose color consistency\nfusion to achieve smooth transitions, which utilizes latent\nfusion in the later denoising stage. LongAnimation can sta-\nbly colorize long animations averaging 500 frames, at least\n5 times more than past methods [12, 25, 43] could achieve.\nOur main contributions are summarized as follows:\n• Concepts.\nWe propose a novel dynamic global-local\nparadigm, which aims to achieve accurate long-term ani-\nmation colorization by dynamically extracting global and\nlocal consistent color features.\n• Technology. We propose LongAnimation, which designs\na novel DGLM module based on dynamic compression of\nlong video understanding model to adaptively extract his-\ntorical color features related to the current generation. To\nfacilitate DGLM module, we design a novel SketchDiT\nto efficiently control the DiT-based video model.\n• Experiments. LongAnimation significantly outperforms\nprevious advanced models on the open-domain long an-\nimation coloring task, improving short-term and long-\nterm performance by 35.1% and 49.1% respectively on\nthe widely used metric Frechet Video Distance (FVD).\n2. Related Work\n2.1. Long Video Generation\nVisual generation has gradually evolved from controllable\nT2I [13, 19, 50] to controllable T2V [21–24, 36], in which\na key issue is long video generation. Training-free studies\n[20, 28] extend the video length by window fusion or fre-\nquency control. DitCTrl [2] follows window fusion method\nin DiT [27] architecture. Training-based studies [8, 30, 48]\nusually rely on the most recent segment to generate the next\nsegment. The most recent study [11] dynamically updates\nhistorical segment features by fine-tuning LoRA during in-\nference, achieving longer memory. However, long video\ngeneration with finer-grained control (e.g., sketch, ID) is\nmore challenging as it requires more precise long-term con-\nsistency of control conditions, not just with text. Some stud-\nies [12, 49] introduce local fusion but rarely consider global\nconsistency. LongAnimation innovatively uses Long Video\nUnderstanding model to extract global temporal features for\nlong controllable video generation without fine-tuning dur-\ning inference.\n2.2. Long Video Understanding\nThe multimodal community has explored many methods for\nlong video understanding (LVU). Early studies [10, 16, 18]\nprocesses videos through sparse sampling (e.g., 16 and\n24 frames). However, sparse sampling discards numerous\nframes, causing biased understanding. To achieve better\nunderstanding, some studies [6, 17, 31, 37, 38, 41] com-\npress video features to feed more frames into LVU mod-\nels. Some studies [6, 17, 41] reduce the token numbers per\nframe or use token merging to compress features. Recent\nVideo-XL [31] and ReTAKE [37] use KV cache to com-\npress visual input, achieving fine-grained visual perception.\nWe use Video-XL, which can process thousands of frames,\nto extract fine-grained global historical features.\n2.3. Reference-based Line Art Video Coloring\nReference-based line art video coloring focuses on inject-\ning reference frame color (not just the first frame) into the\nanimations. Early study ACOF [47] proposes transferring\ncolor for each frame by optical flow and splicing them into\nvideos.\nRecent studies mainly use the sketch to gener-\nate consistent videos based on the Video Diffusion Model.\nToonCrafter [43] proposes an animation interpolation dif-\nfusion model, which requires the first and last reference\nframes. LVCD [12] introduces video ControlNet to control\nthe motion, using local fusion to improve the consistency\nof segment splicing. LayerAnimate [45] proposes a Layer\nControlNet to achieve single object control. AniDoc [25]\ncan generate character animations given a character image\nwithout background. Recent studies [12, 25, 43, 45] fo-\ncus on generating short-term animations (e.g., within 100\nframes) based on the UNet video model. However, long\nanimation shots often last 10 ∼30 seconds (300 to 1000\nframes) in real scenarios. We propose LongAnimation to\nachieve efficient long-term coloring on the DiT architecture.\n3. Methodology\nThe pipeline of LongAnimation is depicted in Fig. 2,\nwhich mainly consists of three parts:\nSketchDiT, Dy-\nnamic Global-Local Memory and Color Consistency Re-\nward.\nDuring training, given a reference image I, F\nframes of sketches {Sf}F\nf=1, and corresponding text de-\nscriptions, LongAnimation extracts hybrid reference fea-\ntures of these inputs through SketchDiT, which is designed\nto facilitate the subsequent Dynamic Global-Local Mem-\nory mechanism.\nWhen generating subsequent segments,\nDynamic Global-Local Memory dynamically compresses\nhistorical features, which adaptively extracts global con-\nsistency features related to the current hybrid features ob-\ntained from SketchDiT, ultimately generating F frames of\nlong-consistency animation {If}F\nf=1.\nColor Consistency\nReward is used to refine long-term color consistency during\ntraining while color consistency fusion is used to smooth\ntransition segments during inference. In this section, we\nwill first introduce preliminaries. Then, we will describe\nthe SketchDiT, Dynamic Global-Local Memory and Color\nConsistency Reward. Next, we will introduce the color con-\nsistency fusion.\n3.1. Preliminaries\nVideo Diffusion Transformer.\nOur work is based on\nCogvideoX [46], one of the excellent performing DiT-based\nvideo generation models [7, 14, 46]. The text encoder Et(·)\nfirst encodes the text into ct. Next, the reference image\nand video are encoded by the 3D VAE encoder Ev(·) into\nreference image token ci and video token z. Then, ci is\npadded and concatenated with z along the channel dimen-\nsion, followed by concatenation with ct along the sequence\ndimension. Finally, the combined sequence is fed into the\nDiT model as input. The DiT denoiser ϵθ(·) is trained by:\nLDiT = E\nz,ϵ,t ∥ϵ −ϵθ (zt, t, ct, ci)∥2\n2 ,\n(1)\nwhere ϵ refers to standard noise and t means denoising\ntimestep. zt is the video hidden tensor at t-th timestep.\nReward model without using the gradient. In image or\nvideo generation, reinforcement learning (RL) algorithms\nuse rewards to align with preferences.\nThere are two\nmain types: gradient-based rewards (GR) [42, 44] and non-\ngradient-based rewards (NGR) [1, 5, 15]. GR requires the\nreward gradient for optimization, whereas NGR does not.\nThe models generate images or videos in pixel space via\nVAE and then score them using the reward. Backpropa-\ngating gradients through the 3D VAEs of most DiT-based\nvideo models [14, 46] is computationally expensive. There-\nfore, we choose NGR to optimize the model. The objective\nfunction Lr is defined as maximizing the expected reward,\nwith its gradient ∇θLr in diffusion models expressed as:\n∇θLr = E\n\"\nr(x0)\nT\nX\nt=1\n∇θ log pθ(zt−1|zt)\n#\n,\n(2)\nwhere x0 donates the generated video in the pixel space and\nzt denotes the latents in the t -th denoising step. T is the\ntotal steps. Omit various control conditions c for simplicity.\nFigure 2. Overview of the LongAnimation. (a) During training, the reference information is fed into the CogvideoX [46] and SketchDiT,\nrespectively, for efficient extraction of hybrid reference features. These reference features are then fused with the historical information\nin Dynamic Global-Local Memory (DGLM) for consistency generation. (b) For the first segment generation, the reference features are\nfed into SketchDiT and then directly sent to the video model. (c) For the subsequent segment generation, DGLM dynamically extracts\nhistorical features, which are adaptively fused with current reference features from the SketchDiT before being fed into the video model.\n3.2. Model Architecture\n3.2.1. SketchDiT\nSketchDiT is designed to efficiently extract the hybrid rep-\nresentation of reference image, sketches and text, facilitat-\ning the implementation of Dynamic Global-Local Memory.\nAs shown in Fig. 2 (a), sketches {Sf}F\nf=1 are first en-\ncoded by 3D VAE encoder Ev(·) to the sketch token ck.\nThe token is then concatenated with the padded reference\nimage token ci along the channel dimension, and subse-\nquently concatenated with text features ct in the sequence\ndimension as the input of SketchDiT. We introduce text con-\ntrol conditions in SketchDiT to enable text and reference\nimage to jointly guide animation coloring (e.g., background\ngeneration), which could not be achieved by past methods.\nIn animation production, the reference image typically does\nnot always align with the first frame of the sketches. Cre-\nators only need to draw a keyframe, from which smooth\nvideos before and after the key frame can be generated us-\ning sketches. Therefore, during training, we propose to ran-\ndomly select the reference image from historical key frame\n(e.g., the keyframe that is hundreds of frames away from the\ncurrent segment.) rather than being fixed as the first frame\nof the segment to enhance the robustness.\nThen, the concatenated tokens are fed into the SketchDiT\nS(·) to get the hybrid multimodal feature S(ct, ci, ck).\nFollowing CogvideoX, SketchDiT adopts L DiT modules\n(where L ≪N to save computing resources and time con-\nsumption), each of which contains a 3D attention layer and\na Feed Forward Network (FFN). To accelerate training, we\nuse the first L layers of CogvideoX as the initial weights for\nSketchDiT. The hybrid multimodal features S(ct, ci, ck)\nare then injected into the vision branches of different lay-\ners’ 3D Attention of CogvideoX parallelly. Specifically, to\nfurther reduce overfitting, we perform skip-layer control of\nthe base model, which is as follows:\nzn = zn + γS(ct, ci, ck)\nn ∈{2, 4, . . . , N},\n(3)\nwhere zn refers to the visual features obtained after the 3D\nAttention in the n-th DiT block. N denotes the number\nof DiT blocks in CogvideoX. γ is a weight factor. During\ntraining, we freeze the weights of CogVideoX to preserve\nits original text-guided capabilities.\nFor the first generation, the features S(ct, ci, ck) are di-\nrectly fed into CogvideoX, as shown in Fig. 2 (b). For sub-\nsequent generation, the features S(ct, ci, ck) are fused with\nrelevant global features of Dynamic Global-Local Memory\nbefore being fed into CogvideoX, as shown in Fig. 2 (c).\n3.2.2. Dynamic Global-Local Memory\nHistorical animations contain both color consistency fea-\ntures related to the current generation and redundant fea-\ntures. To extract the color consistency features related to the\ncurrent generation, we propose the Dynamic Global-Local\nMemory (DGLM) mechanism as shown in Fig. 2(c).\nThe long video understanding (LVU) model Video-XL\n[31] is used to extract historical features. After estimating\nthe visual feature changes between adjacent frames using\nCLIP [29, 39], different segments (e.g., 2, 4, 8 frames, offer-\ning more fine-grained compression than 3D-VAE) are dy-\nnamically selected for the video. Subsequently, these frame\nsegments are fed into the multimodal large language model\n(MLLM), which autoregressively generates a visual token\n<vs> for each segment. Each token <vs> is influenced by\nall previous frame segments and their <vs> tokens, while\nthe original frame features are finally offloaded. The key\nand value of the <vs> token at each layer are stored in the\nKV cache to improve efficiency.\nSome studies [4, 33] indicate that the middle layers of\nMLLM focus more on global visual features than the final\nlayer. Specifically, we define the most recently generated\nsegments as local video Vl and all historically generated\nframes as global video Vg.\nConsidering these segments\ncomprehensively can achieve better temporal consistency.\nSpecifically, the global video Vg and the local video Vl are\nfed into the LVU model, which extracts the keys {kg, kl}M\nm\nand values {vg, vl}M\nm of the visual token <vs> stored in\nthe KV cache, where M refers to the extracted layer num-\nber. {kg, kl}M\nm and {vg, vl}M\nm are then fed into the FFN-\nbased projector to align the dimensions of the hybird fea-\nture S(ct, ci, ck). Then the extracted M-layer KV cache\nfeatures and features S(ct, ci, ck) are sequentially fed into\nM cross-attention layers, adaptively extracting global vi-\nsual features relevant to the current generation as follows:\nSoftmax\n \nQ · K\n\u0000[km\ng , km\nl ]\n\u0001\n√\nd\n!\nV\n\u0000[vm\ng , vm\nl ]\n\u0001\n,\n(4)\nwhere the Query Q = Wq · S(ct, ci, ck), key K = Wk ·\n[km\ng , km\nl ] and value V = Wv · [vm\ng , vm\nl ]. Wq, Wk and Wv\nare weight parameters. [ , ] denotes feature concatenation.\nSummarily, DGLM achieves long-term color consis-\ntency by dynamically compressing historical features and\nadaptively extracting current-relevant features.\nFigure 3. Overview of color consistency fusion during inference.\nThe dashed latent of the overlap part is finally discarded.\n3.2.3. Color Consistency Reward\nWhile the DGLM module significantly improves long-term\ncolor consistency, minor color details remain flawed. In-\nspired by non-gradient reward studies[1, 5] in image gener-\nation, we propose the Color Consistency Reward (CCR) to\nrefine the long-term color consistency of animation.\nMany past studies [32, 35] have shown that self-attention\nin Transformer acts as low-pass filter, which means it cap-\ntures low-frequency features (e.g., colors in anime) bet-\nter than high-frequency features (e.g., sketches). Thus, by\naligning the M-layer KV cache features from the LVU\nmodel of the generated anime with those of the reference\nanime, the generated anime colors can be closer to the real\nanime colors. The reward function is as follows:\nr =\nM\nX\nm=1\n\u0000∥km\nref −km∥2\n2 + ∥vm\nref −vm∥2\n2\n\u0001\n,\n(5)\nWhere {km\nref, vm\nref} donates the features of the m-th KV\ncache of the reference video, and {km, vm} donates the\nfeatures of the m-th KV cache of the generated video.\nThe NGR loss from Eq. (2) is used to optimize CCR.\nAligning the KV caches of the generated video with the\nreference video further reduces the gap between inference\nand training. The KV caches are extracted from reference\nvideos during training, while these features are extracted\nfrom past generated videos during inference. Therefore, the\nCCR can further refine long-term color consistency.\n3.3. Inference Paradigm\nWhile LongAnimation can achieve global consistency be-\ntween segments, smooth transitions between adjacent seg-\nments also need to be processed. Recent Studies [2, 49] ex-\nplore latent blending strategies in other fields of video gen-\neration by blending overlapping segments at all timesteps.\nHowever, we discover using latent blending strategies\nfor all denoising steps would disrupt visual details (e.g.,\nbrightness), which is obvious in dark brightness anima-\ntions, as shown in Fig. 7. This phenomenon mainly oc-\ncurs when blending from the early denoising stage. On the\none hand, the early stage focuses on the overall layout fea-\ntures, while the later stage focuses on the visual details (e.g.,\nMethods\nShort Term (14 Frames)\nLong Term (Average 500 frames)\nLPIPS ↓\nSSIM ↑\nPSNR ↑\nFVD ↓\nFID ↓\nLPIPS ↓\nSSIM ↑\nPSNR ↑\nFVD ↓\nFID ↓\nToonCrafter[43]\n0.196\n0.457\n18.57\n564.48\n52.91\n0.238\n0.440\n18.01\n751.20\n89.87\nLVCD[12]\n0.203\n0.732\n22.86\n520.51\n89.39\n0.223\n0.722\n22.77\n734.85\n104.90\nAniDoc[25]\n0.142\n0.759\n24.13\n427.03\n70.31\n0.169\n0.743\n23.17\n531.32\n76.67\nLVCD*\n0.126\n0.811\n26.66\n288.70\n78.95\n0.162\n0.776\n22.94\n473.02\n94.86\nOurs\n0.054\n0.867\n27.22\n187.48\n37.80\n0.068\n0.868\n26.71\n240.57\n40.75\nImprovement\n∆57.1%\n∆6.9%\n∆2.1%\n∆35.1%\n∆28.6%\n∆58.0%\n∆11.8%\n∆15.3%\n∆49.1%\n∆46.9%\nTable 1. Quantitative comparison with existing methods. LongAnimation achieves the best performance in both short-term and long-\nterm animation coloring. Compared to short-term animation coloring, LongAnimation shows greater improvement in long-term animation\ncoloring. Bolded numbers indicate the best performance.\nFigure 4. Qualitative comparison with existing methods. LongAnimation achieves long-term color consistency through Dynamic Global-\nLocal Memory (e.g., the gril’s dress and leaves). In contrast, previous methods exhibit unstable color changes. We highly recommend\nwatching the videos in the supplement material for a more direct and clear understanding.\nfine-grained color, brightness). The overall layout features\ngained by the early denoising fusion are redundant for the\nanimation coloring task. On the other hand, current DiT-\nbased video models usually jointly decode video latents,\nrather than decoding each frame separately like the previous\nUNet models. It means that unnecessary perturbations to\npartial latents affect the decoding of all latents. Since visual\ndetail features (e.g., color, brightness) are mainly refined in\nlate denoising stage, we propose a fine-grained color con-\nsistency fusion in late denoising stage, as shown in Fig. 3.\nFormally, given two adjacent frame segments Pi−1 and\nPi, each containing F frames. The frame number over-\nlapped by two segments is C. A fusion factor α =\n1\nC+1\nis set to control the fusion intensity. α ensures that frames\ncloser to the boundary have lower weights, while inter-\nnal frames have higher weights. During denoising, when\nt ∈[T, tst], the frame segments are directly concatenated,\nwith the overlapping part taking half from each segment.\nWhen t ∈[tst, 0], latent fusion is applied to the overlapping\nparts of adjacent segments as follows:\nzk\nit = k · α · zk\nit + (1 −k · α) · zF −C+k\n(i−1)t ,\n(6)\nwhere k ∈[1, C + 1] donates the k-th overlapping segment\nframe. zk\nit denotes the video latent of the k-th overlapping\nframe in segment Pi during the denoising step t.\nIn summary, color consistency fusion smoothes color\ntransitions between adjacent segments by fusing overlap\nsegments during the late denoising stage of inference.\nFigure 5. Text and reference image jointly control background\ngeneration, which could not be achieved by previous methods.\n4. Experiments\n4.1. Experimental Setups\nImplementation.\nOur\nmodel\nis\nimplemented\non\nCogVideoX-1.5-5B [46], which can stably generate 81\nframes. Sakuga-42M [26] is used as our training dataset.\nOur work is solely for non-commercial scientific research.\nWe filter high-aesthetic and dynamic video clips with\nlengths greater than 91 frames, retaining about 80k videos\nfor training. The model is trained on 6 A100 GPUs with\na learning rate 1e-5. The model is trained at a resolution\nof 1024 × 576. We set the SketchDiT layers L = 6 while\nCogVideoX-1.5 layers N = 42. SketchDiT is trained for\n30k steps in the first stage, and GLM is trained for 10k\nsteps in the second stage. Finally, we use Color Consistency\nReward to refine the coloring ability for 10k steps further.\nMore details are in Appendix A.1.\nTest Dataset.\nWe randomly select 3k samples from the\nSakuga test set for short-term coloring testing. To ensure\na fair comparison with past models, we only use the first\n14 frames generated by each model (42k frames in total)\nfor short-term analysis. For long-term animation genera-\ntion, we select videos with more than 300 frames from the\nSakuga test and filter samples with aesthetic scores above\n0.8 and dynamic scores above 0.4 (200 videos, approxi-\nmately 100k frames) to test long-term generation.\nBinaryzation. Standard animation creation usually uses bi-\nnary lines. Anidoc [25] points out that LVCD [12] directly\nuses grayscale sketches, which causes the model to recover\nthe hidden colors from the sketch rather than from the ref-\nerence image. During training, we extract the sketches by\nthe method [3] and binarize them by setting values greater\nthan 200 to 0 and all other values to 1.\nEvaluation metrics. We evaluate the animation coloriza-\ntion effect from two aspects: 1) Video quality: FID [9] and\nFVD [34] are used to evaluate the generated frame quality\nand video quality. 2) Frame colorization similarity. Since\nthe sketches in the animation are extracted from the original\nanimation, we measure the generated animation frames and\nthe original frames through PSNR, LPIPS, and SSIM [40].\nFor all metrics, we resize the frame to 256 × 256 and nor-\nFigure 6. Ablation Studies of modules. Compared to SketchDiT,\nDGLM markedly enhances color consistency (e.g., the girl’s hair).\nCCR further refines color details (e.g., the girl’s hairband).\nmalize the pixel to [0, 1] following recent studies [12, 25].\nPrior SOTAs. We compare our model with open source\nstate-of-the-arts. These methods are all trained on UNet-\nbased video diffusion models, which are LVCD [12], Toon-\nCrafter [43], and AniDoc [25].\nNote that in Tab. 1 and\nFig. 4, LVCD∗[12] uses grayscale sketches in the [0, 255]\nrange according to the default settings. All other methods\nuse binary sketches. Autoregressively using the last frame\nof the previous segment as the reference image for the next\ngeneration will lead to the accumulation of noise errors, as\nshown in Appendix B.1. Therefore we set all methods to\nuse the same reference frame for generating all segments.\n4.2. Main Results\nIn this section, We will compare LongAnimation with prior\nSOTAs through qualitative and quantitative analyses.\nQuantitative results. As shown in Tab. 1, our method\nachieves the best performance in both short-term and long-\nterm animation coloring tasks.\nFor short-term anima-\ntion generation, LongAnimation improves frame similarity\n(LPIPS) and video quality (FVD) by 57.1% and 35.1%\ncompared to sub-optimal results, respectively. For long-\nterm video generation, LongAnimation improves frame sim-\nilarity (LPIPS) and video quality (FVD) by 58.0% and\n49.1% compared to sub-optimal results, respectively. Lon-\ngAnimation shows greater improvement in long-term gen-\neration compared to short-term generation, indicating the\neffectiveness of our model in long-term generation.\nQualitative results. Fig. 4 shows the qualitative com-\nparison between LongAnimation and existing methods. Our\nmethod achieves long-term color consistency through Dy-\nnamic Global-Local Memory.\nMore qualitative compar-\nisons are presented in Appendix B.2.\nTo retain the text-guided ability, we introduce text fea-\ntures to SketchDiT. As shown in Fig. 5, LongAnimation can\nachieve long-term text-guided video background generation\nbased on masked reference foregrounds, which past models\nFigure 7. Ablation Studies of timestep on color consistency fu-\nsion (CCF). Adopting CCF can keep the consistency of the spliced\nsegments (e.g., white objects in frames 71 and 72). However, us-\ning CCF in the early stage of denoising (tst = 50&40 ) interferes\nwith other features in latent space, which affects the brightness of\nother frames (e.g., frames 250 and 350). The problem does not\noccur when CCF is used in the late stage of denoising (tst = 20).\nID\nSettings\nLPIPS↓\nSSIM ↑\nPSNR ↑\nFVD ↓\n0\nSketchDiT\n0.086\n0.838\n24.46\n321.62\n1\n0 + Local Memory\n0.080\n0.843\n24.76\n315.05\n2\n0 + Global Memory\n0.078\n0.854\n25.38\n297.93\n3\n0 + DGLM\n0.076\n0.862\n26.11\n261.76\n4\n3 + CCR\n0.068\n0.868\n26.71\n240.57\nTable 2. Ablation studies on each module. Compared with using\nonly SketchDiT (ID-0), introducing both global memory and lo-\ncal memory (DGLM, ID-3) significantly improves performance.\nCompared to local memory (ID-1), global memory (ID-2) per-\nforms better. CCR further refines the coloring performance (ID-4).\nSettings\nLPIPS ↓\nSSIM ↑\nPSNR ↑\nFVD ↓\nw/o CCF (tst = 0)\n0.073\n0.860\n26.39\n266.99\nCCF tst = 50\n0.089\n0.833\n25.40\n337.05\nCCF tst = 40\n0.083\n0.850\n25.71\n301.62\nCCF tst = 20\n0.068\n0.868\n26.71\n240.57\nTable 3. Ablation studies for Color Consistency Fusion (CCF),\nwhere tst represents the timestep for the latent fusion. Injecting\nlocal features from the early stage of denoising (tst = 50 or tst =\n40) reduces the video quality.\n[12, 25, 43] can not achieve. LongAnimation shows strong\ngeneralization and potential for application.\n4.3. Ablation Studies\nTo demonstrate the effectiveness of essential components,\nwe conduct extensive ablation experiments. All ablation\nexperiments are tested on long video (average 500 frames).\nWe present the key ablation study in the main text, with the\nremaining ablation experiments shown in Appendix B.3.\nEffectiveness of Each Module for LongAnimation. As\nindicated in Tab. 2 and Fig. 6, compared to using only\nSketchDiT (ID-0), introducing DGLM (ID-3) significantly\nFigure 8. (a) PSNR with existing methods. LongAnimation out-\nperforms previous methods in PSNR metric. (b) PSNR relative\ndecay ratio in the low-frequency domain. Our method exhibits the\nleast attenuation in low-frequency information (e.g., color), indi-\ncating that our proposed Dynamic Global-Local Memory can bet-\nter maintain long-term color consistency. (c) PSNR relative decay\nratio in the high-frequency domain. Our method exhibits the least\nattenuation in high-frequency information (e.g., sketches). Since\nall methods are controlled by sketches, their attenuation in high\nfrequencies is generally smaller compared to low frequencies.\nimproves long-term color consistency, with the frame sim-\nilarity metric (LPIPS) improving by 11.6% and the video\nquality metric (FVD) improving by 18.6%. ID-1 and ID-\n2 validate the effectiveness of global video and local video\nrespectively. Compared to ID-3, Color Consistency Reward\n(CCR) (ID-4) further refines the long-term coloring effect,\nwith frame similarity metric (LPIPS) improving by 10.5%\nand video quality metric (FVD) improving by 8.0%.\nEffectiveness of Color Consistency Fusion (CCF). As\nindicated in Tab. 3 and Fig. 7, latent fusion from the early\ndenoising stage (tst = 50&40) results in a decrease in\nframe similarity and video quality metrics compared to not\nusing CCF. However, latent fusion from the late denoising\nstage (tst = 20) leads to improvements in these metrics.\nThis indicates that CCF from the late denoising stage not\nonly provides better color consistency in the fusion frames\nbut also maintains the brightness in other frames.\n4.4. Frequency Analysis\nAs shown in Fig. 8 (a), LongAnimation has the best PSNR\nperformance. To further demonstrate the superiority of Lon-\ngAnimation, we perform frequency domain analysis by ap-\nplying the Fourier transform to the videos generated by\neach model and reference videos. After separating high-\nfrequency (e.g., sketch) and low-frequency (e.g., color) fea-\ntures, we calculate the PSNR for each frequency compo-\nnent. For a fair comparison, we calculate the PSNR degra-\ndation rate between long videos and short videos (i.e., 14\nframes) generated by each model. As shown in Fig. 8 (b),\nour method has the weakest decay ratio at low frequencies,\nimproved by 8.2% at 500 frames compared to sub-optimal\nmethod. It shows that our method better preserves the low-\nfrequency (i.e., color) features, while the low-frequency\npreservation of past methods diminishes over time. Since all\nmethods use high-frequency sketch control, high-frequency\ndecay is generally small, as shown in Fig. 8 (c).\n5. Conclusion\nIn this paper, we propose LongAnimation, a novel long ani-\nmation coloring framework to achieve long-term color con-\nsistency. LongAnimation dynamically compresses histori-\ncal features and adaptively extracts features relevant to cur-\nrent generation by a Dynamic Global-Local Memory mech-\nanism. Color consistency is further refined by Color Consis-\ntency Reward. To support DGLM, we propose SketchDiT\nto extract hybrid reference features. Extensive experiments\ndemonstrate the superiority of our method, showing strong\napplication prospects in the animation industry.\nReferences\n[1] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and\nSergey Levine.\nTraining diffusion models with reinforce-\nment learning. arXiv preprint arXiv:2305.13301, 2023. 3,\n5\n[2] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu,\nZhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu\nYue. Ditctrl: Exploring attention control in multi-modal dif-\nfusion transformer for tuning-free multi-prompt longer video\ngeneration. arXiv preprint arXiv:2412.18597, 2024. 2, 5\n[3] Caroline Chan, Fr´edo Durand, and Phillip Isola. Learning to\ngenerate line drawings that convey geometry and semantics.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7915–7925, 2022. 7\n[4] Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui\nHuang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian,\nKun Wang, et al. Explainable and interpretable multimodal\nlarge language models: A comprehensive survey.\narXiv\npreprint arXiv:2412.02104, 2024. 5\n[5] Ying\nFan,\nOlivia\nWatkins,\nYuqing\nDu,\nHao\nLiu,\nMoonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-\nmad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Re-\ninforcement learning for fine-tuning text-to-image diffusion\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 36, 2024. 3, 5\n[6] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu,\nand Hui Wang. Video-ccam: Enhancing video-language un-\nderstanding with causal cross-attention masks for short and\nlong videos. arXiv preprint arXiv:2408.14023, 2024. 3\n[7] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel\nShalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy\nShiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime\nvideo latent diffusion.\narXiv preprint arXiv:2501.00103,\n2024. 3\n[8] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan,\nHayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang,\nShant Navasardyan, and Humphrey Shi. Streamingt2v: Con-\nsistent, dynamic, and extendable long video generation from\ntext. arXiv preprint arXiv:2403.14773, 2024. 2\n[9] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 7\n[10] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu,\nQingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Jun-\nhui Ji, Zhao Xue, et al. Cogvlm2: Visual language mod-\nels for image and video understanding.\narXiv preprint\narXiv:2408.16500, 2024. 3\n[11] Yining Hong, Beide Liu, Maxine Wu, Yuanhao Zhai, Kai-\nWei Chang, Lingjie Li, Kevin Lin, Chung-Ching Lin, Jian-\nfeng Wang, Zhengyuan Yang, et al. Slowfast-vgen: Slow-\nfast learning for action-driven long video generation. arXiv\npreprint arXiv:2410.23277, 2024. 2\n[12] Zhitong Huang, Mohan Zhang, and Jing Liao.\nLvcd:\nreference-based lineart video colorization with diffusion\nmodels. ACM Transactions on Graphics (TOG), 43(6):1–11,\n2024. 2, 3, 6, 7, 8\n[13] Weinan Jia, Mengqi Huang, Nan Chen, Lei Zhang, and\nZhendong Mao. Dˆ 2it: Dynamic diffusion transformer for\naccurate image generation. In Proceedings of the Computer\nVision and Pattern Recognition Conference, pages 12860–\n12870, 2025. 2\n[14] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai,\nJin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang,\net al. Hunyuanvideo: A systematic framework for large video\ngenerative models. arXiv preprint arXiv:2412.03603, 2024.\n3\n[15] Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han\nZhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Jun-\nfeng He, et al. Parrot: Pareto-optimal multi-reward reinforce-\nment learning framework for text-to-image generation. In\nEuropean Conference on Computer Vision, pages 462–478.\nSpringer, 2024. 3\n[16] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,\nYi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al.\nMvbench: A comprehensive multi-modal video understand-\ning benchmark. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22195–\n22206, 2024. 3\n[17] Yanwei Li, Chengyao Wang, and Jiaya Jia.\nLlama-vid:\nAn image is worth 2 tokens in large language models. In\nEuropean Conference on Computer Vision, pages 323–340.\nSpringer, 2024. 3\n[18] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng\nJin, and Li Yuan. Video-llava: Learning united visual rep-\nresentation by alignment before projection. arXiv preprint\narXiv:2311.10122, 2023. 3\n[19] Yijing Lin, Mengqi Huang, Shuhan Zhuang, and Zhendong\nMao. Realgeneral: Unifying visual generation via tempo-\nral in-context learning with video models.\narXiv preprint\narXiv:2503.10406, 2025. 2\n[20] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Free-\nlong: Training-free long video generation with spectralblend\ntemporal attention. arXiv preprint arXiv:2407.19918, 2024.\n2\n[21] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran\nChen, Xiu Li, and Qifeng Chen. Follow your pose: Pose-\nguided text-to-video generation using pose-free videos. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, pages 4117–4125, 2024. 2\n[22] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing\nHe, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung\nShum, Wei Liu, et al. Follow-your-emoji: Fine-controllable\nand expressive freestyle portrait animation. In SIGGRAPH\nAsia 2024 Conference Papers, pages 1–12, 2024.\n[23] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi\nShen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li,\nHeung-Yeung Shum, et al. Follow-your-click: Open-domain\nregional image animation via motion prompts. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, pages\n6018–6026, 2025.\n[24] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng,\nXinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and\nQifeng Chen. Follow-your-motion: Video motion transfer\nvia efficient spatial-temporal decoupled finetuning.\narXiv\npreprint arXiv:2506.05207, 2025. 2\n[25] Yihao Meng, Hao Ouyang, Hanlin Wang, Qiuyu Wang, Wen\nWang, Ka Leong Cheng, Zhiheng Liu, Yujun Shen, and\nHuamin Qu. Anidoc: Animation creation made easier. arXiv\npreprint arXiv:2412.14173, 2024. 2, 3, 6, 7, 8\n[26] Zhenglin Pan, Yu Zhu, and Yuxuan Mu.\nSakuga-42m\ndataset:\nScaling up cartoon research.\narXiv preprint\narXiv:2405.07425, 2024. 7\n[27] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2\n[28] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xin-\ntao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free\nlonger video diffusion via noise rescheduling. arXiv preprint\narXiv:2310.15169, 2023. 2\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 5\n[30] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun\nDu, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhanc-\ning visual consistency for image-to-video generation. arXiv\npreprint arXiv:2402.04324, 2024. 2\n[31] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie\nZhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long\nvision language model for hour-scale video understanding.\narXiv preprint arXiv:2409.14485, 2024. 3, 5\n[32] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao\nWang, and Shuicheng Yan. Inception transformer. Advances\nin Neural Information Processing Systems, 35:23495–23509,\n2022. 5\n[33] Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong\nFeng, and Dongyan Zhao. Probing multimodal large lan-\nguage models for global and local semantic representations.\nIn Proceedings of the 2024 Joint International Conference\non Computational Linguistics, Language Resources and\nEvaluation (LREC-COLING 2024), pages 13050–13056,\nTorino, Italia, 2024. ELRA and ICCL. 5\n[34] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 7\n[35] Peihao Wang,\nWenqing Zheng,\nTianlong Chen,\nand\nZhangyang Wang. Anti-oversmoothing in deep vision trans-\nformers via the fourier domain analysis: From theory to\npractice. arXiv preprint arXiv:2203.05962, 2022. 5\n[36] Wenchuan Wang, Mengqi Huang, Yijing Tu, and Zhen-\ndong Mao.\nDualreal:\nAdaptive joint training for loss-\nless identity-motion fusion in video customization.\narXiv\npreprint arXiv:2505.02192, 2025. 2\n[37] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and\nLiqiang Nie.\nRetake: Reducing temporal and knowledge\nredundancy for long video understanding.\narXiv preprint\narXiv:2412.20504, 2024. 3\n[38] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang,\nand Benyou Wang.\nLongllava: Scaling multi-modal llms\nto 1000 images efficiently via a hybrid architecture. arXiv\npreprint arXiv:2409.02889, 2024. 3\n[39] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng.\nVideollamb: Long-context video understanding with recur-\nrent memory bridges.\narXiv preprint arXiv:2409.01071,\n2024. 5\n[40] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600–612, 2004. 7\n[41] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and\nBohan Zhuang. Longvlm: Efficient long video understand-\ning via large language models. In European Conference on\nComputer Vision, pages 453–470. Springer, 2024. 3\n[42] Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun,\nZhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li.\nDeep reward supervisions for tuning text-to-image diffusion\nmodels. In European Conference on Computer Vision, pages\n108–124. Springer, 2024. 3\n[43] Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xin-\ntao Wang, Ying Shan, and Tien-Tsin Wong.\nTooncrafter:\nGenerative cartoon interpolation.\nACM Transactions on\nGraphics (TOG), 43(6):1–11, 2024. 2, 3, 6, 7, 8\n[44] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation. Advances in Neural Information Pro-\ncessing Systems, 36:15903–15935, 2023. 3\n[45] Yuxue Yang, Lue Fan, Zuzen Lin, Feng Wang, and Zhaoxi-\nang Zhang. Layeranimate: Layer-specific control for anima-\ntion. arXiv preprint arXiv:2501.08295, 2025. 2, 3\n[46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu\nHuang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiao-\nhan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video\ndiffusion models with an expert transformer. arXiv preprint\narXiv:2408.06072, 2024. 3, 4, 7\n[47] Yifeng Yu, Jiangbo Qian, Chong Wang, Yihong Dong, and\nBaisong Liu. Animation line art colorization based on the op-\ntical flow method. Computer Animation and Virtual Worlds,\n35(1):e2229, 2024. 2, 3\n[48] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang\nWei, Yuchen Zhang, and Hang Li. Make pixels dance: High-\ndynamic video generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8850–8860, 2024. 2\n[49] Yuang Zhang,\nJiaxi Gu,\nLi-Wen Wang,\nHan Wang,\nJunqi Cheng, Yuefeng Zhu, and Fangyuan Zou.\nMim-\nicmotion:\nHigh-quality human motion video generation\nwith confidence-aware pose guidance.\narXiv preprint\narXiv:2406.19680, 2024. 3, 5\n[50] Yinhan Zhang, Yue Ma, Bingyuan Wang, Qifeng Chen, and\nZeyu Wang. Magiccolor: Multi-instance sketch colorization.\narXiv preprint arXiv:2503.16948, 2025. 2\n",
    "content": "# Paper Interpretation and Analysis Report: arXiv:2507.01945v1 - LongAnimation\n\n---\n\n## I. Core Content and Main Contributions\n\n### Core Content\nThis paper proposes a **long animation generation framework called LongAnimation**, aimed at solving the problem of **long-term color consistency** in animation coloring. In practical animation production, manually coloring long sequences (e.g., 300–1000 frames) is extremely labor-intensive, making automated long animation coloring a valuable research direction.\n\nMost existing studies focus on coloring **short animations** (e.g., within 100 frames), typically using a **local paradigm** that fuses overlapping features from adjacent segments to ensure smooth transitions. However, this approach neglects global information, leading to long-term color inconsistencies—such as a character’s hat changing colors at different points in time.\n\nTo address this issue, the paper introduces a **dynamic global-local paradigm**, combining historical global features with current local features to achieve more stable color consistency. Specifically, LongAnimation includes four key components:\n\n1. **SketchDiT**: Efficiently extracts hybrid features from reference images, sketches, and text;\n2. **Dynamic Global-Local Memory (DGLM)**: Compresses historical features based on long video understanding models and adaptively integrates them into the current generation process;\n3. **Color Consistency Reward (CCR)**: Optimizes color consistency during training;\n4. **Color Consistency Fusion**: Fuses latent spaces of adjacent segments during the later denoising stage to enhance visual transitions.\n\n### Main Contributions\n- **Conceptual Level**: Introduces a new dynamic global-local paradigm for addressing long-term color consistency.\n- **Technical Level**: Designs a novel architecture based on SketchDiT and DGLM, combined with non-gradient reward mechanisms to improve model performance.\n- **Experimental Level**: LongAnimation significantly outperforms existing methods on open-domain long animation coloring tasks, improving short-term and long-term performance by 35.1% and 49.1%, respectively, in FVD metrics. It can stably generate long animations averaging 500 frames.\n\n---\n\n## II. Breakthroughs and Innovations\n\n### 1. Dynamic Global-Local Paradigm\n- Existing methods rely solely on local segment fusion, ignoring global consistency.\n- This paper introduces the concepts of **global memory** and **local memory**:\n  - **Global memory** extracts low-frequency color consistency features from historical animations to constrain overall color;\n  - **Local memory** focuses on the current segment to ensure smooth transitions.\n- This mechanism enables the model to maintain color stability while handling large movements.\n\n### 2. SketchDiT Architecture\n- Designed specifically for animation coloring, supporting **hybrid control inputs** (image + sketch + text);\n- Improved from the CogVideoX architecture with skip-layer controls to prevent overfitting;\n- Allows reference frames to come from any keyframe during training (not just the first frame), enhancing robustness.\n\n### 3. DGLM Module (Dynamic Global-Local Memory)\n- Uses **long video understanding models** (e.g., Video-XL) to dynamically compress historical features;\n- Adaptively extracts color consistency features relevant to the current generation;\n- Utilizes KV caches to store keyframe features, improving efficiency.\n\n### 4. Color Consistency Reward (CCR)\n- Introduces a **non-gradient reward mechanism** that optimizes color consistency by comparing low-frequency features (e.g., color) between generated and reference animations;\n- Solves the problem of traditional methods failing to capture subtle color deviations.\n\n### 5. Color Consistency Fusion in Inference Phase\n- Proposes a **fine-grained latent space fusion strategy**, applied only in the later denoising stages to avoid affecting brightness and other details;\n- Effectively alleviates color jumps between spliced segments.\n\n---\n\n## III. Entrepreneurial Project Suggestions\n\nBased on the core ideas and technical highlights of this paper, several commercially promising startup directions can be derived:\n\n### 1. **AI Animation Coloring Platform**\n- **Product Positioning**: One-stop automatic coloring service for small-to-medium animation studios or independent creators.\n- **Technical Basis**: Develop a cloud API or desktop software based on the LongAnimation architecture, allowing users to upload sketch sequences, reference images, and text descriptions to output high-quality colored animations.\n- **Key Advantages**:\n  - Supports generating animations longer than 500 frames;\n  - High color consistency reduces manual correction workload;\n  - Supports multi-modal control (image + text + sketch).\n\n### 2. **Intelligent Animation Restoration Tool**\n- **Product Positioning**: Provides intelligent solutions for restoring old animations or remastering classic films.\n- **Technical Basis**: Leverages DGLM and Color Consistency Reward mechanisms from LongAnimation to restore and optimize color consistency in old animations.\n- **Application Scenarios**:\n  - Restoring black-and-white classics with modern styles;\n  - Unifying tone differences across shots;\n  - Recovering color distortions caused by data corruption.\n\n### 3. **Animation Creation Assistant System**\n- **Product Positioning**: A creative assistant tool for professional animators to rapidly generate preliminary animation drafts.\n- **Technical Basis**: Integrates SketchDiT and DGLM modules, supporting animation generation via sketch-driven + text description.\n- **Features**:\n  - Input sketch sequence + text → auto-generate colored animation;\n  - Interactive adjustment of keyframes, system automatically maintains global color consistency;\n  - Exportable in multiple formats for further refinement.\n\n### 4. **Educational / Children's Animation Generator**\n- **Product Positioning**: Personalized children's animation generation service for educational institutions or parents.\n- **Technical Basis**: Combines LayerAnimate (layered control) with LongAnimation's color consistency capabilities.\n- **Business Model**:\n  - Users upload child portraits or define character appearances;\n  - The system generates personalized animated shorts based on story scripts;\n  - Supports voice dubbing, background music, subtitles, and other customizations.\n\n### 5. **Game Animation Asset Generation Tool**\n- **Product Positioning**: Low-cost, high-quality character animation generation tool for game developers.\n- **Technical Basis**: Integrates AniDoc (background-free animation generation) with LongAnimation's color consistency mechanism.\n- **Feature Highlights**:\n  - Input character illustration + action description → output game-style animation;\n  - Automatically maintains character color consistency across scenes;\n  - Exports Unity/Unreal engine-compatible formats.\n\n---\n\n## IV. Conclusion\n\nThe paper *“LongAnimation: Long Animation Generation with Dynamic Global-Local Memory”* makes significant breakthroughs in **long animation color consistency generation**, introducing a novel dynamic global-local paradigm. By integrating key technologies such as SketchDiT, DGLM, and Color Consistency Reward, it achieves high-quality animation generation.\n\nFrom an academic perspective, it advances the field of controllable video generation. From an application standpoint, the technology has strong commercialization potential across multiple verticals, particularly in animation production, content restoration, and educational entertainment.",
    "github": "https://cn-makers.github.io/long_animation_web/",
    "hf": ""
}