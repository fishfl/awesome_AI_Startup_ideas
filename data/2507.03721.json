{
    "id": "2507.03721",
    "title": "Predicting Business Angel Early-Stage Decision Making Using AI",
    "summary": "This article utilizes a large language model to generate a CFA score for a startup pitch and trains a machine learning model to achieve high-precision prediction of investment decisions for startup projects. This approach overcomes the limitations of the original methods and enhances the scalability of decision-making.",
    "abstract": "External funding is crucial for early-stage ventures, particularly technology startups that require significant R&D investment. Business angels offer a critical source of funding, but their decision-making is often subjective and resource-intensive for both investor and entrepreneur. Much research has investigated this investment process to find the critical factors angels consider. One such tool, the Critical Factor Assessment (CFA), deployed more than 20,000 times by the Canadian Innovation Centre, has been evaluated post-decision and found to be significantly more accurate than investors' own decisions. However, a single CFA analysis requires three trained individuals and several days, limiting its adoption. This study builds on previous work validating the CFA to investigate whether the constraints inhibiting its adoption can be overcome using a trained AI model. In this research, we prompted multiple large language models (LLMs) to assign the eight CFA factors to a dataset of 600 transcribed, unstructured startup pitches seeking business angel funding with known investment outcomes. We then trained and evaluated machine learning classification models using the LLM-generated CFA scores as input features. Our best-performing model demonstrated high predictive accuracy (85.0% for predicting BA deal/no-deal outcomes) and exhibited significant correlation (Spearman's r = 0.896, p-value < 0.001) with conventional human-graded evaluations. The integration of AI-based feature extraction with a structured and validated decision-making framework yielded a scalable, reliable, and less-biased model for evaluating startup pitches, removing the constraints that previously limited adoption.",
    "category1": "Application Implementation",
    "category2": "Finance",
    "category3": "Non-Agent",
    "authors": "Yan Katcharovski,Andrew L. Maxwell",
    "subjects": [
        "Machine Learning (cs.LG)",
        "Artificial Intelligence (cs.AI)"
    ],
    "comments": "Comments:Preprint",
    "keypoint": "- The integration of AI with the Critical Factor Assessment (CFA) framework significantly improves the prediction of business angel (BA) investment decisions, achieving 85.0% accuracy.\n- AI-generated CFA scores showed a very strong correlation (Spearman’s ρ = 0.896, p < .001) with human evaluations.\n- A structured and validated decision-making framework combined with AI reduces subjectivity, resource intensity, and biases in evaluating startup pitches.\n- Large language models (LLMs) can effectively assign CFA factor scores to unstructured startup pitch data, enabling automated evaluation.\n- Machine learning classification models trained on LLM-generated CFA features outperformed previous models used for predicting investment outcomes.\n- The top-performing model achieved an F1 score of 0.83, specificity of 0.80, and ROC AUC of 0.82, demonstrating robust predictive performance.\n- Feature importance analysis identified total CFA score, financial expectations, ask amount, features & benefits, and barriers to entry as the most influential factors in investment decisions.\n- A CFA-prompted LLM outperformed an unprompted “stock” LLM in both predictive accuracy (77.4% vs. 58.1%) and quality of feedback provided to entrepreneurs.\n- AI-powered CFA tools offer scalable, cost-effective, and rapid evaluation of startups compared to traditional methods requiring expert human assessment.\n- The study validates Maxwell's original CFA framework while enhancing its practical applicability through automation and AI augmentation.",
    "date": "2025-07-10",
    "paper": "Preprint: July 4, 2025  \n \n1 \n \nPredicting Business Angel Early-Stage Decision Making Using AI \nYan Katcharovski, Andrew L. Maxwell \nYork University, Toronto, Canada \n \nAbstract \nExternal funding is crucial for early-stage ventures, particularly technology startups that require \nsignificant R&D investment. Business angels offer a critical source of funding, but their decision-\nmaking is often subjective and resource-intensive for both investor and entrepreneur. Much research \nhas investigated this investment process to find the critical factors angels consider. One such tool, the \ncritical factor assessment (CFA), deployed more than 20,000 times by the Canadian Innovation \nCentre, has been evaluated post-decision, and found to be significantly more accurate than investors’ \nown decisions. However, a single CFA analysis requires three trained individuals and several days, \nlimiting its adoption. This study builds on previous work validating the CFA to investigate whether \nthe constraints inhibiting its adoption can be overcome using a trained AI model. In this research, we \nprompted multiple large language models (LLMs) to assign the eight CFA factors to a dataset of 600 \ntranscribed, unstructured, startup pitches seeking BA funding with known investment outcomes. We \nthen trained and evaluated machine learning classification models using the LLM-generated CFA \nscores as input features. Our best-performing model demonstrated a very high predictive accuracy \n(85.0% for predicting BA deal/no-deal outcomes) and exhibited very significant correlation (0.896 \nSpearman’s ρ, p < .001) to conventional human-graded evaluations. The integration of AI-based \nfeature extraction with a structured and validated decision-making framework yielded a scalable, \nreliable, and less-biased model for evaluating startup pitches, removing the constraints that limited its \nadoption earlier. \n \nKeywords Business angels, Entrepreneurial finance, Critical Factor Assessment (CFA), Large \nLanguage Models (LLMs), Artificial Intelligence, Startup Pitch Evaluation, Investment Decision-\nMaking \n \n1. Introduction \nAcquiring external funding is a critical milestone for high-potential ventures, allowing entrepreneurs \nto convert innovative concepts into scalable enterprises while funding the development and launch of \ntheir business prior to revenue generation [1]. Business angels (BAs) are a pivotal source of early-\nstage financing, acting as private investors who contribute personal capital for equity. BAs frequently \nPreprint: July 4, 2025  \n \n2 \n \nserve as the initial external equity source, facilitating the early-stage development and validation of \nthe business through seed funding that can subsequently be enhanced through venture capital (VC) \ninvestment [2]. Their contributions go beyond financial assistance to providing mentorship, strategic \ncounsel, and access to valuable networks that profoundly impact a venture's growth trajectory [3]. \nBAs are often motivated by the desire to give back to the community and provide support to the next \ngeneration of entrepreneurs. However, the process of obtaining BA funding is characterized by \nsignificant complexity. Entrepreneurs, especially first-time founders, face a decision-making \nenvironment influenced by subjective assessments, insufficient historical data, and significant \ninformation asymmetries, often complicated by conflicting sources of guidance [4].  \nResearch has identified a large number of factors that can affect BA investment decisions. \nMisunderstanding arises due to a lack of awareness of the decision process, especially how the impact \nof different factors (such as market potential, entrepreneurial experience, product viability, and \nfinancial projections) change as the process evolves [5, 6]. Even in the case of an agreement on \nindividual factors, the assessment of these factors is often based on the investor’s individual \nexperience. BAs use heuristics influenced by personal experiences and biases, which can result in \ninconsistencies in their assessment of the likely success of a venture or of the funding decision [7]. \nFurther, the lack of rigor (or a framework) in the process limits the potential for valuable, objective, \nand consistent feedback. The validated critical factor assessment (CFA) evaluation framework is \nreliable and can be used to provide specific and relevant guidance to the entrepreneur. However, its \nuse in practice has been hampered by the time-consuming and resource-intensive characteristics of its \ndeployment.  \nThis study builds on previous research on the CFA framework, widely used and validated in \nentrepreneur development and investment assessments, to develop an AI operationalization model. \nWe addressed the challenges of widespread deployment and scalability of the original tool, given the \npreviously identified implementation constraints of cost, training, and potential biases [6, 8]. The \nresults of this research reinforce and replicate the original deployment of the CFA tool, using the same \neight factors as in Maxwell’s original research: Features & Benefits, Readiness, Market Size, Barriers \nto Entry, Supply Chain, Entrepreneurial Experience, Financial Expectations, and Adoption [6]. This \nstudy provides evidence of how well-designed and trained AI tools can provide valuable insights for \nentrepreneurs and those working with them to identify specific venture challenges that must be \naddressed to increase the likelihood of receiving funding. \nThis study was facilitated by our access to the original data and assessment techniques used in prior \nwork [6], knowledge of the CFA process, advancements in large language models (LLMs), and \nsupervised machine learning algorithms. This allowed us to automate the evaluation of startup pitches, \nsynthesize factors that impact BA decisions, and provide diagnostic feedback to the entrepreneur. The \ndataset included transcripts from 600 pitches presented on the U.S. television show Shark Tank, \nproviding a diverse sample of real-world interactions between investors and entrepreneurs, resembling \nPreprint: July 4, 2025  \n \n3 \n \nthose in conventional funding settings. The validity of this dataset to replicate real decision-making \nwas discussed in Maxwell’s research that studied full, behind-the-scenes pitches from the Canadian \ntelevision show Dragon’s Den (briefly addressed in this paper) [6]. \nOur research demonstrates that an AI-augmented CFA evaluation tool, using Maxwell's eight-factor \nCFA framework, offers superior reliability in forecasting investment decisions, surpassing \nconventional models in both speed and objectivity while preserving robust predictive accuracy. It \nreinforces many of the insights from Maxwell’s original research, replicating its accuracy (although \nwithout the detailed evolution of the full BA investment-decision process). Drawing on the research \nthat led to the creation of the CFA, this study demonstrates how AI can help prepare funds-seeking \nentrepreneurs for interactions with angels, encouraging them to consider each of the eight critical \nfactors. It highlights the value of understanding why ventures get rejected, contesting the \npresuppositions about the constraints of algorithmic assessment in scenarios requiring nuanced \njudgment by bolstering the ability to offer unbiassed feedback, and reinforcing Noble laureate Daniel \nKahneman’s original insights when referring to the CFA as a mechanism designed to facilitate \ndecision making [7, p. 216]. \nThe study demonstrates a novel approach, by combining a validated venture evaluation framework \nwith an LLM and showing that a framework-aware AI outperforms a generalist AI, thus validating the \noriginal research that facilitated the development of the CFA (a framework is more accurate than no \nframework). Our results offer a number of practical applications and future research implications, for \nentrepreneurs and investors, and their stakeholders keen to rapidly iterate their ventures to increase \ntheir likelihood of venture success (and funding), as well as to those considering how AI can enhance \nexisting entrepreneurship insights and tools to guide those interested in pursuing entrepreneurial \ncareers. This offers a promising area for future research — the ability to leverage existing research to \nprovide AI powered tools to coach or mentor entrepreneurs through their venture creation journey “on \ndemand” and at relatively low cost. Ultimately, this not only allows us to leverage validated research \ninto entrepreneurship education but also increases the likelihood of successful venture development \nand the ability to attract BA investment where required. \n \n2. Why Business Angel Funding is Important \nA BA is a wealthy individual who typically invests their own money in exchange for equity in early-\nstage startups. In 2023, the average BA investment size was $339,390 in return for 9.7% of startup \nequity [21]. In contrast to initial funding from family and friends, which is typically influenced by \npersonal relationships, BA investments are based on evaluations of a venture's growth potential and \nfinancial return prospects as well as the psychological motivation they experience from helping next-\ngen entrepreneurs [17]. Unlike venture capitalists (VCs), BAs do not have to justify their decisions to \nanyone; instead, they allocate their personal capital in return for equity interests, making strategic \nPreprint: July 4, 2025  \n \n4 \n \ndecisions based on considerations such as market opportunity, competitive advantage, and \ncompetencies of the entrepreneurial team [6, 14–16]. \nBA funding plays a crucial role in the entrepreneurial ecosystem, especially during the initial phases \nof venture development when external capital is essential for survival and expansion [2,9]. This is \nparticularly the case for high-growth enterprises, often linked to the development of technologically \ninnovative solutions (and disruptive business models), which can incur significant costs associated \nwith product development, market entry, and operational scaling—expenditures that generally occur \nprior to sufficient revenue generation [10–12]. BAs play a crucial role in bridging this funding gap, \nreferred to as the “valley of death”, by supplying the capital required to connect the initial concept \ndevelopment phase to commercial viability [13].  \nBA participation encompasses more than financial contribution, as their investment frequently acts as \nsignificant validation signals for other stakeholders, including customers and partners. VCs, who \ncome in later, often regard BA support and their relationship with the venture as a prerequisite for \nsubsequent funding, as a signal of the future of their own potential relationship. Thus, BA funding \nserves as an essential conduit, facilitating the transition of startups from the seed stage to the \nacquisition of institutional capital [5, 17–19]. Given that BA investors typically possess significant \nexperience as entrepreneurs or industry veterans, they are seen as “smart money” helping the \nentrepreneur in avoiding pitfalls and establishing industry connections [20]. In a way, BA investment \nillustrates an experimental approach to reducing uncertainty, as they finance early-stage and untested \nventures in anticipation of future investment opportunities [2, 5].  \nIn 2023, U.S. angel investors invested $18.6 billion in 54,735 ventures [21], representing 7% of the \ntotal venture financing. Although this amount is significantly lower than the $248.4 billion invested \nby VC firms across 29,303 deals during the same timeframe [22], BAs participated in the majority of \nearly-stage investment transactions, accounting for 65% of all seed-stage funding deals. Importantly, \nmany BA investments are made in anticipation of raising future funds from VCs or elsewhere—BAs \ntend to consider some of the critical factors that would influence future investment decisions as well \nand relate to actual business performance [19, 20].  \nHowever, the BA investment process, despite its importance, is fraught with deficiencies such as \nsubjective decision-making, information asymmetries, and inconsistent evaluation criteria, leading to \nsuboptimal funding outcomes, as discussed next [6, 7, 23]. \n \n3. Previous Research on BA Decision-Making \nEarly research on BA decision-making has concentrated on determining the primary criteria that \naffect investment results. Factors such as market potential, financial projections, entrepreneurial \nexperience, product viability, and the strength of the business model are critical determinants of \nfunding decisions [6, 24–25]. Much of the foundational research has depended on retrospective self-\nreports from investors, which present challenges such as hindsight and confirmation bias, likely to \nPreprint: July 4, 2025  \n \n5 \n \ndistort perceptions of prior decisions [26]. Research in behavioral decision-making, popularized by \nKahneman in his book Thinking Fast and Slow, indicates that heuristics such as anchoring, \navailability bias, and overconfidence, frequently result in suboptimal judgments in environments \ncharacterized by high uncertainty, exemplified by early-stage investing [7]. Observational studies of \ninvestor–entrepreneur interactions have been suggested as a method to reduce biases in self-reported \ndata [27]. The transition to real-time analysis has guided the creation of structured evaluation \nframeworks designed to improve objectivity in investment decisions.  \nThe critical factor assessment (CFA) framework, developed by the Canadian Innovation Centre (CIC) \nin the 1980s, represents a significant contribution in the domain of evaluating the commercial viability \nof new ventures. The initial framework comprised 37 evaluation criteria including market need, \ntechnical feasibility, and management capability [8], and was refined by Dr. Andrew Maxwell to eight \ncritical factors: Product Adoption, Product Status, Protectability, Customer Engagement, Route to \nMarket, Market Potential, Relevant Experience, and Financial Model. This approach aims to enhance \nthe predictive accuracy of investment decisions and decrease cognitive load on evaluators [6]. \nBetween 1976 and 2004, the Canadian Innovation Assistance Program (IAP), in collaboration with \nthe CIC, used the CFA to evaluate over 14,000 innovative venture applications, proving its robustness \nand adoption [28]. \nEmpirical studies on Maxwell’s CFA framework (hereafter, the CFA) have shown its efficacy in \nstructuring BA decision-making. An investor behavior analysis in quasi-experimental settings, \nexemplified by the Canadian reality television show Dragons’ Den, indicated that BAs utilize \nelimination-by-aspects (EBA) heuristics, leading to the immediate rejection of opportunities with \ncritical flaws [6]. This heuristic screening process allows investors to effectively narrow down the \nventures for investment, thus accelerating decision-making. The CFA framework yielded a predictive \naccuracy of 87.5% when forecasting which ventures progress beyond the initial selection stage, as \nBAs consistently rejected pitches that score poorly on critical factors [6]. The CFA was also found to \nbe an accurate predictor of whether early-stage ventures would go on to succeed at go-to-market \nproduct launches. A study that followed 561 CFA-screened ventures five years post-assessment found \nthat its critical factors predicted venture success with an accuracy of 80.9% [29]. \nWhile the CFA framework has significantly influenced BA decision-making, subsequent research has \nexplored its applications and limitations. Structured evaluation frameworks, such as those modeled on \nthe CFA, enhance decision-making efficiency in BA syndicates during due diligence [30]. Concerns \npersist that the strict application of these frameworks may unintentionally reinforce biases or overlook \nthe nuanced potential of atypical, and often successful, ventures [31]. A systematic review of gender \ndisparities in entrepreneurial equity financing indicated that biases, both conscious and unconscious, \ncan affect BA decisions, especially during early-stage evaluations [31].  \nStructured tools such as the CFA help reduce biases by offering objective benchmarks; however, the \neffectiveness of these frameworks is contingent upon the degree to which their foundational criteria \nPreprint: July 4, 2025  \n \n6 \n \nare devoid of inherent bias. Should the critical factors embody implicit biases, structured decision \ntools may unintentionally sustain, rather than address, inequities in early-stage funding decisions. \n  \n4. Limitations of CFA Deployment and Potential of AI \nThe CFA framework effectively provides structured evaluations of early-stage ventures; however, \nseveral inherent constraints limit its scalability, efficiency, and broader applicability. The limitations \narise from the framework's reliance on manual human assessment [6] and dependence on data quality, \nresulting in challenges especially evident in the fast-paced, high-volume contemporary entrepreneurial \nfinance [32]. The process is time-consuming and resource-intensive, with expert-driven CFA \nassessments typically requiring up to six weeks and costing up to $1,400 per evaluation, creating \nsubstantial obstacles for resource-limited startups and organizations with restricted evaluation \ncapabilities [33]. Entrepreneurs must also make decisions rapidly as markets change and opportunities \narise, with studies indicating that moving quickly is a winning strategy in innovative early-stage \nstartups. For such a startup to succeed, rapid iterations and decision making (often referred to as a \nLean Startup approach, coined by author Eric Ries) must be paired with the ability to focus on the \nright issues, while ignoring the wrong ones [34]. Early-stage startups do not possess sufficient data to \nsupport their business assertions, especially on market validation, financial forecasts, and operational \npreparedness [35, 36]. This problem is exacerbated by information asymmetry, when entrepreneurs \nmay inadvertently exclude essential details or, in certain instances, deliberately withhold information \nto portray their ventures more favorably [37, 38]. \nA second significant limitation is the subjective nature of human assessments. Cognitive biases, \nincluding anchoring, overconfidence, and availability heuristics, can still impact investment decisions, \ndespite the structured criteria inherent in the CFA [39–41]. Kahneman emphasized that even skilled \nevaluators are vulnerable to biases that can impair judgment, especially in high-uncertainty situations \nthat demand swift decision-making [7]. This challenge is intensified in competitive entrepreneurial \necosystems, where swift evaluations are crucial for seizing time-sensitive opportunities and sustaining \ndeal flow momentum [42].  \nThe status quo of the market offers access to a validated, useful and relatively easy-to-use assessment \ntool that was expected to be useful for thousands of entrepreneurs and entrepreneurship centers, but \nthe constraints (and costs) of deployment made this impractical. Advancements in AI offer \nopportunities to address this constraint. \nResearch interest in AI’s potential capabilities in investment decision-making has grown recently. An \nexpertly developed and deployed AI model can efficiently process large datasets with speed and \nconsistency, while minimizing the biases that frequently influence human judgment [43, 44]. These \nmodels are further capable of assessing an extensive range of inputs, including quantitative financial \nmetrics and the nuances of qualitative pitch content, at speeds and scales surpassing manual methods \n[45]. In contrast to static decision frameworks, AI models possess the capability to continuously \nPreprint: July 4, 2025  \n \n7 \n \nrevise their evaluations based on new data, changing market conditions, and emerging trends [46]. \nAdaptability is essential to the new venture ecosystem characterized by uncertainty, rapid \ntechnological changes, and evolving consumer behaviors [47, 48]. As AI systems analyze larger \ndatasets, their predictive models offer investors progressively precise insights derived from new data \n[46]. \nSome research exists on the use of machine learning (ML) techniques to predict investor decision-\nmaking, but the lack of a rigorous framework (such as the CFA) for analysis limits accuracy. \nArtificial neural networks (ANNs) and random forests techniques have been employed to predict \ninvestment outcomes based on variables that include funding requests, startup valuations, and team \ncomposition, resulting in a 67% prediction accuracy [49]. Logistic regression and neural network \nmodels have been used on a dataset of Shark Tank deal records, achieving a predictive accuracy of \n62.5% in forecasting deal outcomes [50]. Furthermore, tree-based models were used to analyze \nstartup valuations and investor decision-making on Shark Tank, from show metadata, with an \naccuracy of 70% [51].  \nBeyond classified ML techniques, recent advancements in LLMs have fundamentally transformed the \npotential for the advanced integration of natural language processing. Today, independent researchers \nare able to utilize powerful and expensive AI models, which would have taken several years and \nmillions of dollars to develop and deploy in the past. These models particularly excel in the analysis \nof unstructured textual data, which has traditionally relied on human evaluators.  \n4.1 Hypothesis Development \nThis research was designed to explore the use of trained AI models (leveraging previous work on the \nCFA) to provide a solution that addressed the cost, training, and timing constraints contributing to \nCFA’s limited deployment. If successful, this novel approach would allow the deployment of an AI-\npowered CFA to benefit funds-seeking entrepreneurs and the broader entrepreneurial ecosystem.  \nThe first question to address was whether a trained LLM can replicate the manual CFA assessment in \nproviding valuable insights, previously exclusively done by trained experts. Given the structure of the \nCFA (i.e., each factor must be independently validated), and the results of our initial analysis, we \npropose the following hypothesis: \n \nH1a: When evaluating startup pitches, a CFA-augmented AI model would produce consistent \nand accurate CFA evaluations across each of the eight critical factors, when compared to the \nscoring of trained human evaluators. \nThe nature of the eight factors in the CFA is that they are non-compensatory; thus, a positive \ninvestment decision is only made if there is no fundamental flaw in any one of the factors, allowing \nthe CFA to predict the actual investment decision on this basis. This allows us to propose the \nfollowing hypothesis: \nPreprint: July 4, 2025  \n \n8 \n \n \nH1b: When forecasting an investment decision of a group of BAs, the CFA-augmented AI \nmodel will accurately predict the outcome.  \nThere is no doubt that commercially available AI LLM models (such as ChatGPT, Gemini, and \nClaude) have been effective in helping entrepreneurs develop their ventures and improve every aspect \nof their business and pitch through the insights they provide. It therefore challenges the usefulness of \nthe CFA-augmented LLM, as it could produce similar results and advice to an unprompted LLM. \nWhile it is likely that an LLM is inherently less biased than any randomly selected individual, and can \nproduce fast and cheap feedback, the whole basis of the previous research on the validity of the CFA \ntool is that this structured approach, analyzing the eight critical factors, provides more accurate \ndecision predictors and advice than an unstructured approach. This leads us to our final set of \nhypotheses: \nH2a: A CFA-prompted LLM would produce better evaluations and investment predictions than \nan unprompted “stock” LLM (i.e., out-of-the-box GPT-4).  \nH2b: A CFA-prompted LLM would produce better and more relevant feedback and advice to \nthe entrepreneur than an unprompted “stock” LLM. \n \n5. Research Methodology \nDeveloping an AI model involves selection of appropriate modeling techniques, data gathering, \npreparation, model training, and effectiveness validation. We deploy multiple prompt-engineered \nOpenAI GPT models to assign CFA letter grades to a dataset consisting of transcribed startup pitches \nfrom the US TV show Shark Tank. The dataset of pitches and their respective CFA letter grades were \nsplit into training and testing sets. A series of ML classification models were then trained, tested, and \ncompared across a range of metrics that evaluated their ability to correctly predict funding outcomes \n(i.e., deal or no deal). \n \n5.1 Data collection \nThis research utilized as its primary data source Shark Tank, a US-based television show where \nentrepreneurs present their business ideas to a panel of typically five investors, referred to as Sharks, \nkeen to make equity funding. The interactions between investors and entrepreneurs are live and \nunscripted, which reflects real BA decision-making processes influenced by product potential, \nbusiness models, and financial projections [52, 53]. Maxwell's research addressed the validity of using \nsuch data by examining Dragons' Den interactions, which are almost identical to Shark Tank in \nformat [6]. Maxwell’s exposure to the complete recordings of the interactions between the \nentrepreneurs and the Dragons enabled him to compare the accuracy of the CFA model when \nPreprint: July 4, 2025  \n \n9 \n \nevaluating the full live interactions and those edited for TV, noting a very high (95%) “inter-category \ngrader reliability” [54].  \nA total of 1,153 pitch records were gathered from the publicly accessible Shark Tank US Dataset on \nthe AI-data platform Kaggle [55]. This dataset encompassed a variety of industries, pitch styles, and \nfunding results, and comprised 53 identified variables describing the startup. Example variables, or \ncolumns (excel), include startup industry, investment amounts, and show season details. Our analysis \nconcentrated on the following key fields: Deal Outcome (binary: 1 for deal secured, 0 for no deal), \nAsk Amount, and Ask Equity. We omitted the remaining fields because the primary focus was to \nassign CFA grades based on the verbal pitch, rather than third party-labeled meta data (by Kaggle \nusers). Ask Amount and Equity were retained, as funding opportunities may be rejected based on the \ninability to reach consensus, which is crucial to funding outcomes. We then obtained subtitle \ntranscripts for 600 pitches from public repositories, including Subdl and OpenSubtitles.org, which \nwere matched to the startup pitches from Kaggle. The data underwent thorough cleaning to rectify \nmissing values, standardize formats, and ensure compatibility with AI modelling. This involved \nrectifying transcription inaccuracies, eliminating extraneous content, and aligning pitch data with \nassociated deal outcomes. \n \n5.2 Hypothesis 1a: Synthesizing CFA Features Using AI  \nWe optimized multiple custom versions of GPT models (GPT 4, 4.1-mini, 4.1, o3) to evaluate each \npitch transcript against the CFA criteria by employing advanced techniques, such as agent/workflow \norchestration, function calling, context chunking, and prompt engineering. The custom model \nassigned grades according to the CIC’s standardized rubric, with scores varying from A+ to C- (Table \n1 and Table 2) [6]. The grades were subsequently codified into structured numerical formats (0 to 10) \nto facilitate machine learning analysis (Table 2). Maxwell’s research indicates that the numbers 3 and \n7 were intentionally excluded from the CFA scale to signify that each grade transition denotes a \nsubstantial increase in investment readiness, thereby ensuring that scoring differences accurately \nreflect variations in startup quality [6]. \nTo achieve human-graded CFA scores, we trained three university students to evaluate and grade \nstartup pitches using the CIC’s literature and rubric concerning CFA. The trained students then graded \nthe same 31 text transcripts from our database of 600 Shark Tank pitches across the eight critical \nfactors. For each pitch, we calculated the arithmetic mean of the three graders’ CFA scoring to \nachieve score consensus. \nTable 1. Critical Factor Assessment (CFA) Criteria \nFactor \nCriteria \nEvaluation Questions \nFeatures & Benefits \nPerformance Advantages \nDoes the product offer \ncompetitive advantages over \ncurrent solutions? \nPreprint: July 4, 2025  \n \n10 \n \nBenefits and Costs \nAre benefits substantial relative \nto costs? \nCustomer Demands \nDoes it meet specific customer \nneeds effectively? \nReadiness \nProduct Delivery \nIs the product ready for market, \nwith key milestones achieved? \nValidation Tests \nAre there beta tests or customer \nvalidations supporting \nreadiness? \nBarriers to Entry \nUniqueness \nDoes the venture have \nproprietary tech or strong IP \nprotection? \nMarket Differentiation \nAre there clear competitive \nadvantages? \nAdoption \nCustomer Engagement \nIs there evidence of customer \ninterest or early adoption? \nMarket Validation \nHave customers committed to \npurchasing? \nSupply Chain \nOperational Readiness \nAre supply chains and \npartnerships well established? \nMarket Size \nRevenue Potential \nIs the market size large enough \nto support high growth? \nEntrepreneurial Experience \nIndustry Expertise \nDoes the team have relevant \nindustry or startup experience? \nFinancial Expectations \nFinancial Viability \nAre the projections realistic and \nsustainable? \n \nTable 2. CFA Grade Conversion Table \nGrade \nScore \nAdjusted \nScore \nA+ \n10 \n80 \nA \n9 \n72 \nA- \n8 \n64 \nB+ \n6 \n48 \nB \n5 \n40 \nB- \n4 \n32 \nC+ \n2 \n16 \nC \n1 \n8 \nC- \n0 \n0 \nN/A \n0 \n0 \n \nWe analyzed the AI-CFA's ability to evaluate startup pitches by comparing the 31 expert CFA grades \n(sum of eight factors) to the AI-CFA grades (using four different models: GPT 4, 4.1, 4.1-mini, o3) \nacross three correlation metrics: Spearman’s ρ (monotonic rank agreement), Pearson’s r (linear \nPreprint: July 4, 2025  \n \n11 \n \ncorrelation), and mean absolute error (average point-wise deviation). This enabled us to determine \nhow accurately a CFA-prompted LLM can replicate human evaluation. \n \n5.3 Hypothesis 1b: AI Model Assessment of Features \nWe employed a multi-model ML pipeline to predict the likelihood of securing funding (binary \noutcome: deal/no deal) based on the synthesized CFA features. Data preparation began with \nexploratory data analysis to visualize the dataset and gain insights into existing patterns and \ncharacteristics. We discovered a class imbalance, with 66% of pitches resulting in a Yes (deal) \ninvestment, which we attempted to address through data sampling techniques. \nNext, we generated model features with different CFA factors (i.e., all 8 factors, 8 choose 7, etc.), as \nwell as the requested funding amount and its corresponding equity conversion (Ask $, Ask %) to test \nthe predictive efficacy of different combinations. While Maxwell’s research yielded the highest \naccuracy with all the eight factors [6], it was prudent to conduct an independent evaluation with our \nmodel. We chose the following popular, accessible, and efficient classification algorithms for training \nand evaluation: Logistic Regression, Support Vector Machines (SVM), Naive Bayes Variants, \nDecision Trees and Random Forests, Gradient Boosting, AdaBoost, XGBoost, CatBoost, Neural \nNetworks, and Linear Discriminant Analysis (LDA). To optimize model performance, we employed \nhyperparameter optimization within a cross-validation framework, testing different configuration and \nsettings for each model across smaller splits of data, which help with overcoming potential model \noverfitting. We also tested various ensemble methods, which are common stacking techniques \ninvolving the use of multiple base classification models (EG Random Forests and Logistic \nRegression) in combination. Within the cross-validation optimization, top-performing models were \nthen retrained on the full dataset and tested on a holdout test set (10–20%) to assess generalizability \nand overall predicative accuracy. \nThe models were evaluated based on the following performance metrics: \n1. Accuracy: Proportion of correctly predicted investment outcomes. \n2. F1 score: Balancing precision and recall, especially important for imbalanced datasets. \n3. Specificity: Measuring the ability to correctly identify negative cases (i.e., predicting “no \ndeal” accurately). \n4. Balanced score: A custom metric averaging F1, accuracy, and specificity for comprehensive \nevaluation. \n5. ROC AUC: The probability that the model will assign a higher “funding likelihood” score to \na randomly chosen “deal” pitch than to a randomly chosen \"no deal” pitch. \n \n5.4 Hypothesis 2a/2b: Untrained “Stock” LLM Pitch Processing \nWe started a ChatGPT chat using GPT-4 with the following prompt: \nPreprint: July 4, 2025  \n \n12 \n \n“Attached are transcripts from Shark Tank pitches, where startups are seeking investments. For each \npitch, please analyze and provide a recommendation on whether an investor should consider funding. \nInclude specific reasons for your recommendation based on the pitch details. Additionally, offer \ntargeted advice to each entrepreneur on how they could refine their business model or pitch to \nenhance their chances of securing an investment. Finally, clearly state a “deal” or “no deal” decision \non whether an investor should fund this startup.” \nThe prompt was accompanied with a Shark Tank pitch transcript, which was repeated with 31 pitches. \nWe created a new evaluation session for each pitch to ensure that no previous memory or \nconversations affected the results. The evaluation, recommendation, and deal/no-deal decision were \nrecorded for comparison with our CFA-AI model. The deal/no-deal predictions were compared with \nthe known deal/no-deal outcomes to result in a confusion matrix, comparing the untrained LLM’s \npredictive capabilities. The text evaluation and recommendation were read and compared manually \nfor length of feedback, contextual relevance, depth of insight, and helpfulness. \n \n6. Results \n6.1 Hypothesis 1a/1b: CFA-AI feature extraction and deal prediction \nAcross our three-correlation metrics, the CFA-AI factor-grader powered by GPT-4.1-mini, \noutperformed the rest, and showed a very strong ability to emulate human expert labelling across the \neight CFA factors. Table 3 compares the CFA scores (sum of eight) of four GPT model variants \nagainst human consensus on a set of 31 pitches. \nTable 3. Comparing CFA-AI and Human Expert Grading \nModel \nSpearman’s ρ \np (Spearman) \nPearson’s r \np (Pearson) \nMean \nAbsolute \nError (pts) \nGPT 4.1-mini \n0.896 \n9.7 × 10⁻¹² \n0.886 \n3.4 × 10⁻¹¹ \n4.4 \nGPT o3 \n0.840 \n3.4 × 10⁻⁹ \n0.841 \n3.2 × 10⁻⁹ \n4.8 \nGPT 4 \n0.787 \n1.5 × 10⁻⁷ \n0.779 \n2.4 × 10⁻⁷ \n7.9 \nGPT 4.1 \n0.780 \n2.3 × 10⁻⁷ \n0.775 \n3.1 × 10⁻⁷ \n7.9 \n \nFor monotonic rank agreement (Spearman’s ρ), GPT 4.1-mini achieved ρ = 0.896, indicating that its \nranking of pitches closely mirrored expert grading (values > 0.8 are considered very strong). Linear \ncorrespondence (Pearson’s r), r = 0.886, showed that the model’s raw score predictions lie close to a \nbest-fit line against the expert scores. For mean absolute error, an average absolute deviation of 4.4 \npoints on an 80-point scale (eight CFA factors x 10 highest possible score) meant GPT 4.1-mini is \nwithin ~5 % of the human consensus. \nOf the classification algorithms evaluated to test H1b, Soft Voting Ensemble, consisting of Naive \nBayes, Logistic Regression, Random Forest, and Gradient Boosting with GPT-4, consistently \nPreprint: July 4, 2025  \n \n13 \n \ndemonstrated superior performance while utilizing Features & Benefits, Barrier to Entry, Financial \nExpectations, the total sum of all factors, Ask $ amount, and the respective company equity investors \nwould get. The primary performance metrics for the leading model demonstrated an impressive \naccuracy of 85.0%, F1 Score of 0.83, Specificity of 0.80, ROC AUC of 0.82, Precision of 0.83, and \nrecall of 0.84 (Table 4, Figure 1 for confusion matrix). Additional models also showed noteworthy \nperformance, as seen in Table 4.  \n \nTable 4. Performance Metrics for Top Models \nModel \nFeature \nCombination \nAccuracy \n(%) \nF1 \nScore \nSpecificity ROC \nAUC \nPrecision Recall \nSoft Voting \nEnsemble – \nNaive Bayes, \nLogistic \nRegression, \nRandom Forest, \nGradient \nBoosting (GPT-\n4) \n1, 3, 8, Total, \nAsk $, Ask % \n85.0 \n0.83 \n0.80 \n0.82 \n0.83 \n0.84 \nCatBoost (GPT-\n4.1-mini) \n1, 4, Total, \nAsk $, Ask % \n85.0 \n0.82 \n0.70 \n0.76 \n0.84 \n0.81 \nCatBoost (GPT-\n4.1-mini) \nAll 8, Total, \nAsk $, Ask % \n78.0 \n0.75 \n0.6 \n0.74 \n0.76 \n0.74 \nSoft Voting \nEnsemble – \nSame as above \n(GPT-4) \nAll 8, Total, \nAsk $, Ask % \n75.0 \n0.72 \n0.65 \n0.76 \n0.72 \n0.73 \n \n \nFigure 1. Soft Voting Ensemble – Naive Bayes, Logistic Regression, Random Forest, Gradient \nBoosting (GPT-4) (1, 3, 8, Total, Ask $, Ask %) Confusion Matrix \n \nPreprint: July 4, 2025  \n \n14 \n \nA feature importance analysis on our top performing stacking ensemble model showed that the total \nCFA score (importance weight = 0.195) was the single strongest predictor, followed by Financial \nExpectations (0.158), Ask Amount (0.148), Features & Benefits (0.145), Barrier to Entry (0.101), and \nAsk Equity (0.077). The results demonstrated that the model effectively identified both successful \n(deal) and unsuccessful (no deal) pitches with 85.0% accuracy, exhibiting strong performance across \nall key ML metrics, and supporting H1a/H1b directly. If the LLM model was not effectively assigning \nCFA grades, the overall predictive accuracy would have suffered.  \n \n6.2 Hypothesis 2a/2b: CFA-Prompted LLM vs. Unprompted LLM \nWhile an untrained GPT-4 showed some predictive capability (accuracy of 58.1%), it was \noutperformed by our CFA-prompted LLM on the same test dataset (N=31, Accuracy = 77.4%) (Table \n6 and Figure 2). The largest predictive gap was seen in the Specificity (of all Shark Tank “no deal” \npitches, the percentage our model correctly labeled as “no deal”) of the two models (0.39 vs. 0.92). \nFurthermore, the CFA-LLM generated contextual evaluations and recommendations in full sentences \nand paragraphs, addressing various aspects of the pitch in detail, drawing specific examples from the \npitch and generating relevant and sophisticated recommendations. In comparison, the GPT-4-\ngenerated evaluations and recommendations were presented in short bullet-point format, and were not \nconsistent across various reports, often omitting crucial feedback or insights. The results therefore \nsupport H2a/2b. \n \nTable 6. CFA-Prompted LLM vs. Unprompted LLM Performance Metrics (N=31) \nMetric \nGPT-4 Unprompted \nCFA-Prompted (GPT-4) \nAccuracy \n58.1% \n77.4% \nPrecision (Deal) \n0.62 \n0.92 \nRecall (Deal) \n0.72 \n0.67 \nF1 Score \n0.55 \n0.77 \nSpecificity (No Deal) \n0.39 \n0.92 \n \nPreprint: July 4, 2025  \n \n15 \n \n \nFigure 2. CFA-Prompted LLM vs. Unprompted LLM Confusion Matrices (N=31) \n \n6.3 Error Analysis \nFor the 31 pitches and 8 factors evaluated by the CFA-prompted GPT4 model, Factors 1, 3, 5 \nexhibited the largest mean difference when comparing to human evaluations (Table 8). For the same \n31 pitches, Table 9 displays the seven where CFA-AI misclassified the final deal outcomes (FN – \nFalse Negative, FP – False Positive).  \n \nTable 8. Factors with Largest CFA Score Differences, CFA-prompted GPT4 vs. Humans \nFactor \nMean difference (pts) \nPearson r \nF5 – Supply Chain \n+3.2 \n0.49 \nF3 – Barrier to Entry \n+2.5 \n0.43 \nF1 – Features & \nBenefits \n+1.6 \n0.38 \n \nTable 9. Misclassified Deal Outcomes of 31 Pitches by CFA-Prompted GPT4  \nPitch \n# \nOutcome \nAI Confidence (> \n50%=Deal) \nGPT-4 \ntotal \nHuman total \nError \ntype  \n3 \n1 \n35.8 % \n57 \n57 \nFN \n12 \n1 \n17.1 % \n27 \n41 \nFN \n14 \n1 \n43.2 % \n51 \n50 \nFN \n17 \n1 \n48.3 % \n44 \n42 \nFN \n27 \n1 \n20.5 % \n36 \n40 \nFN \n29 \n1 \n18.3 % \n37 \n45 \nFN \n10 \n0 \n50.5 % \n61 \n51 \nFP \nPreprint: July 4, 2025  \n \n16 \n \n \n7. Discussion \n7.1 Comparison with Andrew Maxwell's CFA \nThe use of AI in combination with the CFA allowed us to build on Maxwell’s research on BA \ndecision making and propose a novel, more efficient, scalable, and accessible approach to assessing \nearly-stage ventures. To evaluate AI’s validity to deploy Maxwell’s CFA, we first determined whether \nit could replicate human-expert results. Maxwell’s CFA method, dependent on human evaluators, \ndemonstrated notable predictive performance, attaining an overall accuracy of 87.5% in forecasting \nearly-stage investment decisions [6]. Our integrated CFA-AI model demonstrated a test accuracy of \nup to 85% when employing ensemble voting models to predict BA funding outcomes. However, it is \nimportant to highlight several differences in data sources and evaluation methods between the two.  \nMaxwell’s study reviewed extensive conversations between entrepreneurs and investors, lasting 30–\n60 min, offering evaluators more context and potential signals for assessment [6]. By contrast, our \nmodel utilized brief 3–5-min Shark Tank pitches, which offer significant reliability (as discussed \nearlier in the research) but could still lack depth. Maxwell’s model combined evaluations from several \ntrained CFA raters, adding a dimension of human consensus that likely enhanced its accuracy [6]. \nThis can be mimicked in our model through the future integration of multi-agent models. Our CFA-AI \nmodel utilized a single output generated by an LLM for each pitch, which may introduce variance \nstemming from model-specific biases. \nMaxwell’s model demonstrated efficacy in predicting negative decision outcomes, achieving 100% \naccuracy in identifying pitches that were likely to be rejected based on fatal flaws (compared to our \nmodel’s 80% specificity) [6]. The CFA-AI model demonstrated enhanced efficacy in forecasting \nfavorable results (Yes Deal). This divergence may suggest variations in the data context. Pitches aired \non TV often highlight success-oriented narratives designed to attract investor interest and audience \nengagement. Maxwell's CFA study employed a dataset of pitches that were not solely broadcasted, \nthus providing a more precise depiction of the deal/no-deal distribution.  \n \n7.2 Comparison to Other Shark Tank Studies \nOur model's performance surpassed that of previous studies utilizing Shark Tank data, with predictive \naccuracies ranging between 62.5–70%: \n• \nDeodhar et al. utilized ANNs and random forests on a dataset of 117 pitches from Shark Tank \nIndia, achieving an accuracy of 67% [49]. \n• \nSherk et al. applied logistic regression and neural networks to Shark Tank data, achieving a \npredictive accuracy of 62.5% [50]. \n• \nLavanchy et al. employed decision tree models to analyze startup valuations and investor \ndecision-making, attaining an accuracy of 70% [51]. \nPreprint: July 4, 2025  \n \n17 \n \nThe improved performance of our model can be attributed to two main factors. First, while prior \nresearch frequently relied on structured metadata, such as deal size, team demographics, and industry \ntype, our methodology utilized LLM-based feature extraction to examine the entirety of the textual \ncontent in startup pitches. This method captures nuanced and context-dependent information that \nstructured metadata may overlook. Second, by integrating the CFA framework as a systematic \nevaluative tool, our model focuses on empirically validated, domain-specific factors instead of generic \nindicators. The integration of comprehensive, text-based features with the structured CFA framework \nrepresents a notable methodological improvement, enhancing our work's standing relative to previous \ninvestment evaluation methods. \n \n7.3 Analyzing the Results \n7.3.1 Top-Performing Model \nThe top-performing model, combining the LLM-based CFA factor synthesis with soft voting \nensemble, exhibited Recall and F1 scores of 0.84 and 0.83 respectively. Recall accounts for the total \npitches that received an investment, compared to how many our model identifies correctly \n(TP/(TP+FN)). The F1 score denotes a harmonic mean between precision (of pitches that our model \nthought will receive investment, how many did) and recall. Our model’s specificity (TN/(TN+FP)) of \n0.80 denoted its ability to correctly identify a significant portion of non-deal pitches. The overall \naccuracy of 0.85 (85%) denotes the total number of correct predictions divided by the total number of \npitches in test set. In a test set of 60, the model correctly classified 51 pitch outcomes. These results \nindicate that the model did not blindly classify all pitches as “deal” cases, often a sign of an overfitted \nmodel, but effectively differentiated between investment outcomes.  \nA potential explanation for the strong performance of Naive Bayes (as a top-performing base model in \nour ensemble) is its robustness with smaller datasets, and the features derived from text. The \nprobabilistic nature enables effective management of class imbalance, and the assumption of feature \nindependence corresponds with the structure of a CFA-based evaluation, wherein each factor is \nevaluated individually prior to aggregation. \n7.3.2 Importance of Specific CFA Factors \nAn analysis of features among the two top-performing models identified Features & Benefits (1), \nBarrier to Entry (3), Adoption (4), and Financial Expectations (8) as the most predictive individual \nfactors, alongside Total CFA Score, Ask Amount, and the respective Ask Percent (e.g. $100,000 in \nexchange for 10% of the company). Readiness (2), Supply Chain (5), Market Size (6), and \nEntrepreneurial Experience (7) were missing, which may be explained either by our model’s \nshortcomings, or by the nature of the information typically expressed in the transcript of the Shark \nTank interaction. Startups that make it to the show have already exhibited some level of Readiness \nand thereby Supply Chain. Most of the startups displayed strong product readiness and some level of \nPreprint: July 4, 2025  \n \n18 \n \nsales, which implies that the product is in the later stages of readiness and basic supply chain \nprocesses were in place.  \nWhile Maxwell's CFA deemed eight factors important, it is possible that the show producers chose to \nomit specific discussion points from aired pitches. Certain factors may also have been pre-screened \nprior to selection of pitches that will air, with Entrepreneur Experience a likely candidate, to ensure \nthe presenter is engaging and relevant. Such factors would limit our model's ability to derive crucial \ndecision-making data, which explains the omission of certain factors from top performing models. \nFinally, the training and prompting of our LLM-based feature extraction may have been inadequate \nfor underperforming factors, indicating the need for further finetuning and prompt engineering. \n7.3.3 Comparing Performance of Different GPT Models \nIt is logical to assume that new and more advanced LLMs will outperform incumbents (e.g., GPT 4.1 \nvs. GPT 4), as regularly shown in popular LLM benchmarking reports such as LiveBench [56]. While \nGPT4.1-mini outperformed GPT4 in its ability to replicate human grading (Spearman’s ρ 0.896 vs. \n0.787), it did not replicate its superiority in predicting pitch funding outcomes. GPT4 outperformed in \ntwo key metrics — Specificity and ROC AUC (the model’s ability to give a higher funding likelihood \nto a Shark Tank-funded pitch when comparing one deal vs. one no-deal pitch at random), by 10 and 6 \npercentage points respectively. The result gap yielded two potential hypotheses: poor quality of \nhuman grading, or the failure to find an optimal model configuration for the CFA factors extracted by \nGPT4.1-mini. While we assert that newer AI models outperformed across common benchmarks, such \nas coding and reasoning, there is no evidence that these improvements would apply to all applications \nand domain —text context classification using the CFA rubric being one of them.  \n \n7.3.4 CFA-Prompted LLM Model vs. Unprompted GPT-4 \nOur results demonstrated the superior performance and usefulness of a custom-prompted LLM with a \nresearch-backed decision-making framework. During initial phases of our research, a common \ncriticism was: “Why is your tool better than simply using ChatGPT?” The answer was clear while \ncomparing it to CFA-AI with 31 pitches (77.4% Accuracy for CFA-AI vs. 58.1% for an untrained \nGPT-4). While an untrained GPT-4 may excel in a broader context of idea and text generation, it may \nalso suffer from a lack of focused training in identifying investment opportunities correctly, and \nproviding optimally useful pitch feedback and improvement recommendations.  \n \n7.5 Research Limitations \n7.5.1 Dataset Limitations \nThe dependence on Shark Tank data, although a comprehensive and accessible dataset, presented \npossible biases. The program is structured for entertainment, potentially affecting the manner in which \nentrepreneurs deliver their pitches and investor reactions, as discussed in Subsection 7.1. This \nconstraint may have influenced the model's capacity to capture nuanced decision-making factors, \nPreprint: July 4, 2025  \n \n19 \n \nespecially those associated with team dynamics or investor follow-up inquiries. Furthermore, the \ndataset of pitches and their respective outcomes did not take into account additional variables that \ncontribute to BA decision making, such as any disagreement about the startup’s valuation or \ndishonesty. During the review of our model’s results, we noticed a case of deal rejection owing to an \ninflated valuation and investment ask from an entrepreneur, which was not reflected in the initial \nevaluated pitch and was not covered by a CFA factor.  \n7.5.2 Class Imbalance \nClass imbalance refers to the unequal distribution of instances across different categories in a dataset, \nwhich can adversely affect the performance of ML models. The dataset demonstrated a class \nimbalance, with 66% of pitches leading to deals and 34% resulting in none. Despite the application of \ntechniques such as random undersampling to address this issue, class imbalance may have continued \nto affect the model's generalization capabilities, especially in predicting negative outcomes. \n \n8. Implications, Practical Applications, and Future Research \n8.1 CFA Factor Synthesis Using OpenAI Competitors \nA key contribution of this research is validating using an LLM to extract CFA features from startup \npitches. While publicly available LLMs such as GPT-4 are designed with the goal of facilitating broad \nnatural language generation, our research demonstrated that a prompted model could execute complex \nand specific evaluative tasks. This was made possible through prompt engineering using the CFA \nrubric, as well as the intelligent architecture of OpenAI API calls, using varying context windows, \nmodel temperatures, and so on. Utilizing competitor models, such as Anthropic’s Claude or Google’s \nGemini, may yield different results, as they are built using differing architectures and trained with \ndifferent datasets. Comparing our GPT-4 and GPT-4.1-mini to additional OpenAI models (o3, o1, \netc.) would offer new data and results. \n \n8.2 Application for Entrepreneurs and Educators \nWhile not evaluated in-depth in the Section 6 (Results), our CFA-AI model (publicly available under \nExpitch.com) provided text evaluation based on the criteria for each of the eight factors, as well as \ntext recommendations for attaining an A+ rating in each critical factor. This resource functions as an \neducational tool for entrepreneurship courses, which we have validated through dozens of \nconversations with academics, incubators, and accelerators. Expitch has been used in multiple \nhackathons by hundreds of students. Future research can examine the usefulness and effectiveness of \nthe CFA in facilitating conversations with startup mentors, by way of providing actionable metrics to \ntrack and work on over time.  \nFuture research may include longitudinal studies to evaluate the effects of AI-generated feedback on \nentrepreneurial outcomes. Entrepreneurs could be monitored longitudinally to assess improvements in \ninvestment readiness and success rates after applying AI-driven recommendations. Controlled \nPreprint: July 4, 2025  \n \n20 \n \nexperiments comparing groups with and without AI feedback could yield causal evidence regarding \nthe tool's effectiveness. Complementary qualitative research such as surveys can investigate \nentrepreneurs’ perceptions on the tool's helpfulness. \n \n8.3 Application for Investors, Policymakers, and Ecosystem Builders \nThe CFA-AI model provides an efficient and uniform approach for the initial assessment of \ninvestment prospects. Although seasoned investors often depend on their intuition and judgment for \nfinal decisions, the model can function as an efficient triage tool for angel groups, venture capital \nfirms, and accelerators. This approach aids in minimizing cognitive biases, optimizing due diligence \nprocesses, and ensuring that valuable opportunities are not missed due to human error or information \noverload. Although angel investors provided valuable inputs during the development of our model, \nfuture research could investigate the practical benefits of deploying the CFA-AI tool in real-world \ninvestment settings. Rather than solely predicting startup success or failure, this tool has the potential \nto function as an advanced decision-support system, enabling BAs to efficiently identify promising \ninvestment opportunities. Given that human judgment is frequently compromised by cognitive biases \nsuch as overconfidence, anchoring, and confirmation bias [7], the integration of AI assessments may \nserve to mitigate these limitations.  \nTo rigorously test this proposition, future studies could employ experimental designs in which angel \ninvestors make investment decisions both with and without the assistance of the CFA-AI tool. \nAdditionally, researchers could investigate the impact on deploying AI decision-making tools to \nmitigate intrinsic human bias and subconscious decision-making factors. The insights provided can \nguide policy development in entrepreneurial finance, especially in creating programs to support early-\nstage ventures. The AI model may be utilized in public funding initiatives, startup competitions, or \ngrant evaluations to facilitate objective, data-driven decision-making. \n \n8.4 Refinement of Feature Extraction Techniques \nFuture research should explore alternative advanced NLP methods to further improve feature \nextraction. Sentiment analysis, currently not covered by our GPT-4-based model, can be employed to \nassess the emotional tone and confidence levels in startup pitches. Techniques such as topic modelling \ncan identify prevailing themes and subjects in pitches, assessing their alignment with current market \ntrends and investor interests. While we lightly experimented with topic modeling and sentiment \nanalysis, our dataset of 600 pitches was insufficient to capture text patterns. \nBy modifying the API adaptation to our tool, future research can investigate the effect of alternative \nLLMs on model performance. LLMs such as Google’s Gemini and Anthropic’s Claude offer unique \narchitectural advantages, such as better contextual comprehension and improved finetuning abilities. \nMore advanced methodologies may include agent-orchestration of multiple different LLMs to discuss \nCFA grades, similar to how the BAs on Shark Tank or Dragons' Den interact. OpenAI is offering \nPreprint: July 4, 2025  \n \n21 \n \nmodels capable of conducting online research, which may be useful in reinforcing and informing the \nassignment of CFA grades based on researched data. \n8.5 Ethical Considerations and Bias Mitigation \nAs AI models increasingly influence critical decisions in entrepreneurial finance, future research must \nthoroughly examine the ethical challenges involved. It is essential to examine algorithmic bias, \nparticularly to assess whether AI models unintentionally introduce or perpetuate biases associated \nwith gender, race, or socioeconomic status in the evaluation of startups. Enhancing transparency and \nexplainability is essential, and both entrepreneurs and investors must be able to interpret AI-generated \nassessments to foster trust and accountability. Establishing robust data privacy protocols to protect \nsensitive business information, particularly as AI tools are integrated into real-world investment \nprocesses, is crucial. Systematic bias audits and the development of comprehensive frameworks for \nexplainable AI can mitigate risks, ensuring that the CFA-AI model is both effective and ethically \nresponsible.  \n8.6 Practical Spillovers \nThe availability of AI tools to train and educate entrepreneurs will transform how we support \nentrepreneurship, providing real time on-demand tools that: \n• \nLower the cost significantly, while improving the speed of feedback. Based on Maxwell’s \noriginal assessment, the original CFA cost was 3 raters × 2 days or about $1400 per venture; \nwhile the AI version costs less than US$1 and is completed in seconds. \n• \nAllow real-time iteration providing scores and direct feedback/diagnostics for founders to \niteratively learn each component of the CFA. Expitch is already being used in classroom \npilots and at industry pitch events.  \n• \nEnhance the use of toolbox and frameworks that have been developed by researchers, \nenabling a similar prompt-engineering templates to be used to bring other under-used rubrics \n(e.g., TRL, JTBD, BMC) into daily practice. \n \n \nPreprint: July 4, 2025  \n \n22 \n \nReferences \n[1] B. Mrkajic, S. Murtinu, V.G. Scalera, Is green the new gold? Venture capital and green \nentrepreneurship, Small. Bus. Econ. 52(4) (2017) 929–950.                                        \n[2] V. Ramadani, Business angels: who they really are, Strateg. Change. 18(7–8) (2009) 249–258. \nhttps://doi.org/10.1002/jsc.852 \n[3] J. Freear, J.E. Sohl, W.E. Wetzel, Angels: personal investors in the venture capital market. Entrep. \nReg. Dev. 7(1) (1995) 85–94. https://doi.org/10.1080/08985629500000005 \n[4] L. Cohen, P. Wirtz, Characteristics of entrepreneurs, entrepreneurial finance, and growth paths, \n2nd Emerging Trends in Entrepreneurial Finance Conference, SSRN Electronic J (2018). \nhttps://doi.org/10.2139/ssrn.3151908  \n[5] F. Tenca, A. Croce, E. Ughetto, Business angels research in entrepreneurial finance: a literature \nreview and a research agenda, J. Econ. Surv. 32(5) (2018) 1384–1413. \nhttps://doi.org/10.1111/joes.12224 \n[6] A.L. Maxwell, S.A. Jeffrey, M. Lévesque, Business angel early stage decision making, J. Bus. \nVentur. 26(2) (2009) 212–225. https://doi.org/10.1016/j.jbusvent.2009.09.002  \n[7] D. Kahneman, Thinking, fast and slow, Penguin UK, 2011. \n[8] T. Åstebro, Assessing the commercial viability of seed- and early-stage ventures, J. Priv. Equity. \n6(1) (2002) 9–12. https://doi.org/10.3905/jpe.2002.320029 \n[9] S. Blank, B. Dorf, The startup owner’s manual: the step-by-step guide for building a great \ncompany. John Wiley & Sons, 2020. \n[10] M. Cantamessa, V. Gatteschi, G. Perboli, M. Rosano, Startups’ roads to failure, Sustainability. \n10(7) (2017) 2346. https://doi.org/10.3390/su10072346 \n[11] H.J. Kamps, How venture capital works, Apress eBooks, 2020, pp. 9–14. \nhttps://doi.org/10.1007/978-1-4842-6065-4_2 \n[12] A. Vohora, M. Wright, A. Lockett, Critical junctures in the development of university high-tech \nspinout companies, Res. Policy. 33(1) (2003) 147–175. https://doi.org/10.1016/s0048-7333(03)00107-\n0 \n[13] S.K. Markham, S.J. Ward, L. Aiman-Smith, A.I. Kingon, The Valley of Death as context for role \ntheory in product innovation, J. Prod. Innov. Manag. 27(3) (2010) 402–417. \nhttps://doi.org/10.1111/j.1540-5885.2010.00724.x  \n[14] M.V. Osnabrugge, A comparison of business angel and venture capitalist investment procedures: \nan agency theory-based analysis, Ventur. Cap. 2 (2000) 91. \nhttps://doi.org/10.1080/136910600295729. \n[15] M. Gorman, W.A. Sahlman, What do venture capitalists do?, J. Bus. Ventur. 4(4) (1989) 231–\n248. https://doi.org/10.1016/0883-9026(89)90014-1  \n[16] W.R. Kerr, R. Nanda, J. McQuade, Entrepreneurship reading: financing entrepreneurial ventures, \nHarvard Business Publishing, 2014. https://hbsp.harvard.edu/product/8072-PDF-ENG (accessed 15 \nPreprint: July 4, 2025  \n \n23 \n \nFebruary 2025). \n[17] S.A. Macht, J. Robinson, Do business angels benefit their investee companies?, Int. J. Entrep. \nBehav. Res. 15(2) (2009) 187–208. https://doi.org/10.1108/13552550910944575 \n[18] J. Hoyos-Iruarrizaga, A. Fernández-Sainz, M. Saiz-Santos, High value-added business angels at \npost-investment stages: key predictors, Int. Small Bus. J. 35(8), 949–968. \nhttps://doi.org/10.1177/0266242616686401 \n[19] A. Croce, M. Guerini, E. Ughetto, Angel financing and the performance of high-tech start-ups, J. \nSmall. Bus. Manag. 56(2) (2016) 208–228. https://doi.org/10.1111/jsbm.12250 \n[20] G.W. Festel, S.H. De Cleyn, Founding angels as an emerging subtype of the angel investment \nmodel in high-tech businesses, Ventur. Cap. 15(3) (2013) 261–282. \nhttps://doi.org/10.1080/13691066.2013.807059 \n[21] J.E. Sohl, The angel market in 2023: an inflection point for women angels? Center for Venture \nResearch 42, 2023. https://scholars.unh.edu/cvr/42  \n[22] CB Insights, State of Venture 2023 Report, 2024. \nhttps://www.cbinsights.com/research/report/venture-trends-2023/ (accessed 16 February 2025) \n[23] C.M. Mason, R.T. Harrison, Is it worth it? The rates of return from informal venture capital \ninvestments. J. Bus. Ventur. 17(3) (2002) 211–236. https://doi.org/10.1016/s0883-9026(00)00060-4 \n[24] M. Kakati, Success criteria in high-tech new ventures, Technovation. 23(5) (2009) 447–457. \nhttps://doi.org/10.1016/s0166-4972(02)00014-7 \n[25] T.G. Vinig, M. De Haan, How do venture capitalists screen business plans? Evidence from The \nNetherlands and the US, SSRN Electronic J. (2002). https://doi.org/10.2139/ssrn.321860  \n[26] C. Mason, M. Stark, What do investors look for in a business plan? Int. Small Bus. J. 22(3) \n(2004) 227–248. https://doi.org/10.1177/0266242604042377 \n[27] J. Hall, C.W. Hofer, Venture capitalists’ decision criteria in new venture evaluation. J. Bus. \nVentur. 8(1) (1993) 25–42. https://doi.org/10.1016/0883-9026(93)90009-t  \n[28] G. Adomdza, Why do inventors continue when experts say stop? The effects of overconfidence, \noptimism and illusion of control, Master’s thesis, University of Waterloo, Canada, 2004. \nhttp://etd.uwaterloo.ca/etd/gkadomdz2004.pdf \n[29] T. Astebro, Key success factors for technological entrepreneurs’ R&D projects, IEEE Trans. \nEng. Manag. 51(3) (2004) 314–321. https://doi.org/10.1109/tem.2004.830863 \n[30] K. Arundale, C. Mason, Business angel groups as collective action: an examination of the due \ndiligence process, Int. Entrep. Manag. J. 21, 39 (2024). https://doi.org/10.1007/s11365-024-01052-7  \n[31] K. Koziol, M. Schmitz, S. Bort, Gender differences in entrepreneurial equity financing—a \nsystematic literature review, Small Bus. Econ. (2025). https://doi.org/10.1007/s11187-024-00989-x  \n[32] M.A. Gonzales M., O. Terzidis, P. Lütz, B. Heblich, Critical decisions at the early stage of start-\nups: a systematic literature review, Innov. Entrep. 13 (2024) 83. https://doi.org/10.1186/s13731-024-\n00438-9  \nPreprint: July 4, 2025  \n \n24 \n \n[33] Canadian Innovation Centre, CFA Snapshot, 2015. https://innovationcentre.ca/product-\ndetails/cfa-snapshot/ (accessed 16 February 2025) \n[34] E. Ries, The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create \nRadically Successful Businesses, Crown Business, New York, 2011 \n[35] J.E. Sohl, The early-stage equity market in the USA, Ventur. Cap. 1(2) (1999) 101–120.  \n[36] G. Cassar, The financing of business start-ups, J. Bus. Ventur. 19(2) (2009) 261–283. \nhttps://doi.org/10.1016/s0883-9026(03)00029-6. \n[37] C. M. Mason, R.T. Harrison, Auditioning for money, J. Priv. Equity. 6(2) (2003) 29–42. \nhttps://doi.org/10.3905/jpe.2003.320037 \n[38] P. Barbaroux, From market failures to market opportunities: managing innovation under \nasymmetric information, J. Innov. Entrep. 3(1) (2014) 5. https://doi.org/10.1186/2192-5372-3-5 \n[39] H.K. Baker, V. Ricciardi, How biases affect investor behaviour, Eur. Financial. Rev. (2014) 7–\n10. https://ssrn.com/abstract=2457425  \n[40] G.K. Kanaan, Psychology and financial decisions: a literature assessment, Manag. Financ. 19(5) \n(1993) 1–10. https://doi.org/10.1108/eb013720 \n[41] M. Altman, Chapter 3: Behavioral economics, thinking processes, decision-making, and \ninvestment behavior. In: H.K. Baker, V. Ricciardi (Eds.), Investor behavior: the psychology of \nfinancial planning and investing. John Wiley & Sons, 2014, 43–61. \n[42] D. Kliger, A. Kudryavtsev, The availability heuristic and investors’ reaction to company-specific \nevents, J. Behav. Finance. 11(1) (2010) 50–65. https://doi.org/10.1080/15427561003591116 \n[43] S.M. Bartram, J. Branke, M. Motahari, Artificial intelligence in asset management, SSRN \nElectronic J. (2020). https://doi.org/10.2139/ssrn.3692805  \n[44] Z. Chen, Ethics and discrimination in artificial intelligence-enabled recruitment practices, Hum. \nSoc. Sci. Commun. 10(1) (2023). https://doi.org/10.1057/s41599-023-02079-x. \n[45] G. Fatouros, K. Metaxas, J. Soldatos, D. Kyriazis, Can large language models beat Wall Street? \nUnveiling the potential of AI in stock selection, arXiv Cornell University, 2024. \nhttps://doi.org/10.48550/arxiv.2401.03737 \n[46] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature. 521(7553) (2015) 436–444. \nhttps://doi.org/10.1038/nature14539 \n[47] Y.Q. Ang, A. Chia, S. Saghafian, Using machine learning to demystify startups’ funding, post-\nmoney valuation, and success, in: V. Babich, J.R. Birge, G. Hilary (Eds.), Innovative technology at \nthe interface of finance and operations, Springer Series in Supply Chain Management, vol 11, 2021, \npp. 271–296. https://doi.org/10.1007/978-3-030-75729-8_10  \n[48] D. Dellermann, N. Lipusch, P. Ebel, K.M. Popp, J.M. Leimeister, Finding the unicorn: predicting \nearly stage startup success through a hybrid intelligence method, International Conference on \nInformation Systems (ICIS), Seoul, South Korea, SSRN Electronic J. (2017). \nhttps://doi.org/10.2139/ssrn.3159123  \nPreprint: July 4, 2025  \n \n25 \n \n[49] O. Deodhar, S. Bhatkar, P. Dixit, P. Kulkarni, A. Oak, S. Londhe, Shark Tank deal prediction \nusing machine learning techniques, 2022 IEEE 7th International Conference for Convergence in \nTechnology (I2CT), 2024. https://doi.org/10.1109/i2ct61223.2024.10543 \n[50] T. Sherk, M. Tran, T.V. Nguyen, SharkTank deal prediction: dataset and computational model, \n11th International Conference on Knowledge and Systems Engineering (2019) 1–6. \nhttps://doi.org/10.1109/kse.2019.8919477  \n[51] M. Lavanchy, P. Reichert, A. Joshi, Blood in the water: an abductive approach to startup \nvaluation on ABC’s Shark Tank. J. Bus. Ventur. Insights. 17 (2022) e00305. \nhttps://doi.org/10.1016/j.jbvi.2022.e00305 \n[52] B. Smith, A. Viceisza, Bite me! ABC’s Shark Tank as a path to entrepreneurship, Small Bus. \nEcon. 50(3) (2017) 463–479. https://doi.org/10.1007/s11187-017-9880-8  \n[53] A. Bhattarai, A ‘Shark Tank’ appearance can lead to millions — but is it real? The Washington \nPost, 17 January 2014. https://www.washingtonpost.com/business/capitalbusiness/a-shark-tank-\nappearance-can-lead-to-millions--but-is-it-real/2014/01/17/67ccc262-7c9a-11e3-93c1-\n0e888170b723_story.html (accessed 16 February 2025) \n[54] A.L. Maxwell, Failing fast: how and why business angels rapidly reject most investment \nopportunities, Master’s thesis, University of Waterloo, Canada, 2009. \nhttps://uwspace.uwaterloo.ca/handle/10012/4253 \n[55] Shark Tank US Dataset, 2024. https://www.kaggle.com/datasets/thirumani/shark-tank-us-dataset. \n[56] C. White, S. Dooley, M. Roberts, A. Pal, B. Feuer, S. Jain, R. Shwartz-Ziv, N. Jain, K. Saifullah, \nS. Dey, S. Agrawal, S.S. Sandha, S. Naidu, C. Hegde, Y. LeCun, T. Goldstein, W. Neiswanger, M. \nGoldblum, LiveBench: a challenging, contamination-free LLM benchmark. arXiv, Cornell University, \n2024. https://doi.org/10.48550/arxiv.2406.19314 \n  \n \n",
    "content": "# AI Academic Paper Interpretation and Analysis\n\n**Paper Title**: *Predicting Business Angel Early-Stage Decision Making Using AI*  \n**Authors**: Yan Katcharovski, Andrew L. Maxwell  \n**Published**: July 4, 2025 (preprint)  \n**Research Background**: To address the issues of high subjectivity, evaluation cost, and low efficiency in angel investors' decision-making during early-stage startup funding, this paper proposes an AI-based solution.\n\n---\n\n## I. Core Content and Key Contributions\n\n### Core Content\nThis paper introduces a novel assessment method that combines the **Critical Factor Assessment (CFA)** framework with artificial intelligence—specifically large language models (LLMs) and machine learning classification models—to predict **Business Angels (BAs)** investment decisions at early stages.\n\n- **CFA Framework**: Developed by the Canadian Innovation Centre, it includes eight key factors such as product features and advantages, market potential, financial expectations, etc.\n- **Data Source**: Transcripts from 600 startup pitches on the U.S. reality TV show *Shark Tank*, with known outcomes (funded or not).\n- **Technical Approach**:\n  - Use GPT series LLMs to automatically generate CFA scores for each pitch;\n  - Train and test multiple machine learning classification models based on these scores to predict whether a pitch was successful or not.\n\n### Key Contributions\n1. **Validating AI as a Substitute for Manual CFA Evaluation**: The CFA scores generated by LLMs showed a high correlation with human expert scores (Spearman’s ρ = 0.896), proving AI can effectively simulate professional evaluation processes.\n2. **Achieving High Prediction Accuracy**: The final model achieved **85% prediction accuracy**, significantly outperforming traditional methods and LLMs without structured prompts (e.g., GPT-4 without CFA prompting scored only 58.1%).\n3. **Reducing Cost and Time**: Traditional CFA evaluations require three people over three days, costing $1,400, while the AI version completes in seconds at less than $1.\n4. **Enhancing Practical Use of CFA Tools**: By automating the CFA process, this approach makes it more scalable for use in entrepreneurial education, incubators, accelerators, and beyond.\n\n---\n\n## II. Breakthroughs and Innovations\n\n### 1. **Integration of Structured AI with CFA Framework**\n- **Innovatively combines a research-validated CFA framework with LLMs**, rather than using general-purpose LLMs directly.\n- Experiments show that **LLMs prompted with CFA instructions perform better than unstructured ones**, supporting the theory that structure enhances performance.\n\n### 2. **High-Accuracy Prediction Model**\n- After feature extraction using LLMs, an ensemble learning model (Soft Voting Ensemble) was built, achieving **85% accuracy** in predicting funding outcomes—far exceeding the 62.5–70% range reported in previous literature.\n- The model demonstrates strong recall (Recall=0.84), F1 score (0.83), and specificity (Specificity=0.80), indicating robust performance in identifying unsuccessful funding cases.\n\n### 3. **Intelligent Feedback Mechanism**\n- Beyond predicting outcomes, the AI provides **detailed feedback across all eight CFA dimensions**, helping entrepreneurs refine their business models and pitch content.\n- Compared to unstructured LLM outputs, this feedback is more targeted and structured.\n\n### 4. **Open Sourcing and Commercial Exploration**\n- The research team has deployed the system as an online tool, [Expitch.com](https://expitch.com), available for free to entrepreneurs.\n- They propose future applications in entrepreneurship education, incubator mentoring, and initial screening for angel investors.\n\n---\n\n## III. Startup Ideas Inspired by This Research\n\n### 1. **AI-Powered Entrepreneurship Coaching Platform**\n- **Suggested Name**: PitchCoach AI / StartupEvaluator\n- **Functionality**:\n  - Users upload business plans or video pitches; the AI returns CFA-based scores and improvement suggestions.\n  - Supports multilingual and multi-industry adaptation for startups, student competitions, and pre-screening in accelerators.\n- **Monetization Models**:\n  - Free basic version + premium subscription (personalized advice, mentor matching, pitch simulation)\n  - Bulk licensing to universities and accelerators\n\n### 2. **Smart Investment Screening Tool for Angel Investors/VCs**\n- **Suggested Name**: DealFilter AI / AngelScreen\n- **Functionality**:\n  - Investors upload startup materials; the AI quickly filters out projects not meeting CFA standards, saving time in preliminary screening.\n  - Can be integrated into existing investment management platforms for priority ranking and risk alerts.\n- **Monetization Models**:\n  - SaaS subscription based on number of projects reviewed\n  - Custom deployment for angel groups and venture capital funds\n\n### 3. **AI-Assisted Entrepreneurship Courses and Training Systems**\n- **Suggested Name**: FounderLab AI / VentureSim\n- **Functionality**:\n  - Interactive entrepreneurship modules designed around the CFA framework.\n  - Students submit ideas, and the AI provides real-time scoring and feedback simulating real investor interactions.\n  - Teacher dashboard for tracking progress and outcomes.\n- **Monetization Models**:\n  - Licensing for educational institutions\n  - Online courses with certification\n\n### 4. **AI Diagnostic Reports and Funding Readiness Services**\n- **Suggested Name**: FundReadiness Report / CFA Audit\n- **Functionality**:\n  - Provides standardized CFA diagnostic reports as supplementary materials for entrepreneurs presenting to investors.\n  - Applicable to government grant applications, competition entries, and accelerator admissions.\n- **Monetization Models**:\n  - Pay-per-report service\n  - API integration with third-party platforms like AngelList or Crunchbase\n\n---\n\n## IV. Conclusion\n\nThis paper not only theoretically validates the effectiveness of AI in predicting angel investor decisions but also practically demonstrates how to combine established research frameworks (like CFA) with modern AI technologies to solve real-world resource constraints. Its findings offer strong practical and commercial potential, particularly for:\n\n- **Entrepreneurs**: Access affordable, high-quality feedback for funding preparation;\n- **Investors**: Improve screening efficiency and reduce subjective bias;\n- **Educators and Incubators**: Build data-driven entrepreneurial guidance systems;\n- **AI Entrepreneurs**: Develop vertical AI products tailored to specific domains.\n\n> **Keywords**: AI entrepreneurship coaching, angel investment prediction, CFA framework, large language models, entrepreneurship education, investment automation",
    "github": "",
    "hf": ""
}