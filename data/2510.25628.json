{
    "id": "2510.25628",
    "title": "EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis",
    "summary": "This paper presents a large-scale comprehensive Electronic Health Record (EHR) reasoning instruction dataset named EHR-Ins, and develops a large language model enhanced in reasoning capability, EHR-R1, based on this dataset. Additionally, an EHR-Bench benchmark is introduced to evaluate reasoning and predictive capabilities.",
    "abstract": "Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.",
    "category1": "Algorithms and Models",
    "category2": "",
    "category3": "Non-Agent",
    "authors": "Yusheng Liao,Chaoyi Wu,Junwei Liu,Shuyang Jiang,Pengcheng Qiu,Haowen Wang,Yun Yue,Shuai Zhen,Jian Wang,Qianrui Fan,Jinjie Gu,Ya Zhang,Yanfeng Wang,Yu Wang,Weidi Xie",
    "subjects": [
        "Computation and Language (cs.CL)"
    ],
    "comments": "",
    "keypoint": "EHR-Ins is a large-scale, comprehensive EHR reasoning instruction dataset comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks.\nThe core innovation of EHR-Ins is a thinking-graph-driven framework that enables the generation of high-quality reasoning data at scale.\nEHR-R1 is a series of reasoning-enhanced foundational language models with up to 72B parameters tailored for EHR analysis.\nEHR-R1 is trained through a multi-stage training paradigm including domain adaptation, reasoning enhancement, and reinforcement learning.\nEHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis.\nEHR-Bench is a new benchmark curated from MIMIC-IV, spanning 42 tasks, designed to comprehensively assess reasoning and prediction across EHR scenarios.\nEHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs on EHR-Bench and EHRSHOT.\nEHR-R1 surpasses GPT-4o by over 30 points on MIMIC-Bench.\nEHR-R1 achieves a 10% higher zero-shot AUROC on EHRSHOT compared to baseline models.\nEHR-R1 demonstrates superior task adaptability, robustness, and generalizability to unseen datasets.",
    "date": "2025-10-31",
    "paper": "EHR-R1: A Reasoning-Enhanced Foundational Language Model\nfor Electronic Health Record Analysis\nYusheng Liao∗,1, Chaoyi Wu∗,1, Junwei Liu∗,3,4, Shuyang Jiang2,5, Pengcheng Qiu1,2,\nHaowen Wang3, Yun Yue3, Shuai Zhen3, Jian Wang3, Qianrui Fan3, Jinjie Gu3,\nYa Zhang1,2, Yanfeng Wang1,2, Yu Wang1,2,† and Weidi Xie1,2,†\n1Shanghai Jiao Tong University, Shanghai, China\n2Shanghai Artificial Intelligence Laboratory, Shanghai, China\n3Intelligence Healthcare Department, AntGroup, Hangzhou, China\n4Intelligence Computing and Sensing Laboratory, Peking University, Beijing, China\n5Fudan University, Shanghai, China\n∗Equal contributions\n†Corresponding author\nYu Wang: yuwangsjtu@sjtu.edu.cn; Weidi Xie: weidi@sjtu.edu.cn\nElectronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is\ncritical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical\nworkflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-\noriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a\nlarge-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning\ncases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-\ngraph-driven framework that enables to generate high-quality reasoning data at scale. Based\non it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for\nEHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning\nenhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge\nand diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce\nEHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess\nreasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1\nconsistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and\nGPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10% higher zero-shot\nAUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced\nthe development for more reliable and clinically relevant EHR analysis.\n1 INTRODUCTION\nElectronic Health Records (EHRs) are comprehensive digital repositories of patient information, encompassing\nlaboratory tests, medications, diagnoses, procedures, clinical notes, etc. [1, 2, 3, 4, 5]. Systematic analysis\nof EHRs is essential for modern healthcare, as accurate interpretation of patient histories enables early\ndisease detection, personalized treatment planning, and improved clinical outcomes. In everyday clinical\npractice, physicians rely on electronic health records (EHRs) to address a wide spectrum of analytical needs,\nspanning decision-support queries, for example, “what is the most probable next diagnosis for this patient?”,\nto prognostic assessments, such as “what is the risk of hospital readmission within 60 days?” Advanced\nEHR analysis systems can substantially accelerate clinical workflows and augment medical decision-making,\nultimately shaping the quality and timeliness of patient care.\nLarge language models (LLMs) have recently transformed biomedical natural language processing (BioNLP),\nachieving impressive results across a range of medical tasks [6, 7, 8, 9, 10, 11, 12, 13], yet their performance\non EHR-related tasks remains limited [14, 15, 16, 17]. Even state-of-the-art commercial models struggle to\nextract, integrate, and reason over EHR data, which markedly limits their interoperability with daily used\nhospital information systems [18] and, consequently, their wide adoption in clinical practice.\narXiv:2510.25628v1  [cs.CL]  29 Oct 2025\nb. Methods Overview\nc. Results\nEW EMER.\nPotassium\nChloride\nZolpidem\nTartrate\nSodium Chloride\n0.9% Flush\nINR(PT)\nAlbumin\nLactate\nDehydrogenase\nMCH\nMedications\nHematocrit\nCreatinine\nCHEST (PRE-OP \nPA & LAT)\nOther and \nunspecified \nhyperlipidemia\nChronic kidney \ndisease, Stage III\nEsophageal\nAortic valve \ndisorders\nAdmissions\nRadiology\nPrescriptions\nLab Event\nElectronic Health Records\na. EHR Analysis Tasks\nDecision Making Tasks\nEW EMER.\nPotassium\nChloride\nZolpidem\nTartrate\nSodium Chloride\n0.9% Flush\nINR(PT)\nAlbumin\nLactate\nDehydrogenase\nMCH\nMedications\nHematocrit\nCreatinine\nCHEST (PRE-OP \nPA & LAT)\nNext Event Intervention (Diagnoses)\nRisk Prediction Tasks\nRisk\nDay\n60\nINR(PT)\nAlbumin\nLactate\nDehydrogenase\nMCH\nMedications\nHematocrit\nCreatinine\nCHEST (PRE-OP \nPA & LAT)\nOther and \nunspecified \nhyperlipidemia\nChronic kidney \ndisease, Stage III\nEsophageal\nAortic valve \ndisorders\nReadmission\nOther and \nunspecified \nhyperlipidemia\nChronic kidney \ndisease, Stage III\nEsophageal\nAortic valve \ndisorders\nCritical Event Prediction (Readmission)\n< 60 Days\nDecision Making\n2.9M\nRisk Prediction\n640k\nReasoning Data\n300k\nEHR-Ins Data\nBase Model\nDomain Adaptation\nReasoning Enhancement\nReinforcement Learning\nEHR-R1\nThree-Stage Training \nEHR-R1 Series\nParameters\n(Billion)\nSeries\nEHR-R1-1.7B\nEHR-R1-8B\nEHR-R1-72B\n72\n8\n1.7\nFigure 1 | Overview of the EHR tasks and the proposed method. a. EHR Analysis Tasks. EHR analysis tasks\nare defined as consisting of two types of tasks: decision-making and risk-prediction. b. Methods Overview. Our approach\naddresses these challenges with a three-stage training pipeline. First, a large volume of non-reasoning data is used for continual\npre-training. This is followed by an instruction-tuning phase that leverages reasoning data. Finally, reinforcement learning\nwith Group Reward Policy Optimization (GRPO) is applied to further refine the model. c. Results. This figure compares\nthe performance of our model against several baseline LLMs on both decision-making and risk-prediction tasks, showcasing its\nsuperior performance.\nExisting work on EHR analysis with LLMs is fundamentally constrained by two major challenges. First, in\nterms of task coverage, prior research has focused on narrow, task-specific objectives (e.g., risk prediction\nfor a specific condition or outcome), typically restricted to particular diseases or event types, and has yet\nto deliver the holistic capabilities required to support evolving clinical workflows [19, 20, 21]. Second, in\nterms of reasoning ability, existing models struggle to construct reliable, EHR-oriented reasoning chains\nthat demand both selective information filtering and multi-source integration. As a result, they are highly\nsusceptible to the redundancy inherent in EHR data [22, 23] and struggle to integrate isolated findings into a\ncoherent longitudinal understanding of disease progression [22, 24].\nIn this paper, to address these, we propose a holistic framework for EHR analysis with two primary\ncontributions: (i) a large-scale super-instruction dataset that unifies diverse EHR analysis tasks into a\ngenerative format and incorporates reasoning supervision, and (ii) a reasoning-enhanced EHR analysis LLM\ntrained with synthetic reasoning data via a three-stage training paradigm. The overview of the EHR tasks\n|2\nand our proposed method are shown in Figure 1.\nWe present EHR-Ins, a large-scale dataset capturing broad task diversity and explicit medical reasoning for\nEHR analysis. The corpus comprises 300K high-quality reasoning cases and 3.5 million non-reasoning cases\nspanning 42 EHR tasks. The tasks generally fall into two categories: decision-making (e.g., diagnosis and\ntreatment recommendations) and risk-prediction (e.g., mortality and readmission). To ensure clinical fidelity\nand relevance, we develop a ‘thinking-graph’ driven reasoning data synthesis pipeline that (i) applies statistical\nanalysis of entity co-occurrence ratios to identify key related entities, (ii) links them via knowledge from\nUMLS [25], and (iii) prompts GPT-4o to produce structured, step-by-step clinical reasoning. The resulting\ndataset is extensive and clinically grounded, enabling models to acquire diverse, context-rich reasoning\ncapabilities.\nSecond, we introduce EHR-R1, a family of reasoning-enhanced LLMs (up to 72B parameters) tailored for\nEHR analysis and trained on EHR-Ins. The training curriculum is designed to enhance domain knowledge and\ndiverse reasoning patterns through three stages: (i) large-scale domain adaptation on extensive non-reasoning\ndata, (ii) reasoning enhancement on high-quality reasoning cases, and (iii) reinforcement learning with Group\nRelative Policy Optimization (GRPO) on a smaller, curated set. This multi-stage regime improves the models’\nability to handle complex EHR tasks and to produce accurate, clinically meaningful outputs across diverse\nscenarios.\nWe evaluate EHR-R1 on EHRSHOT [19] and MIMIC-IV-CDM [26], as well as on a new benchmark, EHR-\nBench, derived from MIMIC-IV. EHR-Bench spans 42 tasks across decision-making and risk-prediction\nsettings, providing a balanced, comprehensive assessment of both reasoning and task-specific performance.\nTogether, these benchmarks cover two distinct clinical centers—Stanford Medicine and Beth Israel Deaconess\nMedical Center—and encompass a broad range of EHR analysis tasks.\nExperimental results demonstrate that our model consistently outperforms leading commercial and open-\nsourced LLMs. In particular, EHR-R1-72B achieves an average performance improvement of over 30 points\ncompared to GPT-4o across all 42 tasks on EHR-Bench, highlighting the effectiveness of our method in\nenhancing LLMs to tackle the full spectrum of EHR-related challenges. On the out-of-distribution EHRSHOT\ndataset, which features significantly different structures and medical concepts, EHR-R1-72B achieves a\nzero-shot AUROC score that is 10% higher than baseline models. These results highlight not only the superior\ntask adaptability of EHR-R1, but also its robustness and generalizability to unseen datasets, underscoring its\npotential as a transformative tool for clinical decision support.\n2 Results\nIn the following, we first give an overview on the data quality of our constructed training data, EHR-Ins,\nthen describe the benchmarks and evaluation metrics. Afterwards, we report the performance of our final\nreasoning-enhanced model, EHR-R1, highlighting its consistent gains over leading LLMs.\n2.1 Training Data Overview\nTo enhance the LLM’s understanding of EHR data and its ability to identify relationships between medical\nentities within redundant EHR records, we propose a novel ‘thinking-graph’ pipeline to curate a new large-\nscale reasoning-enhanced EHR-analysis instruction data, EHR-Ins. This pipeline is designed to augment\nGPT-4o [27], enabling it to generate high-quality EHR reasoning chains. The details of this methodology can\nbe found in Section 4.2.2. The synthesized reasoning data distribution for each analysis task in EHR-Ins is\nillustrated in Figure 2a, widely covering diverse decision-making tasks.\nTo demonstrate the effectiveness of our thinking-graph pipeline in synthesizing reasoning chains for EHR\nanalysis, we hired medical experts to manually, evaluate 100 synthesized reasoning chains across eight decision-\nmaking tasks to check the data quality. As shown in Figure 2b, the experts are tasked to rate each generated\nreasoning samples on a 5-point scale. The scores from highest to lowest represent how completely the reasoning\nprocess supports the predicted label set. A score of 5 indicates that the reasoning process fully supports the\npredicted result, while a score of 1 indicates the reasoning process is completely irrelevant to the predicted\nresult. Alongside our thinking-graph pipeline, we adopt a naïve data distillation strategy with GPT-4o as\n|3\n## Extraction\n1. **Discharge [2130-06-10 00:00:00]**: \n- The patient has a history of left lower extremity claudication and rest pain, with a floating \nintraluminal thrombus in the descending thoracic aorta and occlusion of the left mid superficial \nfemoral artery.\n- The patient was started on anticoagulation therapy and transitioned to apixaban (Eliquis) for \ndischarge.\n- The discharge diagnosis includes an intra-aortic thrombus and a hypercoagulable state of \nunknown cause.\n2. **Radiology Examinations [2130-06-26 20:44:00]**:\n…\n## Reasoning\nThe patient has a history of thrombotic events, with a documented floating thrombus in \nthe thoracic aorta and occlusions in the femoral and popliteal arteries. The recent CTA findings \nreveal a new occlusion in the left common femoral artery and the proximal portion of the left \ndeep femoral artery, which suggests a progression of thrombotic disease in the lower extremity \nvasculature. The acute onset of left leg pain and numbness, alongside the administration of \nheparin, indicates an acute thrombotic event requiring intervention….\n## Final Results\n- Embolectomy and endarterectomy of lower limbs\nEvaluated Case\n- 5 (Very Satisfied): The extracted information is mostly accurate and largely supports the \nconclusion. \n- 4 (Satisfied): The extracted information supports most of the conclusion. A small part of the \nconclusion lacks evidence but is reasonably justified.\n- 3 (Average): The extracted information supports only a small part of the conclusion. The \nfinal result contains a large portion of content that is not adequately supported.\n- 2 (Poor): The extracted information is related to the final result, but it fails to support the \nconclusion.\n- 1 (Very Poor): The extracted information is highly irrelevant to the diagnostic conclusion. \nThe conclusion lacks a reasonable basis, and the reasoning process is unreliable.\nEvaluation Criteria\na\nb\nc\nFigure 2 | Overview of reasoning data in EHR-Ins. a The sample size of each task in the reasoning data of EHR-Ins.\nb Example of human evaluation on the reasoning data. c Manual evaluation results on EHR reasoning data across eight\ndecision-making tasks, each associated with a distinct type of decision-making event. We compared the quality of synthetic\nreasoning data with and without thinking-graph enhancement, where ‘***’ represents a significance level of p < 0.001.\na baseline, i.e., directly prompting it (using Prompt 1) to derive reasoning chains from the original EHR\nanalysis cases.\nThe corresponding results are shown in Figure 2c. First, in relative comparison, the thinking-graph pipeline\nproduced reasoning chains with more adequate EHR evidence across all eight tasks than the naïve data\ndistillation approach. These findings demonstrate that our extracted thinking-graph yields more reliable\nreferences for GPT-4o, effectively mitigating its knowledge limitations and enhancing curated data quality.\nSecond, in terms of absolute scores, the medical experts expressed satisfaction with most of the synthesized\nreasoning data, assigning it an average rating above 4 on a five-point scale. This evaluation underscores the\nability of the thinking-graph pipeline to effectively extract and utilize sufficient medical evidence from EHR\ndata to support target entities, thereby substantially enhancing the capabilities of the LLM.\n2.2 Evaluation Setting\n2.2.1 Benchmarks\nWe evaluate on three benchmarks to comprehensively assess capabilities. EHR-Bench is our primary in-\ndistribution benchmark, constructed from the same MIMIC-IV source as training and providing a holistic\nassessment across diverse EHR tasks. MIMIC-IV-CDM is also based on MIMIC-IV but uses different task\n|4\nRisk\nPrediction\nDecision\nMaking\nReassignment\nService\nProcedure\nTest & Exam\nDiagnose\nTreatment\nICU Event\nMortality\nReadmission\nLengthOfStay\nTransfer\nOutcome\nAdmissions\nTransfer\nProvider Order Entry\nServices\nProcedures ICD\nProcedures CCS\nLabotary Test Events\nMicrobiology Test \nEvents\nOnline Medical Record\nRadiology\nDiagnoses ICD\nDiagnoses CCS\nED Diagnoses ICD\nED Diagnoses CCS\nElectronic Medicine \nAdministration Record\nPrescriptions\nPrescriptions ATC\nMedrecon\nMedrecon ATC\nIngredient Events\nProcedure Events\nInput Events\nOutput Events\nDatetime Events\nICU Mortality 1Day\nICU Mortality 2Day\nICU Mortality 3Day\nICU Mortality 7Day\nICU Mortality 14Day\nED Inpatient Mortality\nInpatient Mortality\nReadmission 30Day\nReadmission 60Day\nED Reattendance 3Day\nICU Readmission\nLengthOfStay 3Day\nLengthOfStay 7Day\nICU Stay 7Day\nICU Stay 14Day\nED Hospitalization\nED ICU Transfer 24h\nED Critical Outcomes\nEHR-Ins\nEHR-Bench\nCategory\n250,000\n1,000\nReassignment\n170,000\n1,000\nService\n316,299\n1,000\nProcedure\n320,000\n2,000\nTest & Exam\n700,000\n2,000\nDiagnose\n773,644\n2,500\nTreatment\n200,000\n2,500\nICU Event\n160,000\n3,500\nMortality\n160,000\n2,000\nReadmission\n160,000\n2,000\nLengthOfStay\n80,000\n1,000\nTransfer\n40,000\n500\nOutcome\n3,429,943\n21,000\nTotal\nNumber\nItem\n2\nTask Types\n12\nCategories\n42\nTasks\nFigure 3 | Overview of EHR-Ins and EHR-Bench.. The hierarchical ring chart displays the distribution of both datasets.\nThe inner ring partitions tasks into two types: risk prediction and decision making. The middle ring shows 12 task categories\n(subtypes). The outer ring details all 42 specific tasks.\nformulations and preprocessing—including filtered, denoised patient histories—and emphasizes diagnostic\ntasks, testing generalization to task formulation. EHRSHOT is derived from a different healthcare system\nwith distinct event types and medical entities, serving as a benchmark to evaluate generalization on a entirely\nnew patient ditribution. More detailed desription and case demonstration for each dataset can be found in\nSection §4.4.1.\nEHR-Bench. To comprehensively evaluate LLM performance on EHR analysis, we introduce EHR-Bench, a\nbenchmark derived from MIMIC-IV [28]. As summarized in Figure 3, EHR-Bench spans 12 subtypes and\n42 tasks, organized into two major groups—decision making and risk prediction—covering a broad spectrum\nof clinically relevant settings [29, 30, 31].\nThe decision-making tasks are generative, requiring the model to recommend the next appropriate intervention\ngiven a specific medical event. We organize these into seven subtypes—reassignment, service, procedure, test\n& exam, diagnosis, treatment, and ICU event—covering, for example, where a patient should be transferred\n(transfer), which tests to order (test & exam), or the likely disease (diagnosis). These subtypes comprise 24\ntasks that assess the model’s ability to map a patient’s current state to clear, actionable decisions.\nIn contrast, risk-prediction tasks are binary classification problems in which the model forecasts whether a\nsignificant medical event will occur within a specified horizon. We group these into five subtypes—mortality,\nreadmission, length of stay, transfer, and outcome. Specifically, transfer refers to events such as a patient’s\nadmission or transfer to another department, and outcome aggregates severe events such as death or ICU\ntransfer. These account for 18 tasks that probe the model’s capacity to identify risks from longitudinal\npatient data.\nBy spanning both generative and predictive settings, EHR-Bench offers a comprehensive framework that\nmirrors real-world EHR challenges and rigorously tests LLM reasoning and adaptability in clinical contexts.\n|5\nMIMIC-IV-CDM. The MIMIC-IV-CDM [26] benchmark is also derived from MIMIC-IV. Compared with\nEHR-Bench, its distinct patient preprocessing pipeline and task design focus enable the evaluation of models’\ngeneralization across domain shifts in EHR analysis tasks. Specifically, it targets diagnostic accuracy for four\ndiseases—appendicitis, cholecystitis, diverticulitis, and pancreatitis—framed as decision-making tasks. We\nassess two diagnostic granularities: main disease diagnosis and ICD-level full diagnosis, enabling evaluation\nacross coarse and fine levels of clinical specificity.\nEHRSHOT. EHRSHOT, a public dataset from Stanford Medicine, is used to evaluate models’ generalization\nto an entirely new healthcare system, which exhibits domain shifts not only in task formulations but also in\npopulation demographics, event type distributions, and recording practices. The benchmark comprises 14\nrisk-prediction tasks across three subtypes: operational outcomes (long length of stay, 30-day readmission,\nICU transfer), anticipating lab test results (thrombocytopenia, hyperkalemia, hypoglycemia, hyponatremia,\nanemia), and assignment of new diagnoses (hypertension, hyperlipidemia, pancreatic cancer, celiac disease,\nlupus, acute MI). Following the original protocol, we assess both zero-shot performance (directly on unseen\ndata) and few-shot adaptation given 1 to 128 examples, probing robustness and sample-efficient transfer.\n2.2.2 Metrics\nWe evaluate decision-making and risk-prediction tasks with metrics aligned to their outputs, more detailed\nformulation can be found in Section §4.4.2.\nDecision-making tasks are formulated as a multi-label prediction problem, where the model outputs a\nset of medical entities per sample. We report F1 score, balancing precision (fraction of predicted entities\nthat are correct) and recall (fraction of ground-truth entities recovered). Only exact entity matches are\ncounted as correct. To accommodate general-purpose LLMs without EHR-specific fine-tuning (e.g., GPT-4o,\nDeepSeek-R1), we provide a candidate pool: 100 randomly sampled labels from the task’s full label space\ncombined with the true labels for models to select, avoiding penalization for unfamiliar output spaces.\nRisk-prediction tasks are formulated as binary classification, which can be evaluated with Area Under\nthe Receiver Operating Characteristic curve (AUROC). It is a robust metric that measures a model’s\nability to distinguish between positive and negative outcomes across all possible classification thresholds,\nmaking it ideal for medical risk prediction where datasets can often be imbalanced.\nSince large language models don’t have a traditional classification head, we get our probability estimates\nby using the tokens yes and no as our positive and negative classes, under a yes/no question prompt. At\ninference, we apply a technique called logit_biases to isolate the model’s scores for yes and no [6]. We\nthen normalize these scores using a softmax function to get a probability for the positive class, which we use\nto calculate AUROC. Further metric details are provided in Section §4.4.2.\n2.2.3 Baselines\nTo provide a comprehensive performance comparison, we select a diverse type of LLMs as our baselines. More\ndetails can be found in Section §4.4.3.\n• Llama3.3-70B [32]: Developed by Meta, this series is a family of powerful open-source models that\nserve as a robust foundation for a wide range of applications.\n• Qwen2.5-72B [33]: As a versatile, general-purpose multilingual model from Alibaba Cloud, Qwen2.5 is\nrecognized as one of the strongest open-source models available.\n• GPT-4o [27]: As OpenAI’s flagship closed-source model, GPT-4o is a powerful multimodal model\nknown for its advanced reasoning and real-time responsiveness.\n• Qwen3-235B [34]: The reasoning model offers a unique ‘thinking’ and ‘non-thinking’ hybrid reasoning\nengine for enhanced problem-solving.\n• DeepSeek R1 [35]: This is the largest open-source reasoning model we evaluate, primarily focused\non pushing the limits of reinforcement learning to achieve state-of-the-art results in complex reasoning.\n• GPT-OSS-120B [36]: Released by OpenAI, this open-source reasoning model provides strong\n|6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAdmissions\n0.0\n0.2\n0.4\n0.6\n0.8\nOMR\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nEMAR\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nProcedures ICD\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nProcedures CCS\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nDiagnose ICD\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDiagnose CCS\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLabevents\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nMicrobiologyevents\n0.0\n0.2\n0.4\n0.6\n0.8\nServices\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nTransfers\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nPOE\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nRadiology\n0.0\n0.2\n0.4\n0.6\n0.8\nPrescriptions\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nPrescriptions ATC\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nMedrecon\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nMedrecon ATC\n0.0\n0.1\n0.2\n0.3\n0.4\nED Diagnose ICD\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nED Diagnose CCS\n0.0\n0.2\n0.4\n0.6\n0.8\nIngredientevents\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nDatetimeevents\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nProcedureevents\n0.0\n0.2\n0.4\n0.6\n0.8\nInputevents\n0.0\n0.2\n0.4\n0.6\n0.8\nOutputevents\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAverage\nLlama3.3-70B\nGPT-4o\nQwen2.5-72B\nOpenBioLLM-70B\nBaichuan-M2-32B\nMedgemma-27B\nGPT-OSS-120B\nDeepSeek-R1\nQwen3-235B\nEHR-R1-72B\nFigure 4 | Performance comparison of EHR-R1 and nine baseline LLMs across 24 decision-making tasks on\nEHR-Bench. The performance is measured with F1 score. Cross-hatched bars denote reasoning-enhanced models, highlighting\nthe effect of explicit reasoning. In each subplot, our EHR-R1 (rightmost bar) achieves a clear performance advantage on nearly\nall tasks.\nreasoning and agentic capabilities to the open community.\n• Medgemma-27B [37]: Created by Google, this is a specialized medical model optimized for medical\ntext and image comprehension, with a key multimodal capability.\n• OpenBioLLM-70B [38]: An open-source medical model meticulously fine-tuned for the biomedical\nfield based on the Llama3-70B, consistently outperforming larger general-purpose models on biomedical\nbenchmarks.\n• Baichuan-M2-32B [39]: This is a powerful open-source medical reasoning model based on Qwen3-\n32B. It has been specifically optimized for medical scenarios and trained from scratch on an unprecedented\namount of high-quality medical data, enabling it to achieve deep medical expertise.\n|7\n2.3 Results on EHR-Bench\nAs shown in Figure 4 and 5, we report the performance of EHR-R1 on both decision-making and risk-prediction\ntasks, benchmarking against strong baselines to demonstrate the effectiveness across diverse EHR analyses.\n2.3.1 Decision-making Tasks\nWe show the results of evaluated LLMs with bar graph in Figure 4 and the accurate numerical performance\nin the Supplementary Table 1.\nEHR-R1-72B demonstrates superior results. It achieves an average F1 of 0.6744, outperforming the\nnext-best model, Qwen2.5-72B (0.3535), by over 30 points. The advantage is consistent across all decision-\nmaking tasks, validating the effectiveness and scalability of our domain-specific, reasoning-driven approach.\nEHR-R1-72B also surpasses strong closed-source models (GPT-4o: 0.3155) and specialized medical LLMs\n(Med-Gemma-27B: 0.3157), highlighting superior clinical reasoning and EHR-specific knowledge.\nStrong models perform inconsistently. A notable observation is that closed-source commercial models\ndo not reliably outperform open-source ones. For example, GPT-4o (0.3155) performs the second best on\naverage, but is often surpassed by Medgemma-27B (0.3157) and Qwen2.5-72B (0.3535) in inpatient diagnostic\nsettings. Similarly, the general-purpose DeepSeek-R1 model (0.2974) underperformed both Medgemma-27B\nand Qwen2.5-72B. This variability underscores limitations in generalization for prior LLMs. In contrast,\nEHR-R1-72B delivers consistently strong F1 across diverse decision-making tasks, suggesting it can construct\ntask-tailored reasoning pathways absent in existing models.\nGeneral reasoning shows limited benefit. Despite the common consensus that reasoning capabilities\nimprove model performance, existing reasoning models that are not tailored to EHR tasks actually fail to\ndemonstrate a clear advantage over non-reasoning ones. For instance, the top-performing non-reasoning model,\nQwen2.5-72B (0.3535), outperforms the best-performing reasoning baseline model, Qwen3-235B (0.3418).\nSimilarly, the non-reasoning Medgemma-27B (0.3157) achieves a better score than the reasoning-enhanced\nDeepSeek-R1 (0.2977). This indicates that effective reasoning for EHR requires tight integration of medical\nknowledge with case-structured analysis rather than general chain-of-thought scaling.\nIn summary, EHR-R1-72B not only sets a new state-of-the-art for decision-making tasks in EHR analysis but\nalso addresses critical limitations of prior models by seamlessly integrating domain-specific reasoning with\nmedical expertise.\n2.3.2 Risk-prediction Tasks\nEHR-R1-72B consistently leads. Figure 5 illustrates the performance of our proposed model, against\nseveral open-source baselines on 18 risk-prediction tasks, measured by ROC curves. EHR-R1-72B attains an\naverage AUROC of 0.9523, significantly outperforming the second-best baseline model, Qwen3-235B (0.8245).\nThe performance gains are especially pronounced in challenging Emergency Department (ED) tasks, where\nrapid and accurate risk assessment is critical. For instance, on the ED reattendance 3day task, EHR-R1-72B\nreaches 0.9007 versus Qwen2.5-72B’s 0.5540. For core clinical predictions, the model also excels—for example,\ninpatient mortality at 0.9787 versus Qwen3-235B’s 0.9028.\nBaseline LLMs lack task versatility. The baselines’ performance varies significantly across task types,\nindicating limited generalization. Notably, Qwen2.5-72B—strong in decision-making—trails Qwen3-235B\n(0.8245) on risk prediction despite similar or smaller parameter counts, and both Qwen2.5-72B and Med-\nGemma-27B fluctuate across tasks. These inconsistencies suggest current general-purpose and medical LLMs\nstruggle to deliver uniformly reliable performance across the full spectrum of EHR analysis.\nPrediction accuracy decreases with longer horizons. All models exhibit decreasing AUROC as the\nprediction horizon extends, reflecting the complexity of forecasting long-term outcomes from longitudinal,\nmulti-factor trajectories. For ICU Mortality, EHR-R1-72B drops from 0.9898 (1-day) to 0.9610 (14-day). While\nour model consistently outperforms baselines even on these difficult, long-range predictions—for example,\nscoring 0.7562 on the Readmission 30Day task against Qwen2.5-72B’s 0.6735—it’s clear that predicting\nlong-term patient outcomes remains a significant challenge.\n|8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9898\nED Critical Outcomes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9997\nED Hospitalization\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9961\nED ICU Tranfer 12Hour\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9651\nED Inpatient Mortality\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9007\nED Reattendance 3Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9610\nICU Mortality 14Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9898\nICU Mortality 1Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9892\nICU Mortality 2Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9861\nICU Mortality 3Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9584\nICU Mortality 7Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.8020\nICU Readmission\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9562\nICU Stay 14Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9438\nICU Stay 7Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9787\nInpatient Mortality\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9308\nLengthofstay 3Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9201\nLengthofstay 7Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.7562\nReadmission 30Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.7583\nReadmission 60Day\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC: 0.9523\nTotal\nBaichuan-M2-32B\nGPT-OSS-120B\nOpenBioLLM-70B\nMedgemma-27B\nQwen2.5-72B\nLlama3.3-70B\nQwen3-235B\nEHR-R1-72B\nRandom Classifier\nFigure 5 | Performance comparison of EHR-R1 and seven baseline LLMs on 18 risk-prediction tasks on EHR-\nBench. Each subplot shows the ROC curves per task, with EHR-R1 highlighted in orange; the bottom-right corner of each plot\nreports EHR-R1-72B’s AUROC. The final ‘Total’ subplot summarizes aggregated performance across all 18 tasks.\n2.4 Generalization Evaluation on MIMIC-IV-CDM\nThe results of our generalization evaluation on the MIMIC-IV-CDM tasks are shown in Figure 6a. We focus\non the zero-shot setting, where models are directly prompted [40] to perform EHR analysis tasks in MIMIC-\nIV-CDM without any task-specific training. The prompts used for evaluation are shown in Supplementary\nTable 5. We evaluate LLMs’ performance on two levels of diagnosis tasks: the main disease diagnoses and ICD\ncoding level diagnoses. It can be observed that our model, EHR-R1-72B, achieves the highest performance on\nboth types of tasks.\nEHR-R1-72B excels on main disease prediction. For the task of predicting the main disease, which\nprovides a less noisy and more idealized scenario, our model sets a new state-of-the-art with a performance of\n0.8913. While other powerful models like DeepSeek-R1 (0.8841) and GPT-OSS-120B (0.8793) also perform\nwell, our model maintains a slight lead. This demonstrates that our model’s diagnostic capabilities are highly\neffective even in a simplified and refined clinical context.\nEHR-R1-72B leads in multi-level diagnoses. Most models struggle to perform well on both main disease\nand ICD-level coding simultaneously. For example, DeepSeek-R1 (0.8841 Main Disease) and GPT-OSS-120B\n(0.8793) lag on ICD Code prediction (0.2597 and 0.2422), whereas Med-Gemma-27B attains higher ICD\naccuracy (0.2860) but lower Main Disease (0.7939). In contrast, EHR-R1-72B, is the only one that achieves\n|9\na\nb\nc\nFigure 6 | Performance comparison of EHR-R1 and baseline LLMs in generalization evaluation. The score metric for\nMIMIC-IV-CDM and EHRSHOT is F1 and AUROC score, respectively. a Zero-shot on MIMIC-IV-CDM: performance on Main\nDisease and ICD-level diagnoses using the same samples.b Zero-shot on EHRSHOT for 70B-parameters LLMs: aggregated AUROC\nacross all subtasks within each of the three category groups, plus the overall average across all tasks. c Few-shot on EHRSHOT\nfor small-scale language models (Qwen3-1.7B vs. EHR-R1-1.7B): per-task performance acrossk ∈{1, 2, 4, 8, 16, 32, 64, 128} shots.\nthe highest performance on both tasks, with scores of 0.8913 for main disease and 0.3501 for ICD code. These\nresults underscore its ability to generalize zero-shot to a new system while retaining granular coding precision.\n|10\n2.5 Generalization Evaluation on EHRSHOT\nWe follow the EHRSHOT protocol to assess generalization in both zero-shot and few-shot scenarios: (1) zero-\nshot, where we directly evaluate on an unseen dataset with prompts to measure out-of-the-box generalization\nsimilar as n MIMIC-IV-CDM, and (2) few-shot, where we train on a small number of labeled examples to\ngauge how quickly LLMs adapt with minimal supervision. The prompts used for evaluation are shown in\nSupplementary Table 5.\n2.5.1 Zero-shot Setting\nIn the zero-shot setting (Figure 6b), we compare EHR-R1-72B to similarly sized ( 70B) baselines across\nthree categories—operational outcomes, lab test forecasting, and new-diagnosis assignment—and the overall\naverage. EHR-R1-72B attains the highest AUROC in every category and the overall average, outperforming\nstrong general models (e.g., Qwen2.5-72B) and specialized medical models (e.g., Med-Gemma-27B). Given\nthat EHRSHOT differs from EHR-Bench in both data format and task types, these results underscore robust\ncross-dataset generalization, which is a critical feature for practical clinical applications.\n2.5.2 Few-shot Setting\nConsidering that model adaptation in clinical settings is often constrained by limited computational resources.\nTherefore, in the few-shot setting, we use a small-parameter 1.7B LLMs with a minimal number of training\nsamples (at most 128) to evaluate the performance of the LLMs in rapid adaptation scenarios.\nEHR-R1 shows better learning efficiency. Figure 6c and the accompanying table present a detailed\nperformance comparison between our small model, EHR-R1-1.7B, and its base model, Qwen3-1.7B, on a\nvariety of EHRSHOT tasks under a few-shot fine-tuning setting. Following EHRSHOT, the models are\nfine-tuned on a limited number of examples (k) for each task, ranging from 1 to 128. The results show the\nprofound superiority of our model, which achieves an average AUROC of 0.7465 at k = 128, far exceeding\nQwen3-1.7B’s average of 0.5998 at the same shot setting.\nPerformance gap grows with more data. Our model’s performance advantages are primarily manifested\nin two types of tasks: operational outcomes and lab test forecasting. When k < 16, the gap is modest—likely\nbecause extremely limited data under-utilizes large models. For k ≥16, the gap widens markedly, suggesting\nthat EHR-focused training equips EHR-R1-1.7B to leverage small-to-moderate datasets more efficiently.\nGeneral models narrow the gap on diagnosis tasks. For new-diagnosis prediction, the gap peaks around\nk = 16 (e.g., new hypertension: 0.6593 vs. 0.5108 AUROC for EHR-R1-1.7B vs. Qwen3-1.7B) but narrows\nas k increases. This likely reflects that disease diagnosis is a core medical task extensively represented in\ngeneral-model pretraining; with sufficient fine-tuning data, general models can catch up.\nIn summary, our model not only achieves higher performance but also improves faster as data scales, indicating\nsuperior learning efficiency and a stronger EHR-specific knowledge base.\n2.6 Ablation Study\nWe conduct ablation experiments to validate the rationale and effectiveness of our proposed framework from\nthree key perspectives: the impact of incorporating reasoning data into model training, the effectiveness\nof reasoning during inference compared to direct answer output, and the generality of our approach across\nmodel scales.\nExperiments design. We conducted an ablation analysis on five configurations, progressively adding our\ntraining and inference components (Figure 7): (i) BaseModel: the original base model with direct answers (no\nreasoning at inference, no training enhancements). (ii) BaseModel (w/ reasoning inference): the base\nmodel evaluated with test-time reasoning to guide predictions. (iii) EHR-R1 (w/o reasoning training):\nthe model has undergone our continual pre-training to specialize in the EHR domain, but without reasoning\ndata; inference uses direct answers. (iv) EHR-R1 (full training): our three-stage model—continual pre-\ntraining, reasoning-data training, and task-specific fine-tuning—with direct-answer inference. (v) EHR-R1\n(full training, w/ reasoning inference)): the full model augmented with test-time reasoning. Note that,\n|11\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.2\n0.4\n0.6\nF1 Score\nTransfer\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.2\n0.4\n0.6\nF1 Score\nService\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.1\n0.2\n0.3\n0.4\nF1 Score\nProcedure\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.2\n0.4\n0.6\nF1 Score\nTest & Exam\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nF1 Score\nDiagnose\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nF1 Score\nTreatment\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.2\n0.4\n0.6\n0.8\nF1 Score\nICU Event\nQwen3-1.7B\nQwen3-8B\nQwen2.5-72B\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nF1 Score\nAverage\nBase Model\nBase Model (w/ reasoning inference)\nEHR-R1 (w/o reasoning training)\nEHR-R1 (full training)\nEHR-R1 (full training, w/ reasoning inference)\nFigure 7 | Ablation experiments results on decision making tasks of EHR-Bench. The figure show 7 categories of\nsub-type decision making tasks and the average performance of 5 variant of our methods, including Base Model, Base Model\n(with reasoning inference), EHR-R1 (w/o reasoning training), EHR-R1 (full training), and our final model EHR-R1 (full training,\nw/ reasoning inference), which showcase the incremental performance gains from each stage of our training pipeline.\nQwen2.5-72B is not applied with BaseModel (w/ reasoning inference) because it is not a reasoning-enabled\nmodel. The accurate experimental results can be found in the Supplementary Table 2.\nEffectiveness of reasoning data. Incorporating reasoning data into training consistently improves per-\nformance: for EHR-R1-1.7B, direct-answer F1 rises from 0.5060 (without reasoning data) to 0.5300 (with\nreasoning data), and for EHR-R1-72B from 0.6039 to 0.6281, indicating that synthesized reasoning injects\nuseful EHR-specific knowledge. The gains are amplified when reasoning is also used at inference, with the full\nEHR-R1-1.7B reaching 0.5438 F1 compared to 0.5060 without reasoning data, demonstrating that training-time\nreasoning and test-time reasoning are complementary—training equips the model with reasoning primitives,\nand inference-time reasoning leverages them to further refine predictions—yielding a more knowledgeable and\nrobust foundation across inference strategies.\nEffectiveness of reasoning inference. Reasoning at inference only helps when the model has been explicitly\nequipped with EHR-specific reasoning. Applying general reasoning prompts to base models offers little-to-\nnegative benefit (Qwen3-1.7B drops from 0.1624 to 0.1456 F1; Qwen3-8B also drops from 0.2425 to 0.2286),\nunderscoring that general reasoning alone is insufficient for clinical tasks. In contrast, our fully trained\nEHR-R1 models gain substantially from reasoning inference: EHR-R1-1.7B improves from 0.5060 to 0.5438\n(±0.0035), EHR-R1-8B from 0.5549 to 0.5894, and EHR-R1-72B from 0.6039 to 0.6418. These results validate\nthe effectiveness of our ‘thinking-graph’ synthesis pipeline in instilling domain-specific reasoning pathways\nthat inference-time reasoning can reliably exploit.\nEffectiveness across model scales. Our approach generalizes robustly with scale: applying the same\npipeline from 1.7B to 72B parameters yields monotonic gains, with average F1 rising from 0.5438 (Qwen3-\n1.7B–based) to 0.5894 (Qwen3-8B–based) and 0.6481 (Qwen2.5-72B–based). This consistent improvement\nindicates that the benefits of reasoning-data training and reasoning inference are not scale-specific but instead\ncompound as model capacity increases, confirming that our framework scales effectively across parameter\nregimes.\n|12\n3 Discussion\nDespite the remarkable progress of LLMs across diverse medical tasks, their application to clinical EHR\nanalysis remains a significant challenge [14, 15, 16, 17]. Existing approaches fall short in two critical dimensions.\nFirst, task coverage remains narrow, as most efforts are confined to specific objectives or disease cohorts\nrather than enabling holistic analytical support for evolving clinical workflows. Second, reasoning ability is\nunderdeveloped, with current LLMs struggling to generate reliable EHR-focused reasoning processes that\nprogressively highlight key records, integrate fragmented evidence, and construct longitudinal, synthesized\nunderstanding of patient health.\nOverview of Our Approach\nWe introduce a thinking-graph–driven, auto-generation pipeline for EHR reasoning that systematically converts\nraw, longitudinal records into structured, query-ready insights for generative LLM training. The pipeline:\n(1) extracts salient medical entities from heterogeneous EHR sources to create clinical focused insights; (2)\nlinks these entities into a thinking graph that encodes temporal relations and causal hypotheses, transforming\ndisjoint timesteps into a coherent longitudinal narrative; and (3) synthesizes explicit, stepwise reasoning over\nthis graph to support adaptive solutions across diverse clinical queries. This pipeline enables to curate EHR-\nIns, a super-instruction corpus with 300K reasoning cases and 3.5M non-reasoning cases spanning 42 EHR\ntasks. We then train our reasoning-enhanced model, EHR-R1, via three-stage training, including large-scale\ndomain adaptation, reasoning enhancement followed by reinforcement learning with Group Relative Policy\nOptimization (GRPO) to further strengthen analytical fidelity, temporal reasoning, and clinical robustness.\nMain Contribution\nA super-instruction dataset for EHR analysis with reliable reasoning. We release EHR-Ins, a\nlarge-scale super-instruction corpus that pairs 300K high-quality reasoning cases with 3.5M non-reasoning\ncases across 42 EHR tasks. Reasoning traces are auto-generated via our thinking-graph pipeline, which\nextracts entities, links temporal/causal relations, and synthesizes clear stepwise rationales—transforming\nstructured EHR data into LLM-ready, actionable insights. This design enables models to both answer diverse\nclinical queries and explain their decisions.\nFirst comprehensive benchmark for EHR analysis. This paper introduces MIMIC-Bench, a new\nbenchmark based on the MIMIC-IV dataset, which provides a comprehensive evaluation of LLMs on 42 diverse\nEHR tasks. This benchmark is designed to holistically assess a model’s ability to handle the full range of\ndiverse clinical queries that exist in real-world EHRs, addressing coverage gaps in existing evaluations.\nA robust and generalizable foundation model. We introduce EHR-R1, a state-of-the-art model that\ndemonstrates superior performance on EHR analysis. By learning from the explicit reasoning paths in our\ndata, our model not only sets new performance benchmarks on the proposed MIMIC-Bench but also shows\nexceptional zero-shot generalization capabilities on the out-of-distribution EHRSHOT dataset. This highlights\nEHR-R1’s robust ability to translate EHR content into clinical insight and understand temporal data to form\na longitudinal narrative, which is critical for real-world clinical applications.\nKey Findings\nEffective handling of diverse tasks. Trained on our EHR-Ins dataset, EHR-R1 performs a broad range\nof EHR tasks with high accuracy. Across 42 tasks in MIMIC-Bench—including clinical coding and other\nless-common or challenging tasks—it delivers strong, adaptive reasoning over diverse clinical queries. In\naddition, its zero-shot performance on unseen EHR data and tasks from the EHRSHOT dataset confirms that\nthe model captures transferable structure rather than memorizing training data.\nEffectiveness of reasoning for EHR tasks. The gains derive from an auto-synthesis pipeline that leverages\nrelationships among medical entities to teach explicit reasoning over structured EHR data. This enables\nthe model to denoise heterogeneous inputs, analyze temporal relations, and construct longitudinal clinical\nnarratives. Applying reasoning chains at inference yields substantial improvements, supporting the value of\nexplicit, knowledge-informed reasoning.\nGeneralizability of our framework. The effectiveness of our method is not limited to a specific model\nscale. Improvements from our training framework and reasoning data are consistent for models from 1.7B to\n|13\n72B parameters. This finding demonstrates the generalizability and scalability of our approach, confirming\nthat it provides a robust and widely applicable solution for the EHR domain.\nLimitations and Future Work\nScope of reasoning data. Our reasoning-data synthesis was applied only to decision-making tasks, for\nwhich target labels include the medical entities necessary for building the thinking graph. This constrains\ncoverage of diverse clinical queries. Although stage-three reinforcement learning transferred some of these\ncapabilities to risk-prediction tasks, future work should explore procedures to construct explicit reasoning\ndata for these binary classification.\nData attrition during graph construction. We excluded many samples for which the thinking graph\ncould not be built—either due to insufficiently related entity pairs or unsuccessful retrieval of medical relations\nfrom UMLS. This limits our ability to convert temporal data into a longitudinal narrative for every case.\nRefining graph-building heuristics, incorporating additional or domain-specific knowledge bases, and improving\nentity linking could reduce data loss and broaden case coverage.\nSpecialization versus breadth.\nThe approach yields a specialized EHR model and may attenuate\ngeneral-purpose capabilities. Emphasizing structured temporal narratives and prescribed reasoning steps\nrisks overfitting to clinical settings. Future work should explore strategies to balance specialization and\nbreadth—for example, multi-domain continued pretraining, alternating-task curricula, modular adapters, or\nmixture-of-experts routing—to preserve general capabilities while deepening clinical expertise.\n4 Methods\nIn this section, we present the details of our method, starting with the problem formulation, followed by the\nreasoning data curation pipeline, model training, and finally, evaluation.\n4.1 Problem Formulation\nWe formulate the EHR analysis problems in an instruction-tuning-based [41] generative framework.\nAn EHR can be represented as a chronologically ordered sequence of longitudinal records (also referred to as\nevents), including laboratory results, medication administrations, transfers, etc.:\nR = {r1, r2, . . . , rK}.\n(1)\nThe k-th record is defined as rk = (ck, Ek, tk), where ck ∈C denotes the clinical event category and C\nrepresents the set of all possible event types, such as medication, diagnosis, laboratory test, etc.. The term Ek\nrepresents the set of medical entities contained in the event (e.g., test items and results in a laboratory event,\nor diagnoses in a diagnostic event). Finally, tk is the timestamp of the event.\nAt any given prediction time step t, a training sample for EHR analysis is expressed as\nS = (I, R≤t, A),\nwhere I is a free-text instruction specifying the analysis task, R≤t = {rk | tk ≤t} is the observable history of\nthe EHR up to time t, and A is the ground-truth free-text answer.\nOur objective is to train an LLM, ΦLLM, to perform the following conditional text generation task:\nΦLLM(I, R≤t) →A,\n(2)\nthat is, to output an appropriate answer A given the task instructions I and the observed EHR history R≤t.\n4.2 Data Curation\nIn this section, we introduce the curation pipeline of EHR-Ins and EHR-Bench. We begin by processing\nthe raw MIMIC-IV EHR cases, filtering and reorganizing them into an instruction-tuning generative format.\n|14\nadmissions\nN = 431,231\nMIMIC-IV\nRaw Data\nlabevents\nN = 118,171,367\nMerge with subject_id of the records\n(Ranking each event with timestamp and merge the neighbor record with same event)\nGenerate sample with context event and target labels\n(Only keep the sample with context event num < 100 and > 10)\nNon-Reasoning Data \nN = 3,533,433\nEHR-Ins\nReasoning Data\nN = 289,088\nMIMIC-Bench\nN = 21,000\nGenerate Train and Test \nwith subject_id\nCalculating Label-wise sampling \nweight for each sample\nWhole Training Dataet\n269,496 Patients\nWhole Test Dataset\n30,216 Patients\nSplit sample \nwith subject_id\nSampling the whole dataset according to the Label-wise Sampling Weight\nThinking-Graph-Driven \nReasoning Synthesis\ndiagnoses_icd\nN = 4,756,326\nprescription\nN = 15,416,708\nFigure 8 | Overview of Data curation pipeline. The pipeline for our data curation process begins with the original MIMIC-IV\ndataset. From there, we establish EHR-Ins with reasoning enhanced EHR analysis instructions through thinking-graph–driven\nreasoning synthesis, along with a new comprehensive MIMIC-Bench.\nThe data is then carefully split into training and testing sets, where the test set forms EHR-Bench, and\nthe training set is further enhanced with reasoning, resulting in the final EHR-Ins. The overview of the\npreprocessing pipeline is shown in Figure 8.\n4.2.1 MIMIC-IV Processing\nHere, we describe the construction process of both EHR-Ins and EHR-Bench. First, we introduce the\nprocessing details of MIMIC-IV. Then, we describe the free-text formatting of the EHR data, which transforms\nthe long-horizon EHRs into detailed generative training samples. Finally, we describe the approach to achieve\na balanced distribution of the data with label-wise weighted sampling methods and how to split the whole\ndataset into a training and a test set.\nInformation Enrichment. To enable chronological construction from EHR data, we reformat and sort the\noriginal MIMIC-IV dataset at patient level. We first extract all events for each patient and reordered them by\ntimestamp with second precision. Recognizing that diagnoses icd, procedure icd, and diagnosis event types in\nMIMIC-IV lack second-level timestamps, we manually added them to enrich the information. In particular,\nfor diagnoses icd and diagnosis, we associate them with their respective admission events via hadm id. We\nthen set diagnoses icd to 1 minute before the discharge event and diagnosis to 1 minute before the emergency\ndepartment discharge event. For procedure icd events, which only provide day-level timestamps, we default\n|15\ntheir time to 23:59:59 on that day.\nTo further enrich the EHR information, we map the International Classification of Diseases (ICD) codes in\nthe diagnoses icd and procedure icd tables to Clinical Classifications Software (CCS) categories using the\n‘ICD-to-CCS’ script1. Likewise, we translated National Drug Codes (NDCs) in the prescriptions and pharmacy\ntables into Anatomical Therapeutic Chemical (ATC) classification codes adopting the off-the-shelf mapping\nscript2. These enrichment consolidate the originally vast and heterogeneous sets of disease, procedure, and\nmedication codes into clinically coherent categories. We also identify that information within discharge events,\nsuch as social history and chief complaint, as being observable upon admission. We move these details to the\nadmission event to provide more comprehensive input for LLMs. Additionally, to mitigate potential data\nleakage in EHR data, we mask pharmacy event information within prescription events.\nAfter the above processing and enrichment, all MIMIC-IV data are organized into a chronological sequence of\nrecords for each patient.\nTraining Sample Formatting Using the structured EHRs, we construct an instruction dataset on EHR\nanalysis by defining detailed instruction–answer pairs for each training sample, enabling generative training,\nregarding the two EHR analysis task categories: decision-making and risk-prediction.\nDecision-making tasks involve predicting the next-step event based on the history of events. Therefore,\nnaturally, for a given EHR sequence R = {r1, r2, . . . , rK}, we can generate training samples by sampling an\narbitrary timestep t and predicting the next event. Assuming the next-step record as rt+1 = (ct+1, Et+1, tt+1),\nwe formulate the instruction I for decision-making task according to the type of next-step event: ck+1 (detailed\nprompts can be found in Supplementary Table 4), and the target answer set is composed of the entities\ncontained in Ek+1:\nA = {y1, y2, . . . , yN}, where yi ∈Ek+1, i ∈{1, · · · , N},\n(3)\nFor example, in a case where the next-step event is diagnoses, Ek+1 contains all the diagnosis results of\nthe patients and yi represents the name of a disease. In the implementation, A is formatted as free-text by\nconcatenating the entity list with str.join function in Python.\nIn addition, at the time step t, we can also construct risk-prediction samples. We organize the instruction of\nthe risk-prediction taks by filling {ccritic, T} into a predefined template (details can be found in Supplementary\nTable 4), where the objective of the model is to determine whether a critical outcome event of type ccritic\nwill occur within a specified time frame T. Therefore, the binary ground truth answer of the task can be\nformulated as:\nA =\n\n\n\nyes,\nif there exists i ∈{t + 1, . . . , K} such that ci = ccritic and ti < T,\nno,\notherwise.\n(4)\nThus far, we have organized training samples in the form S = (I, R≤t, A), encompassing EHR analysis tasks\nthat fall into two main categories: decision-making and risk-prediction. Notably, since our instructions\nare generated through prompt filling, all training samples associated with a specific task (e.g., diagnostic\ndecision-making) share the same instruction. Hence, in the following, I can also be used to denote a specific\ntask.\nLastly, to convert the tabular observed EHR data R≤t into a format that LLMs can process, we serialize\nthe structured data[42, 43] into Markdown format[21], which is widely present in LLM pre-training corpora,\nfacilitating better model comprehension. Specifically, each event is represented as plain text with a title (event\nname and standardized start time) and content. The content uses bullet-point syntax for events with single\nrecords and table syntax for others with multiple records, as shown in Case 1 and Case 2, respectively. The\nfree-text case of EHR is shown in Case 5.\n1ICD9 to CCS: https://hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp\nICD10 to CCSR: https://hcup-us.ahrq.gov/toolssoftware/ccsr/dxccsr.jsp\n2NDC to ATC: https://github.com/sunlabuiuc/PyHealth\n|16\nLabel-wise Weighted Sampling. Because the constructed dataset over all valid EHR time steps contains a\nlarge number of samples and the target candidate sets in MIMIC-IV are highly imbalanced, we apply sampling\nto form the final training and test sets. We apply weighted sampling to rebalance the target label distribution\nin the training set. In practice, for decision-making tasks, given a certain task prompted by instruction I,\nwe first count the frequency of all possible label entities y appeared in the target answer A and use the\nreciprocal of the frequency as the sampling weight. Due to that, each training sample’s answer label is a list of\nentities, and we assign a weight based on the average of its constituent labels. Formally, for a sample S with\nground-truth answer as A = {y1, y2, . . . , yN}, where N is the size of the label set, its weight wS is given by:\nwS = 1\nN\nN\nX\ni=1\n1\nCountI(yi),\n(5)\nwhere CountI(yi) represents the frequency of label entity yi in a same type of task. This strategy can also be\napplied to risk-prediction tasks, where the entities in answering contain only two binary items (‘yes’ or ‘no’,\nthus N=1). The weighting formula, therefore, simplifies to the reciprocal of the frequency of the sample’s\nsingle label, rebalancing the positive and negative classes.\nThe label-wise weighted sampling strategy is adopted to generate the train and test sets simultaneously. This\napproach ensures that the EHR-Ins maintains a balanced distribution of target labels while addressing the\nissue of class imbalance. Besides, the test set can also cover diverse types of target labels, which makes the\nEHR-Bench evaluating the performance of the models more comprehensive.\nData Split. Prior studies commonly treat each hospital admission (which may come from the same patient)\nas an independent sample for data splitting. However, in our setting, to prevent potential information leakage\nbetween the training and test sets, we adopted a rigorous patient-level data split. This approach ensures that\nno patient, identified by their unique subject_id, appears in both the training and test sets.\nFurthermore, due to context length constraints and computational resources, we limit the observable historical\ntime window to 24 hours. To ensure an appropriate length for historical event trajectories, we filter out\nsamples with fewer than 10 or more than 100 historical events. This process prevents issues of insufficient\ninformation or excessively long inputs.\nFrom the entire training dataset, we sample cases using a label-wise weighting scheme to ensure balanced\nrepresentation across all tasks. This process yielded 3.5M samples designated as non-reasoning data and an\nadditional 300K samples specifically for synthesizing reasoning data. These two sets collectively form our\nproposed super-instruction dataset, EHR-Ins. Similarly, a test set of 21K samples was created from the whole\ntest set using the same label-wise sampling approach to form our benchmark, EHR-Bench.\n4.2.2 Thinking-Graph-Driven Reasoning Synthesis\nIn this section, to enable the model to provide convincing reasoning in clinical scenarios, we further enhance\nthe answers in EHR-Ins with detailed rationales by incorporating the concept distribution in the EHR and\nexternal medical knowledge sources.\nWe mainly consider the decision-making tasks. We first explain how to identify medical entities that are\npotentially related to the task labels. We then filter these entities and adopt bidirectional graph searching on\nexternal medical knowledge bases to gather the thinking graph, which links the context entities and target\nentities with medical relations. Finally, we describe how to synthesize the reasoning subset for decision-making\ntasks in EHR-Ins. The overview of the proposed reasoning synthesis pipeline is shown in Figure 9.\nEntity Relevance Identification. Compared with regular medical data, a distinctive feature of EHRs\nis their tabular format, where most records are clearly indexed by a list of well-defined medical entities.\nTherefore, by analyzing the relationships among these entities across the training set, we can uncover potential\nreasoning pathways that trace key observed entities from noisy raw EHRs to task labels.\nIn particular, we first process some unstructured exception free-text records, such as discharge summaries and\n|17\nCo-exist Metrix\nElectronic Health Records\nDead\nDischarge\nDischarge\nADMISSIONS\nLABEVENTS\nRADIOLOGY\nPRESCRIPTIONS\nDIAGNOSES\nOTHERS\nStatistics\nLift Score \nCalculation\nEW EMER.\nPotassium\nChloride\nZolpidem\nTartrate\nSodium Chloride\n0.9% Flush\nINR(PT)\nAlbumin\nLactate\nDehydrogenase\nMCH\nMedications\nHematocrit\nCreatinine\nCHEST (PRE-OP \nPA & LAT)\nOther and \nunspecified \nhyperlipidemia\nChronic kidney \ndisease, Stage III\nEsophageal\nAortic valve \ndisorders\nPotassium\nChloride\nZolpidem\nTartrate\nSodium Chloride \n0.9% Flush\nINR (PT)\nHematocrit\nCreatinine\nCHEST (PRE-OP\nPA & LAT)\nUnspecified \nhyperlipidemia\nEsophageal\nChronic kidney\ndisease, Stage III\nAortic valve\ndisorders\nEW EMER.\n3.5\n4.8\n5.5\n5.2\n2.1\n2.5\n7.0\n3.8\n0.8\n0.9\n0.9\n1.0\n1.5\n2.1\n2.8\n3.0\n1.2\n3.2\n3.5\n6.1\n1.5\n2.8\n4.2\n4.8\n2.0\n2.5\n8.5\n3.9\n1.1\n7.1\n2.5\n9.5\na. Concept Relevance Identification\nb. Graph-based Medical Relation Search\nLift Score\nTarget Entity\nContext Entity\nCHEST (PRE-OP\nPA & LAT)\nAortic valve\ndisorders\n9.5\nCreatinine\nChronic kidney\ndisease, Stage III\n8.5\nCHEST (PRE-OP\nPA & LAT)\nEsophageal\n7.1\nPotassium\nChloride\nChronic kidney\ndisease, Stage III\n7.0\nUMLS\nc. Reasoning Path Synthesis\nAortic valve \ndisorders\nCHEST (PRE-OP\nPA & LAT)\nSurgical \nProcedures\nCardiac\nPreoperative \nCare\nassociated_with\nrequires\npart_of\nHeart Valve \nDiseases\nCongenital \nHeart \nDefects\nCardiology\nEchocardi-\nography\nCardiologist\nRadiology\nIs a\nmay be\ndiagnosed by\nIs a\nIs a\nfield of\nfield of\nBidirectional Graph Search\nMedical Thinking Graph\nGPT-4o\nAugmented\nGeneration\nReasoning\n1.The presence of Atorvastatin on the medication list is a direct indicator of managing high \ncholesterol, which leads to the diagnosis of Other and unspecified hyperlipidemia.\n2.The elevated Creatinine level, a primary marker for kidney function, along with the \nprescription for Potassium Chloride (used for electrolyte balance), indicates impaired kidney \nfunction, which supports the diagnosis of Chronic kidney disease, Stage III.\n3.The combination of an elevated INR (PT), the use of Warfarin (an anticoagulant), and the \nCHEST (PRE-OP PA & LAT) finding of a metallic heart valve are conclusive evidence for the \ndiagnosis of Aortic valve disorders.\n4.The CHEST (PRE-OP PA & LAT) examination, specifically its findings and pre-operative \ncontext, suggests a co-morbidity or a related diagnostic focus on the Esophageal region.\nCHEST(PRE-OP PA & LAT)\npart of \nPreoperative Care \nrequires \nProcedures Cardiac \nassociated with Surgical \nAortic valve disorders\nThinking Graph \nExtraction\nFigure 9 | Overview of Thinking-Graph-Driven Reasoning Synthesis. The whole pipeline comprises three parts: a\nEntity Relevance Identification analyzes electronic health records to calculate a co-exist matrix and Lift Score for medical\nentities. b Graph-based Medical Relation Search filters the entity pair with a higher lift score and conducts a bidirectional\ngraph search on a large-scale medical knowledge graph, UMLS; The red lines indicate the search paths c Reasoning Path\nSynthesis arguments GPT-4o with retrieved medical relations to generate a coherent reasoning path. The entity with the color\nbox in the reasoning chain indicates the context entity or the target labels.\nexamination reports. We adopt QuickUMLS3 to extract the meaningful medical entities from the raw texts.\nThen, all the medical entities in the observable events can be expressed as E≤t = S\nti<t Ei, Ei = {ei\n1, ei\n2, . . . , ei\nk}\nis the entity set of the i-th events.\nThen we count the co-occurrence frequencies of the context entity ei ∈E≤t with the target label entities yj ∈A\nfor a specific decision-making task denoted by its instruction I and calculate the Lift [44] between them. The\nLift metric can discover strong associations within the data by measuring how much more frequently two\nentities appear together than expected by chance, allowing us to pinpoint the most relevant entities for the\nreasoning pathways. In our method, the Lift metric is calculated as:\nLift(ei, yj, I) =\nPI(ei, yj)\nPI(ei)PI(yj) = CountI(ei, yj)CountI\nCountI(ei)CountI(yj), ei ∈E≤t, yj ∈A\n(6)\nwhere CountI(ei, yj) represents the co-occurrence frequency of ei and yj in the training set for task I and\nCountI represents the total number of samples in task I. Since rare occurrences of ei and yj (for example,\n3https://github.com/Georgetown-IR-Lab/QuickUMLS\n|18\noccurring only once) can disproportionately inflate the Lift score, we only retain co-occurrence pairs where\nboth the frequency of ei and yj are greater than 5. Finally, we retain only the co-occurrence pairs where the\nLift value is greater than 5 for each sample:\nKLift = {(ei, yj) | CountI(ei) > 5, CountI(yj) > 5, Lift(ei, yj, I) > 5}, ei ∈E≤t, yj ∈A\n(7)\nThis filtering process helps to reduce noise while ensuring that the remaining entity-label pairs have a strong\npotential association, thus providing candidate references for evidence in the subsequent reasoning process.\nGraph-based Entity Relation Search. Although Lift can uncover potentially related medical entities\nfrom the historical EHR events, the medical relationships between these entities (including both the context\nentities in input and the label entities) and the labels remain uncovered and thus fail to capture the underlying\nclinical associations. To address this issue, we leverage the Unified Medical Language System (UMLS) [25]\nknowledge graph, which provides a structured representation of medical concepts and their relationships.\nUMLS integrates a wide range of medical terminologies and ontologies, including SNOMED CT, ICD, and\nMeSH, among others, making it an ideal resource to validate and strengthen the medical relationships between\nentities and labels in EHR data. Specifically, we map all medical entities identified in the EHR to their\ncorresponding UMLS concepts. To increase the accuracy and completeness of this mapping, we use fuzzy\nmatching provided in UMLS [25], allowing us to match medical terms in the EHR with UMLS concepts even\nwhen there are slight variations in spelling, abbreviations, or synonyms. This approach ensures a broader and\nmore robust set of entity-to-concept mappings, improving the coverage of medical entities in the EHR data.\nFor entities that cannot be mapped to UMLS concepts, we discard the related co-occurrence pairs, as these\nentities would not contribute valid medical relationships and could introduce noise into the analysis.\nAfter mapping entities to UMLS, we search for medical relations between entity pairs, progressively linking\ncontext input entities towards the final label entities, compensating the missed concept nodes, forming a\ninstance specific thinking-graph. However, many medical entities in UMLS do not have direct associations,\nwhich makes it challenging to acquire their clinical relationships. To overcome this, we employ a bidirectional\ngraph search algorithm to find connections between entity pairs by traversing across multiple nodes and links.\nThis process allows us to uncover implicit medical relationships between entities that are not directly linked\nin the UMLS knowledge graph.\nThe final discovered relationships between the entities are then extracted to form the thinking-graph to\nenhance the subsequent synthesis of reasoning paths for this training case:\nKThink-Graph = {Path(ei, yj) | Path(ei, yj) ∈UMLS, (ei, yj) ∈KLift}\n(8)\nwhere the function Path(ei, yj) is the connection between ei and yj extracted from UMLS.\nReasoning Path Synthesis With the relationships between label entities and historical event entities for\neach sample, we leverage a powerful large language model, GPT-4o, to generate a detailed reasoning chain\nbased on the historical events, entity relationships, and label entities. The prompt used for the reasoning\nsynthesis are shown in Prompt 2.\nSpecifically, the reasoning chain consists of three stages: extraction, reasoning, and final result. In the\nextraction stage, the model is required to extract relevant information from the historical events based on\nthe task’s objective. This extraction process must specify the events and their corresponding timestamps\nto clearly identify the source of the information, ensuring the accuracy and traceability of the extracted\ndata. The reasoning stage requires the model to integrate the extracted information and, using the entity\nrelationship data, link the extracted entities to the corresponding label entities. The model then constructs\na coherent reasoning path that logically connects the historical events to the predicted label, providing an\nexplanation for the clinical decision. In the final result stage, the model outputs the predicted result based on\nthe reasoning path constructed in the previous stage. This output represents the model’s interpretation of the\ntask based on the historical data and reasoning process.\nIt is important to note that each sample may contain multiple label entities. For those label entities for\nwhich the model cannot construct a valid reasoning chain, the synthesized data may introduce hallucinations.\n|19\nTo address this issue, we instruct GPT-4o to generate reasoning chains only for the label entities that are\nlogically inferable from the available data. For those label entities that cannot be reasoned through, we\ndiscard the associated reasoning chains and results. Finally, we retain only those samples where the number\nof inferable label entities constitutes at least 70% of the original label entities. This threshold ensures that\nthe data distribution remains close to the original raw data and minimizes the potential bias introduced by\nthe discarded entities. Figure 1 shows an example of the reasoning synthesis process.\n4.3 Model Training\nIn this section, we detail the training procedure. The proposed model follows a three-stage training process:\nlarge-scale domain adaptation to learn the data distribution and task knowledge in the EHR, reasoning\nenhancement to extract key information from observable historical events and infer prediction results, and\nreinforcement learning to enhance the model’s reasoning capabilities on EHR data.\nWe first serialize the training sample S = (I, R≤t, A) into the final input-output generative format, which\ncan be represented as (X, A). The input X is constructed by concatenating the markdown format of EHR\nhistorical events with the task-specific instruction I:\nX = Concate(Markdown(R≤t), I)\n(9)\nConcurrently, the target output A is formed by joining all items from the corresponding label set, with each\nitem separated by a newline character.\nLarge-scale Domain Adaptation. The initial step in our training pipeline is to adapt the base LLM to the\nclinical domain of EHR data by instruction-tuning on naïve question-answering pairs without reasoning. In\nour experiments, we found that supervising the model on both the input X and the target labels A during\nthis phase yields markedly better performance on downstream EHR tasks than training on A alone. We\nbelieve this improvement stems from the fact that X encodes rich distributional and temporal patterns of\npatient trajectories, which guide the model to learn co-occurrence and sequence dependencies more effectively.\nFormally, we minimize the combined loss:\nLSFT(ΦLLM) = −\n|A|\nX\ni=1\nlog ΦLLM(Ai|A<i, X) −\n|X|\nX\nj=1\nlog ΦLLM(Xj|X<j)\n(10)\nReasoning Enhancement. Following the domain adaptation phase, we conduct instruction-tuning on the\nreasoning samples aiming at teaching the model how to perform explicit reasoning on EHR analysis tasks.\nTo achieve this, we adopt the reasoning data generated by our automatic synthesis pipeline (as detailed in\nSection 4.2.2).\nWhile the input X remains the patient’s observable events and the task instruction, the output A is an\naugmented sequence that contains both a detailed reasoning chain encapsulated within the ‘<think>’ and\n‘</think>’ tags and the final result. The training objective for this stage remains the same as that of\nlarge-scale domain adaptation to ensure consistency and reinforce the model’s ability to learn from both the\ninput sequence and the structured reasoning output.\nReinforcement Learning. Despite the gains achieved through instruction-tuning, the model’s reasoning\ncapacity remains limited by the quality of the generated data. To further unlock its reasoning potential, we\napply Group Reward Policy Optimization (GRPO) on top of the SFT checkpoint, with the expectation that\nend-to-end reinforcement learning will guide the model to self-explore improved reasoning trajectories.\nGRPO frames each reasoning chain as a sequential decision process, rewarding trajectories that yield correct\npredictions and coherent, evidence-grounded explanations. By exploring multiple candidate chains and\nreinforcing the most effective ones, the model learns to prioritize high-quality inference paths. Specifically, we\ndesign GRPO’s reward function RGRPO(τ) for the inference reasoning trajectory τ as a sum of two components:\nformat reward and accuracy reward:\n|20\nTable 1 | Training hyperparameters for three base models. In Instruction Tuning, all models use a high learning rate and large\nbatch sizes for efficient pre-training. For Reasoning Tuning and GRPO, models train for more epochs (3 and 2 respectively) to\ncompensate for smaller, specialized datasets. During GRPO, larger models (Qwen3-8B and Qwen2.5-72B) adopt smaller learning\nrates (5e-7) to ensure training stability given their extensive parameters. Finally, Qwen2.5-72B’s shorter maximum sequence\nlength (8192) across Reasoning Tuning and GRPO is a necessary compromise due to computational resource limitations.\nTraining Stage\nModel\nLR\nBatch Size\nEpoch\nMax Seq Len\nGPU Num\nInstruction Tuning\nQwen3-1.7B\n2e-5\n128\n1\n8192\n64\nQwen3-8B\n2e-5\n256\n1\n8192\n128\nQwen2.5-72B\n2e-5\n512\n1\n8192\n256\nReasoning Tuning\nQwen3-1.7B\n2e-5\n256\n3\n12288\n128\nQwen3-8B\n2e-5\n256\n3\n12288\n128\nQwen2.5-72B\n2e-5\n256\n3\n8192\n128\nGRPO\nQwen3-1.7B\n2e-6\n128\n2\n12288\n128\nQwen3-8B\n5e-7\n32\n2\n12288\n128\nQwen2.5-72B\n5e-7\n32\n2\n8192\n128\nR(τ, ˆ\nA, A, I) = λfmt · Rfmt(τ) + λacc · Racc( ˆ\nA, A, I)\n(11)\nwhere ˆ\nA is the predicted entity set of LLMs, A is the target entity set, and I is the type of EHR analysis\ntask. The format reward Rfmt(τ) checks that whether the model wraps its chain of thought between the tags\n‘<think>’ and ‘</think>’ and respects the three prescribed stages: Extraction, Reasoning, and Final Result:\nRfmt(τ) =\n(\n1,\nif the format of τ is right\n0,\notherwise\n(12)\nThe accuracy reward Racc(τ) uses the task-specific metric. ACC for the risk-prediction task and F1 for the\ndecision-making task are used to score the final prediction against the ground truth:\nRacc( ˆ\nA, A, I) =\n(\nACC( ˆ\nA, A),\nif I is decision-making Task\nF1( ˆ\nA, A),\nif I is Risk Prediction Task\n(13)\nTo guard against runaway drift and hallucination, we restrict GRPO updates to just 500 examples per task,\nensuring that the model refines its reasoning without straying too far from the validated chains generated\nduring instruction tuning. This targeted reinforcement step produces a model that not only reasons more\nflexibly over EHR data but also maintains the reliability and interpretability of its clinical explanations.\nImplementation Details For our experiments, we adopt Qwen3-1.7B, Qwen3-8B, and Qwen2.5-72B [34]\nas our base models. The same three stage training strategy us applied to all three models. All the training\nare conduct based on the code of Transformers4 and TRL5 framework. For GRPO stage, The weighted\nhyperparameters of rewards λfmt and λacc are both set to 1 and the number of rollout is 8 per sample. More\ndetail training hyperparameters are shown in Table 1.\n4.4 Evaluation\nLastly, we introduce the evaluation settings, including used benchmarks, considered baselines, and metrics.\n4https://github.com/huggingface/transformers\n5https://github.com/huggingface/trl\n|21\n4.4.1 Benchmarks\nIn this section, we present the benchmarks considered in this paper and describe how we reformatted them to\nalign with our generative evaluation pipeline, accompanied by detailed case demonstrations.\nEHR-Bench. In EHR-Bench, we’ve included two categories of tasks. One of these, decision-making tasks,\nrequires the model to predict multiple possible target answers. For evaluation, a prediction is only considered\ncorrect if the model’s predicted medical entities precisely match those in the target answer. However, since\nmost baseline models aren’t fine-tuned on MIMIC-IV data, we generate a candidate set by randomly sampling\nfrom the task’s target label set. Baseline models then simply select medical entities from this candidate set.\nIt’s worth noting that our proposed model, EHR-R1, can directly generate answers without needing this\ncandidate set, demonstrating its practical usability. To ensure the difficulty for baseline models selecting from\noptions is comparable to EHR-R1 directly generating answers, we set the candidate set size for each sample\nto 100 (including the number of target labels). If a task has fewer than 100 target label types, we provide all\navailable target labels for the model to choose from. The example is shown in Case 5.\nMIMIC-IV-CDM Benchmark. MIMIC-IV-CDM meticulously selected four diseases—appendicitis, chole-\ncystitis, diverticulitis, and pancreatitis—to test model diagnostic accuracy. Unlike EHR-Bench and EHRSHOT,\nthe historical event information in MIMIC-IV-CDM is not arranged chronologically. Instead, it extracts the\nmost recent information for each event to serve as context. Consequently, the context in MIMIC-IV-CDM\nis less noisy, and the task is simpler. To maintain consistency with the other two benchmarks, we also\nreorganized the MIMIC-IV-CDM data into a Markdown-formatted free text. Given that the information\nwithin it lacks timestamps, the title for each event only includes the event name without the time it occurred.\nThe example case of the free-text input is shown in Case 4.\nEHRSHOT Benchmark. EHRSHOT, a public dataset collected by Stanford University, comprises electronic\nhealth records from 7,000 patients. Its publicly available portion includes 14 risk-prediction tasks categorized\ninto three subtypes: Operational Outcomes, Anticipating Lab Test Results, and Assignment of New Diagnoses.\nEHRSHOT is originally designed for traditional machine learning models, presents a challenge for Large\nLanguage Models (LLMs) due to its direct use of various medical codes, which are inherently difficult for LLMs\nto interpret. To address this, we’ve reformatted the EHRSHOT data into a Markdown-like free-text, similar\nto EHR-Bench, enabling direct testing with LLMs. Specifically, we utilized the descriptive mapping lexicon\nprovided within EHRSHOT to convert item codes into natural language text. However, certain codes, such as\nCPT4 and ICD10PCS, could not be directly transformed. Following previous work on evaluating LLMs with\nEHRSHOT [21], we manually added extra mapping files to resolve these inconsistencies6. Furthermore, for\nEHRSHOT’s measurement and observation events, we semantically clustered the 24 most frequently occurring\nitems. We then enriched these items with additional information, including their units, normal value ranges,\nand anomaly indicators, significantly enhancing the informational content of EHRSHOT. The example case of\nthe free-text input is shown in Case 3.\n4.4.2 Metrics\nBased on the two high-level EHR analysis task types, i.e., decision-making and risk-prediction, different\nevaluation metrics are adopted to quantitatively reflect the performance of LLMs, as formulated below.\nDecision-Making Tasks. For each decision-making sample, let the ground-truth target set be A and the\nmodel’s predicted set be ˆ\nA. We adopt the precision, recall, F1 score to evaluate prediction accuracy in this\nmulti-label setting:\nPrecision = |A ∩ˆ\nA|\n| ˆ\nA|\n,\nRecall = |A ∩ˆ\nA|\n|A|\n,\nF1 = 2 × Precision × Recall\nPrecision + Recall\n.\n(14)\n6CPT4 mapping: https://gist.github.com/lieldulev/439793dc3c5a6613b661c33d71fdd185\nICD10PCS mapping: https://hcup-us.ahrq.gov/toolssoftware/ccsr/PRCCSR_v2025-1.zip\n|22\nRisk-Prediction Tasks. For risk-prediction tasks, we follow prior work [19] and adopt the area under the\nreceiver operating characteristic curve (AUROC) as the evaluation metric. AUROC measures a model’s ability\nto distinguish between positive and negative outcomes across all decision thresholds, and can be written as\nAUROC =\nZ 1\n0\nTPR(t) dFPR(t),\n(15)\nwhere TPR and FPR denote the true positive and false positive rates, respectively.\n4.4.3 Baselines\nThe following LLMs are used as baselines for comparison:\nQwen2.5/3 Series. Developed by Alibaba Cloud, the Qwen2.5/3 series [34] offers versatile, general-purpose\nmultilingual models with a unique ‘thinking’ and ‘non-thinking’ hybrid reasoning engine and Mixture-of-\nExperts (MoE) architecture for efficient inference. Trained on 36 trillion tokens across over 100 languages,\nQwen3 excels in general reasoning, coding, and agent capabilities. While not specifically medical, its robust\nmultilingual support and advanced reasoning could serve as a powerful general foundation for processing\ndiverse medical literature or assisting in global health information dissemination.\nMedgemma 4B/27B. Created by Google, MedGemma [37] is a collection of Gemma 3 variants specifically\noptimized for medical text and image comprehension. Its key innovation lies in its multimodal capability,\nseamlessly integrating natural language processing with computer vision to analyze medical images (e.g.,\nX-rays, histopathology) alongside textual patient data. Trained on extensive de-identified medical datasets,\nMedGemma demonstrates superior performance on clinical benchmarks like MedQA and is designed to\naccelerate healthcare AI applications while supporting local deployment for data privacy.\nLlama3 Series. The Llama3 Series [32], developed by Meta, comprises a family of powerful open-source\ngeneral-purpose LLMs ranging from 8 billion to 405 billion parameters, with evolving multimodal capabilities\nin Llama 3.2. Trained on over 15 trillion tokens, these models excel in broad applications, including complex\nreasoning, coding, and multilingual understanding. While not inherently specialized for medicine, the Llama3\narchitecture serves as a robust foundation, with fine-tuned variants like Me-LLaMA demonstrating superior\nperformance in various medical question-answering and summarization tasks, highlighting its adaptability for\ndomain-specific healthcare solutions.\nOpenBioLLM. Developed by Saama AI Labs, OpenBioLLM [38] is an 8-billion-parameter open-source\nlanguage model specifically designed for the biomedical field, built upon the Llama 3 architecture. It is\nmeticulously fine-tuned on high-quality biomedical data, enabling it to accurately understand and generate\ntext for tasks like clinical entity recognition, medical question answering, and patient data de-identification.\nOpenBioLLM consistently outperforms larger general-purpose models on diverse biomedical benchmarks,\nmaking it a highly effective and transparent choice for healthcare and life sciences applications.\nBaichuan M2 32B. Baichuan-M2 [39], from Baichuan Intelligent, stands out as the first open-source LLM\nspecifically optimized for medical scenarios, trained \"from scratch\" on an unprecedented 20 trillion tokens\nof high-quality medical and general data. This dedicated training, coupled with a multi-stage curriculum\nlearning approach, enables it to achieve deep medical expertise, outperforming models five times its size in\nclinical practice benchmarks while maintaining strong general capabilities. Its specialized focus makes it\nhighly relevant for tasks such as clinical decision support, medical education, and rare disease diagnosis.\nGPT-4o. OpenAI’s GPT-4o (omni) [27] is a flagship multimodal model designed for natural human-computer\ninteraction, accepting and generating any combination of text, audio, image, and video through a single,\nend-to-end neural network . It offers real-time responsiveness and enhanced tokenization for non-English\nlanguages, making it faster and more cost-effective. While a general-purpose model, its advanced multimodal\nunderstanding and reasoning capabilities could be applied to complex medical cases involving diverse data\ntypes, from patient consultations (audio/text) to diagnostic images.\n|23\nDeepSeek R1. Developed by DeepSeek, R1 [35] is an open-source AI model released in January 2025,\nprimarily focused on pushing the limits of reinforcement learning as a post-training technique for complex\nreasoning. Utilizing a Mixture-of-Experts (MoE) architecture, it achieves high accuracy in mathematics,\nprogramming, and general reasoning tasks at a significantly lower operating cost. Notably, DeepSeek R1 has\nshown strong performance on medical benchmarks like MedQA [45], demonstrating its potential for efficient\nand accurate reasoning in healthcare contexts.\nGPT-OSS. The GPT-OSS (Open-Source Series) [36] is OpenAI’s first open-source GPT-class model since\nGPT-2, released under an Apache 2.0 license to provide powerful reasoning and agentic capabilities to the\nopen community. The models, gpt-oss-120b and gpt-oss-20b, utilize a Mixture-of-Experts (MoE) architecture\nfor efficient inference and support a long 128k token context window. While a general-purpose reasoning\nmodel, its performance on complex, health-related queries is noteworthy; the gpt-oss-120b model achieved\na score of 30% on the HealthBench benchmark, outperforming OpenAI’s own o4-mini and o3-mini models\non this specific task. This demonstrates its potential as a strong, customizable foundation for a variety of\napplications, including those with intricate medical reasoning requirements.\n5 Data Availability\nThe data source for EHR-Bench and EHR-Ins derives from MIMIC-IV, a public EHR resource. Both datasets\nare under review of PhysioNet7.\n6 Code Availability\nAll source codes of this paper have been released in https://github.com/MAGIC-AI4Med/EHR-R1.\n7https://physionet.org/\n|24\nReferences\n[1] Seyedmostafa Sheikhalishahi, Vevake Balaraman, and Venet Osmani. Benchmarking machine learning\nmodels on multi-centre eicu critical care dataset. Plos one, 15(7):e0235424, 2020.\n[2] Hugo Yèche, Rita Kuznetsova, Marc Zimmermann, Matthias Hüser, Xinrui Lyu, Martin Faltys, and\nGunnar Rätsch. Hirid-icu-benchmark - A comprehensive machine learning benchmark on high-resolution\nICU data. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information\nProcessing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual, 2021.\n[3] José Roberto Ayala Solares, Francesca Elisa Diletta Raimondi, Yajie Zhu, Fatemeh Rahimian, Dexter\nCanoy, Jenny Tran, Ana Catarina Pinho Gomes, Amir Hossein Payberah, Mariagrazia Zottoli, Milad\nNazarzadeh, Nathalie Conrad, Kazem Rahimi, and Gholamreza Salimi Khorshidi. Deep learning for\nelectronic health records: A comparative review of multiple deep neural architectures. J. Biomed.\nInformatics, 101:103337, 2020.\n[4] Feng Xie, Jun Zhou, Jin Wee Lee, Mingrui Tan, Siqi Li, Logasan S/O Rajnthern, Marcel Lucas Chee,\nBibhas Chakraborty, An-Kwok Ian Wong, Alon Dagan, Marcus Eng Hock Ong, Fei Gao, and Nan Liu.\nBenchmarking emergency department triage prediction models with machine learning and large public\nelectronic health records. In AMIA 2022, American Medical Informatics Association Annual Symposium,\nWashington, DC, USA, November 5-9, 2022. AMIA, 2022.\n[5] Shirly Wang, Matthew B. A. McDermott, Geeticka Chauhan, Marzyeh Ghassemi, Michael C. Hughes,\nand Tristan Naumann. Mimic-extract: a data extraction, preprocessing, and representation pipeline for\nMIMIC-III. In Marzyeh Ghassemi, editor, ACM CHIL ’20: ACM Conference on Health, Inference, and\nLearning, Toronto, Ontario, Canada, April 2-4, 2020 [delayed], pages 222–235. ACM, 2020.\n[6] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of\ngpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.\n[7] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical\nknowledge. Nature, 620(7972):172–180, 2023.\n[8] Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, and Yu Wang. Automatic\ninteractive evaluation for large language models with state aware patient simulator. arXiv preprint\narXiv:2403.08495, 2024.\n[9] Pengcheng Qiu, Chaoyi Wu, Shuyu Liu, Weike Zhao, Zhuoxia Chen, Hongfei Gu, Chuanjin Peng,\nYa Zhang, Yanfeng Wang, and Weidi Xie. Quantifying the reasoning abilities of llms on real-world clinical\ncases. arXiv preprint arXiv:2503.04691, 2025.\n[10] Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, and Weidi Xie.\nTowards evaluating and building versatile large language models for medicine. npj Digit. Medicine, 8(1),\n2025.\n[11] Yusheng Liao, Shuyang Jiang, Yanfeng Wang, and Yu Wang. Reflectool: Towards reflection-aware tool-\naugmented clinical agents. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher\nPilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 13507–13531.\nAssociation for Computational Linguistics, 2025.\n[12] Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M Banda, Nikesh Kotecha,\nTimothy Keyes, Yifan Mai, Mert Oez, et al. Medhelm: Holistic evaluation of large language models for\nmedical tasks. arXiv preprint arXiv:2505.23802, 2025.\n|25\n[13] Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu,\nHyunjin Cho, Chang-In Choi, et al. Medreason: Eliciting factual medical reasoning steps in llms via\nknowledge graphs. arXiv preprint arXiv:2504.00993, 2025.\n[14] Hossein Rouhizadeh, Anthony Yazdani, Boya Zhang, David Vicente Alvarez, Matthias Hüser, Alexandre\nVanobberghen, Rui Yang, Irene Li, Andreas Walter, and Douglas Teodoro. Large language models\nstruggle to encode medical concepts—a multilingual benchmarking and comparative analysis. medRxiv,\npages 2025–01, 2025.\n[15] Yixiang Qu, Yifan Dai, Shilin Yu, Pradham Tanikella, Travis Schrank, Trevor Hackman, Didong Li, and\nDi Wu. Enhancing llms with smart preprocessing for ehr analysis. arXiv preprint arXiv:2412.02868,\n2024.\n[16] Ali Soroush, Benjamin S Glicksberg, Eyal Zimlichman, Yiftach Barash, Robert Freeman, Alexan-\nder W Charney, Girish N Nadkarni, and Eyal Klang.\nLarge language models are poor medical\ncoders—benchmarking of medical code querying. NEJM AI, 1(5):AIdbp2300040, 2024.\n[17] Weijieying Ren, Jingxi Zhu, Zehao Liu, Tianxiang Zhao, and Vasant Honavar. A comprehensive survey\nof electronic health record modeling: From deep learning approaches to large language models. arXiv\npreprint arXiv:2507.12774, 2025.\n[18] Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao\nMa, and Lequan Yu. Healthflow: A self-evolving ai agent with meta planning for autonomous healthcare\nresearch. arXiv preprint arXiv:2508.02621, 2025.\n[19] Michael Wornow, Rahul Thapa, Ethan Steinberg, Jason Fries, and Nigam Shah. Ehrshot: An ehr\nbenchmark for few-shot evaluation of foundation models.\nIn A. Oh, T. Naumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems,\nvolume 36, pages 67125–67137. Curran Associates, Inc., 2023.\n[20] Jiacheng Lin, Zhenbang Wu, and Jimeng Sun. Training llms for ehr-based reasoning tasks via reinforcement\nlearning. arXiv preprint arXiv:2505.24105, 2025.\n[21] Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard\nHindricks, Roland Eils, and Benjamin Wild. Large language models are powerful electronic health record\nencoders. arXiv preprint arXiv:2502.17403, 2025.\n[22] Zhenbang Wu, Anant Dadu, Mike Nalls, Faraz Faghri, and Jimeng Sun. Instruction tuning large language\nmodels to understand electronic health records. Advances in Neural Information Processing Systems,\n37:54772–54786, 2024.\n[23] Ziyi Chen, Mengyuan Zhang, Mustafa Mohammed Ahmed, Yi Guo, Thomas J George, Jiang Bian, and\nYonghui Wu. Narrative feature or structured feature? a study of large language models to identify cancer\npatients at risk of heart failure. In AMIA Annual Symposium Proceedings, volume 2024, page 242, 2025.\n[24] Samuel Kim, In Gu Sean Lee, Mijeong Irene Ban, and Jane Chiang. Detection and prediction of\nclopidogrel treatment failures using longitudinal structured electronic health records. arXiv preprint\narXiv:2310.08757, 2023.\n[25] Olivier Bodenreider. The unified medical language system (umls): integrating biomedical terminology.\nNucleic acids research, 32(suppl_1):D267–D270, 2004.\n[26] Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob\nVielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, and Daniel Rueckert. Evaluation and\nmitigation of the limitations of large language models in clinical decision-making.\n[27] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,\nAkila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276,\n2024.\n|26\n[28] Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom J\nPollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, a freely accessible electronic health\nrecord dataset. Scientific data, 10(1):1, 2023.\n[29] Pawel Renc, Michal K. Grzeszczyk, Nassim Oufattole, Deirdre Goode, Yugang Jia, Szymon Bieganski,\nMatthew B. A. McDermott, Jaroslaw Was, Anthony E. Samir, Jonathan W. Cunningham, David W.\nBates, and Arkadiusz Sitek. Foundation model of electronic medical records for adaptive risk estimation.\nCoRR, abs/2502.06124, 2025.\n[30] Pawel Renc, Yugang Jia, Anthony E. Samir, Jaroslaw Was, Quanzheng Li, David W. Bates, and Arkadiusz\nSitek. Zero shot health trajectory prediction using transformer. npj Digit. Medicine, 7(1), 2024.\n[31] Mehak Gupta, Brennan Gallamoza, Nicolas Cutrona, Pranjal Dhakal, Raphael Poulain, and Rahmatollah\nBeheshti. An extensive data processing pipeline for MIMIC-IV. In Antonio Parziale, Monica Agrawal,\nShalmali Joshi, Irene Y. Chen, Shengpu Tang, Luis Oala, and Adarsh Subbaswamy, editors, Machine\nLearning for Health, ML4H 2022, 28 November 2022, New Orleans, Lousiana, USA & Virtual, volume\n193 of Proceedings of Machine Learning Research, pages 311–325. PMLR, 2022.\n[32] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston\nZhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang,\nBobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian\nKeller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis,\nDamien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv\nMahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina\nLobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle\nLee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey\nNguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M.\nKloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park,\nJay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani,\nKate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR,\nabs/2407.21783, 2024.\n[33] Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.\n[34] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\n[35] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li,\nH. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui\nLi, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L.\nCai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu,\nLean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang,\nMinghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng\nZhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang,\nR. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu\nWang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning\ncapability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025.\n[36] OpenAI. Introducing gpt-oss. https://openai.com/index/introducing-gpt-oss/. Accessed: 2025-08-08.\n|27\n[37] Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo\nKohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, et al. Medgemma technical report.\narXiv preprint arXiv:2507.05201, 2025.\n[38] Felix J Dorfner, Amin Dada, Felix Busch, Marcus R Makowski, Tianyu Han, Daniel Truhn, Jens Kleesiek,\nMadhumita Sushil, Jacqueline Lammert, Lisa C Adams, et al. Biomedical large languages models seem\nnot to be superior to generalist models on unseen medical data. arXiv preprint arXiv:2408.13833, 2024.\n[39] Bingning Wang, Haizhou Zhao, Huozhi Zhou, Liang Song, Mingyu Xu, Wei Cheng, Xiangrong Zeng,\nYupeng Zhang, Yuqi Huo, Zecheng Wang, et al. Baichuan-m1: Pushing the medical capability of large\nlanguage models. arXiv preprint arXiv:2502.12671, 2025.\n[40] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in neural information processing systems, 35:27730–27744, 2022.\n[42] Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. Tapex: Table\npre-training via learning a neural sql executor. In International Conference on Learning Representations,\n2022.\n[43] Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei\nZhang, and Surajit Chaudhuri. Table-gpt: Table fine-tuned gpt for diverse table tasks. Proceedings of\nthe ACM on Management of Data, 2(3):1–28, 2024.\n[44] Rakesh Agrawal, Tomasz Imieliński, and Arun Swami. Mining association rules between sets of items in\nlarge databases. In Proceedings of the 1993 ACM SIGMOD international conference on Management of\ndata, pages 207–216, 1993.\n[45] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease\ndoes this patient have? a large-scale open domain question answering dataset from medical exams.\nApplied Sciences, 11(14):6421, 2021.\n7 Acknowledgments\nThis work is supported by the National Key R&D Program of China (No. 2022ZD0160702), and the Scientific\nResearch Innovation Capability Support Project for Young Faculty (ZY-GXQNJSKYCXNLZCXM-I22).\n8 Author Contributions\nAll listed authors clearly meet the ICMJE 4 criteria. Y.L., C.W., and J.L. contribute equally to this work.\nY.W. and W.X. are the corresponding authors. Specifically, Y.L., C.W., J.L., S.J., P.Q., H.W., Y.Y., S.Z.,\nJ.W., Q.F., J.G., Y.Z., Y.W., Y.W., and W.X. all make contributions to the conception or design of the work,\nand Y.L., C.W., J.L. further perform acquisition, analysis, or interpretation of data for the work. In writing,\nY.L., C.W., J.L. draft the work. S.J., P.Q., H.W., Y.Y., S.Z., J.W., Q.F., J.G., Y.Z., Y.W., Y.W., and W.X.\nreview it critically for important intellectual content. All authors approve of the version to be published and\nagree to be accountable for all aspects of the work to ensure that questions related to the accuracy or integrity\nof any part of the work are appropriately investigated and resolved.\n|28\n9 Supplementary\n9.1 Details of Experiments\nSupplementary Table 1 | Accurate results of decision making tasks in EHR-Bench. The performance are measured\nwith the metric F1 score and shown in in the format ‘mean ± std’.\nTasks\nLlama3.3-70B\nGPT-4o\nQwen2.5-72B\nOpenBioLLM-70B\nBaichuan-M2-32B\nMedgemma-27B\nGPT-OSS-120B\nDeepSeek-R1\nQwen3-235B\nEHR-R1-72B\nAdmissions\n0.1438±0.0169\n0.3151±0.0208\n0.1452±0.0172\n0.0787±0.0122\n0.0633±0.0074\n0.2172±0.0164\n0.1029±0.0141\n0.1728±0.0166\n0.0974±0.0134\n0.7512±0.0186\nOMR\n0.0569±0.0059\n0.0582±0.0080\n0.1591±0.0092\n0.0598±0.0077\n0.1373±0.0088\n0.1739±0.0099\n0.0835±0.0092\n0.0115±0.0032\n0.1989±0.0107\n0.9242±0.0059\nEMAR\n0.2941±0.0156\n0.5543±0.0213\n0.5087±0.0227\n0.2558±0.0166\n0.2639±0.0114\n0.3858±0.0183\n0.4118±0.0198\n0.5290±0.0225\n0.5801±0.0179\n0.6503±0.0192\nProcedures ICD\n0.1737±0.0085\n0.1693±0.0138\n0.2049±0.0156\n0.0848±0.0118\n0.1272±0.0073\n0.2001±0.0103\n0.2074±0.0172\n0.2770±0.0173\n0.1976±0.0139\n0.4079±0.0189\nProcedures CCS\n0.1634±0.0109\n0.2718±0.0196\n0.2557±0.0158\n0.0689±0.0106\n0.1466±0.0086\n0.2047±0.0116\n0.2971±0.0186\n0.3064±0.0174\n0.2907±0.0160\n0.5340±0.0189\nDiagnoses ICD\n0.2432±0.0088\n0.1767±0.0116\n0.2805±0.0129\n0.0786±0.0074\n0.1880±0.0067\n0.2681±0.0088\n0.0972±0.0078\n0.2058±0.0119\n0.2470±0.0111\n0.4088±0.0123\nDiagnoses CCS\n0.3972±0.0107\n0.2847±0.0121\n0.4529±0.0122\n0.1880±0.0119\n0.3366±0.0080\n0.3760±0.0099\n0.2293±0.0137\n0.3451±0.0131\n0.3992±0.0123\n0.5654±0.0134\nLabevents\n0.4558±0.0152\n0.1667±0.0108\n0.4788±0.0123\n0.0702±0.0070\n0.4001±0.0126\n0.4901±0.0138\n0.1878±0.0096\n0.2625±0.0107\n0.4740±0.0135\n0.6681±0.0178\nMicrobiologyevents\n0.1915±0.0091\n0.2437±0.0183\n0.2826±0.0174\n0.0458±0.0093\n0.1494±0.0078\n0.3325±0.0159\n0.2250±0.0200\n0.2627±0.0158\n0.2888±0.0140\n0.7101±0.0215\nServices\n0.2055±0.0185\n0.2421±0.0188\n0.1888±0.0150\n0.0902±0.0129\n0.0860±0.0083\n0.2060±0.0174\n0.2599±0.0199\n0.2834±0.0202\n0.1938±0.0191\n0.8451±0.0169\nTransfers\n0.2997±0.0182\n0.3686±0.0217\n0.3697±0.0227\n0.2668±0.0203\n0.1831±0.0092\n0.3626±0.0221\n0.3035±0.0211\n0.3500±0.0219\n0.3968±0.0195\n0.7634±0.0153\nPOE\n0.2276±0.0101\n0.1716±0.0147\n0.2401±0.0121\n0.1793±0.0144\n0.1457±0.0083\n0.1880±0.0091\n0.1220±0.0139\n0.1699±0.0126\n0.2280±0.0123\n0.5482±0.0200\nRadiology\n0.2115±0.0136\n0.2943±0.0197\n0.2418±0.0192\n0.1364±0.0164\n0.1408±0.0090\n0.2583±0.0169\n0.2540±0.0198\n0.2982±0.0205\n0.2383±0.0163\n0.5542±0.0210\nPrescriptions\n0.2512±0.0110\n0.4533±0.0218\n0.3390±0.0141\n0.2772±0.0192\n0.2124±0.0086\n0.2705±0.0124\n0.2271±0.0158\n0.3157±0.0194\n0.3761±0.0162\n0.8560±0.0116\nPrescriptions ATC\n0.2375±0.0128\n0.2686±0.0183\n0.3996±0.0183\n0.1443±0.0162\n0.1848±0.0091\n0.2739±0.0137\n0.1617±0.0149\n0.2316±0.0174\n0.2974±0.0134\n0.7891±0.0138\nMedrecon\n0.2551±0.0114\n0.3210±0.0159\n0.3030±0.0133\n0.0171±0.0048\n0.2343±0.0102\n0.2436±0.0113\n0.1139±0.0102\n0.2961±0.0149\n0.2545±0.0133\n0.3319±0.0131\nMedrecon ATC\n0.2524±0.0099\n0.3383±0.0137\n0.4006±0.0128\n0.0271±0.0049\n0.2647±0.0093\n0.3345±0.0133\n0.1002±0.0100\n0.2842±0.0159\n0.4003±0.0128\n0.3981±0.0142\nED Diagnoses ICD\n0.1740±0.0076\n0.2583±0.0150\n0.2320±0.0131\n0.0691±0.0087\n0.1421±0.0064\n0.1560±0.0076\n0.2231±0.0144\n0.2514±0.0154\n0.2310±0.0110\n0.4319±0.0168\nED Diagnoses CCS\n0.2593±0.0098\n0.3877±0.0173\n0.3595±0.0127\n0.1365±0.0127\n0.2271±0.0079\n0.2321±0.0076\n0.4240±0.0151\n0.3982±0.0153\n0.3669±0.0118\n0.5674±0.0149\nIngredientevents\n0.6613±0.0124\n0.5826±0.0146\n0.7537±0.0101\n0.4662±0.0117\n0.5176±0.0077\n0.7131±0.0119\n0.4914±0.0165\n0.5704±0.0166\n0.7489±0.0110\n0.8739±0.0080\nDatetimeevents\n0.3659±0.0164\n0.3438±0.0196\n0.5096±0.0206\n0.2956±0.0192\n0.1931±0.0099\n0.4280±0.0181\n0.0572±0.0082\n0.1557±0.0131\n0.4089±0.0167\n0.9758±0.0043\nProcedureevents\n0.0745±0.0077\n0.0938±0.0128\n0.1030±0.0108\n0.1231±0.0146\n0.0655±0.0059\n0.1628±0.0150\n0.0787±0.0122\n0.0884±0.0115\n0.1192±0.0108\n0.8377±0.0179\nInputevents\n0.3504±0.0158\n0.3979±0.0204\n0.4516±0.0192\n0.1565±0.0163\n0.2457±0.0085\n0.4030±0.0181\n0.2621±0.0159\n0.2705±0.0193\n0.3911±0.0152\n0.8763±0.0129\nOutputevents\n0.6730±0.0180\n0.8107±0.0170\n0.8231±0.0154\n0.5745±0.0200\n0.4049±0.0087\n0.6949±0.0184\n0.7556±0.0190\n0.8012±0.0176\n0.7794±0.0168\n0.9176±0.0116\nAverage\n0.2758±0.0025\n0.3155±0.0037\n0.3535±0.0031\n0.1621±0.0027\n0.2108±0.0018\n0.3157±0.0027\n0.2365±0.0035\n0.2974±0.0033\n0.3418±0.0025\n0.6744±0.0029\nSupplementary Table 2 | Accurate results of ablation experiments. The table presents the performance of different\nmodel configurations across various decision-making tasks. We evaluate our method on base models of different sizes, including\nQwen3-1.7B, Qwen3-8B, Qwen2.5-72B, and Qwen3-235B. The performances are measured with the metric F1 score and shown in\nthe format ‘mean ± std’, with the average performance across all tasks provided in the final column.\nDecision Making Tasks\nMethods\nTransfer\nService\nProcedure\nTest&Exam\nDiagnose\nTreatment\nICU Event\nAverage\nQwen3-1.7B\nBase Model\n0.1332±0.0087\n0.1166±0.0059\n0.0820±0.0071\n0.1292±0.0053\n0.1344±0.0035\n0.1225±0.0032\n0.3119±0.0064\n0.1544±0.0020\nBase Model (w/ reasoning inference)\n0.1530±0.0107\n0.0996±0.0093\n0.1068±0.0087\n0.0993±0.0055\n0.1493±0.0055\n0.1182±0.0051\n0.2374±0.0066\n0.1393±0.0024\nEHR-R1 (w/o reasoning training)\n0.6510±0.0144\n0.4953±0.0127\n0.3037±0.0119\n0.5251±0.0081\n0.3319±0.0071\n0.4756±0.0078\n0.7773±0.0071\n0.5060±0.0034\nEHR-R1 (full training)\n0.6257±0.0153\n0.6106±0.0134\n0.2663±0.0121\n0.6135±0.0089\n0.3302±0.0066\n0.4916±0.0067\n0.8031±0.0072\n0.5300±0.0029\nEHR-R1 (full training, w/ reasoning inference)\n0.6863±0.0148\n0.6232±0.0150\n0.2924±0.0136\n0.6350±0.0089\n0.3610±0.0075\n0.5035±0.0075\n0.8280±0.0060\n0.5438±0.0035\nQwen3-8B\nBase Model\n0.1585±0.0119\n0.1445±0.0080\n0.1665±0.0098\n0.2082±0.0062\n0.2478±0.0048\n0.2411±0.0062\n0.4358±0.0077\n0.2425±0.0029\nBase Model (w/ reasoning inference)\n0.2370±0.0123\n0.1815±0.0091\n0.2083±0.0083\n0.1639±0.0062\n0.2890±0.0056\n0.1903±0.0062\n0.3527±0.0061\n0.2286±0.0026\nEHR-R1 (w/o reasoning training)\n0.6670±0.0144\n0.5372±0.0145\n0.3617±0.0132\n0.5793±0.0078\n0.3917±0.0070\n0.5155±0.0073\n0.8249±0.0069\n0.5549±0.0038\nEHR-R1 (full training)\n0.6681±0.0145\n0.5661±0.0117\n0.3192±0.0122\n0.5681±0.0088\n0.3877±0.0071\n0.5348±0.0070\n0.8462±0.0057\n0.5616±0.0031\nEHR-R1 (full training, w/ reasoning inference)\n0.7091±0.0138\n0.6558±0.0124\n0.3531±0.0135\n0.6764±0.0092\n0.4062±0.0072\n0.5523±0.0071\n0.8521±0.0063\n0.5894±0.0037\nQwen2.5-72B\nBase Model\n0.2564±0.0142\n0.2146±0.0092\n0.2283±0.0116\n0.2888±0.0074\n0.3310±0.0053\n0.3900±0.0074\n0.5289±0.0072\n0.3323±0.0031\nEHR-R1 (w/o reasoning training)\n0.7049±0.0151\n0.5572±0.0131\n0.4368±0.0133\n0.6412±0.0085\n0.4304±0.0077\n0.5619±0.0072\n0.8808±0.0061\n0.6039±0.0037\nEHR-R1 (full training)\n0.7176±0.0155\n0.6623±0.0140\n0.4322±0.0126\n0.6797±0.0085\n0.4506±0.0071\n0.5796±0.0070\n0.8840±0.0060\n0.6281±0.0034\nEHR-R1 (full training, w/ reasoning inference)\n0.7379±0.0143\n0.6905±0.0113\n0.4489±0.0143\n0.7013±0.0092\n0.4702±0.0070\n0.5918±0.0060\n0.8894±0.0054\n0.6418±0.0031\n|29\n9.2 Details of EHR-Bench\nSupplementary Table 3 | Details of task information in EHR-Bench. This table presents a comprehensive overview of the 42\ntasks, categorized into 2 main Task Types and further divided into 12 SubTypes. The Target Event column specifies the type\nof event associated with the target label for each task. For decision making tasks, the Item Name indicates the column name\nin MIMIC-IV where the target label is located. It’s important to note that Risk Prediction tasks do not have an Item Name\nas their target labels are not specific entities. Columns marked with ‘*’ represent additional columns created through manual\nmapping. The Candidates column shows the size of the target candidate set for each task.\nTask Type\nSubType\nTask\nTarget Event\nItem Name\nMetric\nCandidates\nDecision Making\nReassignment\nAdmissions\nadmissions\nadmission_type\nF1\n8\nTransfer\ntransfer\neventtype\nF1\n39\nTest & Exam\nOMR\nomr\nresult_name\nF1\n11\nLabevents\nlabevents\nitem_name\nF1\n698\nMicrobiologyevents\nmicrobiologyevents\ntest_name\nF1\n165\nRadiology\nradiology\nexam_name\nF1\n961\nDiagnoses\nDiagnose ICD\ndiagnose_icd\ndiagnoses\nF1\n24467\nDiagnose CCS\ndiagnose_icd\nCCT Type*\nF1\n279\nDiagnosis ICD\ndiagnosis_icd\nicd_title\nF1\n13171\nDiagnosis CCS\ndiagnosis_icd\nCCS Type*\nF1\n271\nProcedures\nProcedures ICD\nprocedures_icd\nprocedures\nF1\n11098\nProcedures CCS\nprocedures_icd\nCCS Type*\nF1\n230\nServices\nServices\nservices\ncurr_service\nF1\n18\nPOE\npoe\norder_type\nF1\n15\nTreatments\nEMAR\nemr\nmedication\nF1\n4153\nPrescriptions\nprescriptions\ndrug\nF1\n9233\nPrescriptions ATC\nprescriptions\nATC Type*\nF1\n913\nMedrecon\nmedrecon\nname\nF1\n18641\nMedrecon ATC\nmedrecon\nATC Type*\nF1\n899\nICU Event\nIngredientevents\ningredientevents\nitem_name\nF1\n15\nDatetimeevents\ndatetimeevents\nitem_name\nF1\n137\nProcedureevents\nprocedureevents\nitem_name\nF1\n138\nInputevents\ninputevents\nitem_name\nF1\n222\nOutputevents\noutputevents\nitem_name\nF1\n63\nRisk Prediction\nTransfer\nED Hospitalization\nedstays\n-\nAUROC\n2\nED ICU Tranfer 12hour\nedstays\n-\nAUROC\n2\nCritical Outcomes\nED Critical Outcomes\nedstays\n-\nAUROC\n2\nReadmission\nReadmission 30day\ndischarge\n-\nAUROC\n2\nReadmission 60day\ndischarge\n-\nAUROC\n2\nICU Readmission\nicustays\n-\nAUROC\n2\nED Reattendance 3day\nedstays\n-\nAUROC\n2\nLengthOfStay\nLengthOfStay 3day\nadmissions\n-\nAUROC\n2\nLengthOfStay 7day\nadmissions\n-\nAUROC\n2\nICU Stay 7day\nicustays\n-\nAUROC\n2\nICU Stay 14day\nicustays\n-\nAUROC\n2\nMortality\nED Inpatient Mortality\nedstays\n-\nAUROC\n2\nInpatient Mortality\nadmissions\n-\nAUROC\n2\nICU Mortality 1day\nicustays\n-\nAUROC\n2\nICU Mortality 2day\nicustays\n-\nAUROC\n2\nICU Mortality 3day\nicustays\n-\nAUROC\n2\nICU Mortality 7day\nicustays\n-\nAUROC\n2\nICU Mortality 14day\nicustays\n-\nAUROC\n2\n|30\n9.3 Prompt Collection\nPrompt\nPrompt 1. Reasoning Chain Evaluation Instruction for Clinicians\nTask Description: Evaluation of Clinical Reasoning Chain Validity\nI. Background\nWe have developed a large language model (LLM) designed to extract key information from electronic\nhealth records (EHRs) and perform clinical reasoning based on that information. For evaluation\npurposes, we have hidden the full EHR context, presenting only the critical information extracted\nby the model and the final diagnostic conclusion. Your task is to assess the relationship and logical\nconsistency between the extracted information and the final conclusion.\nII. Data Description\nThe model’s response consists of three main sections: ##Extraction, ##Reasoning, and ##Final\nResult. You need to evaluate whether the content in ##Extraction and ##Reasoning adequately\nsupports the ##Final Result and provide your final score in the Score column.\nIII. Evaluation Objective\nUsing the provided extracted information, you must determine if it is sufficient and accurate to support\nthe model’s final diagnostic conclusion. Specifically, you need to confirm:\n- Whether the key information extracted by the model is relevant to the diagnosis and accurately\nexpressed.\n- Whether the final diagnostic conclusion is fully supported by the extracted information, without any\nmissing essential evidence.\nNote:\nThe model’s reasoning result is considered correct and does not need to be validated\nfor correctness.\nIV. Evaluation Process\nPlease read the entire reasoning chain, including the extracted information and the diagnostic\nconclusion. Based on your professional judgment, evaluate the degree of correspondence between the\ninformation and the conclusion. The scoring criteria are as follows (five levels):\n- 5 points (Very Satisfied): The extracted information is mostly accurate and largely supports the\nconclusion.\n- 4 points (Satisfied): The extracted information supports most of the conclusion. A small part of\nthe conclusion lacks evidence but is reasonably justified.\n- 3 points (Average): The extracted information supports only a small part of the conclusion. The\nfinal result contains a large portion of content that is not adequately supported.\n- 2 points (Poor): The extracted information is related to the final result, but it fails to support the\nconclusion.\n- 1 point (Very Poor): The extracted information is highly irrelevant to the diagnostic conclusion.\nThe conclusion lacks a reasonable basis, and the reasoning process is unreliable.\nScoring Differences Explained:\n- The main difference between 5 points and 4 points is whether the information extracted by the model\nfully supports the final result.\n- The difference between 5 points and 4 points is whether the final conclusion is fully supported by the\nextracted information.\n- The difference between 4 points and 3 points is whether a large part or a small part of the final\nconclusion is supported.\n- The difference between 3 points and 2 points is whether at least one conclusion is supported.\nIf so, it’s 3 points; if not, but the information is related, it’s 2 points. - The difference between\n|31\n2 points and 1 point is whether the extracted and reasoned information is relevant to the final conclusion.\nV. Evaluation Examples\nExample 1 (5 points, Lab Test)\n1. Task Type:\nPredict the lab tests the patient should undergo.\n2. Model Reasoning Chain:\n##Extraction:\nSource: Outpatient Record [2178-09-23 11:20:00]\n- Key Information: Patient complains of fatigue and pale skin/mucous membranes.\n- Analysis: Typical signs of anemia, requiring clarification of anemia type.\nSource: Lab Results [2178-09-22 15:45:00]\n- Key Information: Hemoglobin 8.2 g/dL (normal 13-17), Creatinine 2.1 mg/dL (normal 0.7-1.3).\n- Analysis: Moderate to severe anemia combined with abnormal kidney function suggests possible renal\nanemia.\nSource: Past Medical History [2177-05-12 09:00:00]\n- Key Information: History of chronic kidney disease (CKD stage 3), not on dialysis.\n- Analysis: Chronic kidney disease often leads to renal anemia. Erythropoietin levels need to be\nassessed.\nSource: Past Surgical History [2176-08-05 10:00:00]\n- Key Information: Appendectomy, good recovery.\n- Analysis: Not directly related to the current anemia evaluation. This is irrelevant information.\n##Reasoning:\nCombining \"anemia symptoms + significantly low hemoglobin + abnormal kidney function + chronic\nkidney disease,\" the following lab tests are recommended to clarify the anemia mechanism:\nSerum iron\nFerritin\nTransferrin saturation\nErythropoietin levels\n##Final Result:\nRecommended Tests: Serum iron, Ferritin, Transferrin saturation, Erythropoietin levels.\n3. Score Explanation:\nScore: 5 points\nReasoning:\nThe first three pieces of extracted information (\"fatigue + pale skin,\" \"Hemoglobin 8.2 + Creatinine 2.1,\"\n\"chronic kidney disease history\") fully support the necessity of performing tests for iron metabolism\nand erythropoietin levels.\nThe \"past appendectomy\" is extraneous information unrelated to the conclusion, but it does not affect\nthe correctness of the conclusion.\nAll key points of the conclusion are fully justified by the first three essential pieces of information, thus\na score of 5 points.\nExample 2 (4 points, Medication Prescription)\n1. Task Type:\nPredict the patient’s medication plan.\n2. Model Reasoning Chain:\n##Extraction:\n|32\nSource: Inpatient Record [2180-02-15 08:10:00]\n- Key Information: Type 2 diabetes mellitus, fasting blood glucose 12.5 mmol/L, postprandial blood\nglucose 16.0 mmol/L.\n- Analysis: Blood glucose is significantly high; the current oral medication regimen is insufficient.\nSource: Lab Results [2180-02-14 07:50:00]\n- Key Information: HbA1c 8.5%, Creatinine 60 µmol/L (normal).\n- Analysis: High HbA1c indicates the need for improved blood glucose control. Normal kidney function\nmeans GLP-1 agonists are a viable option.\nSource: Past Medication History [2179-10-10 09:00:00]\n- Key Information: Taking oral Metformin 1000 mg twice daily and Gliclazide 80 mg once daily.\n- Analysis: The current dual-drug therapy has failed to achieve the target, requiring an upgrade.\n##Reasoning:\nPoor glycemic control (fasting 12.5, postprandial 16.0, HbA1c 8.5%) →Needs escalation of antidiabetic\ntreatment.\nNormal kidney function →GLP-1 agonists are a safe option.\nHas a history of hypertension and is taking Amlodipine →Needs to continue antihypertensive therapy.\n##Final Result:\nPrescription: Add a GLP-1 receptor agonist, continue Amlodipine treatment.\n3. Score Explanation:\nScore: 4 points\nReasoning:\nSupport for adding GLP-1: The information \"fasting 12.5 mmol/L, HbA1c 8.5%\" clearly suggests that\nthe current oral medications are insufficient, and an escalation to a GLP-1 is reasonable. Additionally,\n\"normal kidney function\" proves this drug can be used safely. This part of the conclusion aligns\nperfectly with the extracted information.\nSupport for continuing Amlodipine: The extracted information \"history of hypertension and is taking\nAmlodipine\" indicates the patient is already on antihypertensive treatment. This sub-conclusion is also\ndirectly supported by the extracted information.\nOverall: In this case, both parts of the conclusion—\"add GLP-1\" and \"continue Amlodipine\"—are\nsupported by the corresponding information. The model’s mention of \"continuing Amlodipine\" is a\nreasonable inference (most patients with T2DM have concurrent hypertension) but is not directly\nproven by the extracted information.\nTherefore, the majority of the conclusion (the GLP-1 part) is fully supported by the information, while\na small part of the conclusion (continuing Amlodipine) is a reasonable but unsupported inference. This\nfits the criteria for a \"4-point\" scenario.\nExample 3 (3 points, Diagnosis)\n1. Task Type:\nPredict the patient’s diagnosis.\n2. Model Reasoning Chain:\n##Extraction:\nSource: Outpatient Record [2179-05-10 10:30:00]\n- Key Information: Patient complains of sore throat, cough, fever, and fatigue for 3 days.\n- Analysis: Suggests symptoms of an upper respiratory tract infection (URTI).\nSource: Physical Examination [2179-05-10 10:45:00]\n- Key Information: Temperature 38.5°C, pharyngeal congestion, bilateral tonsillar enlargement (Grade\nII).\n- Analysis: Supports acute tonsillitis or acute pharyngitis.\n|33\nSource: Lab Results [2179-05-10 11:00:00]\n- Key Information: White blood cell count 12.5×109/L (normal 4-10), Neutrophils 80% (normal\n50-75%).\n- Analysis: Both white blood cells and neutrophils are elevated, suggesting a bacterial infection.\n##Reasoning:\nSore throat, fever, cough, temperature 38.5°C, pharyngeal congestion, tonsillar enlargement →Suggests\nURTI.\nElevated white blood cells and neutrophils →Suggests bacterial infection.\nCombining the above information, the diagnosis is \"acute upper respiratory tract infection with\nbacterial infection.\"\nGiven the symptoms and signs, acute bronchitis is also a possibility.\n##Final Result:\nDiagnosis: Acute upper respiratory tract infection with bacterial infection, acute bronchitis.\n3. Score Explanation:\nScore: 3 points\nReasoning:\nThe extracted information (sore throat, fever, elevated temperature, pharyngeal congestion, elevated\nwhite blood cells and neutrophils) fully supports the diagnosis of \"acute upper respiratory tract\ninfection with bacterial infection.\" This part of the conclusion is highly consistent with the information\nand is correct.\nThe diagnostic conclusion also includes \"acute bronchitis,\" but the extracted information does not\ncontain direct evidence to support this diagnosis. For example, there is no mention of worsening cough,\nproductive cough, or abnormal findings on lung auscultation (such as rales or rhonchi).\nWhile the majority of the conclusion (\"acute upper respiratory tract infection with bacterial infection\")\nis supported by the information, the diagnosis of \"acute bronchitis\" lacks a basis. According to the\nscoring criteria, \"The extracted information only supports a small part of the conclusion, and the final re-\nsult contains a large portion of content that is not adequately supported,\" therefore, the score is 3 points.\nExample 4 (2 points, Medication Prescription)\n1. Task Type:\nPredict the patient’s medication plan.\nII. Model Reasoning Chain:\n##Extraction:\nSource: Inpatient Record [2180-03-05 09:00:00]\n- Key Information: Patient admitted due to palpitations and chest tightness. ECG shows atrial\nfibrillation.\n- Analysis: The patient has a cardiac arrhythmia.\nSource: Past Medical History [2178-01-15 14:00:00]\n- Key Information: History of hypertension, long-term use of Irbesartan.\n- Analysis: Blood pressure control needs to be continued.\nSource: Lab Results [2180-03-05 10:30:00]\n- Key Information: Normal kidney function, normal liver function.\n- Analysis: Facilitates drug selection.\n##Reasoning:\nThe patient has atrial fibrillation and needs heart rate control.\nLong-term hypertension requires continued blood pressure management.\nConsidering all factors, Metoprolol is recommended.\n|34\n##Final Result:\nPrescription: Metoprolol.\n3. Score Explanation:\nScore: 2 points\nReasoning:\nThe extracted information—\"palpitations, chest tightness, atrial fibrillation\"—does indicate a cardiac\narrhythmia, and Metoprolol is a commonly used first-line drug for treating atrial fibrillation. From a\npharmacological perspective, this conclusion is related to the extracted information.\nHowever, the extracted information does not provide sufficient evidence to support Metoprolol as the\nsole or preferred drug. The treatment plan for atrial fibrillation requires a comprehensive consideration\nof the patient’s heart rate control needs (type and rate of A-fib), stroke risk (CHA2DS2-VASc score),\npresence of comorbidities (like heart failure), and blood pressure levels.\nBased solely on \"atrial\nfibrillation\" and \"hypertension,\" one cannot directly conclude that Metoprolol should be prescribed, as\nthere are various other drug options (e.g., Diltiazem, Verapamil).\nThe conclusion \"Metoprolol\" lacks sufficient evidence to be fully supported. It might be a plausible\ntreatment, but it is not the only or best choice that the extracted information can definitively\ndetermine. This aligns with the scoring criterion: \"The extracted information is related to the final\nresult, but it fails to support the conclusion.\"\nExample 5 (1 point, Lab Test)\n1. Task Type:\nPredict the lab tests the patient should undergo.\n2. Model Reasoning Chain:\n##Extraction:\nSource: Outpatient Record [2179-11-20 15:00:00]\n- Key Information: Patient complains of abdominal pain and diarrhea.\n- Analysis: Suggests a gastrointestinal issue.\nSource: Physical Examination [2179-11-20 15:20:00]\n- Key Information: Abdominal palpation shows no significant tenderness, bowel sounds are active.\n- Analysis: Suggests no organic lesion or inflammation in the abdomen.\nSource: Past Medication History [2179-11-19 18:00:00]\n- Key Information: Recently took Aspirin.\n- Analysis: No direct correlation with diarrhea.\n##Reasoning:\nPatient with abdominal pain and diarrhea →Gastrointestinal symptoms.\nNo tenderness on abdominal palpation →Rules out conditions like acute peritonitis.\nPatient took Aspirin →May affect the gastrointestinal tract.\nBased on the above information, an autoimmune disease is suspected.\n##Final Result:\nRecommended Tests: Antinuclear antibody (ANA) profile, Complement, Rheumatoid factor.\n3. Score Explanation:\nScore: 1 point\nReasoning:\nThe extracted information is \"abdominal pain, diarrhea,\" \"no tenderness on abdominal palpation,\"\nand \"took Aspirin.\" This information points to common gastrointestinal symptoms and potential\ndrug-related GI discomfort.\nHowever, the final conclusion, \"Recommended Tests: Antinuclear antibody profile, Complement,\n|35\nRheumatoid factor,\" is used to diagnose autoimmune diseases (e.g., Systemic Lupus Erythematosus,\nRheumatoid Arthritis).\nThere are no symptoms or signs in the extracted information (e.g., joint pain, rash, oral ulcers) that\nare related to autoimmune diseases. Linking \"abdominal pain and diarrhea\" to tests for autoimmune\ndiseases lacks any reasonable basis.\nThe conclusion is severely irrelevant to the extracted information, and the reasoning process is highly\nunreliable. Therefore, the score is 1 point.\nPrompt\nPrompt 2. Reasoning Chain Synthesis Prompt for GPT-4o\n========================================\n# Patient EHR Context #\n{context}\n========================================\n# Retrieved Medical Knowledge #\n{medical_knowledge}\n========================================\n# Ground Truth #\n{ground_truth}\n========================================\n# Task #\n{task}\n========================================\n# Data Description\n- # Patient EHR Context #: Contains all medical events and content from the patient’s hospitalization\njourney.\n- # Retrieved Medical Knowledge #: Contains Patient EHR Context elements and their relationships\npotentially relevant to the Ground Truth entity.\n- # Ground Truth #: Contains the correct answers for this task; your predictions must exactly match\nthese.\n- # Task #: Contains the specific task description; you need to complete the task based on the\ninformation in # Patient EHR Context #.\n# Instructions\nPlease provide a logically rigorous medical reasoning process so that the # Ground Truth # can be\nderived from the content in # Patient EHR Context # and # Task #.\n# Requirements\nThe reasoning process should include three stages: Extraction, Reasoning, and Final Results.\n## Extraction\n- In this stage, extract and identify each piece of \"key information\" from the # Patient EHR Context #\naccording to the provided # Retrieved Medical Knowledge #.\n- Don’t pay attention to information that you think is not helpful for the reasoning.\n- Each step in the Extraction stage should follow the format below, **You need to specify the event\nname and time for each extracted information**:\n**Event Name [Event Time]**: list the information extracted from the event and analyze the potential\nrelationship between the key information and the ground truth.\n|36\n## Reasoning\n- Analyze the relationship between the context information and the item in # Ground Truth # in a\nvery specific and professional manner, providing detailed reasoning steps.\n- Your analysis should include the item in # Ground Truth # as much as possible. Items that cannot\nbe inferred from the context can be omitted.\n- Do not use the word \"maybe\", \"possible\" or \"though\" in the generated reasoning. You should do your\nbest to find all the supporting information you can to ensure the correctness of your reasoning.\n- The reasoning process should be concise and rigorous, and each step should explain the specific\nmedical knowledge involved, making the reasoning process more credible.\n- All reasoning must be based on the context information to infer the items in the ground truth and no\nreverse inference can be performed.\n## Final Results - Provide the final result for the task.\n**Note that the final result should\nonly contain the items contained in # Gound Truth # that have been correctly inferred in.\n## Reasoning stage.**\n- Each item in the ## Final Results should be contained in the # Gound Truth # with extactly same\nstring.\n# Important Notes!!!\n- **For each piece of # Retrieved Medical Knowledge # that is relevant to completing the # Task #,\nlocate the exact position of its first item within the # Patient EHR Context # and explicitly annotate\nit during the Extraction phase to ensure a more thorough analysis.**\n- **During the ## Reasoning stage, remember to analyze very carefully how each item in # Ground\nTruth # is inferred**\n- **Most importantly, integrate references to # Ground Truth # and # Retrieved Medical Knowledge\n# in an implicit manner. At any point in the reasoning process, do not use phrases such as “according\nto the medical knowledge above”, \"as shown in ground truth\" or any wording that reveals you are\naware of the underlying medical knowledge or the ground truth.**\n# Output Format\n## Extraction\n[YOUR OUTPUT]\n## Reasoning\n[YOUR OUTPUT]\n## Final Results\n[YOUR OUTPUT]\n|37\nInstruction\nTasks\nGiven the sequence of events that have occurred in a hospital, please give the next Admissions suggestion for the patient.\nAdmissions\nGiven the sequence of events that have occurred in a hospital, please give the next Online Medical Record suggestion for the patient.\nOMR\nGiven the sequence of events that have occurred in a hospital, please give the next Electronic Medicine Administration Record suggestion for the patient.\nEMAR\nGiven the sequence of events that have occurred in a hospital, please give the next Procedures International Classification of Diseases Item suggestion for the\npatient.\nProcedures ICD\nGiven the sequence of events that have occurred in a hospital, please give the next Procedures Clinical Classifications Software Item suggestion for the patient.\nProcedures CCS\nGiven the sequence of events that have occurred in a hospital, please give the next Diagnoses International Classification of Diseases Item suggestion for the \npatient.\nDiagnoses ICD\nGiven the sequence of events that have occurred in a hospital, please give the next Diagnoses Clinical Classifications Software Item suggestion for the patient.\nDiagnoses CCS\nGiven the sequence of events that have occurred in a hospital, please give the next Labotary Test Events suggestion for the patient.\nLabevents\nGiven the sequence of events that have occurred in a hospital, please give the next Microbiology Test Events suggestion for the patient.\nMicrobiologyevents\nGiven the sequence of events that have occurred in a hospital, please give the next Services suggestion for the patient.\nServices\nGiven the sequence of events that have occurred in a hospital, please give the next Transfers suggestion for the patient.\nTransfers\nGiven the sequence of events that have occurred in a hospital, please give the next Provider Order Entry suggestion for the patient.\nPOE\nGiven the sequence of events that have occurred in a hospital, please give the next Radiology Examinations suggestion for the patient.\nRadiology\nGiven the sequence of events that have occurred in a hospital, please give the next Prescriptions suggestion for the patient.\nPrescriptions\nGiven the sequence of events that have occurred in a hospital, please give the next Anatomical Therapeutic Chemical Classification Prescriptions suggestion for \nthe patient.\nPrescriptions ATC\nGiven the sequence of events that have occurred in a hospital, please give the next ED Medrecon suggestion for the patient.\nMedrecon\nGiven the sequence of events that have occurred in a hospital, please give the next ED Medrecon on Anatomical Therapeutic Chemical (ATC) Classification \nsuggestion for the patient.\nMedrecon ATC\nGiven the sequence of events that have occurred in a hospital, please give the next ED Diagnoses on International Classification of Diseases suggestion for the \npatient.\nDiagnosis\nGiven the sequence of events that have occurred in a hospital, please give the next ED Diagnoses on Clinical Classifications Software Item suggestion for the \npatient.\nDiagnosis CCS\nGiven the sequence of events that have occurred in a hospital, please give the next Ingredient Events suggestion for the patient.\nIngredientevents\nGiven the sequence of events that have occurred in a hospital, please give the next Datetime Events suggestion for the patient.\nDatetimeevents\nGiven the sequence of events that have occurred in a hospital, please give the next Procedure Events suggestion for the patient.\nProcedureevents\nGiven the sequence of events that have occurred in a hospital, please give the next Input Events suggestion for the patient.\nInputevents\nGiven the sequence of events that have occurred in a hospital, please give the next Output Events suggestion for the patient.\nOutputevents\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will be hospitalized after the emergency room visit.\nED Hospitalization\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die during hospitalization.\nED Inpatient Mortality\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will be transferred to the ICU within 12 hours after the \nemergency room.\nED ICU Tranfer 12Hour\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will return to the emergency department within 72 hours after the \nemergency visit.\nED Reattendance 3DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die during hospitalization or will be transferred to the ICU \nwithin 12 hours after the emergency room.\nED Critical Outcomes\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will be readmitted to the hospital within 30 days\nReadmission 30Day\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will be readmitted to the hospital within 60 days\nReadmission 60Day\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die during hospitalization.\nInpatient Mortality\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient's hospital stay will exceed 3 days.\nLengthofstay 3DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient's hospital stay will exceed 7 days\nLengthofstay 7DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die within 1 day.\nICU Mortality 1DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die within 2 day.\nICU Mortality 2DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die within 3 day.\nICU Mortality 3DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die within 7 day.\nICU Mortality 7DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will die within 14 day.\nICU Mortality 14Day\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will stay in the ICU for more than 7 days.\nICU STAY 7DAY\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will stay in the ICU for more than 14 days.\nICU STAY 14Day\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will be admitted to the ICU again during this hospitalization\nICU Readmission\nSupplementary Table 4 | Instruction for each task in EHR-Bench.\n|38\nInstruction\nTasks\nDataset\nGiven the sequence of events that have occurred in a hospital, please give the main Diagnoses of  Diseases Item suggestion for the   \npatients.\nMain Disease Diagnoses\nMIMIC-IV-CDM\nGiven the sequence of events that have occurred in a hospital, please give the next Diagnoses in International Classification of\nDiseases Item suggestion for the patients.\nICD Code Diagnoses\nGiven the sequence of events that have occurred in a hospital, please predict whether a patient's total length of stay during a visit to \nthe hospital will be at least 7 days.\nLength of Stay\nEHRSHOT\nGiven the sequence of events that have occurred in a hospital, please predict whether a patient will be re-admitted to the hospital \nwithin 30 days after being discharged from a visit.\nReadmission\nGiven the sequence of events that have occurred in a hospital, please predict whether a patient will be transferred to the ICU during a \nvisit to the hospital.\nICU Transfer\nGiven the sequence of events that have occurred in a hospital, please predict whether an anemia lab comes back as normal (>=120 \ng/L).\nAnemia\nGiven the sequence of events that have occurred in a hospital, please predict whether a hyperkalemia lab comes back as normal(<=5.5 \nmmol/L). \nHyperkalemia\nGiven the sequence of events that have occurred in a hospital, please predict whether a hyponatremia lab comes back as normal\n(>=135 mmol/L).\nHyponatremia\nGiven the sequence of events that have occurred in a hospital, please predict whether a hypoglycemia lab comes back as normal\n(>=3.9 mmol/L).\nHypoglycemia\nGiven the sequence of events that have occurred in a hospital, please predict whether a thrombocytopenia lab comes back as normal \n(>=150 109/L).\nThrombocytopenia\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will have his first diagnosis of an \nacute myocardial infarction within the next year.\nAcute MI\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will have his first diagnosis of celiac \ndisease within the next year.\nCeliac\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will have his first diagnosis of \nhyperlipidemia within the next year.\nHyperlipidemia\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will have his first diagnosis of \nessential hypertension within the next year.\nHypertension\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will have his first diagnosis of lupus \nwithin the next year.\nLupus\nGiven the sequence of events that have occurred in a hospital, please predict whether the patient will have his first diagnosis of \npancreatic cancer within the next year.\nPancan\nSupplementary Table 5 | Instruction for each task in MIMIC-IV-CDM and EHRSHOT.\n9.4 Case Collection\nCase\nCase 1. Markdown Format for Event with Single Item\n## Event Name [Event Time]\n- Info Key 1: Info Value 1\n- Info Key 2: Info Value 2\n...\n- Info Key N: Info Value N\nCase\nCase 2. Markdown Format for Event with Multiple Item\n## Event Name [Event Time]\n| Info Key 1 | Info Key 2 | ... | Info Key N |\n| ———- | ———- | — | ———- |\n| Info Value | Info Value | ... | Info Value |\n| Info Value | Info Value | ... | Info Value |\n...\n| Info Value | Info Value | ... | Info Value |\nCase\nCase 3. Free-text Input Example of EHRSHOT\n## Person [1991-04-12 00:00:00]\n- Birth\n- White\n- Not Hispanic or Latino\n- MALE\n|39\n## Measurement [2015-02-24 02:25:00]\n| Item_Name | Valuenum | Valueuom | Ref_Range_Lower | Ref_Range_Upper | Flag |\n| —— | —— | —— | —— | —— | —— |\n| Lactate [Moles/volume] in Blood | 0.6000000238418579 | mmol/L | nan | nan | nan |\n| Lactate [Mass/volume] in Blood | 0.6000000238418579 | mmol/L | nan | nan | nan |\n| Glasgow coma scale | 15.0 | nan | nan | nan | nan |\n| Mean blood pressure | 103.0 | mmHg | nan | nan | nan |\n| Pain severity [Score] Visual analog score | 6.0 | nan | nan | nan | nan |\n| Respiratory rate | 18.0 | breaths/min | 12 | 18 | normal |\n| Systolic blood pressure | 152.0 | mmHg | 90 | 140 | abnormal |\n| Body temperature | 98.4000015258789 | F | 95 | 100.4 | normal |\n| Diastolic blood pressure | 79.0 | mmHg | 60 | 90 | normal |\n| Heart rate | 97.0 | bpm | 60 | 100 | normal |\n## Observation [2015-02-24 02:25:00]\n| Item_Name | Valuenum | Valueuom | Ref_Range_Lower | Ref_Range_Upper | Flag |\n| —— | —— | —— | —— | —— | —— |\n| Body temperature measurement site | 1.0 | nan | nan | nan | nan |\n| Oxygen saturation | 100.0 | % | 95 | 100 | normal |\n## Drug_Exposure [2015-02-24 04:22:00]\n- oxycodone hydrochloride 1 MG/ML Oral Solution\n- ibuprofen 400 MG Oral Tablet\n- gabapentin 300 MG Oral Capsule\n- levetiracetam 500 MG Oral Tablet\n- 1 ML hydromorphone hydrochloride 2 MG/ML Prefilled Syringe\n- acetaminophen 325 MG Oral Tablet\n- 0.4 ML enoxaparin sodium 100 MG/ML Prefilled Syringe\n- zolpidem tartrate 5 MG Oral Tablet\n- triamcinolone acetonide 1 MG/ML Topical Cream\n- oxycodone hydrochloride 5 MG Oral Tablet\n## Procedure_Occurrence [2015-02-24 23:59:00]\n- Routine venipuncture\n- Taking patient vital signs assessment\n- Ambulating patient\n## Condition_Occurrence [2015-02-25 11:07:00]\n- Tobacco dependence syndrome\n- Benign neoplasm of colon\n- Inflammatory dermatosis\n- Epilepsy\nCase\nCase 4. Free-text Input Example of MIMIC-IV-CDM\n## Patient Demographics\n- Patient History: ___ s/p emergency tissue AVR and type A aortic dissection repair ___ and\nsternal washout ___, c/b tamponade requiring clot evacuation ___, trach ___, PEG ___. Overall,\nother complications included seizures, renal failure requiring CRRT, shock liver, HITT, sepsis, afib,\nleft colonic ischemia. Discharged at end of ___ and returned to ___ in ___, at which time he\nunderwent a bronch. Has been at rehab. Was on tube feeds until several weeks ago and now only on\n|40\nregular diet. He has been eating, but decreased appetite and intake due to post-prandial epigastric\nabdominal pain. This has been occuring for approximately three weeks, and he initially attributed it\nto indigestion. He had routine follow up in thoracic clinic with Dr ___, at which time his PEG tube\nwas removed. Then he was then sent to the ED for his abdominal pain. PEG site was clean prior to\nremoval.\n- Past Medical History: Hypertension Appendectomy Right facial cyst drainage\n- Social History: ___ Family History: None\n- Physical Examination: Physical exam VS: 98.6, 80, 135/72, 18, 96% RA Gen: NAD CV: RRR Pulm:\nCTA b/l Abd: soft, nondistended. slightly tender in epigastric area and right upper quadrant without\nrebound/guarding/rigidity Ext: left hand with ischemic digits, b/l ___ wrapped with gauze PE\n## Labotary Test Events\n| Item_name | Valuenum | Valueuom | Ref_range_lower | Ref_range_upper |\n| —— | —— | —— | —— | —— |\n| Alanine Aminotransferase (ALT) | 18.0 | IU/L | 0.0 | 40.0 |\n| PTT | 32.8 | sec | 25.0 | 36.5 |\n| PT | 14.9 | sec | 9.4 | 12.5 |\n| INR(PT) | 1.4 | None | 0.9 | 1.1 |\n| White Blood Cells | 25.8 | K/uL | 4.0 | 11.0 |\n| Red Blood Cells | 3.84 | m/uL | 4.6 | 6.2 |\n| RDW | 14.1 | % | 10.5 | 15.5 |\n| Platelet Count | 177.0 | K/uL | 150.0 | 440.0 |\n| Neutrophils | 91.5 | % | 50.0 | 70.0 |\n| Monocytes | 3.6 | % | 2.0 | 11.0 |\n| MCV | 92.0 | fL | 82.0 | 98.0 |\n| MCHC | 33.0 | % | 31.0 | 35.0 |\n| Lymphocytes | 4.8 | % | 18.0 | 42.0 |\n| Hemoglobin | 11.6 | g/dL | 14.0 | 18.0 |\n| Hematocrit | 35.2 | % | 40.0 | 52.0 |\n| Eosinophils | 0.1 | % | 0.0 | 4.0 |\n| MCH | 30.2 | pg | 27.0 | 32.0 |\n| Light Green Top Hold | HOLD. | None | None | None |\n| Albumin | 3.3 | g/dL | 3.5 | 5.2 |\n| Alkaline Phosphatase | 117.0 | IU/L | 40.0 | 130.0 |\n| Anion Gap | 15.0 | mEq/L | 8.0 | 20.0 |\n| Basophils | 0.1 | % | 0.0 | 2.0 |\n| Bicarbonate | 28.0 | mEq/L | 22.0 | 32.0 |\n| Bilirubin, Total | 1.2 | mg/dL | 0.0 | 1.5 |\n| Chloride | 94.0 | mEq/L | 96.0 | 108.0 |\n| Asparate Aminotransferase (AST) | 20.0 | IU/L | 0.0 | 40.0 |\n| Estimated GFR (MDRD equation) | Using this patient’s age, gender, and serum creatinine value\nof 1.2,. Estimated GFR = 61 if non African-American (mL/min/1.73 m2). Estimated GFR = 74\nif African-American (mL/min/1.73 m2).\nFor comparison, mean GFR for age group 60-69 is 85\n(mL/min/1.73 m2). GFR<60 = Chronic Kidney Disease, GFR<15 = Kidney Failure. | None | None |\nNone |\n| Glucose | 141.0 | mg/dL | 70.0 | 100.0 |\n| Lipase | 15.0 | IU/L | 0.0 | 60.0 |\n| Creatinine | 1.2 | mg/dL | 0.5 | 1.2 |\n| Potassium | 4.6 | mEq/L | 3.3 | 5.1 |\n| Sodium | 132.0 | mEq/L | 133.0 | 145.0 |\n| Urea Nitrogen | 27.0 | mg/dL | 6.0 | 20.0 |\n|41\n| Specimen Type | VEN. | None | None | None |\n| Lactate | 1.3 | mmol/L | 0.5 | 2.0 |\n| Protein | 30.0 | mg/dL | None | None |\n| Yeast | NONE | None | None | None |\n| WBC | <1. | None | 0.0 | 5.0 |\n| Urobilinogen | NEG. | None | 0.2 | 1.0 |\n| Urine Color | Yellow. | None | None | None |\n| Specific Gravity | >1.050*. | None | 1.001 | 1.035 |\n| RBC | 3.0 | #/hpf | 0.0 | 2.0 |\n| pH | 6.0 | units | 5.0 | 8.0 |\n| Urine Appearance | Clear. | None | None | None |\n| Leukocytes | NEG. | None | None | None |\n| Ketone | NEG. | None | None | None |\n| Glucose | NEG. | None | None | None |\n| Epithelial Cells | 0.0 | #/hpf | None | None |\n| Blood | TR. | None | None | None |\n| Bilirubin | NEG. | None | None | None |\n| Bacteria | NONE. | None | None | None |\n| Nitrite | NEG. | None | None | None |\n| Calcium, Total | 9.2 | mg/dL | 8.4 | 10.3 |\n| Magnesium | 1.8 | mg/dL | 1.6 | 2.6 |\n| Phosphate | 3.4 | mg/dL | 2.7 | 4.5 |\n## Microbiology Test Events\n| Item_name | Valuestr |\n| —— | —— |\n| Blood Culture, Routine | NO GROWTH. |\n| ANAEROBIC CULTURE | NO ANAEROBES ISOLATED. |\n| FLUID CULTURE | PSEUDOMONAS AERUGINOSA, ENTEROCOCCUS SP. |\n## Radiology Examinations\n| Exam_name | Text |\n| —— | —— |\n| LIVER OR GALLBLADDER US (SINGLE ORGAN) | EXAMINATION: LIVER OR GALLBLAD-\nDER US (SINGLE ORGAN):\nTECHNIQUE: Grey scale and color Doppler ultrasound images of the abdomen were obtained.\nFINDINGS:\nLIVER: The hepatic parenchyma appears within normal limits. The contour of the liver is smooth.\nAgain seen is a hyperechoic lesion measuring approximately 6 mm in segment V of the liver, unchanged\nsince prior study and likely represents a hemangioma. The additional hemangioma seen on previous\nstudy is not clearly identified on today’s exam. Main portal vein is patent with hepatopetal flow. There\nis no ascites.\nBILE DUCTS: There is no intrahepatic biliary dilation. The CBD measures 6 mm.\nGALLBLADDER: The gallbladder is distended and filled with sludge. There is mild gallbladder wall\nedema measuring up to 6 mm. There is no pericholecystic fluid.\nPANCREAS: The pancreas is not well seen secondary to overlying bowel gas.\nKIDNEYS: The right kidney measures 8.1 cm. Survey views of the right kidney do not demonstrate\nany masses, hydronephrosis, or stones.\nRETROPERITONEUM: Visualized portions of aorta and IVC are within normal limits. |\n|42\nCase\nCase 5. Free-text Input Example of EHR-Bench\n# Patient Demographics [None]\n- Anchor_Age: 88\n- Gender: F\n## Admissions [2127-04-18 16:53:00]\n- Admission_Type: EW EMER.\n- Admission_Location: PROCEDURE SITE\n- Admission_Info: None\n## Provider Order Entry [2127-04-18 15:21:26]\n| Order_Type | Order_Subtype |\n| —— | —— |\n| Medications | nan |\n| ADT orders | Admit |\n| General Care | Vitals/Monitoring |\n| Nutrition | Diet Order |\n| General Care | Other |\n| Medications | nan |\n## Services [2127-04-18 16:54:06]\n- Curr_Service: CMED\n## Transfers [2127-04-18 16:54:06]\n- Eventtype: admit\n- Careunit: Medicine/Cardiology\n## Pharmacy [2127-04-18 16:57:15]\n| Medication | Proc_Type | Status |\n| —— | —— | —— |\n| Potassium Chloride | Unit Dose | Discontinued via patient discharge |\n| Potassium Chloride | Unit Dose | Discontinued via patient discharge |\n| Oxybutynin | Unit Dose | Discontinued via patient discharge |\n| Zolpidem Tartrate | Unit Dose | Discontinued via patient discharge |\n| Nitroglycerin SL | Unit Dose | Discontinued |\n| Sodium Chloride 0.9| Hydrochlorothiazide | Unit Dose | Discontinued via patient discharge |\n| Potassium Chloride | Unit Dose | Discontinued via patient discharge |\n| Lisinopril | Unit Dose | Discontinued via patient discharge |\n| Multivitamins | Unit Dose | Discontinued via patient discharge |\n| Acetaminophen | Unit Dose | Discontinued via patient discharge |\n| Ferrous Sulfate | Unit Dose | Discontinued via patient discharge |\n| Aluminum-Magnesium Hydrox.-Simethicone | Unit Dose | Discontinued via patient discharge |\n| Pneumococcal Vac Polyvalent | Unit Dose | Discontinued |\n| Simvastatin | Unit Dose | Discontinued via patient discharge |\n| Aspirin | Unit Dose | Discontinued via patient discharge |\n| Diltiazem Extended-Release | Unit Dose | Discontinued via patient discharge |\n## Prescriptions [2127-04-18 17:00:00]\n| Drug | Atc Type | Prod_Strength | Dose_Val_Rx | Dose_Unit_Rx |\n| —— | —— | —— | —— | —— |\n| Potassium Chloride | potassium chloride | 20mEq Packet | 20 | mEq |\n|43\n| Potassium Chloride | potassium chloride | 20mEq Packet | 40 | mEq |\n| Zolpidem Tartrate | zolpidem | 5mg Tablet | 5 | mg |\n| Nitroglycerin SL | glyceryl trinitrate | 0.3mg SL Tablet Bottle | 0.3 | mg |\n| Sodium Chloride 0.9| Potassium Chloride | potassium chloride | 20mEq Packet | 60 | mEq |\n| Acetaminophen | paracetamol | 325mg Tablet | 650 | mg |\n| Aluminum-Magnesium Hydrox.-Simethicone | aluminium hydroxide | 30 mL UDCup | 30 | mL |\n| Pneumococcal Vac Polyvalent | nan | 25mcg/0.5mL Vial | 0.5 | mL |\n## Provider Order Entry [2127-04-18 17:13:19]\n| Order_Type | Order_Subtype |\n| —— | —— |\n| Medications | nan |\n| Medications | nan |\n| Medications | nan |\n## Pharmacy [2127-04-18 17:26:01]\n| Medication | Proc_Type | Status |\n| —— | —— | —— |\n| Fentanyl Citrate | Unit Dose | Expired |\n| Ibuprofen | Unit Dose | Expired |\n## Provider Order Entry [2127-04-18 17:55:22]\n| Order_Type | Order_Subtype |\n| —— | —— |\n| Radiology | Ultrasound |\n| General Care | Other |\n| General Care | Other |\n| General Care | Other |\n| General Care | Other |\n| Blood Bank | Blood tests |\n| Lab | nan |\n| Lab | nan |\n| Radiology | General Xray |\n| Cardiology | ECG |\n| General Care | Other |\n| General Care | Other |\n## Prescriptions [2127-04-18 18:00:00]\n| Drug | Atc Type | Prod_Strength | Dose_Val_Rx | Dose_Unit_Rx |\n| —— | —— | —— | —— | —— |\n| Fentanyl Citrate | fentanyl | 100mcg/2mL Amp | 25 | mcg |\n| Ibuprofen | ibuprofen | 600mg Tablet | 600 | mg |\n## Provider Order Entry [2127-04-18 18:12:48]\n| Order_Type | Order_Subtype |\n| —— | —— |\n| General Care | Vitals/Monitoring |\n| General Care | Other |\n| General Care | Other |\n| General Care | Other |\n| General Care | Other |\n| General Care | Activity |\n|44\n| General Care | Other |\n| General Care | Other |\n| IV therapy | IV fluids |\n| Medications | nan |\n| General Care | Other |\n## Pharmacy [2127-04-18 18:44:04]\n| Medication | Proc_Type | Status |\n| —— | —— | —— |\n| nan | IV Large Volume | Expired |\n| Atropine Sulfate | Unit Dose | Discontinued via patient discharge |\n## Prescriptions [2127-04-18 19:00:00]\n| Drug | Atc Type | Prod_Strength | Dose_Val_Rx | Dose_Unit_Rx |\n| —— | —— | —— | —— | —— |\n| 5| Sodium Bicarbonate | sodium bicarbonate | 50mEq Vial | 150 | mEq |\n| Atropine Sulfate | atropine | 1mg/10mL Syinge | 0.5 | mg |\n## Provider Order Entry [2127-04-18 19:11:50]\n| Order_Type | Order_Subtype |\n| —— | —— |\n| Lab | nan |\n| Medications | nan |\n## Pharmacy [2127-04-18 19:14:10]\n- Medication: Acetylcysteine 20- Proc_Type: Unit Dose\n- Status: Discontinued via patient discharge\n## Labotary Test Events [2127-04-18 19:19:00]\n| Item_Name | Valuenum | Valueuom | Ref_Range_Lower | Ref_Range_Upper | Flag | Comments |\n| —— | —— | —— | —— | —— | —— | —— |\n| | INR(PT) | 1.2 | nan | 0.9 | 1.1 | abnormal | nan |\n| PT | 13.7 | sec | 10.4 | 13.4 | abnormal | nan |\n| PTT | 25.6 | sec | 22.0 | 35.0 | nan | nan |\n| Alanine Aminotransferase (ALT) | 9.0 | IU/L | 0.0 | 40.0 | nan | nan |\n| Albumin | 3.8 | g/dL | 3.4 | 4.8 | nan | nan |\n| Alkaline Phosphatase | 61.0 | IU/L | 39.0 | 117.0 | nan | nan |\n| Anion Gap | 13.0 | mEq/L | 8.0 | 20.0 | nan | nan |\n| Asparate Aminotransferase (AST) | 13.0 | IU/L | 0.0 | 40.0 | nan | nan |\n| Bicarbonate | 27.0 | mEq/L | 22.0 | 32.0 | nan | nan |\n| Bilirubin, Total | 0.2 | mg/dL | 0.0 | 1.5 | nan | nan |\n| Chloride | 99.0 | mEq/L | 96.0 | 108.0 | nan | nan |\n| Creatinine | 1.3 | mg/dL | 0.4 | 1.1 | abnormal | nan |\n| Estimated GFR (MDRD equation) | nan | nan | nan | nan | nan | Using this patient’s age, gender,\nand serum creatinine value of 1.3,. Estimated GFR = 39 if non African-American (mL/min/1.73 m2).\nEstimated GFR = 47 if African-American (mL/min/1.73 m2). For comparison, mean GFR for age\ngroup 70+ is 75 (mL/min/1.73 m2). GFR<60 = Chronic Kidney Disease, GFR<15 = Kidney Failure.\n|\n| Glucose | 176.0 | mg/dL | 70.0 | 105.0 | abnormal | nan |\n| Lactate Dehydrogenase (LD) | 134.0 | IU/L | 94.0 | 250.0 | nan | nan |\n| Potassium | 3.8 | mEq/L | 3.3 | 5.1 | nan | nan |\n| Sodium | 135.0 | mEq/L | 133.0 | 145.0 | nan | nan |\n|45\n| Urea Nitrogen | 28.0 | mg/dL | 6.0 | 20.0 | abnormal | nan |\n| Hematocrit | 28.5 | | Hemoglobin | 9.6 | g/dL | 12.0 | 16.0 | abnormal | nan |\n| MCH | 29.2 | pg | 27.0 | 32.0 | nan | nan |\n| MCHC | 33.7 | | MCV | 87.0 | fL | 82.0 | 98.0 | nan | nan |\n| Platelet Count | 260.0 | K/uL | 150.0 | 440.0 | nan | nan |\n| RDW | 14.7 | | Red Blood Cells | 3.29 | m/uL | 4.2 | 5.4 | abnormal | nan |\n| White Blood Cells | 4.6 | K/uL | 4.0 | 11.0 | nan | nan |\n## Prescriptions [2127-04-18 20:00:00]\n- Drug: Acetylcysteine 20- Atc Type: acetylcysteine\n- Prod_Strength: 800mg/4ml Vial\n- Dose_Val_Rx: 600\n- Dose_Unit_Rx: mg\n## Labotary Test Events [2127-04-18 21:20:00]\n| Item_Name | Valuenum | Valueuom | Ref_Range_Lower | Ref_Range_Upper | Flag | Comments |\n| —— | —— | —— | —— | —— | —— | —— |\n| Bacteria | nan | nan | nan | nan | nan | NONE. |\n| Bilirubin | nan | mg/dL | nan | nan | nan | NEG. |\n| Blood | nan | nan | nan | nan | nan | MOD. |\n| Epithelial Cells | 1.0 | #/hpf | nan | nan | nan | nan |\n| Glucose | nan | mg/dL | nan | nan | nan | NEG. |\n| Ketone | nan | mg/dL | nan | nan | nan | NEG. |\n| Leukocytes | nan | nan | nan | nan | nan | NEG. |\n| Nitrite | nan | nan | nan | nan | nan | NEG. |\n| pH | 7.5 | units | 5.0 | 8.0 | nan | nan |\n| Protein | nan | mg/dL | nan | nan | nan | NEG. |\n| RBC | nan | #/hpf | 0.0 | 2.0 | nan | <1. |\n| Specific Gravity | 1.015 | | 1.001 | 1.035 | nan | nan |\n| Transitional Epithelial Cells | nan | #/hpf | nan | nan | nan | <1. |\n| Urine Appearance | nan | nan | nan | nan | nan | Hazy. |\n| Urine Color | nan | nan | nan | nan | nan | Straw. |\n| Urobilinogen | nan | mg/dL | 0.2 | 1.0 | nan | NEG. |\n| WBC | 3.0 | #/hpf | 0.0 | 5.0 | nan | nan |\n| Yeast | nan | nan | nan | nan | nan | nan |\n## Provider Order Entry [2127-04-19 02:55:46]\n- Order_Type: Medications\n- Order_Subtype: nan\n## Pharmacy [2127-04-19 02:58:06]\n- Medication: Zolpidem Tartrate\n- Proc_Type: Unit Dose\n- Status: Discontinued via patient discharge\n## Prescriptions [2127-04-19 03:00:00]\n- Drug: Zolpidem Tartrate\n- Atc Type: zolpidem\n- Prod_Strength: 5mg Tablet\n- Dose_Val_Rx: 5\n- Dose_Unit_Rx: mg\n|46\n## Provider Order Entry [2127-04-19 05:41:43]\n- Order_Type: Lab\n- Order_Subtype: nan\n## Labotary Test Events [2127-04-19 07:16:00]\n| Item_Name | Valuenum | Valueuom | Ref_Range_Lower | Ref_Range_Upper | Flag | Comments |\n| —— | —— | —— | —— | —— | —— | —— |\n| Hematocrit | 29.0 | | Creatinine | 1.4 | mg/dL | 0.4 | 1.1 | abnormal | nan |\n| Urea Nitrogen | 28.0 | mg/dL | 6.0 | 20.0 | abnormal | nan |\n## Radiology Examinations [2127-04-19 09:49:00]\n- Note_Type: RR\n- Exam_Name: [’CHEST (PRE-OP PA & LAT)’]\n- Text: REASON FOR EXAMINATION: Preoperative evaluation of the patient with aortic stenosis\nbefore aortic valve replacement. PA and lateral chest radiograph was compared to ___. Heart size\nis normal. Mediastinal position, contour and width are stable except for dextroscoliosis, mild to\nmoderate. The lungs are clear except for linear bibasilar opacities, unchanged since ___, consistent\nwith scarring. There is no pleural effusion or pneumothorax. The lateral view demonstrates contrast\nmaterial in expected location of the distal esophagus that might be related to recent study involving\nthe administration of oral contrast, please correlate with clinical history. There is no pleural effusion or\npneumothorax demonstrated.\n## Provider Order Entry [2127-04-19 12:11:28]\n| Order_Type | Order_Subtype |\n| —— | —— |\n| ADT orders | Discharge |\n| ADT orders | Discharge |\n## Transfers [2127-04-19 13:10:00]\n- Eventtype: discharge\n- Careunit: nan\n|47\nType: Decision Making\nSubject ID: 14122030 \nCase Information\nTask: Diagnoses ICD\nHadm ID: 25609889Task \n## Radiology Examinations [2129-04-04 23:00:00]\n- Note_Type: RR\n- Exam_Name: ['CT HEAD W/O CONTRAST']\n- Text: EXAMINATION:  CT HEAD W/O CONTRAST Q111 CT HEAD\nINDICATION:  History: ___ with fall, fractured dentures, facial pain, L-arm, L-hip/ankle, R-leg pain.  \nEvaluate for traumatic injury.\nTECHNIQUE:  Contiguous axial images of the brain were obtained without\ncontrast. Coronal and sagittal reformations as well as bone algorithm\nreconstructions were provided and reviewed.\n<item info>…\nIMPRESSION:\n1. No hemorrhage.\n2. Aerosolized secretions in bilateral partially imaged sphenoid sinuses.\n3. Please refer to the dedicated CT neck and CT sinus/mandible/maxillofacial\nexam performed concurrently for findings specific to these regions.\n<events>…\n## Medrecon [2129-04-04 23:20:00]\n| Name | Atc Type |\n| ------ | ------ |\n| glipizide | glipizide |\n| Lipitor | atorvastatin |\n| metformin | nan |\n| *hctz | None |\n| omeprazole | omeprazole |\n<events>…\n## ED Diagnoses on International Classification of Diseases [2129-04-05 15:51:00]\n| Icd_Title | Ccs Type |\n| ------ | ------ |\n| Oth fracture of upper and lower end of left fibula, init | Fracture of lower limb |\n| Fall (on) (from) unspecified stairs and steps, init encntr | E Codes: Fall |\n## Patient Demographics [None]\n- Anchor_Age: 70\n- Gender: F\n## Admissions [2129-04-05 04:06:00]\n- Admission_Type: EU OBSERVATION\n- Admission_Location: EMERGENCY ROOM\n- Admission_Info: None\n## Transfers [2129-04-04 20:41:00]\n- Eventtype: ED\n- Careunit: Emergency Department\n## EDstays [2129-04-04 20:41:00]\n- Gender: F\n- Race: BLACK/CARIBBEAN ISLAND\n## Triage [2129-04-04 20:41:01]\n- Temperature: 97.9\n- Heartrate: 103.0\n- Resprate: 16.0\n- O2Sat: 100.0\n- Sbp: 163.0\n- Dbp: 91.0\n- Pain: 10\n- Acuity: 3.0\n-\nChiefcomplaint: s/p Fall, Facial injury\n<events>…\n## Labotary Test Events [2129-04-04 23:00:00]\n| Item_Name | Valuenum | Valueuom | Ref_Range_Lower | Ref_Range_Upper | Flag | Comments |\n| ------ | ------ | ------ | ------ | ------ | ------ | ------ |\n…\n| Glucose | 168.0 | mg/dL | 70.0 | 100.0 | abnormal | IF FASTING, 70-100 NORMAL, >125 \nPROVISIONAL DIABETES. |\n| Potassium | 7.0 | mEq/L | 3.3 | 5.1 | abnormal | ___ |\n<item info>…\n<events>…\nEHR Trajectory\nLaceration without foreign body of lip, initial encounter\nType 2 diabetes mellitus without complications\nNondisplaced fracture of lateral malleolus of left fibula, initial encounter for closed fracture\nOther fall on same level, initial encounter\nHyperlipidemia, unspecified\nEssential (primary) hypertension\nTask Label List\n[\"Unspecified fracture of left talus, initial encounter for closed fracture\", \"Inverse is a\", \"Fractures\", \"is PCDC EWS authorized value for variable\", \"Yes (indicator)\", \"has seronet authorized value\", \"Hypertension\", \n\"May be treated by\", \"amiloride hydrochloride\", \"Contraindicated class of\", \"Potassium\"]\n[\"Unspecified fracture of left talus, initial encounter for closed fracture\", \"Inverse is a\", \"Fractures\", \"Is Related to\", \"Foot Injuries and Disorders\", \"Is Related to\", \"Difficulty walking\", \"Is Related to\", \"E Codes: \nFall\"]\n[ [\"Laceration without foreign body of lip, initial encounter\", \"Is Related to\", \"Wounds and Injuries\", \"Is Related to\", \"Fall (on) (from) unspecified stairs and steps, init encntr\"]\n[\"Laceration without foreign body of lip, initial encounter\", \"Is Related to\", \"Wounds and Injuries\", \"May treat\", \"zinc chloride\", \"Inverse is a\", \"Dietary Supplement\", \"Is Related to\", \"Dietary Potassium\", \n\"Mapped from\", \"Potassium\"]\n[\"Laceration without foreign body of lip, initial encounter\", \"Is Related to\", \"Wounds and Injuries\", \"Is Related to\", \"Surgery\", \"is OORO PC value of\", \"Multi-Specialty Consultation\", \"has OORO PC value\", \n\"Radiology\"]\n[\"Laceration without foreign body of lip, initial encounter\", \"Is Related to\", \"Wounds and Injuries\", \"Is Related to\", \"E Codes: Fall\"]\n….\nMedical Relation\n## Extraction\n1. **Triage [2129-04-04 20:41:01]**: \n- Chief complaint: s/p Fall, Facial injury\n- Pain: 10\\n - This indicates a fall leading to a facial injury, which may relate to fractures or lacerations.\n2. **Radiology Examinations [2129-04-04 23:00:00]**: \n- CT HEAD W/O CONTRAST: No fracture or hemorrhage, but aerosolized secretions in sinuses.\n- CT SINUS/MANDIBLE/MAXILLOFACIAL W/O CONTRAST: Maxillary teeth fractures, soft tissue hematoma, no mandibular fracture.\n- KNEE (AP, LAT & OBLIQUE) RIGHT: No fracture, minimal degenerative changes.\n- CT C-SPINE W/O CONTRAST: Possible chronic fracture, moderate degenerative changes.\n- ANKLE (AP, MORTISE & LAT) LEFT: Nondisplaced fracture of the distal tibia and talus avulsion fracture.\n- GLENO-HUMERAL SHOULDER (W/ Y VIEW) LEFT: No fracture, moderate degenerative changes.\n3. **Labotary Test Events [2129-04-04 23:00:00]**: \n- Potassium: 7.0 mEq/L (abnormal, hyperkalemia).\n- Glucose: 168.0 mg/dL (abnormal, potential diabetes).\n- These lab results point towards metabolic issues such as diabetes and hyperkalemia.\n4. **ED Diagnoses on International Classification of Diseases [2129-04-05 15:51:00]**:\n- Oth fracture of upper and lower end of left fibula, init\n- Fall (on) (from) unspecified stairs and steps, init encntr\n- This confirms a fracture due to a fall.\n5. **Medrecon [2129-04-04 23:20:00]**:\n- Medications: glipizide, Lipitor, metformin, hctz, omeprazole.\n- Glipizide and metformin indicate management of diabetes.\n## Reasoning\n1. The patient\\'s fall and subsequent injuries, including facial and dental trauma, are consistent with the diagnosis of \"Laceration without foreign body of lip, initial encounter\" due to the presence of facial pain and \nfractured teeth.\n2. The elevated glucose level of 168 mg/dL, along with the use of diabetes medications (glipizide and metformin), strongly supports the diagnosis of \"Type 2 diabetes mellitus without complications.”\n3. The radiology findings of a nondisplaced fracture of the distal tibia and talus avulsion fracture confirm the presence of a \"Nondisplaced fracture of lateral malleolus of left fibula, initial encounter for closed \nfracture.”\n4. The fall from unspecified stairs leading to multiple injuries aligns with the diagnosis of \"Other fall on same level, initial encounter.”\n5. The patient\\'s medication list includes atorvastatin (Lipitor), commonly used for managing hyperlipidemia, which supports the diagnosis of \"Hyperlipidemia, unspecified.”\n6. The use of hydrochlorothiazide (hctz), a diuretic often used for hypertension, supports the diagnosis of \"Essential (primary) hypertension.”\n## Final Results\nLaceration without foreign body of lip, initial encounter\nType 2 diabetes mellitus without complications\nNondisplaced fracture of lateral malleolus of left fibula, initial encounter for closed fracture\nOther fall on same level, initial encounter\nHyperlipidemia, unspecified\nEssential (primary) hypertension\nReasoning Chain\nSupplementary Figure 1 | A case study of EHR Trajectory, Medical Relation, and Reasoning Chain. (a) EHR Trajectory for a\npatient, where <events>... and <item info>... represent the omission of a large amount of information for display purposes. (b)\nMedical Relation, showing the connections between the context medical entities and target items. (c) Reasoning Chain, detailing\nthe process of inferring a diagnosis from the patient’s EHR data. The parts highlighted in bold are the content commonly found\nin the EHR Trajectory, Medical Relation, and Reasoning Chain. This indicates that the medical graph is effective in identifying\nvalid medical entities from the trajectory and using them to enhance reasoning.\n|48\n",
    "content": "# EHR-R1 Paper Interpretation and Analysis\n\n## 1. Core Content and Key Contributions\n\nThis paper proposes a comprehensive solution for Electronic Health Record (EHR) analysis, aiming to address two major bottlenecks in the application of large language models (LLMs) in clinical settings: **narrow task coverage** and **lack of domain-specific reasoning capabilities**.\n\n### Core Content\n- **Construction of a large-scale, high-quality EHR reasoning dataset, EHR-Ins**: Includes 300,000 high-quality reasoning cases and 3.5 million non-reasoning cases, covering 42 distinct EHR tasks.\n- **Introduction of a \"Thought Graph\"-driven data generation framework**: Identifies key entities through statistical co-occurrence relationships and leverages the UMLS medical knowledge base to establish causal and temporal connections between entities, guiding GPT-4o to generate structured and interpretable clinical reasoning chains.\n- **Development of the EHR-R1 series—reasoning-enhanced foundation models**: Built upon base models like Qwen, using a three-stage training paradigm (domain adaptation → reasoning enhancement → reinforcement learning), resulting in specialized LLMs for EHR analysis with up to 72 billion parameters.\n- **Release of a new comprehensive evaluation benchmark, EHR-Bench**: Constructed from the MIMIC-IV dataset, covering 42 tasks to comprehensively assess model performance in decision support and risk prediction.\n\n### Major Contributions\n1. **Pioneering reasoning dataset EHR-Ins**: The first systematic integration of *explicit medical reasoning* into EHR instruction data, filling a critical gap in existing datasets that lack explainable reasoning paths.\n2. **Innovative \"Thought Graph\" synthetic method**: Solves the challenge of automatically extracting reliable reasoning evidence from complex, redundant EHRs, offering a scalable technical pathway for generating high-quality reasoning data.\n3. **High-performance domain-specific model EHR-R1**: Significantly outperforms advanced models such as GPT-4o and DeepSeek-V3 across multiple authoritative benchmarks, validating the effectiveness of domain-customized training.\n4. **Open research resources**: Public release of code, models, and the new benchmark promotes fair comparison and sustainable development in the medical AI field.\n\n---\n\n## 2. Breakthroughs and Innovations\n\n### (1) Methodological Innovation: \"Thought Graph\"-Driven Reasoning Data Synthesis  \nTraditional methods rely on manual annotation or simple prompt engineering to generate reasoning chains, which are costly and inconsistent in quality. The proposed \"Thought Graph\" framework achieves:\n- **Automation**: Generates millions of reasoning samples without expert labeling;\n- **Reliability**: Validates entity relationships via the UMLS knowledge graph, reducing hallucinations;\n- **Structured output**: Enforces a three-step logic flow (\"extract → reason → conclude\"), enhancing interpretability.\n\n> This is the first time knowledge-graph-guided automatic reasoning chain generation has been applied to the EHR domain, significantly improving data quality and training efficiency.\n\n### (2) Multi-Stage Progressive Training Strategy  \nUnlike conventional end-to-end fine-tuning, the authors designed a three-phase training process:\n1. **Domain Adaptation**: Continuous pre-training on massive non-reasoning data to capture EHR distribution characteristics;\n2. **Reasoning Enhancement**: Instruction fine-tuning using reasoning data from EHR-Ins to teach the model *how to think*;\n3. **Reinforcement Learning Optimization**: Using the GRPO algorithm to refine reasoning trajectories, enabling the model not only to answer correctly but also to provide more logical and coherent explanations.\n\n> This hierarchical approach effectively addresses the common limitation of general-purpose LLMs in specialized domains—knowing *what* but not *why*.\n\n### (3) Comprehensive and Rigorous Evaluation Framework\n- Introduced **EHR-Bench**, currently the most comprehensive EHR evaluation benchmark (42 tasks);\n- Designed cross-institutional and cross-system generalization tests (MIMIC-IV-CDM, EHRSHOT) to validate real-world applicability;\n- Proposed a combined zero-shot and few-shot evaluation mechanism to simulate real clinical scenarios with limited labeled data.\n\n> Experimental results show that EHR-R1 achieves an AUROC 10% higher than GPT-4o under zero-shot conditions, demonstrating superior transferability and robustness.\n\n---\n\n## 3. Entrepreneurial Project Recommendations\n\nBased on the core technology and application scenarios of this paper, here are several commercially viable startup ideas:\n\n### 🚀 Startup Project 1: AI-Powered Clinical Decision Support System (CDSS) SaaS Platform\n\n#### Project Name  \n**MedThinker Pro — AI Reasoning-Based Real-Time Clinical Assistant**\n\n#### Pain Points Addressed\n- Clinicians face information overload and struggle to integrate longitudinal patient histories for accurate decisions;\n- Existing CDSS systems often rely on rigid rule engines, lacking flexibility for complex cases.\n\n#### Technical Implementation\n- Integrate the EHR-R1 model as the core reasoning engine;\n- Develop lightweight API interfaces compatible with hospital HIS/PACS/LIS systems;\n- Support natural language queries: *\"What tests should be ordered next?\"*, *\"What's the 30-day readmission risk?\"*\n\n#### Business Model\n- Subscription pricing per department or hospital;\n- Offer customized model fine-tuning services (e.g., specialty-specific disease models);\n- Partner with EHR vendors for embedded integration.\n\n#### Differentiation\n- Delivers not just answers, but **auditable reasoning processes**, increasing clinician trust;\n- Handles complex scenarios including rare diseases and multi-morbidity;\n- Compatible with mainstream EHR standards (HL7/FHIR), ensuring easy deployment.\n\n---\n\n### 🏥 Startup Project 2: Hospital Operations Intelligence & Early Warning Platform\n\n#### Project Name  \n**HospAlert — Institutional-Level Risk Prediction Hub**\n\n#### Pain Points Addressed\n- Hospital administrators lack macro-level visibility into dynamic patient risks;\n- Metrics like ICU occupancy and readmission rates impact insurance reimbursement and institutional ratings.\n\n#### Technical Implementation\n- Leverage EHR-R1’s risk prediction capability to analyze entire patient populations;\n- Deliver real-time alerts for:\n  - High-risk patients (mortality, ICU transfer, readmission)\n  - Resource strain (bed availability, nursing workload)\n  - Potential malpractice risks (untreated abnormal lab values)\n\n#### Key Features\n- Dashboard visualization of hospital-wide risk maps;\n- Auto-generate intervention suggestions (e.g., initiate multidisciplinary consultation);\n- Support compliance checks (e.g., DRG/DIP cost control reminders).\n\n#### Revenue Model\n- Annual licensing based on bed count;\n- Collaborate with insurers on risk-sharing programs;\n- Sell anonymized analytics reports for research partnerships.\n\n---\n\n### 💡 Startup Project 3: AI Tutor for Medical Education and Training\n\n#### Project Name  \n**CliniCoach — Next-Gen Clinical Reasoning Training Platform**\n\n#### Pain Points Addressed\n- Medical students and residents lack sufficient exposure to real-world complex cases;\n- Traditional teaching offers limited personalized feedback on diagnostic thinking.\n\n#### Technical Implementation\n- Repurpose EHR-R1 as a \"virtual tutor\" simulating real patient encounters;\n- After trainees submit initial assessments, AI generates annotated reasoning chains:\n  > *\"You overlooked the link between elevated creatinine and potassium chloride use, suggesting possible chronic kidney disease...\"*\n- Supports case review, error attribution analysis, and knowledge graph navigation.\n\n#### Use Cases\n- Supplementary tool for medical school curricula;\n- Standardized residency training and assessment system;\n- Preparation platform for medical licensing exams.\n\n#### Commercial Value\n- Bulk licensing to medical schools;\n- Launch consumer app with premium content and tutoring;\n- Partner with continuing education providers for accredited courses.\n\n---\n\n## Summary\n\n| Dimension | Content |\n|---------|--------|\n| **Core Breakthrough** | First scalable construction of explainable reasoning in EHR data and model training |\n| **Technical Innovation** | \"Thought Graph\" + Three-stage training + GRPO reinforcement learning |\n| **Application Prospects** | Viable in clinical decision support, hospital management, and medical education |\n| **Entrepreneurial Recommendation** | Prioritize entry into the SaaS-based CDSS market, leveraging \"transparent reasoning\" as a key differentiator |\n\n> This work is not merely an academic achievement—it provides a **replicable, deployable, and profitable** technical paradigm for AI-powered healthcare, with exceptional potential for industrialization.",
    "github": "",
    "hf": ""
}